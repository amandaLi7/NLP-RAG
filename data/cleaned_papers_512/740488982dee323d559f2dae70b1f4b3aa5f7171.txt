Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Language Technologies Institute Carnegie Mellon University aps1@andrew.cmu.eduShuyi Chen Heinz Information College Carnegie Mellon University shuyic@andrew.cmu.eduKejun Zhou Heinz Information College Carnegie Mellon University kejunz@andrew.cmu.edu Heinz Information College Carnegie Mellon University yuechen2@andrew.cmu.
eduBhiksha Raj Language Technologies Institute Carnegie Mellon Institute Pittsburgh, PA, 15213 General-purpose embedding is highly desirable for few-shot even zero-shot learning in many application scenarios, including the audio tasks. In order to understand representations better, we conducted thorough error analysis and visualization of HEAR 2021 submission results.
Inspired by the analysis, this work experiments with different front-end audio preprocessing methods, including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT), and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover a more holistic simulation of the frequency information received by the human auditory system. We tested the models on the suite of HEAR 2021 tasks, which encompass a broad category of tasks.
Preliminary results show (1) the proposed BECR can incur a more dispersed embedding on the test set, (2) BECR improves the PaSST model without extra computation complexity, and (3) STFT preprocessing outperforms CQT in all tasks we tested. Github: https://github.com/ankitshah009/general_audio_embedding_hear_2021 General-purpose representation learning is still an open question in audio datasets.
Therefore, Holistic Evaluation of Audio Representations 2021 (HEAR 2021) challenge was proposed, aiming at providing longitudinal insights into different generalized audio representation models [ 1]. The challenge was to train one audio representation model that is  exible enough to represent unseen audio datasets. The representations were evaluated by training and testing a shallow network built on the embedding output of the models. The end-to-end process of HEAR2021 is summarized in Fig. 1.
Inspired by the results of the challenge, this work  rst compares Short-time Fourier transform (STFT) with Constant-Q Transform (CQT), which was not used by any team in the challenge as the audio preprocessing method. Secondly, based on thorough error analysis and visualization of HEAR 2021 submission results, we propose Batch Embedding Covariance Regularization (BECR), Draft.arXiv:2303.03591v1  [cs.
SD]  7 Mar 2023a regularizing term that utilizes the Gini Index to measure the statistical dispersion of eigenvalues of the covariance matrix of the embedding on a training task. More speci cally, it encourages the projection of representations of a speci c pre-training task in all its eigenvectors to be as evenly dispersed as possible. Therefore, it aims to learn a deep representation network that is more versatile in low-dimension space when trained with only one dataset of a speci c domain.
We also propose an optimized implementation algorithm to reduce the time complexity. We tested the two proposals along with a baseline model on four HEAR 2021 tasks, which encompass tasks from three audio domains including speech, music, and broad. Results show BECR improves the PaSST baseline in all tasks while CQT-trained results are inferior compared to Mel STFT models. 2.1 Audio Data Preprocessing Techniques 2.1.
1 Short-time Fourier transform (STFT) and Mel Spectrogram An approach to better solve the general representation learning challenge is by applying different hand-crafted transformations based upon domain expertise for different tasks, for example, using Short-time Fourier transform (STFT) [ 1]. STFT is a powerful audio signal processing tool that can be applied in many tasks. It speci es complex amplitude versus time and frequency for every signal and de nes a valuable class of time-frequency distributions.
However, the STFT has its disadvantages, such as the limit in its time-frequency resolution capability. Low frequencies can be hardly depicted with short windows, whereas short pulses can only poorly be localized in time with long windows.[ 2] As humans don t perceive the sound in linear scale, Mel scale is proposed such that equal distances in pitch are equally distant to the listener, the human. The Mel spectrogram converts the values in hertz to Mel scale.
This transformation can better stimulate human hearing than STFT [3]. 2.1.2 Constant Q-transform (CQT) Another way suitable to preprocess the music, human voice, and other sound varying data is Constant Q-transform. In 1991, Brown proposed CQT to simulate the human auditory system by using a transform with a  xed quality factor Q [ 4]. Constant Q-transform is different from STFT in several ways.
The Constant-Q transform has logarithmically spaced frequency bins, while the frequency component of STFT is linear. Further, the Constant-Q transform has octaves bin widths other than absolute value bin width. As the output of the transform is effectively amplitude/phase against log frequency, fewer frequency bins are required to cover a given range effectively [ 3].
Therefore, some argue Constant-Q transform better describes what is received by the human auditory system and is thus better positioned in the musical area [5]. 2.2 Gini Index in Machine Learning Gini Index is a data purity measure. A small Gini Index indicates a high purity of the encodings or signals. Gini Index has been widely applied in Machine Learning.
For example, Randall [ 6] proposed neural decision trees (NDT) based on decision trees and MLP in the practice of combining Gini Index with neural networks. Park [ 7] also proposed a deep learning model using the Gini Index algorithm for the extraction of features from datasets.
We noticed the need for a summary statistic that describes the overall geometric property of the embedding matrix on an evaluation dataset during the analysis of HEAR2021 results   speci cally, how spread out the embedding for different tasks. Therefore we got the idea to apply Gini Index to normalized eigenvalues of embedding matrix as a regularization term. We believe this work presents the  rst application of Gini Index with such a de nition in audio tasks.
The end-to-end process of this work is summarized in Fig. 1. We  rst compare the effects of two preprocessing methods. Secondly, we experiment with a novel Gini Index-based regularization to improve the versatility of the model. The resulting models are used to generate embedding on a 2variety of unknown datasets of HEAR2021 dataset, which will be used to train shallow MLP layers to get a  nal evaluation score. Figure 1: The End-to-end Training and Evaluation Process for HEAR2021.
In this work, we made two modi cations of it. (1) We compared the effects of CQT and STFT, and (2) we designed a regularization term in the training time. 3.1 Preprocessing Methods Short-time Fourier transform (STFT)  rst divides the long recording signal into short equal segments in the time domain. Then, it computes a Fourier transform on each segment and generate several frequency spectrums.
Discrete STFT can be expressed as: n= x[n] [n m]e j n(1) CQT transform mirrors the human auditory system, whereby at lower-frequencies spectral resolution is better [8]. Discrete CQT can be expressed as: 3.2 Baseline Architecture We choose PaSST model as the baseline structure [ 9] which achieved overall top results in the 19 tasks (14 are secret tasks) of HEAR2021 challenge [ 1].
PaSST is the state-of-the-art transformer-based audio model that can achieve SOTA results with less memory and time complexity compared to other CNN-based models [ 9]. As shown in Fig. 2, the input of the model is an audio spectrogram generated by preprocessing methods. In part 1, it experiences a patch extraction and linear projection. In part 2, frequency and time positional encodings are added. Then a Patchout operation will be applied.
The Patchout idea is to encourage the transformer to classify with an incomplete sequence, similar to dropout. Finally, the sequence is  attened and then passed through Self-attention layer. In the last, a classi er MLP layer operates on the classi cation token and generates predictions [9]. 3.3 Batch Embedding Covariance Regularization (BECR) 3.3.
1 Analysis of Low-dimension Representation Projections Through analysis of the embedding of top models in various tasks, we observed that given the same task, those models that perform better in the downstream task generally have higher embedding 3Figure 2: The Patchout transformer (PaSST) architecture [9].
(a) Gini Index of normalized eigenvalues (b) F-test of K-means results (c) Calinski-harabasz score of K-means results (d) Variance explained by top 2 principal vectors Figure 3: Summary of Embedding Performance and Dispersion Metrics. Each data point is either a submitted or a replicated model of HEAR2021. Nsynth Pitch, FSD50K, and CREMA-D are music, broad, and speech tasks respectively. dispersion, for example with lower variance explained by top principal components and lower Kmeans F-test score.
See Fig. 3 for a summary. Thus, we conjecture the high dispersion of the embedding produced by a model is helpful in downstream tasks. Since we have restricted the training process to only using one task dataset, the eigenvalues of the covariance matrix are suitable to simulate the variance of embedding when projected to different dimensions for different testing tasks.
The Gini Index of normalized eigenvalues is therefore a summary statistic that describes how evenly dispersed the embedding is across the eigenspace. So we de ne BECR in the following way: For embedding layer with Doutputs, Kis theDdimensional covariance matrix of each batch 4K(f (Xi)) = (f (Xi) E[f (Xi)])(f (Xi) E[f (Xi)])T(3) Kis always positive semi-de nite with real nonnegative eigenvalues.
Gapplies the de nition of Gini Index to the normalized eigenvalues of K, i=1 i(K(f (Xi))))2(4) Ris the proposed regularization term, R(Xi, ) = max(0, G(f (Xi)))2(5) where is a hyperparameter, de ning the upper bound of the Gini Index when incurring a loss. Finally, the total loss is de ned by L (Xi, ) = (1 )L(Xi, ) + R(Xi, ) (6) In our case, the vanilla loss Lis Binary Cross Entropy loss, whose range is [0, 1].
By adding the regularization term, it encourages a large Gini Index that is encouraging evenly distributed eigenvalues. Also, the regularization term is a convex function added to the original loss function, which has desirable convergence property. 3.3.3 Implementation Details of BECR Let batch size be N and embedding dimension be K. The eigenvalue decomposition algorithm takes O(K3)complexity.
Adding the covariance matrix calculation, the total additional complexity per batch isO(K3+K2 N), which is categorically impractical in our case since N is around 10 and K is around 1000. Through our experiment, training one 10-sample batch of FSD50K dataset with embedding eigenvalue decomposition takes around 6 hours in an RTX 3090 machine, 18 times longer than that without the loss. Therefore, we propose an optimized implementation without eigenvalue decomposition, that is with a complexity of O(K2 N).
i i(A)2. Therefore, Eq. 4 can be simpli ed to ( i)2= 1 tr(K(f (Xi))2) Through experiment, the speed of this implementation method is similar to vanilla loss calculation. Simpli ed BECR takes 33 hours compared to vanilla loss s 36 hours in training as shown in Table 3. So we can apply BECR with little extra complexity. 4 Experimental Evaluation 4.1 Datasets and Evaluation Metric We evaluate the performance of the models on three types of data sets: music, speech, and broad sounds.
See Table 1 for dataset and evaluation metrics summary. For music, we use the NSynth Pitch containing 305,979 musical notes, each with a unique pitch, timbre, and envelope. For 1,006 instruments from commercial sample libraries, the dataset was generated in four seconds, monophonic 16kHz audio snippets by ranging over every pitch of a standard MIDI piano (21-108) as well as  ve different velocities (25, 50, 75, 100, 127).
The goal of this task is to classify instrumental sounds into one of 88 pitches [ 10]. The Pitch Accuracy was used for evaluation on NSynth Pitch task. 5We also use Beijing Opera Percussion for music task evaluation. Beijing Opera Percussion Instrument Dataset is based on recordings from The Beijing Opera [ 11]. It contains six main percussion instruments that can be classi ed into four main categories: Bangu, Naobo, Daluo, and Xiaoluo. There are 236 audio clips in total.
Classi cation accuracy is used for evaluation on Beijing Opera For speech, we use the CREMA-D for emotion recognition [ 12]. This dataset contains audio data of actors reciting sentences with one of six different emotions (anger, disgust, fear, happy, neutral, and sad). The goal of this task is to identify the type of emotion the actors are in when they say the sentences. Classi cation accuracy is used for evaluation on CREMA-D task.
For broad sounds, we use the FSD50K, each of the audio clips in this dataset is labeled using one or more of 200 classes in environmental sounds, speech, and music [ 13]. This dataset contains over 51K audio clips, totaling over 100 hours of audio, and is extracted from the AudioSet Ontology. We also use FSD50K for training. mAP was used for multi-label evaluation on FSD50K task. Task Name Predictor Type Split Mode Evaluation Metric NSynth Pitch 5h C TVT Pitch Acc.
Beijing Opera C 5-fold Accuracy CREMA-D C 5-fold Accuracy Table 1: Summary of the four evaluation tasks selected from HEAR 2021 [ 1]. For all four tasks, the embedding type are either scene based. The predictor types are either multiclass (C) or multilabel (L). The split method used during downstream evaluation are either train/validation/test (TVT) or K-fold. 4.2 Hyperparameter Tuning The proposed BECR involves two hyperparameters,  and . We only experimented with  of 0.05 and 0.
10 as through our experiment  0.1ensures the BECR does not cannibalize all the loss in the beginning few epochs. For determination of search space of  , we observed that the Gini Index of vanilla PaSST s embedding on FSD50K is 0.92 after 100 epochs (See Table 4). Also, in the initial batches, the Gini Index is around 0.3-0.6 in experiments. So it would not make sense if we set epsilon smaller than 0.6 which makes little difference in the  nal output (See Eq. 5). So we experimented with  larger than 0.7.
The tuning results in Table 2 show ( , ) = (0.05,0.7)is the best combination. # Epochs Test Set Performance 0.7 0.05 50 (19hr) 30.7 0.8 0.05 50 (18hr) 24.4 0.9 0.05 50 (18hr) 24.9 0.7 0.10 50 (19hr) 25.6 0.8 0.10 50 (18hr) 25.7 0.9 0.10 50 (18hr) 29.8 Table 2: Hyperparameter Tuning Results on Training Set (FSD50K) Results show that CQT-preprocessing is worse a choice than STFT+Mel in all four tasks.
Additionally, the computational complexity of CQT transformation is larger than STFT+Mel, taking more than two times than original STFT+Mel (approximately 1 hour per epoch in FSD50K with a batch size of 6(a) T-SNE of CQT-based PaSST (b) T-SNE of STFT-based PaSST (c) T-SNE of STFT-based PaSST with BECR Model Embedding (d) PCA of CQT-based PaSST (e) PCA of STFT-based PaSST (f) PCA of STFT-based PaSST with Figure 4: Projection of Embedding with PCA and T-SNE.
The points are colored using the groundtruth labels. Dataset is CREMA-D. The models were trained on FSD-50K, not the CREMA-D dataset. 6). The proposed BECR combined with Mel+STFT outperforms the baseline model in all four tasks with similar training complexity. Models # EpochsDownstream Evaluation Scores (%) Beijing Opera Nsynth Pitch 5 CREMA-D FSD50K STFT Mel+PaSST (Baseline) 100 (33hr) 90.6 50.9 46.5 27.8 STFT Mel+PaSST (BECR) 100 (36hr) 92.7 51.2 47.6 36.8 CQT+PaSST 50 (50hr) 36.8 4.8 19.4 3.
5 CP-JKU PaSST in HEAR2021 [1] Unknown 96.6 25.6 61.0 55.8 Table 3: Summary of Model Performances.  and of BECR are tuned on training set in section 4.2. 5.1 Discussion on CQT s Results We tried to  nd some explanation for CQT s worse performance than STFT. First, by comparing the resulting embedding of STFT and CQT-based PaSST model dimension reduction methods of PCA and T-SNE, we found projection of CQT are usually in linearly unseperatable shape, while those of STFT seems more cluster-like.
Considering evaluation process builds a 2-layer MLP on the embedding results (see Fig 1), it s reasonable to assume that embedding of CQT-based PaSST being unseperatable accounts for its bad performance in the downstream tasks. Secondly, CQT-based PaSST performance and some of its metrics follow the pattern mentioned in Section 3.3.1. Compared with STFT-based PaSST models, CQT-based PaSST embedding results are less dispersed in unseen datasets. See Table 4.
Thirdly, the experimental results aside, we suspect a relatively "good" preprocessing method partly depends on the choice of model. The original implementation of PaSST uses STFT transformation [9], so it s natural that PaSST model works best with STFT preprocessing method instead of others. 7(a) Total loss descent and BECR s percentage of (b) BECR term descent Figure 5: BECR descent and the total training loss descent comparison (  = 0.05, = 0.
7) (a) PaSST with STFT + Mel (Baseline) (b) PaSST with BECR and STFT + Mel Figure 6: Training loss descent of the models 5.2 Discussion on BECR s Results To verify that the BECR is making the improvement on baseline PaSST model, we show the regularization term successfully converges to zero after the  rst few epochs. See Fig 5. In particular, the percentage of BECR term of total loss steadily descent from 15% to 0%, and does not cannibalize the total loss even in the early stage.
Therefore compared with the baseline model, the validation loss descent is not signi cantly affected in the 100-epoch training process (Fig 6). Another piece of evidence is after adding BECR to the baseline PaSST, the variance explained by top eigenvectors decreases, F-test score decreases, and Gini Index of normalized eigenvalues increases, as BECR is intended for (see Table 4).
These changes indicate the embedding projection is more spread out than before, which leads to its better performance according to our analysis in Section 8Dataset Models Gini Index Top 2 eigenvalues Nsynth Pitcht 5hCQT+PaSST 0.20 0.88 0.048 STFT+PaSST 0.90 0.31 0.509 STFT+PaSST (BECR) 0.91 0.29 0.512 FSD50KCQT+PaSST 0.29 0.77 0.035 STFT+PaSST 0.92 0.28 0.278 STFT+PaSST (BECR) 0.94 0.21 0.368 CREMA-DCQT+PaSST 0.02 0.95 0.195 STFT+PaSST 0.89 0.34 0.465 STFT+PaSST (BECR) 0.90 0.31 0.
476 Table 4: Effect of BECR. Models are trained on FSD50K dataset with 100 epochs of batch size 10. In this paper, we verify that CQT is a less ideal audio preprocessing method than STFT+Mel when used with PaSST model, not only decreasing downstream task performances but also dramatically increasing the computational complexity when transforming the audio data into the frequency domain.
We also propose Batch Embedding Covariance Regularization (BECR), a Gini Index-based regularization term, which encourages widespread embedding in its eigenspace, and a fast implementation algorithm for it. We tested BECR with SOTA audio model PaSST in a wide variety of audio domains: music, speech, and broad and achieve better performance when compared with using PaSST alone trained with the same dataset and similar hours.
The simplicity in intuition and low complexity of implementation to apply the method, together with the encouraging results in challenging unknown test tasks, demonstrate the promising potential BECR has for general-purpose audio representation We would like to note some limitations of this work. First, different models work well with different preprocessing methods. So the conclusion of CQT is better than STFT limits to our experiment setting which uses a PaSST model.
Second, we have not veri ed if BECR is a generalizable technique working beyond PaSST model. It would be interesting to apply BECR to other common baselines in the future, for example, OpenL3 and wav2vec2 models, and see the difference it makes. Thanks to Chaoran Zhang, Yuxiang Zhang for their helpful comments on the work. [1]Joseph Turian, Jordie Shier, Humair Raj Khan, Bhiksha Raj, Bj rn W Schuller, Christian J Steinmetz, Colin Malloy, George Tzanetakis, Gissel Velarde, Kirk McNally, et al.
,  Hear: Holistic evaluation of audio representations,  in NeurIPS 2021 Competitions and Demonstrations Track . PMLR, 2022, pp. 125 145. [2] Local time-frequency analysis and short time fourier transform.,  https://www.math. ucdavis.edu/~strohmer/research/gabor/gaborintro/node3.html , 2010, [Online; accessed Retrieved May 2, 2022,]. [3]Ziqiang Shi, Huibin Lin, Liu Liu, Rujie Liu, and Jiqing Han,  Is cqt more suitable for monaural speech separation than stft? an empirical study,  arXiv preprint arXiv:1902.
00631 , 2019. [4]DeLiang Wang and Guy J Brown, Computational auditory scene analysis: Principles, algorithms, and applications , Wiley-IEEE press, 2006. 9[5]Christian Sch rkhuber and Anssi Klapuri,  Constant-q transform toolbox for music processing, in7th sound and music computing conference, Barcelona, Spain , 2010, pp. 3 64. [6] Randall Balestriero,  Neural decision trees,  arXiv preprint arXiv:1702.07360 , 2017.
[7]Heum Park,  Deep learning with gini-index algorithm for extraction of major features: Major adverse cardiac bvents from kamir,  International Information Institute (Tokyo). Information , vol. 21, no. 2, pp. 631 638, 2018. [8] Constant-q transform,  https://en.wikipedia.org/wiki/Constant-Q_transform , 2019, [Online; accessed Retrieved May 2, 2022,]. [9]Khaled Koutini, Jan Schl ter, Hamid Eghbal-zadeh, and Gerhard Widmer,  Ef cient training of audio transformers with patchout,  arXiv preprint arXiv:2110.
05069 , 2021. [10] Jesse Engel, Cinjon Resnick, Adam Roberts, Sander Dieleman, Mohammad Norouzi, Douglas Eck, and Karen Simonyan,  Neural audio synthesis of musical notes with WaveNet autoencoders, in Proceedings of the 34th International Conference on Machine Learning , Doina Precup and Yee Whye Teh, Eds., International Convention Centre, Sydney, Australia, 06 11 Aug 2017, vol. 70 of Proceedings of Machine Learning Research , pp. 1068 1077, PMLR.
[11] Mi Tian, Ajay Srinivasamurthy, Mark Sandler, and Xavier Serra,  Beijing opera percussion instrument dataset,  Mar. 2014. [12] Houwei Cao, David G Cooper, Michael K Keutmann, Ruben C Gur, Ani Nenkova, and Ragini Verma,  Crema-d: Crowd-sourced emotional multimodal actors dataset,  IEEE transactions on affective computing , vol. 5, no. 4, pp. 377 390, 2014.
[13] Eduardo Fonseca, Xavier Favory, Jordi Pons, Frederic Font, and Xavier Serra,  Fsd50k: an open dataset of human-labeled sound events,  IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 30, pp. 829 852, 2021.