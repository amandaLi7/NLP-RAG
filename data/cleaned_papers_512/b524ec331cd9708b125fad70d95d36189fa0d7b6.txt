arXiv:2305.01620v2  [cs.CL]  6 May 2023A STUDY ON THE INTEGRATION OF PIPELINE AND E2E SLU SYSTEMS FOR SPOKEN SEMANTIC PARSING TOWARD STOP QUALITY CHALLENGE Siddhant Arora1, Hayato Futami2, Shih-Lun Wu1,Jessica Huynh1, Yifan Peng1,Yosuke Kashiwagi2, Emiru Tsunoo2, Brian Yan1, Shinji Watanabe1 1Carnegie Mellon University,2Sony Group Corporation, Japan Recently there have been efforts to introduce new benchmark tasks for spoken language understanding (SLU), like semant ic parsing.
In this paper, we describe our proposed spoken semantic parsing system for the quality track (Track 1) in Spoken Language Understanding Grand Challenge which is part of ICASSP Signal Processing Grand Challenge 2023. We experiment with both end-to-end and pipeline systems for this task. Strong automatic speech recognition (ASR) models lik e Whisper and pretrained Language models (LM) like BART are utilized inside our SLU framework to boost performance.
We also investigate the output level combination of various models to get an exact match accuracy of 80.8, which won the 1st place at the challenge.
Index Terms  STOP Challenge, spoken language understanding, Spoken Language Understanding Grand Challenge or Spoken Task Oriented Parsing (STOP) Challenge, which is part of ICASSP Signal Processing Grand Challenge 2023, aims to build systems that can convert a spoken utterance to a semantic parse sequence to facilitate the execution of tasks b y the voice assistant.
This work discusses our team PittOsaki  s approach for Track 1 to improve the quality of generated semantic parse using open source model and ASR datasets. In this work, we experiment with various pipeline, and end-to-end (E2E) SLU approaches. Pretrained self-supervi sed speech (SSL) representations like WavLM and Hubert are employed in our SLU framework. We also incorporate pretrained LMs like BART large.
Finally, a system combination of various models shows a signi cant performance gain over the baseline systems. We formulate the SLU task of semantic parsing as a uni ed sequence-to-sequence problem. The input is a sequence of speech features extracted from the raw audio, and the outputis a semantic parse represented as a linearized tree structu re. The attention-based encoder-decoder architecture is adop ted in our end-to-end (E2E) SLU approaches.
Our ASR model is also based on encoder-decoder architecture. For both ASR and SLU training, we employ SSL representations like WavLM as a frontend. A weighted sum of multiple hidden states is utilized and the parameters are frozen during trai ning. We also experiment with utilizing the recently release d Whisper model in our SLU framework. The entire Whisper model is  ne-tuned instead of it being used as a frontend since it achieved superior performance in our initial exper imentations.
Our NLU model is incorporated by  ne-tuning pre-trained LMs. Similar to prior work [1], we improve the semantic modeling of our E2E SLU models by adopting a 2-pass SLU approach [2], where the second pass combines both acoustic and semantic information generated by pretrained LM from ASR hypotheses.
Inspired by the principles of task compositionality, we also train compositional E2E SLU model [3] that  rst convert the spoken utterance to a sequence of token representations, which can then be used in the traditional NLU framework. To combine hypotheses from multiple models, we directly apply the recognizer output vo ting error reduction (ROVER) method and extract exact match (EM) accuracy from the combined semantic parse sequence. We adopt the evaluation metrics in the STOP benchmark i.e. EM accuracy.
The encoder of our E2E SLU model is a 12layer Conformer, while the decoder is a 6-layer Transformer . The number of heads and dimension of a self-attention layer is set to 8 and 512. The linear units are 2048 for the encoder and the decoder. Speed perturbation and SpecAugment are performed for data augmentation. Our ASR model has the same architecture as our E2E SLU model. We investigate using external ASR datasets like Librispeech and Commonvoice for pretraining.
For 2 pass SLU models, our deliberation encoder consists of 4-layer conformer, and second-pas s decoder has the same architecture as the ASR decoder. Our Compositional model consists of the same architecture as th e ASR model in it s ASR component and 6-layer transformerPre-trained Model Test EM (  ) Pipeline Wav2Vec2+BART-L 72.36 w/ Compositional E2E SLU Our Pipeline approach Table 1 : Exact Match (EM) accuracy for semantic parsing on Pre-trained Model Test EM (  ) w/ Whisper transcripts 77.
0 w/ Compositional E2E SLU w/ Whisper transcripts 77.4 Our Pipeline approach Our System Combination Table 2 : Exact Match (EM) accuracy for semantic parsing using Whisper on STOP dataset. encoder, and 6-layer transformer decoder in it s NLU component. Dropout and label smoothing are applied. For pretrained LMs, we use BART large and T5 large, which are trained with HuggingFace Seq2Seq Trainer. We add special tokens in the vocabulary for slot and intent tags.
We also experiment with Whisper medium for ASR and both Whisper medium and large models for SLU. For ASR, we train in 3 settings: (a) using lowercase transcripts (b) t ranscripts with original casing (as in  utterance   eld) refer red to as Whisper w/ casing (c) original casing transcripts and mor e frequent saving of checkpoints (i.e. after 1k iterations in stead of epoch) referred to as Whisper w/ freq. checkpoint.
Simila r to pretrained LMs, we add special tokens in vocabulary for slot and intent tags while training E2E SLU models. More details about our models and the con g  les will be publicly available as part of the ESPnet-SLU [4] toolkit. Our results on the semantic parsing task without using the Whisper model are shown in Table 1. 2-pass and compositional E2E SLU models perform better than traditional E2E models.
We further observe that the pipeline model is sig-Pre-trained Model Pretrained Dataset Test WER (  ) WavLM w/ LM Librispeech+Commonvoice+STOP 2.7 Whisper w/ casing STOP 2.3 Whisper w/ freq. checkpoint STOP 2.3 Our System Combination Table 3 : Word Error Rate (WER) on STOP dataset. ni cantly better than all the E2E SLU models. Table 2 shows that  netuning Whisper can be very helpful in improving E2E SLU performance since Whisper has been trained on large amounts of labeled data.
Table 3 similarly shows that Whispe r achieves very good improvement on the ASR task. We further experiment with using Whisper transcripts in our 2 pass and compositional E2E SLU model directly during inference and observe signi cant performance gains. We were not able to investigate incorporating the Whisper model in our Composi tional and 2 pass E2E SLU model due to time constraints, but we will investigate this in future work.
Finally, we use the ROVER combination on the hypothesis produced by our 4 best ASR models (Table 3) and achieve WER 2.2. Using this ASR transcript, we are able to significantly boost the performance of our pipeline systems. We further use ROVER to combine the 4 best SLU models (Table 2) to achieve EM 80.8.
This work used Bridges2 system at PSC and Delta system at NCSA through allocation CIS210014 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296. Jessica Huynh was supported by NSF Graduate Research Fellowship grants DGE1745016 and DGE2140739. The opinions expressed in this paper do not necessarily re ect those of that funding agency. [1] D. Le, A.
Shrivastava, P. D. Tomasello, et al. ,  Deliberation model for on-device spoken language understanding,  in Proc. Interspeech , 2022, [2] S. Arora, S. Dalmia, X. Chang, et al. ,  Two-pass low latency end-toend spoken language understanding,  in Arxiv preprint arXiv:2207.06670 , [3] S. Arora, S. Dalmia, B. Yan, et al. ,  Token-level sequence labeling for spoken language understanding using compositional end-to -end models, in Proc. EMNLP , 2022.[4] S. Arora, S. Dalmia, P. Denisov, et al.
,  Espnet-slu: Advancing spoken language understanding through espnet,  in Proc. ICASSP , 2022, [5] P. Tomasello, A. Shrivastava, D. Lazar, et al. ,  STOP: A dataset for Spoken Task Oriented Semantic Parsing,  in CoRR .