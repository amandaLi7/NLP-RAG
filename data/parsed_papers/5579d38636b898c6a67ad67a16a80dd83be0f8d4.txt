Towards Multilingual Automatic Open-Domain Dialogue Evaluation
John MendonÃ§a1,2,âˆ—, Alon Lavie3,4and Isabel Trancoso1,2
1INESC-ID, Lisbon
2Instituto Superior TÃ©cnico, University of Lisbon
3Carnegie Mellon University, Pittsburgh
4Phrase, Pittsburgh
{john.mendonca, isabel.trancoso}@inesc-id.pt
alavie@cs.cmu.edu
Abstract
The main limiting factor in the development of
robust multilingual open-domain dialogue eval-
uation metrics is the lack of multilingual data
and the limited availability of open-sourced
multilingual dialogue systems. In this work,
we propose a workaround for this lack of data
by leveraging a strong multilingual pretrained
encoder-based Language Model and augment-
ing existing English dialogue data using Ma-
chine Translation. We empirically show that
the naive approach of finetuning a pretrained
multilingual encoder model with translated data
is insufficient to outperform the strong baseline
of finetuning a multilingual model with only
source data. Instead, the best approach consists
in the careful curation of translated data using
MT Quality Estimation metrics, excluding low
quality translations that hinder its performance.
1 Introduction
Open-domain dialogue systems have gained sub-
stantial attention in the NLP (Natural Language
Processing) and ML (Machine Learning) fields,
thanks to their increasingly human-like behaviour
(Thoppilan et al., 2022; Shuster et al., 2022). Their
impressive generation capabilities can be attributed
to new milestones in model development and scal-
ing (Adiwardana et al., 2020), and the amount of
data used during training. Despite this research
and development effort, advertised generation ca-
pabilities were only attainable in a select few lan-
guages (typically English or Chinese) due to low
resources in dialogue for other languages (Zhang
et al., 2022b). More recently, however, the advent
of LLMs (Large Language Models) finetuned with
Reinforcement Learning from Human Feedback
such as ChatGPT (Ouyang et al., 2022) has opened
the path for high-quality and easily accessible mul-
tilingual dialogue generation.
Similarly, automated open-domain dialogue eval-
uation has also been largely limited to evaluating a
âˆ—Work conducted as a visiting scholar at CMU.
LANG
MT Quality EstimationEN(ğ‘!,ğ‘Ÿ!)(ğ‘",ğ‘Ÿ")(ğ‘#,ğ‘Ÿ#)(ğ‘$,ğ‘Ÿ$)(ğ‘%,ğ‘Ÿ%)(ğ‘&,ğ‘Ÿ&)(ğ‘!,ğ‘Ÿ!)(ğ‘",ğ‘Ÿ")(ğ‘#,ğ‘Ÿ#)(ğ‘$,ğ‘Ÿ$)(ğ‘%,ğ‘Ÿ%)(ğ‘&,ğ‘Ÿ&)
LANG1.ğ‘#,ğ‘Ÿ#2.ğ‘',ğ‘Ÿ'...ğ‘.ğ‘%,ğ‘Ÿ%Dialogue Submetrictop ğ‘˜Figure 1: Proposed architecture. The original dia-
logue dataset is transformed into context-response pairs
(cn, rn)and translated using MT. The final dialogue
submetric is trained using a combination of the original
English data and the top ksentences or (cn, rn)from
each language, depending on the submetric.
select few languages. Word-overlap based metrics
from NLG (Natural Language Generation) such
as BLEU (Papineni et al., 2002) and METEOR
(Banerjee and Lavie, 2005) are agnostic to lan-
guage, only requiring a reference response. How-
ever, these metrics are known to correlate poorly
with human judgments due to the multifaceted na-
ture of dialogue (Liu et al., 2016). Reference-free
metrics such as USR (Mehri and Eskenazi, 2020)
and USL-H (Phy et al., 2020), however, require
dialogue data for training. Considering most open-
source dialogue data is in English, these models
are expected to underperform significantly in other
languages. Additionally, most open sourced dia-
logue systems are also limited to English, further
disincentivising multilingual research.
One solution to the issues previously mentioned
is to leverage MT (Machine Translation). With MT
services becoming more affordable and consistent,
some authors resort to translation when developing
their multilingual dialogue systems (Schuster et al.,arXiv:2308.16795v1  [cs.CL]  31 Aug 20232019; Anastasiou et al., 2022). This can either
be included as a module in the systemâ€™s pipeline
â€“ allowing the use of proven English generation
models for other languages; or as a cross-lingual
transfer method â€“ by translating training data.
In this paper, we extend the approach of training
using data generated by MT for the development of
multilingual models for evaluation of open-domain
dialogue responses. We experiment with and evalu-
ate several different possible workarounds for this
problem. Namely, we leverage the availability of
strong pretrained multilingual encoders as a foun-
dation for training multilingual dialogue evalua-
tion models. As a first step, we translate existing
publicly-available English dialogue data into the
target languages. We then explore multiple alterna-
tive ways to leverage this translated data in order to
finetune and train monolingual and multilingual di-
alogue evaluation models for two specific dialogue
submetrics. To address the impact of low quality
translations, we propose using an MT Quality Es-
timation (QE) model to rank the translations and
investigate the impact of finetuning models with
varying amounts of quality-ranked data. Figure 1
illustrates the proposed approach.
The performance of these alternative models is
evaluated on a curated test set of dialogues which
were human-annotated with dialogue quality scores
for two subqualities. The original English test
set was translated using MT and then post-edited
by editors into six different target languages (PT-
Portuguese, DE-German, FR-French, ZH-Chinese,
ES-Spanish and JA-Japanese). The quality scores
from the human annotations of the original En-
glish dialogues were then carried over to the target-
language dialogues. Our finetuned multilingual dia-
logue evaluation models exhibit strong correlations
with human judgements, comparable to LLMs, in-
dicating it is possible to leverage multilingual di-
alogue evaluation metrics without the constraints
LLMs currently possess (costs, latency, etc.). We
hope this will encourage other researchers to update
existing metrics using our proposed multilingual
finetuning approach.
In summary, the primary contributions of this
work are as follow:
â€¢We evaluate cross-lingual transfer and transla-
tion augmented training approaches using MT
for the task of training multilingual dialogue
evaluation models, showing that, on average,
the best performance is achieved by finetun-ing with subsets consisting of only the best
translations. We found that, depending on the
subquality and target language, the optimal
amount of translated data can be as low as 5%
and as high as 75%.
â€¢We translate and release DailyDialog and a
corresponding test set of human quality an-
notations in 6 languages to facilitate future
benchmarking of multilingual dialogue evalu-
ation metrics1.
2 Background
2.1 Open-Domain Dialogue Evaluation
Metrics
The recent trend in open-domain dialogue evalu-
ation is to train dialogue submetrics using well-
defined self-supervised tasks which correlate well
with their corresponding subqualities. The most
used self-supervised task is Next Sentence Predic-
tion (NSP), as it is known to correlate well with
subqualities that evaluate "Context Awareness" . Ex-
amples of this include: Uses Context (Mehri and
Eskenazi, 2020), Sensibleness (Phy et al., 2020;
Mendonca et al., 2022) and Relevance (Zhao et al.,
2020; Zhang et al., 2022a). Other subqualities in-
clude: Fluency, Grammatically Correct orUnder-
standability , which use word-level noising tech-
niques to generate negative samples (Phy et al.,
2020; Mendonca et al., 2022; Zhang et al., 2022a);
and Specificity , which uses an MLM (Masked
Language Modelling) score (Mehri and Eskenazi,
2020; Phy et al., 2020; Zhang et al., 2022a). For
overall quality, these submetrics are typically com-
bined using different methods (e.g. empirical ob-
servation, trained Linear Regression or multilayer
perceptrons).
To the best of our knowledge, there has not been
any published research on cross-lingual transfer
and/or development of trained multilingual metrics
for open-domain dialogue evaluation.
2.2 Multilingual Text Classification
Despite the lack of research on multilingual dia-
logue evaluation, extending text classification to
other languages is a well established subfield of
research in NLP. The main constraint for mul-
tilingual performance parity is the lack of task-
specific resources in the vast majority of written
languages. Given the creation of these resources is
1github.com/johndmendonca/DialEvalMLboth time consuming and expensive, most research
effort has been geared towards general-purpose
cross-lingual representations that are learned in
an unsupervised way, therefore leveraging the un-
structured data available in the wild. Large mul-
tilingual Transformer-based models (e.g mBERT,
XLM-RoBERTa, and mT5) have been successfully
used in a variety of classification tasks (Conneau
et al., 2020; Pires et al., 2019; Xue et al., 2021).
The standard approach for cross-lingual transfer is
to finetune on existing domain data in a source lan-
guage and perform inference in a target language.
However, this approach typically lags behind mod-
els specifically trained with in-domain (both task
and language) data.
As a solution to this problem, Pfeiffer et al.
(2020) propose learning language-specific adapter
modules via MLM on unlabelled target-language
data followed by task-specific adapter modules by
optimising a target task on labelled data in the
source language. Task and language adapters are
stacked, allowing cross-lingual transfer to the tar-
get language by substituting the target-language
adapter at inference.
Bornea et al. (2021) propose an augmentation
strategy where a corpus of multilingual silver-
labelled QA pairs is generated by combining the
original English training data with MT-generated
data. A language adversarial training and arbi-
tration framework bring the embeddings closer to
each other, making the model language invariant.
To the best of our knowledge, there has not been
any research on the utilization of MT Quality Es-
timation (QE) scoring as a means for identifying
and demoting or excluding poorly translated data
in such cross-language training scenarios.
3 Problem Formulation
The goal of reference-free turn-level dialogue eval-
uation is, given a dialogue history (frequently de-
noted as context) cof varying amount of turns, and
a response r, to learn a scoring function that assigns
a score f(c, r)â†’s. This scoring function is com-
pared against human judgements, which annotate
the same context-response pairs. These responses
are evaluated using a scaling method, for instance,
a binary (0,1)judgement or a [1,5]scale, where
the lowest value means lowest quality and high-
est value maximum quality. The notion of quality
varies wildly depending on the annotation. In this
work, we evaluate dialogue in two dimensions:â€¢Understandability An understandable re-
sponse is one that can be understood without
context. Such responses may contain minor
typos that do not hinder the comprehension of
the response.
â€¢Sensibleness A sensible response is one that
takes into account its preceding context.
Most automatic evaluation metrics reformulate
the problem as regression. Performance is then
evaluated using Pearson and Spearman correlations
with human annotations.
3.1 Automatic Dialogue Evaluation Metrics
The majority of competitive metrics for dia-
logue evaluation include models trained in a self-
supervised way for Valid Sentence Prediction
(VSP) and Next Sentence Prediction (NSP) (Yeh
et al., 2021; Zhang et al., 2021). As such, the focus
of this work was to evaluate multilingual dynamics
for these models, which can then be employed on
existing metrics.
VSP: Valid Sentence Prediction In this paper,
we followed the approach used by Phy et al. (2020)
and initially proposed by Sinha et al. (2020). A re-
gression model was trained to differentiate between
positive samples and synthetic negative samples.
Positive samples are perturbed by randomly apply-
ing one of the following: (1) no perturbation, (2)
punctuation removal, (3) stop-word removal. Neg-
ative samples are generated by randomly applying
one of the following rules: (1) word reorder (shuf-
fling the ordering of the words); (2) word-drop; and
(3) word-repeat (randomly repeating words).
NSP: Next Sentence Prediction The task of pre-
dicting sensibleness can be considered a binary
(NSP) task, distinguishing a positive example from
a semantically negative one, given a context. A dis-
criminative regression model was trained using the
following sampling strategy: positive responses are
drawn directly from the dialog; negative responses
are randomly selected and a token coverage test dis-
cards semantically similar sentences. All responses
are processed using the positive-sample heuristic
used by VSP.
4 Cross-lingual Transfer Learning
The goal of the experiments described in this sec-
tion was to evaluate different basic approaches ofcross-lingual transfer for the task of automatic dia-
logue evaluation. For encoder model training, we
leveraged Machine Translation (MT) by fully trans-
lating an English source dialogue dataset and then
finetuning monolingual and multilingual models
using these translations.
4.1 Experimental Setup
4.1.1 Dataset
All experiments in this paper were based on the Dai-
lyDialog (Li et al., 2017) dataset, a high-quality
human-human open-domain dialogue dataset fo-
cused on day-to-day conversations. After process-
ing, we obtained train/dev splits of 58,515/25,078
and 89,707/38,449 per language for the VSP and
NSP models, respectively. For training and evalua-
tion, the post-processed dataset was translated into
the target languages using MBART50 (Liu et al.,
2020). We opted for using MBART50 as it is a
relatively lightweight open sourced model with a
large language coverage.
For the test set, we leveraged the annotations
from Phy et al. (2020). These human annotations
evaluate five responses from two retrieval methods,
two generative methods, and one human-generated
response for 50 contexts. These responses were
annotated in terms of Understandability andSensi-
bleness2. We translated this set using Unbabelâ€™s3
translation service. A total of 300 sentences were
translated, corresponding to the 50 shared contexts
and 250 responses. The translations were then
split into smaller tasks and were corrected by ed-
itors from a commercial provider. Editors were
specifically asked to retain any source disfluencies
or hallucinations stemming from low quality re-
sponse generation (e.g. "Iâ€™m afraid you canâ€™t. Iâ€™m
afraid you canâ€™t." ;"Au contraire, you need to be a
bahn." ). This ensured the original human quality
annotations remained valid for the translation. A
secondary senior editor reviewed the edited content
as a whole.
4.1.2 Finetuned Encoders
We used XLM-RoBERTa (Conneau et al., 2020) as
the encoder model for the experiments. This model
is the multilingual version of RoBERTa, pretrained
on CommonCrawl data containing 100 languages.
2Annotations for Specificity andOverall Quality were also
conducted, but were excluded since they do not map to the
learned metrics under study.
3unbabel.comFor both the VSP and NSP models, we added a
regression head on top of the encoder model.
EN â€“ Zero-shot inference As a baseline for our
results, we conducted zero-shot inference on the
target languages using a model finetuned only on
the original English data.
LANG â€“ Target-Language Finetuning We fine-
tuned the encoder with target-language translated
dialogue data only. The downside of this approach
is that a unique model needs to be trained for each
target language. However, this method can be
scaled to every language, including new ones, and
is optimised to perform best in that language.
ML â€“ Multilingual Finetuning Instead of fine-
tuning a new model for each target language, one
can finetune a single multilingual model by combin-
ing all of the translated data. In this case, the result-
ing single trained model is then used to evaluate re-
sponses in all languages. However, its performance
may suffer in languages it has not seen during fine-
tuning, even if they are supported by the encoder
model. Furthermore, unlike target-language fine-
tuned, the multilingual model is optimised jointly
for all included languages.
MAD-X In this approach, we trained a VSP and
NSP task adapter using the original English data by
stacking the task adapter with a pretrained English
language adapter (kept frozen during training). For
zero-shot inference, the English language adapter
was replaced by the target-language counterpart,
while keeping the trained task adapter in place.
4.1.3 Large Language Model
As an additional strong baseline, we leveraged
gpt-3.5-turbo (colloquially known as ChatGPT)
as an evaluator of Understandability and Sensible-
ness. The context (exclusively for Sensibleness)
and response was provided as input, together with
the prompt "{Given the context,} evaluate from 1-
5 the response in terms of {dimension}. Provide
the score and nothing else." . This prompt, paired
with a temperature setting of 0.0 attempted to min-
imises the variability of the output. Nevertheless,
we report a standard deviation of (.003, .003) and
(.001, .001) for Understandability and Sensibleness
correlations, respectively, across 3 runs.
4.2 Results
The correlation results for all subqualities and the
overall quality are presented in Table 1.EN PT DE FR ZH ES JA A VG
Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp.
Understandability
EN .376 .187 .366 .167 .328 .172 .351 .120 .318 .202 .342 .204 .204 .176 .327 .194
LANG - - .176 .164 .214 .138 .052 .034 .274 .156 .219 .144 .185 .132 .214 .146
ML .336 .117 .176 .167 .262 .150 .012 .015 .225 .138 .117 .158 .091 .092 .174 .126
MAD-X .363 .166 .189 .103 .237 .122 .168 .078 .305 .168 .217 .119 .119 .129 .228 .126
ChatGPT .397 .334 .365 .230 .332 .263 .369 .273 .276 .182 .394 .263 .228 .223 .337 .263
Sensibleness
EN .658 .676 .636 .651 .657 .655 .646 .656 .640 .656 .646 .657 .590 .599 .639 .649
LANG - - .649 .661 .669 .699 .635 .655 .634 .671 .629 .669 .617 .640 .642 .664
ML .651 .691 .606 .675 .634 .680 .605 .669 .642 .667 .596 .676 .599 .637 .619 .664
MAD-X .660 .681 .614 .604 .664 .652 .624 .624 .608 .647 .688 .661 .558 .595 .631 .638
ChatGPT .746 .724 .636 .626 .683 .675 .695 .666 .655 .645 .680 .677 .625 .610 .674 .662
Table 1: Average correlation results across 3 runs with different seeds. Pr.denotes Pearson and Sp. denotes
Spearman. Bold denotes best performance, Italic p <0.05.
Understandability The results show that, on av-
erage, the best performing encoder approach is
the zero-shot inference using the English model
(EN). Both the target-language finetuning ( LANG )
and multilingual finetuning approaches ( ML) have
much lower performances, indicating that transla-
tion augmentation is detrimental for this task. We
also note that the MAD-X approach, although per-
forming slightly better than ML and LANG, still
lags behind EN considerably. In any case, ChatGPT
largely outperforms other models on both metrics.
Sensibleness The best performing encoder ap-
proach for this subquality is LANG. Intuitively
this makes sense, given that during finetuning the
model is exposed to target-language data for the
language it is being evaluated on. Furthermore,
the performance difference between the different
approaches is relatively much smaller, which indi-
cates the Sensibleness subquality is less sensitive
to MT quality. When comparing these results with
ChatGPT, we observe a much smaller performance
gap, with the best encoder models slightly outper-
forming on Spearman.
5 MT Quality-aware finetuning
The effects of noise introduced to the training data
is a subject of intense research in the literature
(Zhang et al., 2017; Hu et al., 2020; Swayamdipta
et al., 2020). It is expected that, for this task, noise
is introduced by low quality translations, reducing
the performance of trained models. This issue was
identified in Section 4, where for the VSP model
in particular, the models trained using translations
performed much worse than the baseline approach.
Our hypothesis is that some translations heavily dis-
rupt morphosyntactic cues used to infer response
fluency, as shown in Table 2. We acknowledge
that these low quality translations may also reduceEN: Yes, Iâ€™d like to see the receipt .
Oh ! I see you bought the watch last week.
PT:Sim, gostava de ver o receio.
Oh! Vejo-te a fazer o relÃ³gio na semana passada.
QE score: -0.670
EN: Just look around ? Ah, thatâ€™s boring.
ES:Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡Â¡
QE score: -1.481
EN: Eight tens , six ones and large silver for others.
ZH:å…«ä¸ªåä¸ª,å…­ä¸ªåä¸ª,å…¶ä»–åä¸ªåä¸ªåä¸ªåä¸ªå...
QE Score: -1.312
Table 2: Examples of low quality translations with cor-
responding QE score. Red denotes MT error, with
underline in the source sentence indicating the closest
alignment of the error. Blue denotes keywords that refer
to prior context.
the quality of the response by disrupting keywords
that point to the context (which is important for
Sensibleness), or even more subtle quality cues
(e.g. loss of empathy, inconsistency with named
entities). However, the NSP model is trained to
discriminate between the original response and ran-
domly selected response from the corpus. As such,
the modelâ€™s prediction will remain invariant to most
translation errors.
These observations, paired with the fact encoder
models only slightly underperform ChatGPT (a
much larger and expensive model), motivate the
work described in this section. We hypothesise
that, by ameliorating the MT noise via identifying
and filtering low quality translations, the encoder
model performance can outperform LLMs such as
ChatGPT, at a fraction of the cost.
Since there are no available references, an MT
QE (Specia et al., 2018) automatic metric is used
for this purpose. Formally, an MT QE model is a
scoring function that assigns a score given a source
sentence and hypothesis translation. The unbound-
edness and uncalibrated nature of this score across
languages results in the need for a cumbersome0.05 0.10 0.20 0.50 0.751.00
Amount of Translations0.00.20.40.60.81.0Normalised CorrelationPearson Correlation for Understandability
PT
DE
FR
ZH
ES
JA(a) Pearson Correlation, Understandability.
0.05 0.10 0.20 0.50 0.751.00
Amount of Translations0.00.20.40.60.81.0Normalised CorrelationSpearman Correlation for Understandability
PT
DE
FR
ZH
ES
JA (b) Spearman Correlation, Understandability.
0.05 0.10 0.20 0.50 0.751.00
Amount of Translations0.800.850.900.951.001.051.10Normalised CorrelationPearson Correlation for Sensibleness
PT
DE
FR
ZH
ES
JA
(c) Pearson Correlation, Sensibleness.
0.05 0.10 0.20 0.50 0.751.00
Amount of Translations0.800.850.900.951.001.051.10Normalised CorrelationSpearman Correlation for Sensibleness
PT
DE
FR
ZH
ES
JA (d) Spearman Correlation, Sensibleness.
Figure 2: Normalised Pearson and Spearman correlation for the Understandability and Sensibleness submetric with
varying amount of translated training data. Numeric results available in Appendix B.
analysis for each individual language in order to
determine a threshold for filtering. Instead, we pro-
pose to use QE scores for response ranking, for
each target language. This ensures a standardised
method for filtering, improving the scalability of
this method to new languages.
5.1 Experimental setup
2.5
 2.0
 1.5
 1.0
 0.5
 0.0 0.5 1.0
MT QEPT
DE
FR
ZH
ES
JALanguageMT QE score distributions
Figure 3: MT QE unnormalised score boxplot per lan-
guage.
In order to confirm our hypothesis, we retrained
all models using different amounts of translated
data (100, 75, 50, 20, 10 and 5%). The ranking of
the translations was conducted by scoring them us-
ing the WMT20 COMET-QE-DA model (Rei et al.,2020). For the VSP model, we ranked the individ-
ual sentences, and then applied negative sampling.
For the NSP model, we ranked the positive and
negative samples separately and then merged them
together. Figure 3 presents the unnormalised score
boxplot per language for all sentences (context and
responses) for DailyDialog.
One of the things we noticed when finetuning
the monolingual models was that the VSP models
had large variations in performance. This can be
attributed to (1) the low amount of training data, es-
pecially when using very few examples (5%, 10%),
and (2) low quality translations, which is the re-
search question this experiment attempts to answer.
Since the true impact of low quality translations is
obfuscated by other factors, we decided to finetune
the LANG models starting from the EN checkpoint
instead of the pretrained XLM-RoBERTa, and in-
clude the zero-shot results as 0%.
5.2 Results
LANG For the monolingual models, we plot nor-
malised correlation results with the amount of MT
data used during finetuning in Figure 2. The Un-
derstandability correlation results show that the
optimal amount of translated data is language de-
pendent, but with a clear indication that the inclu-EN PT DE FR ZH ES JA A VG
Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp.
Understandability
0 (EN) .376 .187 .366 .167 .328 .172 .351 .120 .318 .202 .342 .204 .204 .176 .327 .194
5 .403 .182 .490 .219 .344 .172 .385 .091 .320 .235 .429 .236 .230 .179 .372 .211
10 .377 .180 .514 .227 .381 .193 .294 .091 .338 .214 .385 .212 .216 .175 .358 .206
20 .384 .177 .478 .236 .333 .203 .153 .087 .318 .219 .315 .214 .174 .168 .308 .202
50 .413 .201 .481 .242 .381 .213 .103 .053 .310 .200 .315 .221 .219 .149 .317 .200
75 .311 .145 .247 .211 .320 .195 .047 .048 .163 .149 .111 .198 .108 .127 .187 .158
100 .336 .117 .176 .167 .262 .150 .012 .015 .225 .138 .117 .158 .091 .092 .174 .126
ChatGPT .397 .334 .365 .230 .332 .263 .369 .273 .276 .182 .394 .263 .228 .223 .337 .263
Sensibleness
0 (EN) .658 .676 .636 .651 .657 .655 .646 .656 .640 .656 .646 .657 .590 .599 .639 .649
5 .637 .674 .629 .632 .627 .648 .637 .656 .629 .646 .626 .647 .567 .596 .621 .640
10 .642 .675 .639 .664 .661 .669 .636 .661 .637 .656 .635 .668 .575 .604 .632 .654
20 .650 .689 .627 .670 .649 .681 .627 .666 .621 .661 .637 .673 .568 .614 .626 .660
50 .667 .691 .642 .687 .650 .672 .621 .662 .652 .664 .629 .673 .600 .642 .637 .666
75 .677 .712 .629 .694 .679 .702 .633 .679 .661 .673 .643 .695 .593 .635 .645 .679
100 .651 .691 .606 .675 .634 .680 .605 .669 .642 .667 .596 .676 .599 .637 .619 .664
ChatGPT .746 .724 .636 .626 .683 .675 .695 .666 .655 .645 .680 .677 .625 .610 .674 .662
Table 3: Average correlation results across 3 runs with different seeds for multilingual models when varying the
amount of translated data.
sion of more translations decreases performance
significantly. Instead, a lower amount of transla-
tions (5-10%) yields optimal performance. This
shows that this small finetuning step is essentially
adapting a model that was already finetuned for
the downstream task to the target-language domain.
ForSensibleness , we see that the inclusion of more
translations yields the best results. As such, we can
conclude that low-quality MT does not adversely
affect performance. We hypothesise this is due to
MT being able to correctly translate keywords that
indicate context awareness. Since we are only con-
cerned about relevance, the overall sentence may
still contain MT errors and be scored highly.
ML The correlation results for the multilingual
models are presented in Table 3. For Understand-
ability , we note that, on average, and similar to
LANG, the best performance is attained with the
minimum amount of translated data (ML-5), with
the performance decreasing when more translations
are added. Comparing these results with ChatGPT,
we observe an improvement in performance, but
our encoder models are still generally weaker when
using Spearman as a metric. For Sensibleness ,
decreasing the amount of data reduces the perfor-
mance of the model. However, we note a decrease
in performance when including the full amount of
translated data (ML-100). This may be due to the
inclusion of the worst translations â€“ typically hallu-
cinations â€“ which is compounded by training on all
languages. Unlike in Understandability, here we
see that ChatGPT still outperforms the best encoder
model in terms of Pearson correlation.5.3 Effect of low-quality translation during
prediction
One might ask if a low-quality translation can in-
duce the submetrics to output a different score. In-
tuitively, we hypothesise each model will attribute
different scores in the face of low quality transla-
tions. More specifically, given the results presented
in previous sections, we expect the test prediction
error to be:
â€¢Negatively correlated with the MT QE
scores for VSP. We know this model is highly
sensitive to low quality translations, since MT
errors frequently affect the fluency of the re-
sponse (as identified in previous sections);
â€¢Weakly correlated for the NSP model. The
model showed robustness when including
more translations during training, with per-
formance decreasing only when we included
all translations (ML-100) during training.
In order to evaluate these assumptions, the cor-
relation plots of the MT QE z-scores (obtained
independently for each language) against the sub-
metric absolute error using the best ML models
(ML-5 for VSP and ML-75 for NSP) for the test
set are presented in Figure 4.
For the Understandability subquality, we note
that there is a slight negative correlation between
the absolute error and the MT QE score. This is
also confirmed by a calculated Pearson Correla-
tion value of -0.245. For the Sensibleness sub-
quality, the relationship between these two mea-
sures is less obvious. For instance, we note that,
unlike for Understandability, maximum deviations1.5
 1.0
 0.5
 0.0 0.5
MT QE103
102
101
100Absolute ErrorQE Score vs Understandability Absolute Error
Language
pt
de
fr
zh
es
ja(a) Understandability scatter plot.
1.5
 1.0
 0.5
 0.0 0.5
MT QE104
103
102
101
100Absolute ErrorQE Score vs Sensibleness Absolute Error
Language
pt
de
fr
zh
es
ja (b) Sensibleness scatter plot.
Figure 4: Scatter plot comparing the test set MBART50 per-language QE z-scores (x-axis) versus the per sample
Absolute Prediction Error (y-axis in log scale) for Understandability and Sensibleness subqualities.
are spread evenly across the QE scale, which points
to the model erroneously predicting Sensibleness
irrespective of the translation quality. Conversely,
we also note a higher density of accurate predic-
tions with lower QE scores. These results, paired
with the calculated Pearson Correlation value of
-0.129, confirm our hypothesis that the NSP model
is more agnostic of MT quality than VSP.
CTX: TambÃ©m me apercebi desta questÃ£o. E a
automatizaÃ§Ã£o dos processos do escritÃ³rio Ã© essencial.
RES: Sim, fazer tudo manualmente demora demasiado.
EN-VSP: .394 EN-NSP: .824
ML-VSP: 1.00 ML-NSP: 1.00
Unders.: 1.00 Sensibl: 0.00
CTX: Ja, ich leite die Jungs am Kai.
RES: Wow, das klingt nach einem fantastischen Job, de
du da bekommen hast.
EN-VSP: .963 EN-NSP: .315
ML-VSP: .941 ML-NSP: .981
Unders.: 1.00 Sensibl: 1.00
Table 4: Examples of subquality predictions from the
test set.
5.4 Example test predictions
We present representative examples of our best ML
modelsâ€™ prediction (ML 5/75) in Table 4. In the
first example, the baseline English model fails to
appropriately identify the understandability of the
response. In the second example, we see that the
multilingual model is able to correctly identify that
the response takes into account the job presented in
the context (manager) by complimenting it ("fan-
tastic job"), which the EN model failed to identify.6 Conclusions
This paper explored the use of cross-lingual knowl-
edge transfer for the novel task of automatic mul-
tilingual dialogue evaluation. We evaluated dif-
ferent strategies for this task, including zero-shot
inference, MAD-X and Machine Translation aug-
mentation. Empirically we showed that the naive
approach of leveraging MT for augmentation is
insufficient to outperform the baseline of English
finetuning with a multilingual encoder-based LM,
let alone a strong LLM. Instead, by filtering out
low quality translations, we were able to reduce the
gap of performance on ChatGPT, outperforming
it on select correlation metrics. Experimental re-
sults showed that we obtain the best performance
when training encoder models with the following
proportions of MT-QE: 5% for Understandability
and 75% for Sensibleness.
One could argue the notion of quality is intrin-
sically related to cultural norms. For instance,
Japanese speakers may prefer a polite conversation,
whereas German speakers might prefer a more di-
rect interaction. A future research direction is to
evaluate generative model responses in different
languages using annotators exposed to the culture
associated with a given language. In addition to
ensuring the evaluation of the response meets the
criteria of "quality" in different cultures, it would
also allow for a qualitative analysis of the differ-
ences in the notion of quality between languages.Limitations
Perhaps the main limitation of this work is the
restricted amount of languages studied. Ideally,
we would have used a more comprehensible set of
languages, including low-resource ones, to evaluate
the consistency of the conclusions drawn from the
experiments.
Another limitation is the focus on a single open-
domain dialogue dataset. Dialogue evaluation met-
rics are known to correlate poorly when evaluated
on unseen datasets (Yeh et al., 2021). As such,
it is not certain that the observations presented in
this work would hold for other datasets, or even
different annotations (Mehri et al., 2022).
Finally, the pretrained encoder, MT and QE mod-
els used in this work are not fully representative
of all available models. We acknowledge that the
optimal amount of filtering is likely to be different,
depending on the combination of models used.
Ethics Statement
This work leverages dialogues and annotations de-
veloped exclusively by English-speakers. This in-
troduces an English-centric bias with respect to the
notion of quality (and subqualities) in dialogues.
Although not evaluated in depth in this work, there
could be a chance that the models erroneously yield
lower scores to responses not conforming to En-
glish notions of quality responses.
The original dialogue dataset and generated re-
sponses were checked for personally identifiable
information or offensive content by the original
authors. Although highly unlikely, we acknowl-
edge the translations may contain offensive content
resulting from decoding.
The post-editing conducted in this work used a
crowdsourcing platform that awarded users a fair
wage according to their location.
Acknowledgements
This research was supported by the Portuguese
Recovery and Resilience Plan through project
C645008882-00000055 (Responsible.AI), and by
national funds through FundaÃ§Ã£o para a CiÃªn-
cia e a Tecnologia (FCT) with references
PRT/BD/152198/2021 and UIDB/50021/2020, and
by the P2020 program MAIA (LISBOA-01-0247-
FEDER-045909).References
Daniel Adiwardana, Minh-Thang Luong, David R So,
Jamie Hall, Noah Fiedel, Romal Thoppilan, Zi Yang,
Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,
et al. 2020. Towards a human-like open-domain chat-
bot. arXiv preprint arXiv:2001.09977 .
Dimitra Anastasiou, Anders Ruge, Radu Ion, Svet-
lana Seg Ë˜arceanu, George Suciu, Olivier Pedretti,
Patrick Gratz, and Hoorieh Afkari. 2022. A machine
translation-powered chatbot for public administra-
tion. In Proceedings of the 23rd Annual Conference
of the European Association for Machine Translation ,
pages 329â€“330, Ghent, Belgium. European Associa-
tion for Machine Translation.
Satanjeev Banerjee and Alon Lavie. 2005. METEOR:
An automatic metric for MT evaluation with im-
proved correlation with human judgments. In Pro-
ceedings of the ACL Workshop on Intrinsic and Ex-
trinsic Evaluation Measures for Machine Transla-
tion and/or Summarization , pages 65â€“72, Ann Arbor,
Michigan. Association for Computational Linguis-
tics.
Mihaela Bornea, Lin Pan, Sara Rosenthal, Radu Florian,
and Avirup Sil. 2021. Multilingual transfer learning
for qa using translation as data augmentation. In
Proceedings of the AAAI Conference on Artificial
Intelligence , volume 35, pages 12583â€“12591.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal,
Vishrav Chaudhary, Guillaume Wenzek, Francisco
GuzmÃ¡n, Edouard Grave, Myle Ott, Luke Zettle-
moyer, and Veselin Stoyanov. 2020. Unsupervised
cross-lingual representation learning at scale. In Pro-
ceedings of the 58th Annual Meeting of the Asso-
ciation for Computational Linguistics , pages 8440â€“
8451, Online. Association for Computational Lin-
guistics.
Wei Hu, Zhiyuan Li, and Dingli Yu. 2020. Simple and
effective regularization methods for training on nois-
ily labeled data with generalization guarantee. In
8th International Conference on Learning Represen-
tations, ICLR 2020, Addis Ababa, Ethiopia, April
26-30, 2020 . OpenReview.net.
Diederik P. Kingma and Jimmy Ba. 2015. Adam: A
method for stochastic optimization. In 3rd Inter-
national Conference on Learning Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015,
Conference Track Proceedings .
Yanran Li, Hui Su, Xiaoyu Shen, Wenjie Li, Ziqiang
Cao, and Shuzi Niu. 2017. DailyDialog: A manually
labelled multi-turn dialogue dataset. In Proceedings
of the Eighth International Joint Conference on Nat-
ural Language Processing (Volume 1: Long Papers) ,
pages 986â€“995, Taipei, Taiwan. Asian Federation of
Natural Language Processing.
Chia-Wei Liu, Ryan Lowe, Iulian Serban, Mike Nose-
worthy, Laurent Charlin, and Joelle Pineau. 2016.
How NOT to evaluate your dialogue system: Anempirical study of unsupervised evaluation metrics
for dialogue response generation. In Proceedings of
the 2016 Conference on Empirical Methods in Natu-
ral Language Processing , pages 2122â€“2132, Austin,
Texas. Association for Computational Linguistics.
Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey
Edunov, Marjan Ghazvininejad, Mike Lewis, and
Luke Zettlemoyer. 2020. Multilingual denoising pre-
training for neural machine translation. Transac-
tions of the Association for Computational Linguis-
tics, 8:726â€“742.
Shikib Mehri, Jinho Choi, Luis Fernando Dâ€™Haro, Jan
Deriu, Maxine Eskenazi, Milica Gasic, Kallirroi
Georgila, Dilek Hakkani-Tur, Zekang Li, Verena
Rieser, Samira Shaikh, David Traum, Yi-Ting Yeh,
Zhou Yu, Yizhe Zhang, and Chen Zhang. 2022. Re-
port from the nsf future directions workshop on auto-
matic evaluation of dialog: Research directions and
challenges.
Shikib Mehri and Maxine Eskenazi. 2020. USR: An
unsupervised and reference free evaluation metric
for dialog generation. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics , pages 681â€“707, Online. Association for
Computational Linguistics.
John Mendonca, Alon Lavie, and Isabel Trancoso. 2022.
QualityAdapt: an automatic dialogue quality estima-
tion framework. In Proceedings of the 23rd Annual
Meeting of the Special Interest Group on Discourse
and Dialogue , pages 83â€“90, Edinburgh, UK. Associ-
ation for Computational Linguistics.
Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Car-
roll L. Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, John
Schulman, Jacob Hilton, Fraser Kelton, Luke Miller,
Maddie Simens, Amanda Askell, Peter Welinder,
Paul Christiano, Jan Leike, and Ryan Lowe. 2022.
Training language models to follow instructions with
human feedback.
Kishore Papineni, Salim Roukos, Todd Ward, and Wei-
Jing Zhu. 2002. Bleu: A method for automatic evalu-
ation of machine translation. In Proceedings of the
40th Annual Meeting on Association for Computa-
tional Linguistics , ACL â€™02, page 311â€“318, USA.
Association for Computational Linguistics.
Jonas Pfeiffer, Ivan Vuli Â´c, Iryna Gurevych, and Se-
bastian Ruder. 2020. MAD-X: An Adapter-Based
Framework for Multi-Task Cross-Lingual Transfer.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 7654â€“7673, Online. Association for Computa-
tional Linguistics.
Vitou Phy, Yang Zhao, and Akiko Aizawa. 2020. Decon-
struct to reconstruct a configurable evaluation metric
for open-domain dialogue systems. In Proceedings of
the 28th International Conference on ComputationalLinguistics , pages 4164â€“4178, Barcelona, Spain (On-
line). International Committee on Computational Lin-
guistics.
Telmo Pires, Eva Schlinger, and Dan Garrette. 2019.
How multilingual is multilingual BERT? In Proceed-
ings of the 57th Annual Meeting of the Association for
Computational Linguistics , pages 4996â€“5001, Flo-
rence, Italy. Association for Computational Linguis-
tics.
Ricardo Rei, Craig Stewart, Ana C Farinha, and Alon
Lavie. 2020. Unbabelâ€™s participation in the WMT20
metrics shared task. In Proceedings of the Fifth Con-
ference on Machine Translation , pages 911â€“920, On-
line. Association for Computational Linguistics.
Sebastian Schuster, Sonal Gupta, Rushin Shah, and
Mike Lewis. 2019. Cross-lingual transfer learning
for multilingual task oriented dialog. In Proceedings
of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers) , pages 3795â€“3805, Minneapolis, Min-
nesota. Association for Computational Linguistics.
Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju,
Eric Michael Smith, Stephen Roller, Megan Ung,
Moya Chen, Kushal Arora, Joshua Lane, Morteza
Behrooz, W.K.F. Ngan, Spencer Poff, Naman Goyal,
Arthur D. Szlam, Y-Lan Boureau, Melanie Kam-
badur, and Jason Weston. 2022. Blenderbot 3: a
deployed conversational agent that continually learns
to responsibly engage. ArXiv , abs/2208.03188.
Koustuv Sinha, Prasanna Parthasarathi, Jasmine Wang,
Ryan Lowe, William L. Hamilton, and Joelle Pineau.
2020. Learning an unreferenced metric for online
dialogue evaluation. In Proceedings of the 58th An-
nual Meeting of the Association for Computational
Linguistics , pages 2430â€“2441, Online. Association
for Computational Linguistics.
Lucia Specia, Carolina Scarton, and Gustavo Henrique
Paetzold. 2018. Quality estimation for machine trans-
lation. Synthesis Lectures on Human Language Tech-
nologies , 11(1):1â€“162.
Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie,
Yizhong Wang, Hannaneh Hajishirzi, Noah A. Smith,
and Yejin Choi. 2020. Dataset cartography: Mapping
and diagnosing datasets with training dynamics. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 9275â€“9293, Online. Association for Computa-
tional Linguistics.
Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam
Shazeer, Apoorv Kulshreshtha, Heng-Tze Cheng,
Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, et al.
2022. Lamda: Language models for dialog applica-
tions. arXiv preprint arXiv:2201.08239 .
Linting Xue, Noah Constant, Adam Roberts, Mihir Kale,
Rami Al-Rfou, Aditya Siddhant, Aditya Barua, andColin Raffel. 2021. mT5: A massively multilingual
pre-trained text-to-text transformer. In Proceedings
of the 2021 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies , pages 483â€“498, On-
line. Association for Computational Linguistics.
Yi-Ting Yeh, Maxine Eskenazi, and Shikib Mehri. 2021.
A comprehensive assessment of dialog evaluation
metrics. In The First Workshop on Evaluations and
Assessments of Neural Conversation Systems , pages
15â€“33, Online. Association for Computational Lin-
guistics.
Chen Zhang, JoÃ£o Sedoc, L. F. Dâ€™Haro, Rafael E.
Banchs, and Alexander I. Rudnicky. 2021. Auto-
matic evaluation and moderation of open-domain
dialogue systems. ArXiv , abs/2111.02110.
Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin
Recht, and Oriol Vinyals. 2017. Understanding deep
learning requires rethinking generalization. In 5th
International Conference on Learning Representa-
tions, ICLR 2017, Toulon, France, April 24-26, 2017,
Conference Track Proceedings . OpenReview.net.
Pengfei Zhang, Xiaohui Hu, Kaidong Yu, Jian Wang,
Song Han, Cao Liu, and Chunyang Yuan. 2022a.
MME-CRS: Multi-Metric Evaluation Based on Cor-
relation Re-Scaling for Evaluating Open-Domain Di-
alogue. arXiv preprint arXiv:2206.09403 .
Qingyu Zhang, Xiaoyu Shen, Ernie Chang, Jidong Ge,
and Pengke Chen. 2022b. Mdia: A benchmark for
multilingual dialogue generation in 46 languages.
Tianyu Zhao, Divesh Lala, and Tatsuya Kawahara. 2020.
Designing precise and robust dialogue response eval-
uators. In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
26â€“33, Online. Association for Computational Lin-
guistics.
A Training setup and Hyperparamters
We used the XLM-R Large encoder model down-
loaded from HuggingFace4for all experiments. A
token representing the speaker was added for each
turn, and a history length of 3 turns was used. We
applied a regression head consisting of a 2-layer
MLP with a hidden size of 1024 and a hyperbolic
tangent function as activation for prediction. All
parameters were trained/finetuned using Adam op-
timizer (Kingma and Ba, 2015).
The task adapters were trained using the recipe
from Mendonca et al. (2022), using a learning rate
of 1e-4 and training for 10 epochs, with a batch
size of 32. We used the existing language adapters
from AdapterHub whenever possible (EN, ZH, JA)
4huggingface.co/xlm-roberta-largeand trained the remaining using the AdapterHubâ€™s
MLM recipe5on Wikipedia data6. The fully fine-
tuned models used a learning rate of 3e-6 and were
trained for 3 epochs using a batch size of 16. Eval-
uation was conducted every 1,000 steps for the
smaller training sets and 10,000 steps for the larger
ones (75% and 100 %). The best performing model
on the evaluation set was selected for testing.
For the dialogue data preprocessing we
used spaCy7and the corresponding core lan-
guage models. For the translations we used
facebook/mbart-large-50-one-to-many-mmt
from HuggingFace. Batch size was set to 16 and
decoding was conducted using beam search, with
the number of beams set to 4.
We used a single Quadro RTX 6000 24GB GPU
for all experiments.
B Additional Results
Table 5 presents the monolingual model results for
the experiments of Section 5. Due to time and
computational constraints, we only conduct these
experiments using a single seed.
5github.com/adapter-hub
6dumps.wikimedia.org
7spacy.ioEN PT DE FR ZH ES JA A VG
Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp. Pr. Sp.
Understandability
0 .347 .192 .381 .176 .353 .184 .349 .106 .406 .251 .372 .210 .268 .223 .354 .212
5 .534 .259 .469 .223 .347 .095 .318 .263 .459 .223 .236 .208 .387 .231
10 .563 .236 .489 .227 .199 .102 .300 .233 .300 .191 .303 .206 .357 .218
20 .499 .233 .356 .211 .153 .082 .323 .223 .257 .163 .251 .191 .312 .201
50 .433 .214 .418 .185 .117 .017 .250 .198 .233 .140 .225 .163 .289 .175
75 .186 .189 .306 .158 .089 .026 .319 .198 .243 .156 .226 .185 .245 .169
100 .240 .165 .347 .144 .082 .043 .248 .206 .191 .109 .216 .146 .239 .155
Sensibleness
0 .621 .654 .618 .627 .667 .668 .621 .644 .605 .647 .628 .628 .577 .592 .620 .635
5 .615 .636 .687 .657 .632 .628 .618 .629 .599 .631 .538 .553 .616 .626
10 .647 .646 .672 .655 .562 .596 .607 .626 .635 .637 .587 .606 .619 .630
20 .639 .644 .680 .679 .627 .640 .620 .633 .615 .634 .582 .595 .626 .638
50 .651 .680 .654 .671 .601 .631 .637 .665 .613 .639 .603 .609 .626 .647
75 .634 .670 .640 .681 .643 .664 .629 .673 .615 .639 .608 .635 .627 .656
100 .671 .693 .681 .698 .631 .666 .650 .688 .589 .659 .617 .633 .637 .666
Table 5: Average correlation results for the monolingual models when varying the amount of translated data.