Computationally Assisted Quality Control for Public Health Data Streams
Ananya Joshi ,Kathryn Mazaitis ,Roni Rosenfeld ,Bryan Wilder
Carnegie Mellon University
{aajoshi, kmazaitis, rrosenfeld, bwilder }@andrew.cmu.edu
Abstract
Irregularities in public health data streams (like COVID-19
Cases) hamper data-driven decision-making for public health
stakeholders. A real-time, computer-generated list of the
most important, outlying data points from thousands of daily-
updated public health data streams could assist an expert re-
viewer in identifying these irregularities. However, existing
outlier detection frameworks perform poorly on this task be-
cause they do not account for the data volume or for the statis-
tical properties of public health streams. Accordingly, we de-
veloped FlaSH ( Flagging Streams in public Health), a prac-
tical outlier detection framework for public health data users
that uses simple, scalable models to capture these statistical
properties explicitly. In an experiment where human experts
evaluate FlaSH and existing methods (including deep learn-
ing approaches), FlaSH scales to the data volume of this task,
matches or exceeds these other methods in mean accuracy,
and identifies the outlier points that users empirically rate as
more helpful. Based on these results, FlaSH has been de-
ployed on data streams used by public health stakeholders.
1 Motivation and Introduction
During the COVID-19 pandemic, daily-updated real-time
public health data was used directly [6] or as input to meth-
ods that informed critical healthcare decisions and policies
[36] in support of Sustainable Development Goals such as
good health and well being. However, aspects of public health
data have hampered this data-driven decision-making in sev-
eral ways. These include issues like data delays, corrections,
and recording errors [9, 28] that may have masked impor-
tant trends in disease progression [19], as shown in Fig. 1.
Additionally, COVID variants or policy changes often cause
sudden, notable distribution shifts in the data [37]. Finally,
public health data streams are known to be biased or incom-
plete [21]. For example, regions with low healthcare resource
availability may not have accurate COVID case counts.
Addressing these issues is a significant challenge for any
organization that curates public health data streams [18], in-
cluding the Delphi Group at Carnegie Mellon University
(Delphi). Delphi employs a team of full-time developers,
statisticians, researchers, and product managers to maintain
Jan 2022 Feb 2022 Mar 2022 Apr 2022 May 2022 Jun 2022 Jul 2022 Aug 2022020k40k60k80k0.5 Quantile Forecasted Values
Actual CountsForecasted COVID Case Counts for Nevada, US 
 Box plots from 30 Forecasters that report to the US CDC COVID CasesFigure 1: Temporal irregularities in actual case counts, shown by
the large spikes in March and July 2022, when cases were trending
down, resulted in similar spikes for predicted counts (highlighted in
red) that were then sent to the US Centers for Disease Control and
Surveillance.
an accurate and performant public health data source1. Del-
phi’s publicly available API [11] and other data products
are regularly used by public health authorities in the United
States (US), along with researchers, forecasters, journalists,
and other users (totaling visits from over 78k unique IP ad-
dresses in January 2022). These stakeholders recommended
that Delphi continuously monitor their data streams for ir-
regularities so that Delphi’s data users have more informa-
tion about data quality issues, the state of the pandemic, and
changes in regional disease behavior, to directly support data-
driven decision-making.
To act on this recommendation, expert human reviewers
in Delphi would need to regularly monitor at least ten thou-
sand data streams for stakeholders (e.g. cases, deaths, and
hospitalizations, at several geographical resolutions, includ-
ing county, state, territory, and national level resolutions). If
done manually, this type of monitoring is prohibitively ex-
pensive [18]. Even if it were feasible, trained reviewers fre-
quently miss critical irregularities due to the sheer reviewing
load. While some outliers are so extreme that they require
no human review, many outliers that signify irregularities are
more nuanced and require close human attention. Compu-
tationally assisted quality control, where a reviewer only in-
spects the top entries from a computer-generated ranked list
of outlier streams that a human should review, is promising
because it could prioritize the reviewer’s time for irregularity
detection while retaining the trust and expertise a reviewer
1Delphi’s open source repositories can be found at
https://github.com/cmu-delphi
1arXiv:2306.16914v2  [cs.AI]  2 Jan 2024brings.
Creating this computationally ranked list of outliers in
public health streams is a difficult task. In addition to the
practical constraints of operating over the large data volume
necessitated by this task, outlier detection methods must be
robust to the statistical noise ,nonstationarity ,day of week
effects , and limited historical data that are prevalent in pub-
lic health streams in order to provide helpful recommenda-
tions [23, 27, 32]. Further, the outlier detection methods must
be simple and intuitive for reviewers to understand and trust
them on this task.
To address these challenges, we present FlaSH ( Flagging
Streams in public Health), available open source at
https://github.com/cmu-delphi/covidcast-indicators/tree/
main/ delphi utils python/delphi utils/flash eval. FlaSH is
a new outlier detection framework that produces a ranked
list of recent values from data streams that most warrant
human inspection . FlaSH uses simple, scalable, and intuitive
models to explicitly capture the statistical properties of
public health data. To address challenges in evaluating
unsupervised outlier detection methods in time series data
like FlaSH, we also developed and conducted a classification
and ranking evaluation of FlaSH’s performance using input
from several expert human reviewers. This is especially
important given that many recent works in anomaly detection
use semi-synthetic or simulation evaluations that may not
truly reflect an expert user’s assessment of the method utility.
In this evaluation, FlaSH matches or outperforms previous
outlier detection methods, including recent deep learning
baselines.
2 Practical Irregularity Detection Goals
Our goal is to develop a framework that assists reviewers in
detecting important irregularities in Delphi’s data streams on
behalf of public health data users. The data streams available
to the Delphi Group vary by source (local governments, hos-
pitals, private companies, and surveys), and each source has
its own dynamics and measurement definitions. For example,
the Johns Hopkins Centers for System Science and Engineer-
ing (JHU CSSE) COVID-19 source only curates data from
publicly available reports [9]. They also only report real-time
cumulative estimates. Thus, subsequent corrections to the cu-
mulative figures can appear as large spikes or even negative
values in derived daily case counts.
Detecting such irregularities across many sources is
uniquely challenging for typical outlier detection methods,
leading to a range of failure modes observed in our experi-
ments. First, modern deep learning methods for outlier detec-
tion struggle with the large number of time series, each with
a short history and rapid distribution shifts [24]. To perform
well, these highly parameterized models require long training
histories often unavailable in public health settings. More-
over, high computational costs mean these methods scale
poorly to real-time operation over thousands of distinct time
series. Second, simpler statistical methods are not attuned to
the specific structure of public health data and struggle to ac-
curately identify irregularities [34]. Third, neither class canleverage features of public health data streams that could as-
sist with diagnosing irregularities. Because of these limita-
tions, Delphi currently relies on volunteers and group mem-
bers to manually report issues on all data streams as they en-
counter them, but this process is unsystematic and expensive.
To start, the proposed outlier detection method should de-
tect specific types of outliers present in public health streams
that are relevant to Delphi’s stakeholders so that the method is
both context and user-dependent [29]. To identify these out-
lier categories, we conducted an exploratory analysis on data
streams2of COVID Case Counts and Ratios, COVID Deaths,
Hospital Visits, Google Symptoms Trends, and Doctors Vis-
its at the national, state, territory, and county level resolutions
from the first available date of the streams until December
2021. Using these streams, which each have a different pos-
sible range of values based on the region’s population and the
measurement quantity, we defined the following categories of
outliers based on their ability to assist reviewers with identi-
fying irregularities:
Out of Range Values and Global Outliers. These outliers
are typically due to retrospective updates made by a data
source in the value of a cumulative quantity. Out of range ex-
amples include “negative” new cases (if the cumulative total
was revised down) or more cases reported on a day than the
population of the geographic area due to multi-day batched
reports. Similarly, global outliers usually appear as large pos-
itive or negative spikes, but the ‘global’ outlier thresholds
may change over time as rapidly shifting disease dynamics
undermine static thresholds. Still, both of these outliers are
relatively easy to identify and very rarely require human re-
view.
Day of Week Outliers. Many public health streams have
systematic day of week effects [27]. For example, fewer
COVID cases are reported on weekends partly because fewer
people test on weekends. Day of week outliers occur when
reported data points are anomalous relative to the expectation
for their day of the week (even if they are within distribution
for the stream as a whole). Unlike out of range or global out-
liers, day of week outliers are more difficult for humans to
notice but may still indicate an irregularity in the stream.
Trendline Outliers. Data that deviate strongly from the re-
cent trend (e.g. case counts were rising last week, but to-
day’s count is low) or from the recent trends of close geo-
graphic regions warrant attention. These phenomena are the
most difficult for humans to detect and can indicate critical
irregularities in the context of recent data.
To address failure modes from existing methods and de-
tect these outlier categories, the proposed method must be in-
tuitive, scale to the data volume, and provide outputs (outlier
scores) that are correct and complement human judgment in
2The streams were from National, Texas, New York, LA County
(CA), and Loving County (TX) sourced from JHU CSSE, Depart-
ment of Health and Human Services, Google, and USA Facts.
2this task. Practically, this requires the method to be a single-
pass, point detection algorithm that integrates explainable AI
and human computing interaction insights. Further, the rank-
ing for the FlaSH list shown to the expert reviewers will be
based on the trendline outlier scores because they can indicate
critical irregularities in the context of recent data. Reviewers
will also benefit from inspecting the global and day of week
outlier scores reported alongside. Finally, each of these de-
sired criteria must be evaluated to justifiably compare differ-
ent outlier detection methods for this task.
3 FlaSH Outlier Detection Method
FlaSH formalizes the outlier detection problem discussed in
the previous section as a model-based hypothesis test [4]. We
denote a single data stream as a time series Xt,t=s...T .
Here, sis the starting time for the stream analysis3, and Tis
the current time. When it is necessary to discuss multiple ge-
ographic regions, we use Xrto denote the stream for a given
quantity in geographic region r(e.g. the stream of COVID
cases in a given US county).
Suppose that Xs:T−1∼mfor some m∈ M , where
Mis a set of models. We test the hypothesis that the most
recent point in the stream is drawn from the same model
(H0:XT∼m). If the observed data has a low probabil-
ity under this hypothesis, it means that XTwas likely not
generated from the same model mas the historical data. This
sudden shift from the data-generating distribution indicates a
potential irregularity. We conduct the hypothesis test by first
calculating a test statistic measuring the discrepancy between
observed values and values predicted by m. We then obtain
ap-value by comparing the real-time test statistic value to a
historical distribution of test statistics P. FlaSH instantiates
this entire method via a sequence of 3 steps:
S1: Process Data. We want to fit a model msuch that
points with irregularities appear in the most extreme tails of
them’s predictive distribution. However, training mon out
of range, global, and day of week outliers both distorts the
model and inflates the tails of the distribution of prediction
error so that more subtle deviations no longer stand out. We
process the data to identify and impute these outliers prior to
training. The key challenge in this step is to accommodate
the statistical properties of public health data.
S2: Obtain Predicted Values. After processing, we fit a
parametric model mfrom a model class Mthat uses the
history of the stream to predict future values. Choosing an
appropriate Mis nontrivial. Heavily parameterized models,
like many deep learning models, are unsuitable because of
the limited data history available to tune the model, the ex-
pensive ground truth labels, and the rapid distribution shifts
in the types of irregularities per stream. Further, stakeholders
prioritize interpretability, so the model class must be intuitive.
3Often, there is a ramp-up period before streams report reliable
measurements, so we do not start at t=0.
Jul 2021Jan 2022
Historical Data StreamsXrs:T-1 Ɐ r ∈ R
Real Time DataXrT Ɐ r ∈ R
FlaSH Specifications
FlaSH Testing
FlaSH Training
O3. Trendline Outlier
O2. Day of Week
FlaSH Output ScoresO1. Global Outlier
Step 0: Raw DataStep 1: Process DataStep 2: Obtain Predicted ValuesStep 3: Compare Prediction & Actual200 k COVID Cases0
01FlaSH Steps in Detail
Figure 2: In the FlaSH outlier detection method, data stream in-
puts are processed through FlaSH to generate informational outlier
scores. FlaSH itself has three steps. The raw data (gray) is pro-
cessed [S1] (purple), and model mis used to predict future values
[S2] (blue). Then, the historical performance of model mis cap-
tured with the test statistic distribution (gold), and this distribution
is used to compare predicted and actual values [S3].
S3: Compare Predicted and Observed Values. Finally,
FlaSH compares the observed and predicted values to test if
XTcould have been generated from mgiven the historical
performance of observed and predicted values. The critical
decision in this step is the choice of the test statistic and con-
struction of its distribution under the null hypothesis, which
are complicated by short training histories and the resulting
need to share information across geographic regions.
We now discuss each step, as displayed in Fig. 2.
3.1 Process Data
Trendline outliers cannot be reliably identified if the model
is trained on data that also includes out of range, global, and
day of week outliers. However the thresholds for determin-
ing these outliers change with distribution shifts in the stream.
To address this challenge, first, different COVID regimes, or
waves, are identified via changepoints. Then, within each
regime, existing outliers are detected and imputed.
3.1.1 Identifying Changepoints in Nonstationary
Streams
Values that would be outliers when there is no COVID wave
may not be outliers during a COVID wave. This phenomenon
of distinct, underlying waves, or regimes, in public health
streams is why they are known to be statistically nonstation-
ary[7]4. To identify these regimes in historical data, FlaSH
uses the Pelt Changepoint Algorithm [17, 31], parametrized
with a Gaussian model and a minimum of four weeks be-
4Operationally, we consider regimes present in streams that are
updated daily with at least 60 historical data points. On streams with
fewer than 60 data points, we provide interquartile range-defined
outliers.
3tween change points5. However, individual streams may be
very noisy, and Pelt sometimes overfits to this noise to re-
turn regimes inconsistent with expert knowledge of disease
dynamics. Therefore, we take advantage of geographical de-
pendencies6by searching for changepoints that are jointly ap-
plicable across a set of nearby regions. Specifically, we run
the Pelt algorithm on the streams for all counties within a
given state, jointly optimizing Pelt’s objective across these re-
gions to find changepoint days that describe the regimes well
across these streams.
While Pelt can identify changepoints in historical data, it
does not identify if real-time data represents a changepoint.
Instead of retraining FlaSH daily to find new changepoints,
which would be computationally expensive, FlaSH assumes
there is no changepoint until there is sufficient evidence of
nonstationarity to trigger retraining as follows. Under the
null hypothesis, there is no changepoint, and p-values are uni-
formly distributed by definition. If the distribution of the test
statistic for the hypothesis test H0significantly shifts, then
the Kolmogrov-Smirnov test can identify whether the em-
pirical p-value distribution since the last retraining deviates
significantly from the uniform distribution. The user can se-
lect the test significance level αaccording to their desired
trade-off between the computational expense of retraining
and increased accuracy. Even if a new changepoint is not
detected, FlaSH is retrained every 3 months, which roughly
corresponds to a change in season, and the expert reviewer
can retrain at any time.
3.1.2 Identifying Outliers Within Regimes
Within each changepoint-defined regime, FlaSH identifies
out of range, day of week, and global outliers, and it imputes
non-outlying values that are later used for modeling. First,
out of range outliers, like negative COVID Cases, are identi-
fied and imputed to be in range. Second, data is separated by
day of week, and points where |zscore| ≥3with respect to
the points for that day of the week in the regime are identified
as day of week outliers. The imputed value for downstream
analysis is the median value of the difference in day of the
week added to the value for the prior day. Day of week sensi-
tivity is important here because of systematic patterns across
the week, like that the median value for Sundays is usually
lower than the median for Tuesdays.
Third, we process the time series to remove systematic
day of week effects (unlike the previous step, which han-
dled points far outside the typical pattern for their week-
day). FlaSH uses a Poisson regression method w(part of Del-
phi’s public API7) which outputs a weekday-corrected value
w(Xt). This model removes systematic differences in mean
values across days (e.g. by scaling values on Saturday up and
scaling Mondays down) to obtain a time series without day of
5Four weeks is the maximum horizon for many short-term fore-
casts [8], likely because health dynamics change drastically after
that horizon.
6Data reporting and health policies are generally consistent at the
state level [30].
7https://github.com/cmu-delphi/covidcast-indicators/blob/main/
delphi utils python/delphi utils/weekday.pyweek effects. Removing such systematic periodicity enables
downstream predictive models to fit the data-generating pro-
cess using fewer parameters.
Finally, after the day of week correction, FlaSH addresses
thenoisiness of the stream by identifying global outliers in
the day of week corrected data as those with |zscore| ≥3,
calculated from all weekdays in the day of week corrected
data. These points are imputed using the mean value of the
current regime. Having removed out of range, global, and day
of week outliers, FlaSH treats the processed data across all
regimes as the null distribution and can now identify trendline
outliers as specified by the following two steps of FlaSH.
3.2 Obtain Predicted Values
To identify trendline outliers, FlaSH uses a small sample of
the processed historical data to train a predictive model m
forXTfrom model class Mand then uses the remaining
processed historical data to characterize the performance of
the model. Specifically, the training set for FlaSH’s null hy-
pothesis model is the maximum of 10% of the historical data
or 30 points. FlaSH then uses M: Linear Autoregressive
(AR) models (lag=7) , where mis characterized by the lin-
ear weights, ˆβ, fit during training. This class of models is
preferred in public health applications for its simplicity and
performance with limited historical data [23]. The remaining
processed historical data (not used to fit the model) is used to
generate predictions ˆXt.
3.3 Compare Predicted and Observed Values
Models from any model class Mfit with the null historical
data will not perform uniformly across all streams. Accord-
ingly, out-of-sample data is essential to quantify the typical
discrepancy between model predictions and observed values
per stream. Outliers can then be identified when the discrep-
ancy between predictions and observations is more than typi-
cal, as determined by a distribution of historical performance.
For example, if a model consistently predicts higher values
than what is observed, then the outlier score should reflect
the fact that ˆXT> X Tis not surprising.
To quantify the discrepancy between predicted and ob-
served values, let Nrdenote the total population of geo-
graphic region r. The day of week corrected observed values
(w(Xr
t), corrected to be comparable to the predicted values)
and the predicted values ( ˆXr
t=ˆβ∗w(Xr
t−1:t−7)) are used to
calculate the test statistic kt:
kt= (P(w(Xr
t)< D))
D∼Bin 
n=Nr, p=ˆXr
t
Nr!
This test models the counts in a region as a binomial dis-
tribution D. The probability of infection per person is the
number of predicted counts divided by the region’s popula-
tion size. Intuitively, we test the hypothesis that the actual
observed counts are drawn from a distribution parameterized
4by our predictions. Extreme values of the test statistic indi-
cate that the observations were much bigger or smaller than
expected given the predictions. Each stream model’s typi-
cal performance discrepancy is specified by a distribution Pr,
composed of test statistics kr
30:T−1, that compares observed
values and the predicted values for the out-of-sample histor-
ical data Xr
30:T−1. However, there is often too little history
to approximate the null distribution of an individual stream
effectively, with a minimum of 30 points characterizing each
distribution if there are only 60 days of historical data. Ac-
cordingly, we define the pooled test statistic distribution P,
specified byS
r∈Rkr
30:T−1, where Ris all the counties in
a state if ris a county, else Ris all states and territories
in a nation, because these streams share geographic context.
Note that pooling is enabled by the design of our test statis-
tic, which is chosen to ensure comparable distributions across
regions (e.g. via normalizing by the population).
3.4 FlaSH Output
The final output of FlaSH is a list of real-time points ranked
by how extreme their test statistic is via the transformation
|2p−1|, where pis the p-value for the real-time test statis-
tic in the pooled historical test statistic distribution P. This
transformation ensures that the most outlying points (from ei-
ther distribution tail) will top the ranked list.
4 FlaSH Labels, Evaluation, & Feedback
As noted in the literature, accurately evaluating algorithms
for unsupervised time series outlier detection is challenging
[35]. In most previous work, human-generated labels have
not been provided by experts (instead coming from readily
available subjects such as students or Mechanical Turk work-
ers). Non-expert labels are noisy since identifying outliers
often requires domain-specific knowledge. However, outlier
detection method performance on simulated data or data with
synthetically injected outliers [20] rarely translates to practi-
cal performance on real-world data in epidemiology generally
[34].
One of our key contributions is to address this limitation
in the outlier detection literature via a rigorous, real-world
evaluation of outlier detection methods for public health data.
We obtained high-quality labeled data from human subject
matter experts- Delphi members who are directly involved in
building statistical or software systems using public health
data and who regularly encounter the impact of data irreg-
ularities. In contrast to the binary labels standard in previ-
ous work, which may not be sufficient as different experts
have different thresholds for outlier determination, asking ex-
perts to rank outliers that warrant human inspection provides
a more informative comparison for FlaSH’s output.
For additional evaluation rigor, we preregistered the
FlaSH version, survey design, and analysis before data col-
lection began [15]. This ensures that our algorithm was final-
ized before any data collection occurred, giving an unbiased
(prospective) evaluation of FlaSH’s performance. Real-time
COVID Case data streams (3341 streams at county, state,
Feb 2022 Mar 2022 Apr 2022 May 2022010k20k30kHolidays Investigate UninterestingCOVID Cases: PACOVID Cases1 23Figure 3: Example of a Survey Task. Respondents click on the
time series plot to mark points as unevaluated, uninteresting, or war-
rants investigation. They also rank points that warrant investigation,
and these rankings appear on the plot in yellow. Respondents could
zoom, pan, and see a 7 day average per graph.
territory, and national levels available from May 2020-May
2022) were initially sourced daily from JHU CSSE. Of these,
five streams, including the national stream, were randomly
chosen from the following sets to ensure stream variety for
the evaluation: the top 10% of populous states (Pennsylva-
nia), bottom 90% of populous states (Arkansas), top 10%
populous FIPS regions (36081 Queens County, NY), and bot-
tom 90% populous FIPS regions (72043 Coamo Municipality,
PR), as per the US Census.
4.1 Survey and Analysis
To gather ground truth in order to understand FlaSH’s perfor-
mance on empirical data, we designed a web survey that has
10 interactive questions (Fig. 3)8. First, respondents classify
candidate data points from a public health stream as ‘warrants
human investigation’ or ‘uninteresting’. Then, they are asked
to rank (with possible ties) the subset of these candidates they
think would warrant additional human inspection.
To form this candidate set of evaluation points, we needed
to select stream values that are at least somewhat anomalous.
That way, survey respondents could meaningfully distinguish
between points that are potentially anomalous. In practice,
we expect <1% of all points in a stream to represent irreg-
ularities. Accordingly, this candidate set is formed by taking
the union of the top outlying points output by both FlaSH and
8 previously proposed outlier detection methods given all his-
torical data (see Sec. 5). By filtering to data points at the top
of at least one algorithm, the candidate set is limited to points
that are considered anomalous by some method. This empir-
ically meant the candidate set comprised of points that were
at least interesting enough to classify and rank.
In Questions 1-5 (Q1-5), the candidate set was formed
from the top 5% of points from at least one algorithm for each
of the 5 possible data streams. In Q6-10, respondents were
asked to reconsider the top 2% of points from at least one
algorithm, a subset of these candidates from Q1-5, in more
detail to test for respondent internal consistency. They were
also asked how likely they would have flagged each point for
human review had it not been identified by an algorithm (‘un-
likely’, ‘somewhat unlikely’, ‘neither’, ‘somewhat likely’, or
8https://github.com/Ananya-Joshi/IJCAI23 Supplemental
5‘likely’). This allows us to measure the value added by the
algorithm over what would have been obvious to a human.
We evaluate the algorithm’s performances in a realistic
setting of only 60 days of history for training (12/21/2021-
1/31/2022). Our test set was the following 100 days (2/1-
5/12/2022). To compare the survey results to the outlier de-
tection method outputs, we use a range of metrics to capture
the complexity of real-world outlier detection. Both tradi-
tional binary classification and ranking metrics provide in-
formation on how well the system finds points that the ma-
jority of respondents thinks warrants human inspection. We
also seek to understand which points from outlier detection
algorithms provided the most benefit to users based on self-
reports. The points which were rated as both highly anoma-
lous and unlikely to have been flagged without an algorithm
are the most valuable potential contributions of computation-
ally assisted quality control.
Survey Quality. The total number of survey participants
(n=13) is a significant increase over previous work (e.g. n=3
in [34]). We tested for internal consistency in respondents
that answered both sets of survey questions (Q1-5 and Q6-
10) by measuring response centrality between the Copeland
aggregate per paired question (e.g. Q1 & Q6, Q2 & Q7)
and the raw ranks per person. High centrality values (0.83
±0.13) suggest that respondents generally were consistent
in their pairwise preferences between the two sets. Still, the
average number of points that warranted inspection per per-
son varied from <2to>6, supporting that the threshold
for identifying points of interest varies greatly by individual
and reinforcing the importance of sampling a wider range of
experts than historical standards suggest.
5 Results and Analysis
We compare the trendline outlier scores from FlaSH to outlier
scores from the following off-the-shelf outlier detection algo-
rithm baselines implemented in TODS9that span recent deep
learning methods, classical machine learning, and statistical
approaches: DeepLog [10], Telemanom (Telem.) [13], Vari-
ational Autoencoder (V AE) [2], Local Outlier Factor (LOF)
[5], Lightweight Online Detector of Anomalies (LODA) [26],
Isolation Forest (IF) [22], k-Nearest Neighbors (KNN) [3],
and Linear AR Model [12]. These methods have in-built data
processing [S1] and prediction comparison [S3] steps, just
like FlaSH. We use default hyperparameters for the TODS
implementations because there is not enough recent data to
select hyperparameters. In fact, many of these models are too
costly to even train once on the full set of streams, much less
to do hyperparameter selection with many repeated runs. One
strength of FlaSH is that it has no hyperparameters because it
is designed for this task.
Additionally, for an ablation study, we compare results
from the TODS AR model implementation, which has the
same model class Mas FlaSH, to a mixed implementation
9Each algorithm had a setting 7 day windows where applicable
to account for day of week effects .(Mixed), where the processing step [S1] is the same as FlaSH,
and the prediction comparison step [S3] is from TODS.
FlaSH is computationally scalable. We find that FlaSH
easily scales to a large number of data streams, while many
deep learning methods become infeasible. Performance
statistics (Table 5) were reported from experiments using a
2.6 GHz 6-Core Intel Core i7 machine. Each algorithm was
trained on the full 3341 JHU CSSE COVID-19 case streams
with 60 days of history. This setup mimics the setting that
we expect algorithms to scale to in deployment. A few algo-
rithms (mainly deep learning algorithms) did not finish train-
ing within one day (DNF). Training time can only increase
for these deep learning implementations as historical data
increases. While GPU acceleration may benefit deep learn-
ing models, such specialty hardware may not be available in
many public health settings.
FlaSH performs well on outlier detection metrics. Al-
though many of the existing outlier detection methods have
infeasibly long training times for daily deployment, we com-
pare the performance of all algorithms using the labeled data
from the survey. Table 5 shows the 95% CI of various tradi-
tional binary and ranking outlier detection metrics across all
participants for Q1-5 per algorithm.
In the binary analysis, points identified by the majority of
respondents as to-investigate were marked as outliers (ground
truth)10. To calculate binary labels from each algorithm to
compare to this ground truth, we used the following process.
Letkdenote the number of human-identified outliers for a
stream. For each algorithm, we took the top kpoints, ranked
according to the algorithm’s outlier scores, as the predicted
outliers for binary classification tasks and compared these re-
sults to the ground truth labels. We report the 95% CI metrics
per person and per question for accuracy, balanced accuracy
score, F1 score, and the ROC-AUC score. On average, FlaSH
meets or exceeds the performance of all baselines in the bi-
nary analysis. FlaSH performs slightly better than DeepLog,
an unusable, but performant, deep learning method. Some
model classes like Telemanom and LODA performed poorly
on the ROC-AUC score because while they identified global
outliers very clearly, they failed to capture other kinds of out-
liers (e.g. trendline or day of week outliers). For the ranking
analysis, each algorithm’s ranking of the subset points avail-
able in Q1-5 that a majority of participants marked as war-
rants suspicion was compared to each respondent’s rankings
using Hamming distance (lower is better), Ranked-Biased
Overlap (RBO) [33], and swap correlation (corr). Once again,
FlaSH performs comparably to DeepLog and is competitive
with the other algorithms.
Finally, FlaSH shows strong improvements over the
TODS AR implementation. While the TODS AR method is
uncompetitive with other approaches, by using data processed
using FlaSH’s first step (Mixed), the AR model can better
build a null model of the data. Still, because the TODS out-
lier scoring uses the absolute difference between the predicted
10The base rates were: US (2/14), Pennsylvania (9/14), Arkansas
(3/16), FIPS 36081 (6/24) and FIPS 72043 (5/21).
6Model Class AR DeepLog Telem. VAE LOF LODA IF KNN
Implementation TODS Mixed † FlaSH TODS
Training (s) 10.1±0.3 169±0.8 DNF DNF DNF 8±0.271±0.1 DNF 7±0.08✓BinaryAccuracy 0.78±0.020.71±0.040.8±0.03✓ 0.8±0.04✓0.6±0.040.76±0.040.69±0.010.68±0.040.79±0.040.74±0.03
Bal.Acc. 0.68±0.020.59±0.060.73±0.05✓0.72±0.050.42±0.030.67±0.070.55±0.030.54±0.050.7±0.070.62±0.05
F1 0.54±0.050.43±0.090.64±0.08✓0.63±0.070.19±0.070.53±0.120.33±0.080.34±0.090.56±0.110.42±0.09
ROCAUC 0.79±0.020.73±0.060.75±0.06 0.82±0.05✓0.42±0.070.68±0.060.62±0.040.44±0.070.66±0.080.65±0.07RankingDistance 0.66±0.391±0 0.62±0.39✓0.63±0.360.83±0.240.66±0.370.66±0.390.71±0.390.67±0.390.66±0.39
RBO 0.84±0.10.89±0.080.84±0.1 0.84±0.10.84±0.10.89±0.070.88±0.080.93±0.06✓0.91±0.110.88±0.08
Corr. 0.2±0.630.42±0.450.37±0.57 0.43±0.54✓−0.13±0.710.18±0.640.21±0.670.24±0.690.17±0.680.22±0.66
Assistive Rank*8.00±63.66±11.33±0.7✓2.33±0.741.33±3832.00±5724.00±4070.67±5147.33±395.33±5
*Mean rank of points somewhat unlikely or unlikely to be caught by human
†Mixed model with FlaSH data processing [S1] and TODS comparison of predicted and observed values [S3].
Table 5: Summary of Algorithm Comparison with 60 Days Historical Data. ✓marks the best algorithm in each row.
and observed values to rank points, the mixed approach per-
forms poorly on streams with small case counts, as reflected
in the results. Compared to the TODS implementation with
the same model class M, FlaSH’s processing [S1] and com-
parison [S3] steps together provide clear performance bene-
fits.
FlaSH can complement human judgment. We find that
FlaSH identifies useful points that were unlikely to have been
inspected without computational assistance (via an algorithm
identifying the point), as shown in the Assistive Rank section
of Table 5. Specifically, we examine the set of points that (a)
the majority of humans rated as warranting investigation after
a full examination, and (b) at least 40% of such respondents
said that they were “unlikely” or “somewhat unlikely” to have
identified the point without algorithmic assistance. We re-
port the mean rank assigned to such points, where a smaller
rank indicates that the algorithm would prioritize those points
more for human inspection. We find that FlaSH consistently
ranks these points near the top of its list (more so than other
methods), indicating that FlaSH can usefully direct human at-
tention to points that would have been missed otherwise. This
is a result of FlaSH’s emphasis on discovering trendline out-
liers, which our prototyping showed are difficult for humans
to recognize in public health data streams.
Overall, FlaSH’s strength lies in leveraging specific fea-
tures of public health data, a simple model class to meet de-
ployment criteria, and an intuitive test statistic. The combina-
tion of these ideas is why FlaSH can scale to the data volumes
required, perform well on traditional outlier detection met-
rics, especially compared to the best-performing deep learn-
ing models, and crucially, prioritize points for human review
that would not have been discovered otherwise.
6 Deployment and Lessons Learned
Based on FlaSH’s empirical performance and design, it has
been deployed as part of Delphi’s daily workflow since Febru-ary 2023. It runs on selected streams, and an expert reviewer
inspects the ranked, outlying points. To support this interac-
tion, we added a dashboard where expert reviewers can visu-
alize each of FlaSH’s calculations before flagging them. As
new types of irregularities arise, an analyst in the loop can
modify FlaSH to detect those respective outliers.
Lessons Learned. For outlier detection methods that pro-
duce actionable outputs, intuitive methods with informative
outputs that explicitly navigate contextual nuances (like how
FlaSH directly leverages the statistical properties of pub-
lic health streams) innately enhance trust in method outputs
that may also translate to performance gains. Additionally,
method evaluations should consider expert-generated ground
truth tasks that cover classification, because classification can
be more straightforward for humans, and ranking, because
thresholds for classification may vary.
7 Related Works
There are numerous outlier and anomaly detection methods
[4], but recent advancements in the field focus on deep learn-
ing applications [25]. In our experiments, we find deep learn-
ing methods perform poorly on this task for various reasons.
Accordingly, only a handful of real-time outlier streaming al-
gorithms have been adapted for public health streaming data.
Specifically, point outlier detection approaches for COVID-
19 streams like [14, 16, 32, 1] consider the nonstationar-
ityof the data streams but use simulations for evaluation or
only consider a limited set of outlier categories. Hence, they
are not fully applicable to our setting. Some source-specific
COVID outlier detection methods [9] that operate on data
streams before Delphi receives them do not have publicly-
available methods, but the continued presence of irregulari-
ties in those streams that impacts Delphi stakeholders under-
scores the importance of FlaSH.
78 Conclusion
This paper presents FlaSH, a practical framework for com-
putationally assisted quality control in public health data
streams. FlaSH creates a list of the most important outly-
ing recent data points for domain experts to review by us-
ing simple models to explicitly account for the nuances of
public health streaming data. In our experimental evalua-
tion, which addressed some open design and evaluation chal-
lenges in unsupervised time series outlier detection, FlaSH
scaled to the task requirements, outperformed other meth-
ods (including deep learning approaches) in traditional out-
lier detection metrics, and successfully prioritized points that
would not have been discovered without algorithmic assis-
tance. Our results demonstrate that effective, practical outlier
detection systems require careful, user-informed design and
sustained effort. These efforts will have considerable benefits
for Delphi’s stakeholders and, ultimately, for public health
data users.
Ethical Statement
This research was conducted in accordance with the prin-
ciples embodied in the Declaration of Helsinki and in ac-
cordance with local statutory requirements. All participants
consented to the study and could exit the study at any time.
Approval was granted from Carnegie Mellon Univeristy IRB
number STUDY2022 00000240.
Acknowledgements
We want to thank the whole Delphi team and members of the
Carnegie Mellon community for their input to and support
of this project. This work was supported by the Centers for
Disease Control and Prevention of the U.S. Department of
Health and Human Services (HHS) as part of a cooperative
agreement funded solely by CDC/HHS under federal award
identification number U01IP001121, “Delphi Influenza Fore-
casting Center of Excellence”. The contents are those of the
authors and do not necessarily represent the official views
of, nor an endorsement by, CDC/HHS or the U.S. Govern-
ment. This material is based upon work supported by the
National Science Foundation Graduate Research Fellowship
under Grant No. DGE1745016 and DGE2140739. Any opin-
ion, findings, and conclusions or recommendations expressed
in this material are those of the authors and do not necessarily
reflect the views of the National Science Foundation.
References
[1] A GARWAL , P., A LURU , P., AND PRAKASH , B. A.
Real-time anomaly detection in epidemic data streams.
InEpidemiology meets Data Mining and Knowledge
discovery (2022).
[2] A N, J., AND CHO, S. Variational autoencoder
based anomaly detection using reconstruction probabil-
ity.Special lecture on IE 2 , 1 (2015), 1–18.[3] A NGIULLI , F., AND PIZZUTI , C. Fast outlier detec-
tion in high dimensional spaces. In Principles of Data
Mining and Knowledge Discovery: 6th European Con-
ference (2002), Springer, pp. 15–27.
[4] B L´AZQUEZ -GARC´IA, A., C ONDE , A., M ORI, U.,
AND LOZANO , J. A. A review on outlier/anomaly de-
tection in time series data. ACM Computing Surveys
(CSUR) 54 , 3 (2021), 1–33.
[5] B REUNIG , M. M., K RIEGEL , H.-P., N G, R. T., AND
SANDER , J. Lof: identifying density-based local out-
liers. In Proceedings of the 2000 ACM SIGMOD in-
ternational conference on Management of data (2000),
pp. 93–104.
[6] CDC, U. Covid-19 by county. https:
//www.cdc.gov/coronavirus/2019-ncov/your-health/
covid-by-county.html, publisher=Centers for Disease
Control and Prevention, Aug 2022.
[7] C HIMMULA , V., K UMAR , R., AND ZHANG , L. Time
series forecasting of covid-19 transmission in canada
using lstm networks. Chaos, Solitons & Fractals 135
(2020), 109864.
[8] C RAMER , E. Y., H UANG , Y., W ANG , Y., R AY, E. L.,
CORNELL , M., B RACHER , J., B RENNEN , A., R I-
VADENEIRA , A. J. C., G ERDING , A., H OUSE , K.,
ET AL . The united states covid-19 forecast hub dataset.
Scientific data 9 , 1 (2022), 462.
[9] D ONG , E., R ATCLIFF , J., G OYEA , T. D., K ATZ, A.,
LAU, R., N G, T. K., G ARCIA , B., B OLT, E., P RATA ,
S., Z HANG , D., ET AL . The johns hopkins univer-
sity center for systems science and engineering covid-
19 dashboard: data collection process, challenges faced,
and lessons learned. The Lancet Infectious Diseases
(2022).
[10] D U, M., L I, F., Z HENG , G., AND SRIKUMAR , V.
Deeplog: Anomaly detection and diagnosis from sys-
tem logs through deep learning. In Proceedings of the
2017 ACM SIGSAC conference on computer and com-
munications security (2017), pp. 1285–1298.
[11] F ARROW , D. C., B ROOKS , L. C., R UMACK , A., T IB-
SHIRANI , R. J., AND ROSENFELD , R. Delphi epi-
data api. The Lancet Infectious Diseases. https://github.
com/cmu-delphi/delphi-epidata (2015).
[12] G UPTA , M., G AO, J., A GGARWAL , C. C., AND HAN,
J. Outlier detection for temporal data: A survey. IEEE
Transactions on Knowledge and data Engineering 26 , 9
(2013), 2250–2267.
[13] H UNDMAN , K., C ONSTANTINOU , V., L APORTE , C.,
COLWELL , I., AND SODERSTROM , T. Detecting space-
craft anomalies using lstms and nonparametric dynamic
thresholding. arXiv preprint arXiv:1802.04431 (2018).
8[14] J OMBART , T., G HOZZI , S., S CHUMACHER , D., T AY-
LOR, T. J., L ECLERC , Q. J., J IT, M., F LASCHE ,
S., G REAVES , F., W ARD, T., E GGO , R. M., ET AL .
Real-time monitoring of covid-19 dynamics using au-
tomated trend fitting and anomaly detection. Philo-
sophical Transactions of the Royal Society B 376 , 1829
(2021), 20200266.
[15] J OSHI , A., M AZAITIS , K., R OSENFELD , R., AND
WILDER , B. Osf pre-data collection registration: To-
wards detecting points of interest from public health
data streams. https://osf.io/2v8f5, 2023.
[16] K ARADAYI , Y., A YDIN , M. N., AND ¨OˇGRENC ´I, A. S.
Unsupervised anomaly detection in multivariate spatio-
temporal data using deep learning: early detection of
covid-19 outbreak in italy. Ieee Access 8 (2020),
164155–164177.
[17] K ILLICK , R., F EARNHEAD , P., AND ECKLEY , I. A.
Optimal detection of changepoints with a linear compu-
tational cost. Journal of the American Statistical Asso-
ciation 107 , 500 (2012), 1590–1598.
[18] K RAEMER , M. U., S CARPINO , S. V., M ARIVATE , V.,
GUTIERREZ , B., X U, B., L EE, G., H AWKINS , J. B.,
RIVERS , C., P IGOTT , D. M., K ATZ, R., ET AL . Data
curation during a pandemic and lessons learned from
covid-19. Nature Computational Science 1 , 1 (2021),
9–10.
[19] K REPS , S. E., AND KRINER , D. L. Model uncertainty,
political contestation, and public trust in science: Evi-
dence from the covid-19 pandemic. Science advances
6, 43 (2020), eabd4563.
[20] L AI, K.-H., Z HA, D., X U, J., Z HAO, Y., W ANG , G.,
AND HU, X. Revisiting time series outlier detection:
Definitions and benchmarks. In Thirty-fifth conference
on neural information processing systems datasets and
benchmarks track (2021).
[21] L ESLIE , D., M AZUMDER , A., P EPPIN , A., W OLTERS ,
M. K., AND HAGERTY , A. Does “ai” stand for aug-
menting inequality in the era of covid-19 healthcare?
bmj 372 (2021).
[22] L IU, F. T., T ING, K. M., AND ZHOU , Z.-H. Isolation
forest. In 2008 eighth ieee international conference on
data mining (2008), IEEE, pp. 413–422.
[23] M CDONALD , D. J., B IEN, J., G REEN , A., H U, A. J.,
DEFRIES , N., H YUN , S., O LIVEIRA , N. L., S HARP -
NACK , J., T ANG , J., T IBSHIRANI , R., ET AL . Can
auxiliary indicators improve covid-19 forecasting and
hotspot prediction? Proceedings of the National
Academy of Sciences 118 , 51 (2021), e2111453118.
[24] P ALEYES , A., U RMA , R.-G., AND LAWRENCE , N. D.
Challenges in deploying machine learning: a survey of
case studies. ACM Computing Surveys 55 , 6 (2022), 1–
29.[25] P ANG , G., S HEN, C., C AO, L., AND HENGEL , A.
V. D. Deep learning for anomaly detection: A review.
ACM computing surveys (CSUR) 54 , 2 (2021), 1–38.
[26] P EVN `Y, T. Loda: Lightweight on-line detector of
anomalies. Machine Learning 102 (2016), 275–304.
[27] R EINHART , A., B ROOKS , L., J AHJA , M., R UMACK ,
A., T ANG , J., A GRAWAL , S., A LSAEED , W.,
ARNOLD , T., B ASU, A., B IEN, J., ET AL . An open
repository of real-time covid-19 indicators. Proceedings
of the National Academy of Sciences 118 , 51 (2021),
e2111452118.
[28] S ´AEZ, C., R OMERO , N., C ONEJERO , J. A., AND
GARC´IA-G´OMEZ , J. M. Potential limitations in covid-
19 machine learning due to data source variability: A
case study in the ncov2019 dataset. Journal of the Amer-
ican Medical Informatics Association 28 , 2 (2021),
360–364.
[29] S EJR, J. H., AND SCHNEIDER -KAMP, A. Explainable
outlier detection: What, for whom and why? Machine
Learning with Applications 6 (2021), 100172.
[30] S IMON , S. Inconsistent reporting practices hampered
our ability to analyze covid-19 data. here are three com-
mon problems we identified. The COVID Tracking
Project (Apr 2021).
[31] T RUONG , C., O UDRE , L., AND VAYATIS , N. Selective
review of offline change point detection methods. Signal
Processing 167 (2020), 107299.
[32] W ANG , G., G U, Z., L I, X., Y U, S., K IM, M., W ANG ,
Y., G AO, L., AND WANG , L. Comparing and in-
tegrating us covid-19 data from multiple sources with
anomaly detection and repairing. Journal of Applied
Statistics (2021), 1–27.
[33] W EBBER , W., M OFFAT , A., AND ZOBEL , J. A similar-
ity measure for indefinite rankings. ACM Transactions
on Information Systems (TOIS) 28 , 4 (2010), 1–38.
[34] W ONG , W.-K. Data mining for early disease outbreak
detection . Carnegie Mellon University, 2004.
[35] W U, R., AND KEOGH , E. Current time series anomaly
detection benchmarks are flawed and are creating the
illusion of progress. IEEE Transactions on Knowledge
and Data Engineering (2021).
[36] Y U, S., Q ING, Q., Z HANG , C., S HEHZAD , A., O AT-
LEY, G., AND XIA, F. Data-driven decision-making
in covid-19 response: A survey. IEEE Transactions on
Computational Social Systems 8 , 4 (2021), 1016–1029.
[37] Z HU, D., Y E, X., AND MANSON , S. Revealing the
spatial shifting pattern of covid-19 pandemic in the
united states. Scientific reports 11 , 1 (2021), 1–9.
9