I D  TRANSFORMER ARCHITECTURES WITH INPUT-DEPENDENT DYNAMIC DEPTH
FOR SPEECH RECOGNITION
Yifan Peng1, Jaesong Lee2, Shinji Watanabe1 1Carnegie Mellon University NA VER Corporation
ABSTRACT
Transformer-based end-to-end speech recognition has achieved great
success. However, the large footprint and computational overhead
make it difÔ¨Åcult to deploy these models in some real-world applications.
 Model compression techniques can reduce the model size and
speed up inference, but the compressed model has a Ô¨Åxed architecture
 which might be suboptimal. We propose a novel Transformer
encoder with Input- Dependent Dynamic Depth (I D) to achieve
strong performance-efÔ¨Åciency trade-offs. With a similar number of
layers at inference time, I D-based models outperform the vanilla
Transformer and the static pruned model via iterative layer pruning.
We also present interesting analysis on the gate probabilities and the
input-dependency, which helps us better understand deep encoders.
Index Terms  Dynamic depth, transformer, speech recognition

1. INTRODUCTION
Recently, end-to-end automatic speech recognition (ASR) has
gained popularity. Typical frameworks include Connectionist Temporal
 ClassiÔ¨Åcation (CTC) [1], Attention-based Encoder-Decoder
(AED) [2 4], and Recurrent Neural Network Transducer (RNNT)
 [5]. Many types of networks can be used as encoders in these
frameworks, such as Convolutional Neural Networks (CNNs),
RNNs, Transformers [6] and their combinations [7 9]. Transformers
 have achieved great success in various benchmarks [10].
However, they usually contain many cascaded blocks and thus have
high computation, which hinders deployment in some real-world applications
 with limited resource. To reduce computation and speed
up inference, researchers have investigated different approaches.
A popular method is to compress a large pre-trained model using
distillation [11 13], pruning [14 16], and quantization [15]. However,
 the compressed model has a Ô¨Åxed architecture for all types of
inputs, which might be suboptimal. For example, this Ô¨Åxed model
may be too expensive for very easy utterances but insufÔ¨Åcient for difÔ¨Åcult
 ones. To better trade off performance and computation, prior
studies have explored dynamic models [17], which can adapt their
architectures to different inputs. Dynamic models have shown to
be effective in computer vision [18 23], which are mainly based on
CNNs. For speech processing, [24] trains two RNN encoders of different
 sizes and dynamically switches between them guided by keyword
 spotting. [25] proposes a dynamic encoder transducer based on
layer dropout and collaborative learning. [26] adopts two RNN encoders
 to tackle close-talk and far-talk speech. [27] also designs two
RNN encoders that are compressed to different degrees and switches
between them on a frame-by-frame basis. [28] extends this idea to
Transformer-transducers and considers more Ô¨Çexible subnetworks,
but it continues to focus on streaming ASR and the architecture isstill determined on a frame-by-frame basis, which requires a special
 design for the Ô¨Åned-grained key and query operations in selfattention.
 For nonstreaming (or chunk-based streaming) ASR, the
frame-level prediction may be expensive and suboptimal, as it only
captures frame-level local features.
We propose a Transformer encoder with Input- Dependent
Dynamic Depth (I D) for end-to-end ASR. Instead of using carefully
designed Ô¨Åne-grained operations within submodules like attention,
I D predicts whether to skip an entire self-attention block or an
entire feed-forward network through a series of local gate predictors
or a single global gate predictor. The prediction is made at the utterance
 level (or chunk level if extended to streaming cases), which
is easier to implement and reduces additional cost. It also captures
global statistics of the input. As analyzed in Sec. 3.4, the length of
an utterance affects the inference architecture. Some blocks may be
useful for longer inputs. Results show that I D models consistently
outperform Transformers trained from scratch and the static pruned
models via iterative layer pruning [29], when using a similar number
 of layers for inference. We also perform interesting analysis on
predicted gate probabilities and input-dependency, which helps us
better understand the behavior of deep encoders.
2. METHOD
2.1. Transformer encoder
A Transformer [6] encoder layer contains a multi-head self-attention
(MHA) module and a feed-forward network (FFN), which are combined
 sequentially. The function of the l-th layer is as follows 
Y(l) X(l 1) MHA(l)(X(l 1)), (1)
X(l) Y(l) FFN(l)(Y(l)), (2)
where X(l)is the output of the l-th Transformer layer and X(l 1)is
thus the input to the l-th layer. Y(l)is the output of the MHA at the
l-th layer, which is also the input of the FFN at the l-th layer. These
sequences all have a length of Tand a feature size of d.
2.2. Overall architecture of I D encoders
Fig. 1a shows the overall architecture of I D encoders. A waveform
is Ô¨Årst converted to a feature sequence by a frontend, and further
processed and downsampled by a CNN, after which positional embeddings
 are added. Later, the sequence is processed by a stack of N
I D encoder layers to produce high-level features. This overall design
 follows that of Transformer. However, Transformer always uses
a Ô¨Åxed architecture regardless of the input. Our I D selects different
combinations of MHA and FFN depending on the input utterance.
To determine whether a module should be executed or skipped, a binary
 gate is introduced for each MHA or FFN module. The functionarXiv 2303.07624 1  [cs.CL]  14 2023FrontendCNNPositionalEmbeddingAudio WaveformI D Encoder Layer ùëÅEncoder Output SequenceEncoder Input Sequence(a) Overall encoder architecture.
Multi-HeadSelf-AttentionFeed-ForwardNetwork MeanLocal Gate PredictorI D Encoder Layer (b) I D encoder layer with a local gate predictor.
Multi-HeadSelf-AttentionFeed-ForwardNetwork
 MeanGlobal Gate PredictorLayer 1Layer ùëÅ ùëÅ 
Encoder Input SequenceEncoder Output Sequence (c) I D encoder with a global gate predictor.
Fig. 1   Architectures of our proposed I D encoders.
of thel-th layer (see Eqs. (1) and (2) for the vanilla Transformer)
now becomes 
Y(l) X(l 1) g(l)
MHA MHA(l)(X(l 1)), (3)
X(l) Y(l) g(l)
FFN FFN(l)(Y(l)), (4)
whereg(l)
MHA,g(l)
FFN {0,1}are input-dependent gates. If a gate is
predicted to be 0, then the corresponding module will be skipped,
which effectively reduces computation. The total training loss is 
Ltotal LASR Œª Lutility, (5)
Lutility 1 2 1(
g(l)
MHA g(l)
FFN)
, (6)
whereLASRis the standard ASR loss and Lutility is a regularization
loss measuring the utility rate of all MHA and FFN modules. Œª 0
is a hyper-parameter to trade off the recognition accuracy and computational
 cost.1Note that the utility loss in Eq. (6) is deÔ¨Åned for an
individual utterance so the utterance index is omitted. In practice, a
mini-batch is used and the loss is averaged over utterances.
A major issue with this training objective is that binary gates
are not differentiable. To solve this problem, we apply the GumbelSoftmax
 [30, 31] trick, which allows drawing hard (or soft) samples
from a discrete distribution. Consider a discrete random variable Z
with probabilities P(Z k) Œ±kfor anyk  1,...,K . To draw
a sample from this distribution, we can Ô¨Årst draw Ki.i.d. samples
{gk}K
k 1from the standard Gumbel distribution and then select the
index with the largest perturbed log probability 
z  arg max
k {1,...,K}logŒ±k gk. (7)
The argmax is not differentiable, which can be relaxed to the softmax.
 It is known that any sample from a discrete distribution can be
denoted as a one-hot vector, where the index of the only non-zero
entry is the desired sample. With this vector-based notation, we can
draw a soft sample as follows 
z softmax ((logŒ± g)/œÑ), (8)
whereŒ±  (Œ±1,...,Œ± K),g  (g1,...,g K), andœÑis a temperature
 constant. Eq. (8) is an approximation of the original sampling
1Our method can be extended to consider different costs of MHA and
FFN. In Eq. (6), we can use a weighted average of the two types of gates,
where the weights depend on their computational costs. Then, the training
will minimize the overall computation instead of simply the number of layers.process, but it is differentiable w.r.t. Œ±and thus suitable for gradientbased
 optimization. As œÑ 0, the approximation becomes closer
to the discrete version. We use œÑ  1in our experiments.
For thel-th MHA, a discrete probability distribution p(l)
MHA R2
over two possible gate values (0 1) is predicted, where 0 1 means executing it. Then, a soft sample
is drawn from this discrete distribution using Eq. (8), which is used
as the gate in Eq. (3) during training. Similarly, for the l-th FFN, a
distribution p(l)
FFN R is predicted, from which a soft gate is drawn
and used in Eq. (4). The gate distributions are generated by a gate
predictor based on the input features, as deÔ¨Åned in Sec. 2.3.
2.3. Local and global gate predictors
We propose two types of gate predictors, namely the local gate predictor
 andglobal gate predictor . We employ a multi-layer perceptron
 (MLP) with a single hidden layer of size 32 in all experiments,
which has little computational overhead.
Thelocal gate predictor (LocalGP or LGP) is associated with a
speciÔ¨Åc I D encoder layer, as illustrated in Fig. 1b. Every layer has
its own gate predictor whose parameters are independent. Consider
thel-th encoder layer with an input sequence X(l 1) RT d. This
sequence is Ô¨Årst converted to a d-dimensional vector x(l 1) Rd
through average pooling along the time dimension. Then, this pooled
vector is transformed to two 2-dimensional probability vectors for
the MHA gate and the FFN gate, respectively 
p(l)
MHA,p(l)
FFN LGP(l)(x(l 1)), (9)
where p(l)
MHA,p(l)
FFN R are introduced in Sec. 2.2, and LGP(l)is the
local gate predictor at the l-th layer. With this formulation, the decision
 of executing or skipping any MHA or FFN module depends on
the input to the current layer, which further depends on the decision
made at the previous layer. Hence, the decisions are made sequentially
 from lower to upper layers. During inference, a Ô¨Åxed threshold
Œ≤ [0,1]is utilized to produce a binary gate for every module 
g(l)
MHA  1 if(p(l)
MHA)1 Œ≤ else0, (10)
g(l)
FFN  1 if(p(l)
FFN)1 Œ≤ else0, (11)
where (p(l)
MHA)1is the probability of executing the MHA and (p(l)
FFN)1
is the probability of executing the FFN. We use Œ≤  0.5by default,
but it is also possible to adjust the inference cost by changing Œ≤.18 20 22 24 26 28 30 32 34 3611.512.012.513.013.5
Average number of layers% WER (  )Transformer LayerDrop I D-LocalGP
I D-GlobalGP I D-GlobalGP ( Œ≤varies)
(a) LibriSpeech test clean
18 20 22 24 26 28 30 32 34 3626.027.028.029.030.0
Average number of layers% WER (  )
(b) LibriSpeech test other
Fig. 2   Word error rates (%) of CTC -based models vs. average number
 of layers used for inference on LibriSpeech test sets.Œ≤is the
threshold for generating binary gates as deÔ¨Åned in Eqs. (10) (11).
18 20 22 24 26 28 30 32 34 3611.411.611.812.012.212.4
Average number of layers% WER (  )Transformer
LayerDrop
I D-GlobalGP
(a) LibriSpeech test clean
18 20 22 24 26 28 30 32 34 3626.026.527.027.5
Average number of layers% WER (  ) Transformer
LayerDrop
I D-GlobalGP
(b) LibriSpeech test other
Fig. 3   Word error rates (%) of InterCTC -based models vs. average
number of layers used for inference on LibriSpeech test sets.
The global gate predictor (GlobalGP or GGP), on the other
hand, is deÔ¨Åned for an entire I D encoder, as shown in Fig. 1c. It predicts
 the gate distributions for all layers based on the encoder s input,
which is also the input to the Ô¨Årst layer  X X(0) RT d. In particular,
 the sequence is transformed to a single vector x x(0) Rd
by average pooling. Then, it is mapped to two sets of probability
distributions for all NMHA and FFN gates, respectively 
{p(l)
MHA}N
l 1,{p(l)
FFN}N
l 1 GGP(x), (12)
where p(l)
MHA,p(l)
FFN R are the gate probability distributions at the
l-th layer, and the I D encoder has Nlayers in total. Here, the decisions
 of executing or skipping modules are made immediately after
seeing the encoder s input, which has lower computational overhead
than LocalGP and allows for more Ô¨Çexible control over the inference
architecture. During inference, we can still use a Ô¨Åxed threshold
Œ≤ [0,1]to generate binary gates as in Eqs. (10) and (11).
3. EXPERIMENTS
3.1. Experimental setup
We use PyTorch [32] and follow the ASR recipes in ESPnet [33] to
train all models. We mainly use the CTC framework on LibriSpeech18 20 22 24 26 28 30 32 34 3610111213
Average number of layers% WER (  )Transformer I D-LocalGP I D-GlobalGP
Fig. 4   Word error rates (%) of CTC -based models vs. average number
 of layers used for inference on the Tedlium2 test set.
Table 1   Word error rates (%) and average number of inference layers
 of AED -based models on LibriSpeech 100h .
Modeldev clean test clean
Ave #layers WER (  ) Ave #layers WER (  )
Transformer36 7.8 36 8.0 27 8.2 27 8.5
I D-LGP-36 27.3 7.9 27.1 8.3
I D-GGP-36 27.2 7.8 27.1 8.2 100h [34]. In Sec. 3.5, we also show that I D can be applied to
AED and another corpus, Tedlium2 [35]. Our I D encoders have
36 layers in total. They are initialized with trained standard Transformers
 and Ô¨Åne-tuned with a reduced learning rate ( 1 3) and
variousŒª(usually ranging from 1 13) in Eq. (5) to trade off WER
and computation. The Ô¨Åne-tuning epochs for LibriSpeech 100 2 50 35, respectively. We compare I D with two
baselines. First, we train standard Transformers with a reduced number
 of layers. Second, we train a 36-layer Transformer with LayerDrop
 [36, 37] or Intermediate CTC (InterCTC) [38] and perform
iterative layer pruning [29] using the validation set to get a variety
of models with smaller and Ô¨Åxed architectures. This baseline is denoted
 as  LayerDrop  in Figs. 2 3. We can compare I D, whose
layers are dynamically reduced based on the input, against the static
pruned models.
3.2. Main results
Fig. 2 compares our I D models with two baselines. We train I DCTC
 models with different Œªin Eq. (5) to adjust the operating point.
We calculate the number of layers as the average of the number of
MHA blocks and the number of FFN blocks. Both I D-LocalGP and
I D-GlobalGP outperform the standard Transformer and the pruned
version using iterative layer pruning [29]. We can reduce the average
 number of layers to around 20 while still matching the Transformer
 trained from scratch. LocalGP achieves similar performance
as GlobalGP, but GlobalGP has only one gate predictor, which can
be more efÔ¨Åcient for inference. The reason why LocalGP is not better
 than GlobalGP may be that LocalGP decides whether to execute
or skip a block based on the current layer s input, which depends on
decisions at previous layers. This sequential procedure can lead to
more severe error propagation. We also show that it is possible to
adjust the computational cost of a trained I D model by changing Œ≤
(see Eqs. (10) (11)) at inference time. Three I D-GlobalGP models
are decoded with different Œ≤. AsŒ≤decreases, more blocks are used,
and the WER is usually improved.
Fig. 3 shows the results using InterCTC [38]. The WERs are
lower than those in Fig. 2, thanks to the auxiliary CTC loss which
regularizes training. Again, I D is consistently better than the Transformer
 trained from scratch and the pruned model.
3.3. Analysis of gate distributions
Fig. 5 shows the mean and standard deviation (std) of the gate probabilities
 generated by an I D-GlobalGP model using CTC on Lib-1 6 11 16 21 26 31 3600.51
Layer indexProbability
(a) MHA gate probabilities.
1 6 11 16 21 26 31 3600.51
Layer indexProbability
(b) FFN gate probabilities.
Fig. 5   Predicted gate probabilities (mean and std) at different layers
 of an I D-GlobalGP model on LibriSpeech test other. A higher
probability means the layer is more likely to be executed.
1 6 11 16 21 26 31 3600.51
Layer indexProbability
(a) MHA gate probabilities (trained with InterCTC).
1 6 11 16 21 26 31 3600.51
Layer indexProbability
(b) FFN gate probabilities (trained with InterCTC).
Fig. 6   Predicted gate probabilities (mean and std) at different layers
of an I D-GlobalGP model with InterCTC on LibriSpeech test other.
A higher probability means the layer is more likely to be executed.
riSpeech test-other. Most layers have a stable probability. Several
layers have larger variations depending on the input. For both MHA
and FFN, upper layers are executed with high probabilities while
lower layers tend to be skipped, which is consistent with [28].
We also show the gate probabilities from an I D-GlobalGP
model trained with InterCTC [38] in Fig. 6. Interestingly, the overall
 trend is very different from Fig. 5. Now, the upper layers are
almost skipped while the lower layers are executed with very high
probabilities, indicating that lower layers of this encoder can learn
powerful representations for the ASR task. This is probably because
auxiliary CTC losses inserted at intermediate layers can facilitate
the gradient propagation to lower parts of a deep encoder, which
effectively improves its capacity and also the Ô¨Ånal performance.
We believe this gate analysis can provide a way to interpret the
layer-wise behavior of deep networks.
3.4. Analysis of input-dependency
It has been shown that our I D models can dynamically adjust the
encoder depth based on the characteristics of an input utterance,
which achieves strong performance even with reduced computation.
But it is unclear which features are important for the gate predictor
 to determine the modules used during inference. We have found
that the speech length generally affects the inference architecture.
Fig. 7 shows the speech length distributions categorized by the number
 of MHA or FFN blocks used by an I D-GlobalGP model during
inference. We observe that utterances using more blocks tend to
be longer. This is probably because longer utterances contain more
complex information and longer-range dependency among frames,
which require more blocks (especially MHA) to process.
We also considered two other factors that may affect the infer-0 5 10 15 20 2500.20.4 18 19 20 21 blocks
(a) MHA
0 5 10 15 20 2500.1 19 20 21 22 blocks
(b) FFN
Fig. 7   Distributions of speech lengths categorized by the number of
MHA or FFN blocks used for inference. This is an I D-GlobalGP
model evaluated on LibriSpeech test other. Utterances using more
blocks tend to be longer.
ence architecture, namely the difÔ¨Åculty of utterances measured by
WERs, and the audio quality measured by DNSMOS scores [39].
However, in general, we didn t observe a clear relationship between
these metrics and the number of layers used for inference.
3.5. Generalizability
We demonstrate that the proposed I D encoders can be directly applied
 to other datasets and ASR frameworks. Fig. 4 shows the results
of CTC-based models on Tedlium2. Our I D models consistently
achieve lower WERs than the standard Transformer with similar or
even fewer layers during inference.2We further apply I D to the
attention-based encoder-decoder (AED) framework. Only the encoder
 is changed while the decoder is still a standard Transformer
decoder. Table 1 100h. With
around 27 layers on average during inference, our I D models outperform
 the 27-layer Transformer trained from scratch on both dev
clean and test clean sets. The I D with a global gate predictor is
slightly better than that with a local gate predictor.
4. CONCLUSION
In this work, we propose I D, a Transformer-based encoder which
dynamically adjusts its depth based on the characteristics of input
utterances to trade off performance and efÔ¨Åciency. We design two
types of gate predictors and show that I D-based models consistently
 outperform the vanilla Transformer trained from scratch and
the static pruned model. I D can be applied to various end-to-end
ASR frameworks and corpora. We also present interesting analysis
 on the predicted gate probabilities and the input-dependency to
better interpret the behavior of deep encoders and the effect of intermediate
 loss regularization techniques. In the future, we plan to
apply this method to large pre-trained models. We will explore only
Ô¨Åne-tuning gate predictors to signiÔ¨Åcantly reduce training cost.
5. ACKNOWLEDGEMENTS
This work used Bridges2 210014 from the Advanced Cyberinfrastructure Coordination
 Ecosystem  Services & Support (ACCESS) program, which
is supported by National Science Foundation grants #2138259,
#2138286, #2138307, #2137603, and #2138296.
2 960h. Observations are consistent
 with LibriSpeech 100 2.6. REFERENCES
[1] A. Graves, S. Fern  andez, et al.,  Connectionist temporal classiÔ¨Åcation 
 labelling unsegmented sequence data with recurrent
neural networks,  in Proc. ICML , 2006.
[2] K. Cho, B. Merrienboer, et al.,  Learning phrase representations
 using RNN encoder-decoder for statistical machine translation, 
 in Proc. EMNLP , 2014.
[3] D. Bahdanau, K. Cho, et al.,  Neural machine translation by
jointly learning to align and translate,  in Proc. ICLR , 2015.
[4] W. Chan, N. Jaitly, et al.,  Listen, attend and spell  A neural
network for large vocabulary conversational speech recognition, 
 in Proc. ICASSP , 2016.
[5] A. Graves,  Sequence transduction with recurrent neural networks, 
 arXiv 1211.3711 , 2012.
[6] A. Vaswani, N. Shazeer, N. Parmar, et al.,  Attention is all you
need,  in Proc. NeurIPS , 2017.
[7] A. Gulati, J. Qin, C.-C. Chiu, et al.,  Conformer  Convolutionaugmented
 Transformer for Speech Recognition,  in Proc. Interspeech
 , 2020.
[8] Y . Peng, S. Dalmia, et al.,  Branchformer  Parallel MLPattention
 architectures to capture local and global context for
speech recognition and understanding,  in Proc. ICML , 2022.
[9] K. Kim, F. Wu, Y . Peng, et al.,  E-branchformer  Branchformer
 with enhanced merging for speech recognition, 
arXiv 2210.00077 , 2022.
[10] S. Karita, N. Chen, T. Hayashi, et al.,  A comparative study
on transformer vs rnn in speech applications,  in Proc. ASRU ,
2019.
[11] G. Hinton, O. Vinyals, J. Dean, et al.,  Distilling the knowledge
 in a neural network,  arXiv 1503.02531 , 2015.
[12] H. Chang, S. Yang, and H. Lee,  Distilhubert  Speech representation
 learning by layer-wise distillation of hidden-unit
bert,  in Proc. ICASSP , 2022.
[13] R. Wang, Q. Bai, et al.,  LightHuBERT  Lightweight and ConÔ¨Ågurable
 Speech Representation Learning with Once-for-All
Hidden-Unit BERT,  in Proc. Interspeech , 2022.
[14] P. Dong, S. Wang, et al.,  RTMobile  Beyond Real-Time
Mobile Acceleration of RNNs for Speech Recognition,  in
ACM/IEEE Design Automation Conference (DAC) , 2020.
[15] K. Tan and D.L. Wang,  Compressing deep neural networks
for efÔ¨Åcient speech enhancement,  in Proc. ICASSP , 2021.
[16] C. J. Lai, Y . Zhang, et al.,  Parp  Prune, adjust and re-prune for
self-supervised speech recognition,  in Proc. NeurIPS , 2021.
[17] Y . Han, G. Huang, S. Song, et al.,  Dynamic neural networks 
A survey,  IEEE Trans. Pattern Anal. Mach. Intell. , vol. 44,
no. 11, pp. 7436 7456, 2022.
[18] E. Bengio, P. Bacon, et al.,  Conditional computation in neural
networks for faster models,  arXiv 1511.06297 , 2015.
[19] A. Veit and S. Belongie,  Convolutional networks with adaptive
 inference graphs,  in Proc. ECCV , 2018.
[20] X. Wang, F. Yu, et al.,  Skipnet  Learning dynamic routing in
convolutional networks,  in Proc. ECCV , 2018.
[21] Z. Wu, T. Nagarajan, et al.,  Blockdrop  Dynamic inference
paths in residual networks,  in Proc. CVPR , 2018.[22] J. Shen, Y . Wang, et al.,  Fractional skipping  Towards Ô¨Ånergrained
 dynamic cnn inference,  in Proc. AAAI , 2020.
[23] C. Li, G. Wang, et al.,  Dynamic slimmable network,  in Proc.
CVPR , 2021.
[24] J. Macoskey, G. P. Strimel, and A. Rastrow,  Bifocal neural
asr  Exploiting keyword spotting for inference optimization, 
inProc. ICASSP , 2021.
[25] Y . Shi, V . Nagaraja, C. Wu, et al.,  Dynamic encoder transducer 
 a Ô¨Çexible solution for trading off accuracy for latency, 
arXiv 2104.02176 , 2021.
[26] F. Weninger, M. Gaudesi, R. Leibold, R. Gemello, and P. Zhan,
 Dual-encoder architecture with encoder selection for joint
close-talk and far-talk speech recognition,  in Proc. ASRU ,
2021.
[27] J. Macoskey, G. P. Strimel, J. Su, and A. Rastrow,  Amortized
 neural networks for low-latency speech recognition, 
arXiv 2108.01553 , 2021.
[28] Y . Xie, J. J. Macoskey, et al.,  Compute Cost Amortized Transformer
 for Streaming ASR,  in Proc. Interspeech , 2022.
[29] J. Lee, J. Kang, and S. Watanabe,  Layer pruning on demand
with intermediate CTC,  in Proc. Interspeech , 2021.
[30] E. Jang, S. Gu, and B. Poole,  Categorical reparameterization
with gumbel-softmax,  in Proc. ICLR , 2017.
[31] C. J. Maddison, A. Mnih, and Y . Teh,  The concrete distribution 
 A continuous relaxation of discrete random variables,  in
Proc. ICLR , 2017.
[32] A. Paszke, S. Gross, F. Massa, et al.,  Pytorch  An imperative
 style, high-performance deep learning library,  in Proc.
NeurIPS , 2019.
[33] S. Watanabe, T. Hori, S. Karita, et al.,  ESPnet  End-to-End
Speech Processing Toolkit,  in Proc. Interspeech , 2018.
[34] V . Panayotov, G. Chen, D. Povey, and S. Khudanpur,  Librispeech 
 An ASR corpus based on public domain audio
books,  in Proc. ICASSP , 2015.
[35] A. Rousseau, P. Del  eglise, Y . Esteve, et al.,  Enhancing the
ted-lium corpus with selected data for language modeling and
more ted talks.,  in Proc. LREC , 2014.
[36] G. Huang, Y . Sun, Z. Liu, D. Sedra, and K. Weinberger,  Deep
networks with stochastic depth,  in Proc. ECCV , 2016.
[37] A. Fan, E. Grave, and A. Joulin,  Reducing transformer depth
on demand with structured dropout,  in Proc. ICLR , 2020.
[38] J. Lee and S. Watanabe,  Intermediate loss regularization for
ctc-based speech recognition,  in Proc. ICASSP , 2021.
[39] C. K. Reddy, V . Gopal, and R. Cutler,  Dnsmos p.835  A nonintrusive
 perceptual objective speech quality metric to evaluate
noise suppressors,  in Proc. ICASSP , 2022.