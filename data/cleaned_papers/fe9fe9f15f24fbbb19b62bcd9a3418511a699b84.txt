Policy Representation via Di usion Probability Model for Reinforcement Learning1 Long Yang1, ,Zhixiong Huang2, ,Fenghao Lei2,Yucun Zhong3,Yiming Yang4, Cong Fang1,Shiting Wen5,Binbin Zhou6,Zhouchen Lin1 1School of Arti cial Intelligence, Peking University, Beijing, China 2College of Computer Science and Technology, Zhejiang University, China 3MOE Frontiers Science Center for Brain and Brain-Machine Integration & College of Computer Science, Zhejiang University, China. 4Institute of Automation Chinese Academy of Sciences Beijing, China 5School of Computer and Data Engineering, NingboTech University, China 6College of Computer Science and Technology, Hangzhou City University, China {yanglong001,fangcong,zlin }@pku.edu.cn , {zx.huang,lfh,yucunzhong }@zju.edu.cn ,{wensht}@nit.zju.edu.cn yangyiming2019@ia.ac.cn ,bbzhou@hzcu.edu.cn Code: https://github.com/BellmanTimeHut/DIPO Popular reinforcement learning (RL) algorithms tend to produce a unimodal policy distribution, which weakens the expressiveness of complicated policy and decays the ability of exploration. The di usion probability model is powerful to learn complicated multimodal distributions, which has shown promising and potential applications to RL. In this paper, we formally build a theoretical foundation of policy representation via the di usion probability model and provide practical implementations of di usion policy for online model-free RL. Concretely, we character di usion policy as a stochastic process induced by stochastic di erential equations, which is a new approach to representing a policy. Then we present a convergence guarantee for di usion policy, which provides a theory to understand the multimodality of di usion policy. Furthermore, we propose the DIPO, which implements model-free online RL with DI usion POlicy. To the best of our knowledge, DIPO is the  rst algorithm to solve model-free online RL problems with the di usion model. Finally, extensive empirical results show the e ectiveness and superiority of DIPO on the standard continuous control Mujoco benchmark. 1* L.Yang and Z.Huang share equal contributions. 1arXiv:2305.13122v1  [cs.LG]  22 May 2023Contents 1.1 Our Main Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 3 1.2 Paper Organization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2 Reinforcement Learning 5 3 Motivation: A View from Policy Representation 5 3.1 Policy Representation for Reinforcement Learning . . . . . . . . . . . . . . . . 5 3.2 Di usion Model is Powerful to Policy Representation . . . . . . . . . . . . . . . 6 4.1 Stochastic Dynamics of Di usion Policy . . . . . . . . . . . . . . . . . . . . . . 7 4.2 Exponential Integrator Discretization for Di usion Policy . . . . . . . . . . . . 8 4.3 Convergence Analysis of Di usion Policy . . . . . . . . . . . . . . . . . . . . . . 9 5 DIPO: Implementation of Di usion Policy for Model-Free Online RL 10 5.1 Training Loss of DIPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 10 5.2 Playing Action of DIPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 5.3 Policy Improvement of DIPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . 11 6.1 Di usion Models for Reinforcement Learning . . . . . . . . . . . . . . . . . . . 12 6.2 Generative Models for Policy Learning . . . . . . . . . . . . . . . . . . . . . . . 12 7.1 Comparative Evaluation and Illustration . . . . . . . . . . . . . . . . . . . . . . 13 7.2 State-Visiting Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 7.3 Ablation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 A Review on Notations 25 B Auxiliary Results 26 B.1 Di usion Probability Model (DPM). . . . . . . . . . . . . . . . . . . . . . . . . 26 B.2 Transition Probability for Ornstein-Uhlenbeck Process . . . . . . . . . . . . . . 26 B.3 Exponential Integrator Discretization . . . . . . . . . . . . . . . . . . . . . . . . 27 B.4 Fokker Planck Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27 B.5 Donsker-Varadhan Representation for KL-divergence . . . . . . . . . . . . . . . 28 B.6 Some Basic Results for Di usion Policy . . . . . . . . . . . . . . . . . . . . . . 28 C Implementation Details of DIPO 30 C.1 DIPO: Model-Free Learning with Di usion Policy . . . . . . . . . . . . . . . . . 31 C.2 Loss Function of DIPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31 C.3 Playing Actions of DIPO . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 33 2D Time Derivative of KL Divergence Between Difu usion Policy and True D.1 Time Derivative of KL Divergence at Reverse Time k= 0 . . . . . . . . . . . . 35 D.2 Auxiliary Results For Reverse Time k= 0 . . . . . . . . . . . . . . . . . . . . . 35 D.3 Proof for Result at Reverse Time k= 0 . . . . . . . . . . . . . . . . . . . . . . 42 D.4 Proof for Result at Arbitrary Reverse Time k. . . . . . . . . . . . . . . . . . . 44 E Proof of Theorem 4.3 45 F Additional Details 48 F.1 Proof of Lemma F.1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48 F.2 Proof of Lemma D.7 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49 G Details and Discussions for multimodal Experiments 50 G.1 Multimodal Environment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 50 G.2 Plots Details of Visualization . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51 G.3 Results Report . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53 H Additional Experiments 55 H.1 Hyper-parameters for MuJoCo . . . . . . . . . . . . . . . . . . . . . . . . . . . 55 H.2 Additional Tricks for Implementation of DIPO . . . . . . . . . . . . . . . . . . 56 H.3 Details and Additional Reports for State-Visiting . . . . . . . . . . . . . . . . . 57 H.4 Ablation Study on MLP and VAE . . . . . . . . . . . . . . . . . . . . . . . . . 601 Introduction Existing policy representations (e.g., Gaussian distribution) for reinforcement learning (RL) tend to output a unimodal distribution over the action space, which may be trapped in a locally optimal solution due to its limited expressiveness of complex distribution and may result in poor performance. Di usion probability model [Sohl-Dickstein et al., 2015, Ho et al., 2020, Song et al., 2021] is powerful to learn complicated multimodal distributions, which has been applied to RL tasks (e.g., [Ajay et al., 2023, Reuss et al., 2023, Chi et al., 2023]). Although the di usion model (or di usion policy) shows its promising and potential applications to RL tasks, previous works are all empirical or only consider o ine RL settings. This raises some fundamental questions: How to character di usion policy? How to show the expressiveness of di usion policy? How to design a di usion policy for online model-free RL? Those are the focuses of this paper. a0  aT a1 1( |s)  at t( |s)  aT T( |s) N(0,I)  a0 ( |s)input a0 0( |s) =  T( |s) aT t T t( |s)  aT 1 T 1( |s)  aT T( |s) 2g(t) atdt+g(t)dwtForward SDE: a ( |s) N (0,I) 2g2(T t) [ at+ 2 logpT t( at)] dt+g(T t)d wt Reverse SDE:N(0,I) ( |s) Figure 1: Di usion Policy: Policy Representation via Stochastic Process. For a given state s, the forward stochastic process { at|s}maps the input  a0=:a ( |s) to be a noise; then we recover the input by the stochastic process { at|s}that reverses the reversed SDE if we know the score function  logpt( ), wherept( ) is the probability distribution of the forward process, i.e.,pt( ) =  t( |s). In this paper, we mainly consider di usion policy from the next three aspects. Charactering Di usion Policy as Stochastic Process. We formulate di usion policy as a stochastic process that involves two processes induced by stochastic di erential equations (SDE), see Figure 1, where the forward process disturbs the input policy  to noise, then the reverse process infers the policy  according to a corresponding reverse SDE. Although this view is inspired by the score-based generative model [Song et al., 2021], we provide a brand new approach to represent a policy: via a stochastic process induced by SDE, neither via value function nor parametric function. Under this framework, the di usion policy is  exible to 3 D={st,at,st+1,rt+1}data policy improvement e.g.,Q-learning/SAC/PPO Figure 2: Standard Training Framework for Model-free Online RL. D={st,at,st+1,rt+1}data action gradient D ={st,at}at+ aQ (st,at) at Figure 3: Framework of DIPO: Implementation for Model-free Online RL with DI usion generate actions according to numerical SDE solvers. Convergence Analysis of Di usion Policy. Under mild conditions, Theorem 4.3 presents a theoretical convergence guarantee for di usion policy. The result shows that if the score estimator is su ciently accurate, then di usion policy e ciently infers the actions from any realistic policy that generates the training data. It is noteworthy that Theorem 4.3 also shows that di usion policy is powerful to represent a multimodal distribution, which leads to su cient exploration and better reward performance, Section 3 and Appendix G provide more discussions with numerical veri cations for this view. Di usion Policy for Model-free Online RL. Recall the standard model-free online RL framework, see Figure 2, where the policy improvement produces a new policy according to the data D. However, Theorem 4.3 illustrates that the di usion policy only  ts the distribution of the policy  but does not improve the policy  . We can not embed the di usion policy into the standard RL training framework, i.e., the policy improvement in Figure 2 can not be naively replaced by di usion policy. To apply di usion policy to model-free online RL task, we propose the DIPO algorithm, see Figure 3. The proposed DIPO considers a novel way for policy improvement, we call it action gradient that updates each at D along the gradient  eld (over the action space) of state-action value: where for a given state s,Q (s,a) measures the reward performance over the action space A. Thus, DIPO improves the policy according to the actions toward to better reward performance. To the best of our knowledge, this paper  rst presents the idea of action gradient, which provides an e cient way to make it possible to design a di usion policy for online RL. 1.2 Paper Organization Section 2 presents the background of reinforcement learning. Section 3 presents our motivation from the view of policy representation. Section 4 presents the theory of di usion policy. Section 5 presents the practical implementation of di usion policy for model-free online reinforcement learning. Section 7 presents the experiment results. 42 Reinforcement Learning Reinforcement learning (RL)[Sutton and Barto, 2018] is formulated as Markov decision process M= (S,A,P( ),r, ,d 0), whereSis the state space; A Rpis the continuous action space; P(s |s,a) is the probability of state transition from stos after playing a;r(s |s,a) denotes the reward that the agent observes when the state transition from stos after playing a; (0,1) is the discounted factor, and d0( ) is the initial state distribution. A policy  is a probability distribution de ned on S A , and (a|s) denotes the probability of playing a in state s. Let{st,at,st+1,r(st+1|st,at)}t 0 be the trajectory sampled by the policy  , where s0 d0( ),at ( |st),st+1 P( |st,at). The goal of RL is to  nd a policy  such that 3 Motivation: A View from Policy Representation In this section, we clarify our motivation from the view of policy representation: di usion model is powerful to policy representation, which leads to su cient exploration and better 3.1 Policy Representation for Reinforcement Learning Value function and parametric function based are the main two approaches to represent policies, while di usion policy expresses a policy via a stochastic process (shown in Figure 1) that is essentially di cult to the previous representation. In this section, we will clarify this view. Additionally, we will provide an empirical veri cation with a numerical experiment. 3.1.1 Policy Representation via Value Function A typical way to represent policy is  -greedy policy [Sutton and Barto, 1998] or energy-based policy [Sallans and Hinton, 2004, Peters et al., 2010], arg maxa AQ (s,a ) w.p. 1 ; randomly play a A w.p. ;or (a|s) =exp{Q (s,a)} t=0 tr(st+1|st,at)|s0=s,a0=a] the normalization term Z (s) = Rpexp{Q (s,a)}da, and  w.p.  is short for  with probability . The representation (1) illustrates a connection between policy and value function, which is widely used in value-based methods (e.g., SASRA [Rummery and Niranjan, 1994], Q-Learning [Watkins, 1989], DQN [Mnih et al., 2015]) and energy-based methods (e.g., SQL [Schulman et al., 2017a, Haarnoja et al., 2017, 2018a], SAC [Haarnoja et al., 2018b]). 3.1.2 Policy Representation via Parametric Function Instead of consulting a value function, the parametric policy is to represent a policy by a parametric function (e.g., neural networks), denoted as  , where is the parameter. Policy gradient theorem [Sutton et al., 1999, Silver et al., 2014] plays a center role to learn  , which is fundamental in modern RL (e.g., TRPO [Schulman et al., 2015], DDPG [Lillicrap et al., 2016], PPO [Schulman et al., 2017b], IMPALA [Espeholt et al., 2018], et al). Figure 5: Policy representation comparison of di erent policies on multimodal environment. 3.1.3 Policy Representation via Stochastic Process It is di erent from both value-based and parametric policy representation; the di usion policy (see Figure 1) generates an action via a stochastic process, which is a fresh view for the RL community. The di usion model with RL  rst appears in [Janner et al., 2022], where it proposes the di user that plans by iteratively re ning trajectories, which is an essential o ine RL method. Ajay et al. [2023], Reuss et al. [2023] model a policy as a return conditional di usion model, Chen et al. [2023a], Wang et al. [2023], Chi et al. [2023] consider to generate actions via di usion model. The above methods are all to solve o ine RL problems. To the best of our knowledge, our proposed method is the  rst di usion approach to online model-free reinforcement learning. 3.2 Di usion Model is Powerful to Policy Representation Figure 4: Unimodal Distribution vs Multimodal Distribution.This section presents the di usion model is powerful to represent a complex policy distribution. by two following aspects: 1)  tting a multimodal policy distribution is e cient for exploration; 2) empirical veri cation with a numerical experiment. The Gaussian policy is widely used in RL, which is a unimodal distribution, and it plays actions around the region of its mean center with a higher probability, i.e., the red regionAin Figure 4. The unimodal policy weakens the expressiveness of complicated policy and decays the agent s ability to explore the environment. While for a multimodal policy, it plays actions among the di erent regions: A1 A2 A3. Compared to the unimodal policy, the multimodal policy is powerful to explore the unknown world, making the agent 6understand the environment e ciently and make a more reasonable decision. We compare the ability of policy representation among SAC, TD3 [Fujimoto et al., 2018], PPO and di usion policy on the  multi-goal  environment [Haarnoja et al., 2017] (see Figure 5), where the x-axis andy-axis are 2D states, the four red dots denote the states of the goal at (0,5), (0, 5), (5,0) and ( 5,0) symmetrically. A reasonable policy should be able to take actions uniformly to those four goal positions with the same probability, which characters the capacity of exploration of a policy to understand the environment. In Figure 5, the red arrowheads represent the directions of actions, and the length of the red arrowheads represents the size of the actions. Results show that di usion policy accurately captures a multimodal distribution landscape, while both SAC, TD3, and PPO are not well suited to capture such a multimodality. From the distribution of action direction and length, we also know the di usion policy keeps a more gradual and steady action size than the SAC, TD3, and PPO to  t the multimodal distribution. For more details about 2D/3D plots, environment, comparisons, and discussions, please refer to Appendix G. In this section, we present the details of di usion policy from the following three aspects: its stochastic dynamic equation (shown in Figure 1), discretization implementation, and  nite-time analysis of its performance for the policy representation. 4.1 Stochastic Dynamics of Di usion Policy Recall Figure 1, we know di usion policy contains two processes: forward process and reverse process. We present its dynamic in this section. Forward Process. To simplify the expression, we only consider g(t) = parallel to the general setting in Figure 1. For any given state s, the forward process produces a sequence{( at|s)}t=0:Tthat starting with  a0 ( |s), and it follows the Ornstein-Uhlenbeck process (also known as Ornstein-Uhlenbeck SDE), Let at t( |s) be the evolution distribution along the Ornstein-Uhlenbeck  ow (2). According to Proposition B.1 (see Appendix B.2), we know the conditional distribution of  at| a0is That implies the forward process (2) transforms policy  ( |s) to the Gaussian noise N(0,I). Reverse Process. For any given state s, if we reverse the stochastic process {( at|s)}t=0:T, then we obtain a process that transforms noise into the policy  ( |s). Concretely, we model the policy as the process {( at|s)}t=0:Taccording to the next Ornstein-Uhlenbeck process (running d at= ( at+ 2 logpT t( at)) dt+ wherept( ) is the probability density function of  t( |s). Furthermore, according to [Anderson, 1982], with an initial action  a0 T( |s), the reverse process { at}t=0:Tshares the same 7Algorithm 1: Di usion Policy with Exponential Integrator Discretization to Approximate ( |s) 1:input: state s, horizonT, reverse length K, step-sizeh=T 2:initialization: a random action  a0 N(0,I); 4: a random zk N(0,I), settk=hk; atk+ 2 S( atk,s,T tk)) distribution as the time-reversed version of the forward process { aT t}t=0:T. That also implies t( |s) =  T t( |s),if a0 T( |s). (5) Score Matching. The score function  logpT t( ) de ned in (4) is not explicit, we consider an estimator  S( ,s,T t) to approximate the score function at a given state s. We consider the next problem, S( ,s,T t) =: arg min s( ) FEa t( |s)[ s(a,s,t) logpT t(a) 2 s( ) FEa t( |s)[ s(a,s,t) log  t(a|s) 2 whereFis the collection of function approximators (e.g., neural networks). We will provide the detailed implementations with a parametric function approximation later; please refer to Section 5 or Appendix C.2. 4.2 Exponential Integrator Discretization for Di usion Policy In this section, we consider the implementation of the reverse process (4) with exponential integrator discretization [Zhang, 2022, Lee et al., 2023]. Let h>0 be the step-size, assume h N, andtk=:hk,k= 0,1, ,K. Then we give a partition on the interval [0,T] as follows, 0 = t0<t1< <tk<tk+1< <tK=T. Furthermore, we take the discretization to the reverse process (4) according to the following equation, d at=( at+ 2 S( atk,s,T tk)) 2dwt, t [tk,tk+1], (8) where it runs initially from  a0 N(0,I). By It  o integration to the two sizes of (8) on the k-th interval [tk,tk+1], we obtain the exact solution of the SDE (8), for each k= 0,1,2, ,K 1, eh 1)( atk+ 2 S( atk,s,T tk)) e2h 1zk,zk N(0,I). (9) For the derivation from the SDE (8) to the iteraion (9), please refer to Appendix B.3, and we have shown the implementation in Algorithm 1. 84.3 Convergence Analysis of Di usion Policy In this section, we present the convergence analysis of di usion policy, we need the following notations and assumptions before we further analyze. Let  (x) and (x) be two smooth probability density functions on the space Rp, the Kullback Leibler (KL) divergence and relative Fisher information (FI) from  (x) to (x) are de ned as follows, Assumption 4.1 (Lipschitz Score Estimator and Policy) .The score estimator is Ls-Lipschitz over action space A, and the policy  ( |s)isLp-Lipschitz over action space A, i.e., for any a, a A, the following holds, S(a,s,t) S(a ,s,t) Ls a a , log (a|s) log (a |s) Lp a a . Assumption 4.2 (Policy with  -LSI Setting) .The policy ( |s)satis es -Log-Sobolev inequality (LSI) that de ned as follows, there exists constant   >0, for any probability distribution Assumption 4.1 is a standard setting for Langevin-based algorithms (e.g., [Wibisono and Yang, 2022, Vempala and Wibisono, 2019]), and we extend it with RL notations. Assumption 4.2 presents the policy distribution class that we are concerned, which contains many complex distributions that are not restricted to be log-concave, e.g. any slightly smoothed bound distribution admits the condition (see [Ma et al., 2019, Proposition 1]). Theorem 4.3 (Finite-time Analysis of Di usion Policy) .For a given state s, let{ t( |s)}t=0:T and{ t( |s)}t=0:Tbe the distributions along the  ow (2) and (4) correspondingly, where { t( |s)}t=0:Tstarts at  0( |s) = ( |s)and{ t( |s)}t=0:Tstarts at  0( |s) =  T( |s). Let k( |s)be the distribution of the iteration (9) at the k-th timetk=hk, i.e.,  atk k( |s) denotes the di usion policy (see Algorithm 1) at the time tk=hk. Let{ k( |s)}k=0:Kbe starting at  0( |s) =N(0,I), under Assumption 4.1 and 4.2, let the reverse length K , where constants  0and T0will be special later. Then the KLdivergence between di usion policy  K( |s)and input policy  ( |s)is upper-bounded as follows, convergence of forward process (2)+( errors from discretization (9)+20 exp S(a,s,T hk) log  t(a|s) 2 errors from score matching (7). Theorem 4.3 illustrates that the errors involve the following three terms. The  rst error that represents how close the distribution of the input policy is to the standard Gaussian noise, which is bounded by the exponential convergence rate of Ornstein-Uhlenbeck  ow (2) [Bakry et al., 2014, Wibisono and Jog, 2018, Chen et al., 2023c]. The second error is sourced from exponential integrator discretization implementation (9), which scales with the discretization step-size h. The discretization error term implies a  rstorder convergence rate with respect to the discretization step-size hand scales polynomially 9on other parameters. The third error is sourced from score matching (7), which represents how close the score estimator  Sis to the score function  logpT t( ) de ned in (4). That implies for the practical implementation, the error from score matching could be su ciently small if we  nd a good score estimator  S. Furthermore, for any   > 0, if we  nd a good score estimator that makes the score matching error satisfy  score<1 20 , the step-size h=O( , and reverse length K= 4 hlog3KL(N(0,I) ( |s)) , then Theorem 4.3 implies the output of di usion policy (  K( |s) makes a su cient close to the input policy  ( |s) with the measurement by KL(   K( |s) ( |s)) . 5 DIPO: Implementation of Di usion Policy for Model-Free In this section, we present the details of DIPO, which is an implementation of DI usion POlicy for model-free reinforcement learning. According to Theorem 4.3, di usion policy only  ts the current policy  that generates the training data (denoted as D), but it does not improve the policy  . It is di erent from traditional policy-based RL algorithms, we can not improve a policy according to the policy gradient theorem since di usion policy is not a parametric function but learns a policy via a stochastic process. Thus, we need a new way to implement policy improvement, which is nontrivial. We have presented the framework of DIPO in Figure 3, and shown the key steps of DIPO in Algorithm 2. For the detailed implementation, please refer to Algorithm 3 (see Appendix C). 5.1 Training Loss of DIPO It is intractable to directly apply the formulation (7) to estimate the score function since logpt( ) = log  t( |s) is unknown, which is sourced from the initial distribution  a0 ( |s) is unknown. According to denoising score matching [Vincent, 2011, Hyv  arinen, 2005], a practical way is to solve the next optimization problem (10). For any given s S, 0 (t)E a0 ( |s)E at| a0[ s ( at,s,t) log t( at| a0) 2 where (t) : [0,T] R+is a positive weighting function;  t( at| a0) =N( denotes the transition kernel of the forward process (3); E at| a0[ ] denotes the expectation with respect to  t( at| a0); and is the parameter needed to be learned. Then, according to Theorem C.1 (see Appendix C.2), we rewrite the objective (10) as follows, L( ) =Ek U({1,2, ,K}),z N(0,I),(s,a) D[ whereU( ) denotes uniform distribution, ( , ,k) = 1 k s ( , ,T tk), and kwill be special. The objective (11) provides a way to learn  from samples; see line 14-16 in Algorithm 2. 10Algorithm 2: (DIPO) Model-Free Reinforcement Learning with DI usion POlicy 1:initialize , critic network Q ;{ i}K 3: datasetD  ; initialize s0 d0( ); 4: #update experience 6: playatfollows (12); st+1 P( |st,at);D D { st,at,st+1,r(st+1|st,at)}; 7: #update value function 9: sample ( st,at,st+1,r(st+1|st,at)) D i.i.d; take gradient descent on  Q( ) (14); 12: replace each action at Dfollows at at+ aQ (st,a)|a=at; 15: sample ( s,a) fromDi.i.d, sample index k U({1, ,K}),z N(0,I); 16: take gradient decent on the loss  d( ) = z ( ka+ 1 kz,s,k) 2 17:until the policy performs well in the environment. 5.2 Playing Action of DIPO Replacing the score estimator  S(de ned in Algorithm 1) according to  , after some algebras (see Appendix C.3), we rewrite di usion policy (i.e., Algorithm 1) as follows, ak 1 k 1 k ( ak,s,k)) wherek= 0,1, ,K 1 runs forward in time, the noise zk N(0,I). The agent plays the last (output) action  aK. 5.3 Policy Improvement of DIPO According to (11), we know that only the state-action pairs ( s,a) D are used to learn a policy. That inspires us that if we design a method that transforms a given pair ( s,a) D to be a  better  pair, then we use the  better  pair to learn a new di usion policy  , then . About  better  state-action pair should maintain a higher reward performance than the originally given pair ( s,a) D. We break our key idea into two steps: 1) rst, we regard the reward performance as a function with respect to actions, J (a) =Es d0( )[Q (s,a)], which quanti es how the action aa ects the performance; 2)then, we update all the actions a D through the direction  aJ (a) by gradient ascent method: a a+ aJ (a) =a+ Es d0( )[ aQ (s,a)], (13) where >0 is step-size, and we call  aJ (a) asaction gradient . To implement (13) from samples, we need a neural network Q to estimate Q . Recall{st,at,st+1,r(st+1|st,at)}t 0 , we train the parameter  by minimizing the following Bellman residual error, r(st+1|st,at) + Q (st+1,at+1) Q (st,at))2. (14) 11Finally, we consider each pair ( st,at) D, and replace the action at Das follows, at at+ aQ (st,a)|a=at. (15) Due to the di usion model being a fast-growing  eld, this section only presents the work that relates to reinforcement learning, a recent work [Yang et al., 2022] provides a comprehensive survey on the di usion model. In this section,  rst, we review recent advances in di usion models with reinforcement learning. Then, we review the generative models for reinforcement 6.1 Di usion Models for Reinforcement Learning The di usion model with RL  rst appears in [Janner et al., 2022], where it proposes the di user that plans by iteratively re ning trajectories, which is an essential o ine RL method. Later Ajay et al. [2023] model a policy as a return conditional di usion model, Chen et al. [2023a], Wang et al. [2023], Chi et al. [2023] consider to generate actions via di usion model. SE(3)-di usion  elds [Urain et al., 2023] consider learning data-driven SE(3) cost functions as di usion models. Pearce et al. [2023] model the imitating human behavior with di usion models. Reuss et al. [2023] propose score-based di usion policies for the goal-conditioned imitation learning problems. ReorientDi  [Mishra and Chen, 2023] presents a reorientation planning method that utilizes a di usion model-based approach. StructDi usion [Liu et al., 2022] is an object-centric transformer with a di usion model, based on high-level language goals, which constructs structures out of a single RGB-D image. Brehmer et al. [2023] propose an equivariant di user for generating interactions (EDGI), which trains a di usion model on an o ine trajectory dataset, where EDGI learns a world model and planning in it as a conditional generative modeling problem follows the di user [Janner et al., 2022]. DALL-E-Bot [Kapelyukh et al., 2022] explores the web-scale image di usion models for robotics. AdaptDi user [Liang et al., 2023] is an evolutionary planning algorithm with di usion, which is adapted to unseen The above methods are all to solve o ine RL problems, to the best of our knowledge, the proposed DIPO is the  rst di usion approach to solve online model-free RL problems. The action gradient plays a critical way to implement DIPO, which never appears in existing RL literature. In fact, the proposed DIPO shown in Figure 3 is a general training framework for RL, where we can replace the di usion policy with any function  tter (e.g., MLP or VAE). 6.2 Generative Models for Policy Learning In this section, we mainly review the generative models, including VAE [Kingma and Welling, 2013], GAN [Goodfellow et al., 2020], Flow [Rezende and Mohamed, 2015], and GFlowNet [Bengio et al., 2021a,b] for policy learning. Generative models are mainly used in cloning diverse behaviors [Pomerleau, 1988], imitation learning [Osa et al., 2018], goal-conditioned imitation learning [Argall et al., 2009], or o ine RL [Levine et al., 2020], a recent work [Yang et al., 2023] provides a foundation presentation for the generative models for policy learning. VAE for Policy Learning. Lynch et al. [2020], Ajay et al. [2021] have directly applied auto-encoding variational Bayes (VAE) [Kingma and Welling, 2013] and VQ-VAE [Van Den Oord et al., 2017] model behavioral priors. Mandlekar et al. [2020] design the low-level 12policy that is conditioned on latent from the CVAE. Pertsch et al. [2021] joint the representation of skill embedding and skill prior via a deep latent variable model. Mees et al. [2022], RoseteBeas et al. [2023] consider seq2seq CVAE [Lynch et al., 2020, Wang et al., 2022] to model of conditioning the action decoder on the latent plan allows the policy to use the entirety of its capacity for learning unimodal behavior. GAN for Imitation Learning. GAIL [Ho and Ermon, 2016] considers the Generative Adversarial Networks (GANs) [Goodfellow et al., 2020] to imitation learning. These methods consist of a generator and a discriminator, where the generator policy learns to imitate the experts  behaviors, and the discriminator distinguishes between real and fake trajectories, which models the imitation learning as a distribution matching problem between the expert policy s state-action distribution and the agent s policy [Fu et al., 2018, Wang et al., 2021]. For several advanced results and applications, please refer to [Chen et al., 2023b, Deka et al., 2023, Rafailov et al., Taranovic et al., 2023]. Flow and GFlowNet Model for Policy Learning. Singh et al. [2020] consider normalizing ows [Rezende and Mohamed, 2015] for the multi-task RL tasks. Li et al. [2023a] propose diverse policy optimization, which consider the GFlowNet [Bengio et al., 2021a,b] for the structured action spaces. Li et al. [2023b] propose CFlowNets that combines GFlowNet with continuous control. Stochastic GFlowNet [Pan et al., 2023] learns a model of the environment to capture the stochasticity of state transitions. Malkin et al. [2022] consider training a GFlowNet with trajectory balance. Other Methods. Decision Transformer (DT) [Chen et al., 2021] model the o ine RL tasks as a conditional sequence problem, which does not learn a policy follows the traditional methods (e.g., Sutton [1988], Sutton and Barto [1998]). Those methods with DT belong to the task-agnostic behavior learning methods, which is an active direction in policy learning (e,g., [Cui et al., 2023, Brohan et al., 2022, Zheng et al., 2022, Konan et al., 2023, Kim et al., 2023]). Energy-based models [LeCun et al., 2006] are also modeled as conditional policies [Florence et al., 2022] or applied to inverse RL [Liu et al., 2021]. Autoregressive model [Vaswani et al., 2017, Brown et al., 2020] represents the policy as the distribution of action, where it considers the distribution of the whole trajectory [Reed et al., 2022, Sha ullah et al., 2022]. In this section, we aim to cover the following three issues: How does DIPO compare to the widely used RL algorithms (SAC, PPO, and TD3) on the standard continuous control benchmark? How to show and illustrate the empirical results? How does the di usion model compare to VAE [Kingma and Welling, 2013] and multilayer perceptron (MLP) for learning distribution? How to choose the reverse length Kof DIPO for the reverse inference? 7.1 Comparative Evaluation and Illustration We provide an evaluation on MuJoCo tasks [Todorov et al., 2012]. Figure 6 shows the reward curves for SAC, PPO, TD3, and DIPO on MuJoCo tasks. To demonstrate the robustness of the proposed DIPO, we train DIPO with the same hyperparameters for all those 5 tasks, where we provide the hyperparameters in Table 3, see Appendix H.1. For each algorithm, we plot the average return of 5 independent trials as the solid curve and plot the standard deviation across 5 same seeds as the transparent shaded region. We evaluate all the methods with 106iterations. Results show that the proposed DIPO achieves the best score across all those 5 tasks, and DIPO learns much faster than SAC, PPO, and TD3 on the tasks of Ant-3v 13/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013 /uni00000033/uni00000033/uni00000032 /uni00000036/uni00000024/uni00000026 /uni00000037/uni00000027/uni00000016 /uni00000027/uni0000002c/uni00000033/uni00000032(a) Ant-v3 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000013 /uni00000033/uni00000033/uni00000032 /uni00000036/uni00000024/uni00000026 /uni00000037/uni00000027/uni00000016 /uni00000027/uni0000002c/uni00000033/uni00000032 (b) HalfCheetah-v3 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000016/uni00000018/uni00000013/uni00000013 /uni00000033/uni00000033/uni00000032 /uni00000036/uni00000024/uni00000026 /uni00000037/uni00000027/uni00000016 /uni00000027/uni0000002c/uni00000033/uni00000032 (c) Hopper-v3 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000033/uni00000033/uni00000032 /uni00000036/uni00000024/uni00000026 /uni00000037/uni00000027/uni00000016 /uni00000027/uni0000002c/uni00000033/uni00000032 (d) Humanoid-v3 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000033/uni00000033/uni00000032 /uni00000036/uni00000024/uni00000026 /uni00000037/uni00000027/uni00000016 /uni00000027/uni0000002c/uni00000033/uni00000032 (e) Walker2d-v3 Figure 6: Average performances on MuJoCo Gym environments with  std shaded, where the horizontal axis of coordinate denotes the iterations (  106), the plots smoothed with a window Ant-v3 DIPO SAC TD3 PPO Figure 7: State-visiting visualization by each algorithm on the Ant-v3 task, where states get dimension reduction by t-SNE. The points with di erent colors represent the states visited by the policy with the style. The distance between points represents the di erence between HalfCheetah-v3 DIPO SAC Figure 8: State-visiting visualization for comparison between DIPO and SAC on HalfCheetahv3. and Walker2d-3v. Although the asymptotic reward performance of DIPO is similar to baseline algorithms on other 3 tasks, the proposed DIPO achieves better performance at the initial iterations, we will try to illustrate some insights for such empirical results of HalfCheetah-v3 in Figure 8, for more discussions, see Appendix H. 7.2 State-Visiting Visualization From Figure 6, we also know that DIPO achieves the best initial reward performance among all the 5 tasks, a more intuitive illustration has been shown in Figure 7 and 8, where we only consider Ant-v3 and HalfCheetah-v3; for more discussions and observations, see Appendix H.3. We show the state-visiting region to compare both the exploration and  nal reward performance, where we use the same t-SNE [Van der Maaten and Hinton, 2008] to transfer the high-dimensional states visited by all the methods for 2D visualization. Results of Figure 7 show that the DIPO explores a wider range of state-visiting, covering TD3, SAC, and PPO. Furthermore, from Figure 7, we also know DIPO achieves a more dense state-visiting at the nal period, which is a reasonable result since after su cient training, the agent identi es and 14DIPO VAE MLP SAC TD3 PPO algorithm0100020003000400050006000reward DIPO VAE MLP SAC TD3 PPO algorithm20004000600080001000012000reward (b) HalfCheetah-v3 DIPO VAE MLP SAC TD3 PPO algorithm05001000150020002500300035004000reward DIPO VAE MLP SAC TD3 PPO algorithm0100020003000400050006000reward DIPO VAE MLP SAC TD3 PPO algorithm010002000300040005000reward Figure 9: Reward Performance Comparison to VAE and MLP with DIPO, SAC, PPO and /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013 /uni0000002e/uni00000020/uni00000015/uni00000013 /uni0000002e/uni00000020/uni00000018/uni00000013 /uni0000002e/uni00000020/uni00000014/uni00000013/uni00000013 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni0000002e/uni00000020/uni00000015/uni00000013 /uni0000002e/uni00000020/uni00000018/uni00000013 /uni0000002e/uni00000020/uni00000014/uni00000013/uni00000013 (b) HalfCheetah-v3 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000018/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000016/uni00000018/uni00000013/uni00000013 /uni0000002e/uni00000020/uni00000015/uni00000013 /uni0000002e/uni00000020/uni00000018/uni00000013 /uni0000002e/uni00000020/uni00000014/uni00000013/uni00000013 (c) Hopper-v3 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni0000002e/uni00000020/uni00000015/uni00000013 /uni0000002e/uni00000020/uni00000018/uni00000013 /uni0000002e/uni00000020/uni00000014/uni00000013/uni00000013 (d) Humanoid-v3 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013 /uni0000002e/uni00000020/uni00000015/uni00000013 /uni0000002e/uni00000020/uni00000018/uni00000013 /uni0000002e/uni00000020/uni00000014/uni00000013/uni00000013 (e) Walker2d-v3 Figure 10: Learning curves with di erent reverse lengths K= 20,50,100. Reverse length Ant-v3 HalfCheetah-v3 Hopper-v3 Humanoid-v3 Walker2d-v3 K= 100 5622.30 487.09 10472.31 654.96 3123.14 636.23 4878.41 822.03 4409.18 469.06 K= 50 4877.41 1010.35 9198.20 1738.25 3214.83 491.15 4513.39 1075.94 4199.34 1062.31 K= 20 5288.77 970.35 9343.69 986.82 2511.63 837.03 4294.79 1583.484467.20 368.13 Table 1: Average return over  nal 6E5 iterations with di erent reverse lengths K= 20,50,100, and maximum value is bolded for each task. avoids the  bad  states, and plays actions transfer to  good  states. On the contrary, PPO shows an aimless exploration in the Ant-v3 task, which partially explains why PPO is not so good in the Ant-v3 task. From Figure 8 we know, at the initial time, DIPO covers more regions than SAC in the HalfCheetah-v3, which results in DIPO obtaining a better reward performance than SAC. This result coincides with the results of Figure 5, which demonstrates that DIPO is e cient for exploration, which leads DIPO to better reward performance. While we also know that SAC starts with a narrow state visit that is similar to the  nal state visit, and SAC performs with the same reward performance with DIPO at the  nal, which implies SAC runs around the  good  region at the beginning although SAC performs a relatively worse initial reward performance than DIPO. Thus, the result of Figure 8 partially explains why DIPO performs better than SAC at the initial iterations but performs with same performance with SAC at the  nal for the HalfCheetah-v3 task. In this section, we consider the ablation study to compare the di usion model with VAE and MLP for policy learning, and show a trade-o  on the reverse length Kfor reverse inference. 157.3.1 Comparison to VAE and MLP Both VAE and MLP are widely used to learn distribution in machine learning, a fundamental question is: why must we consider the di usion model to learn a policy distribution? what the reward performance is if we use VAE and MLP to model a policy distribution? We show the answer in Figure 9, where the VAE (or MLP) is the result we replace the di usion policy of DIPO (see Figure 3) with VAE (or MLP), i.e., we consider VAE (or MLP)+action gradient for the tasks. Results show that the di usion model is more powerful than VAE and MLP for learning a distribution. This implies the di usion model is an expressive and  exible family to model a distribution, which is also consistent with the  eld of the generative model. 7.3.2 Comparison with Di erent Reverse Lengths Reverse length Kis an important parameter for the di usion model, which not only a ects the reward performance but also a ects the training time, we show the results in Figure 10 and Table 1. The results show that the reverse time K= 100 returns a better reward performance than other cases (except Hopper-v3 task). Longer reverse length consumes more reverse time for inference, we hope to use less time for reverse time for action inference. However, a short reverse length K= 20 decays the reward performance among (except Walker2d-v3 task), which implies a trade-o  between reward performance and reverse length K. In practice, we setK= 100 throughout this paper. We have formally built a theoretical foundation of di usion policy, which shows a policy representation via the di usion probability model and which is a new way to represent a policy via a stochastic process. Then, we have shown a convergence analysis for di usion policy, which provides a theory to understand di usion policy. Furthermore, we have proposed an implementation for model-free online RL with a di usion policy, named DIPO. Finally, extensive empirical results show the e ectiveness of DIPO among the Mujoco tasks. Anurag Ajay, Aviral Kumar, Pulkit Agrawal, Sergey Levine, and O r Nachum. Opal: O ine primitive discovery for accelerating o ine reinforcement learning. In International Conference on Learning Representations (ICLR) , 2021. (Cited on page 12.) Anurag Ajay, Yilun Du, Abhi Gupta, Joshua Tenenbaum, Tommi Jaakkola, and Pulkit Agrawal. Is conditional generative modeling all you need for decision-making? In International Conference on Learning Representations (ICLR) , 2023. (Cited on pages 3, 6, and 12.) Brian DO Anderson. Reverse-time di usion equation models. Stochastic Processes and their Applications , 12(3):313 326, 1982. (Cited on pages 7 and 26.) Brenna D Argall, Sonia Chernova, Manuela Veloso, and Brett Browning. A survey of robot learning from demonstration. Robotics and autonomous systems , 57(5):469 483, 2009. (Cited Dominique Bakry, Ivan Gentil, Michel Ledoux, et al. Analysis and geometry of Markov di usion operators , volume 103. Springer, 2014. (Cited on page 9.) 16Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, and Yoshua Bengio. Flow network based generative models for non-iterative diverse candidate generation. Advances in Neural Information Processing Systems (NeurIPS) , 34:27381 27394, 2021a. (Cited on pages 12 Yoshua Bengio, Salem Lahlou, Tristan Deleu, Edward J Hu, Mo Tiwari, and Emmanuel Bengio. G ownet foundations. arXiv preprint arXiv:2111.09266 , 2021b. (Cited on pages 12 and 13.) Johann Brehmer, Joey Bose, Pim De Haan, and Taco Cohen. Edgi: Equivariant di usion for planning with embodied agents. In Workshop on Reincarnating Reinforcement Learning at ICLR , 2023. (Cited on page 12.) Anthony Brohan, Noah Brown, Justice Carbajal, Yevgen Chebotar, Joseph Dabis, Chelsea Finn, Keerthana Gopalakrishnan, Karol Hausman, Alex Herzog, Jasmine Hsu, Julian Ibarz, Brian Ichter, Alex Irpan, Tomas Jackson, Sally Jesmonth, Nikhil Joshi, Ryan Julian, Dmitry Kalashnikov, Yuheng Kuang, Isabel Leal, Kuang-Huei Lee, Sergey Levine, Yao Lu, Utsav Malla, Deeksha Manjunath, Igor Mordatch, O r Nachum, Carolina Parada, Jodilyn Peralta, Emily Perez, Karl Pertsch, Jornell Quiambao, Kanishka Rao, Michael Ryoo, Grecia Salazar, Pannag Sanketi, Kevin Sayed, Jaspiar Singh, Sumedh Sontakke, Austin Stone, Clayton Tan, Huong Tran, Vincent Vanhoucke, Steve Vega, Quan Vuong, Fei Xia, Ted Xiao, Peng Xu, Sichun Xu, Tianhe Yu, and Brianna Zitkovich. Rt-1: Robotics transformer for real-world control at scale. In arXiv preprint arXiv:2212.06817 , 2022. (Cited on page 13.) Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners. Advances in neural information processing systems (NeurIPS) , 33: 1877 1901, 2020. (Cited on page 13.) Hongrui Chen, Holden Lee, and Jianfeng Lu. Improved analysis of score-based generative modeling: User-friendly bounds under minimal smoothness assumptions. arXiv preprint arXiv:2211.01916 , 2022. (Cited on page 29.) Huayu Chen, Cheng Lu, Chengyang Ying, Hang Su, and Jun Zhu. O ine reinforcement learning via high- delity generative behavior modeling. In International Conference on Learning Representations (ICLR) , 2023a. (Cited on pages 6 and 12.) Jinyin Chen, Shulong Hu, Haibin Zheng, Changyou Xing, and Guomin Zhang. Gail-pt: An intelligent penetration testing framework with generative adversarial imitation learning. Computers & Security , 126:103055, 2023b. (Cited on page 13.) Lili Chen, Kevin Lu, Aravind Rajeswaran, Kimin Lee, Aditya Grover, Misha Laskin, Pieter Abbeel, Aravind Srinivas, and Igor Mordatch. Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems(NeurIPS) , 34: 15084 15097, 2021. (Cited on page 13.) Sitan Chen, Sinho Chewi, Jerry Li, Yuanzhi Li, Adil Salim, and Anru R Zhang. Sampling is as easy as learning the score: theory for di usion models with minimal data assumptions. InInternational Conference on Learning Representations (ICLR) , 2023c. (Cited on page 9.) Cheng Chi, Siyuan Feng, Yilun Du, Zhenjia Xu, Eric Cousineau, Benjamin Burch el, and Shuran Song. Di usion policy: Visuomotor policy learning via action di usion. arXiv preprint arXiv:2303.04137 , 2023. (Cited on pages 3, 6, and 12.) 17Kai Lai Chung and Ruth J Williams. Introduction to stochastic integration , volume 2. Springer, 1990. (Cited on page 27.) Zichen Je  Cui, Yibin Wang, Nur Muhammad, Lerrel Pinto, et al. From play to policy: Conditional behavior generation from uncurated robot data. International Conference on Learning Representations (ICLR) , 2023. (Cited on page 13.) Ankur Deka, Changliu Liu, and Katia P Sycara. Arc-actor residual critic for adversarial imitation learning. In Conference on Robot Learning (CORL) , pages 1446 1456. PMLR, 2023. (Cited on page 13.) Monroe D Donsker and SR Srinivasa Varadhan. Asymptotic evaluation of certain markov process expectations for large time. iv. Communications on pure and applied mathematics , 36(2):183 212, 1983. (Cited on page 28.) Lasse Espeholt, Hubert Soyer, Remi Munos, Karen Simonyan, Vlad Mnih, Tom Ward, Yotam Doron, Vlad Firoiu, Tim Harley, Iain Dunning, et al. Impala: Scalable distributed deeprl with importance weighted actor-learner architectures. In International conference on machine learning (ICML) , pages 1407 1416. PMLR, 2018. (Cited on page 5.) Pete Florence, Corey Lynch, Andy Zeng, Oscar A Ramirez, Ayzaan Wahid, Laura Downs, Adrian Wong, Johnny Lee, Igor Mordatch, and Jonathan Tompson. Implicit behavioral cloning. In Conference on Robot Learning (CORL) , pages 158 168. PMLR, 2022. (Cited on Adriaan Dani  el Fokker. Die mittlere energie rotierender elektrischer dipole im strahlungsfeld. Annalen der Physik , 348(5):810 820, 1914. (Cited on page 27.) Justin Fu, Katie Luo, and Sergey Levine. Learning robust rewards with adverserial inverse reinforcement learning. In International Conference on Learning Representations (ICLR) , 2018. (Cited on page 13.) Scott Fujimoto, Herke Hoof, and David Meger. Addressing function approximation error in actor-critic methods. In International conference on machine learning (ICML) , pages 1587 1596. PMLR, 2018. (Cited on page 7.) Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial networks. Communications of the ACM , 63(11):139 144, 2020. (Cited on pages 12 and 13.) Tuomas Haarnoja, Haoran Tang, Pieter Abbeel, and Sergey Levine. Reinforcement learning with deep energy-based policies. In International conference on machine learning (ICML) , pages 1352 1361. PMLR, 2017. (Cited on pages 5, 7, and 50.) Tuomas Haarnoja, Vitchyr Pong, Aurick Zhou, Murtaza Dalal, Pieter Abbeel, and Sergey Levine. Composable deep reinforcement learning for robotic manipulation. In 2018 IEEE international conference on robotics and automation (ICRA) , pages 6244 6251. IEEE, 2018a. Tuomas Haarnoja, Aurick Zhou, Pieter Abbeel, and Sergey Levine. Soft actor-critic: O -policy maximum entropy deep reinforcement learning with a stochastic actor. In International conference on machine learning (ICML) , pages 1861 1870. PMLR, 2018b. (Cited on page 5.) 18Hado Hasselt. Double q-learning. Advances in Neural Information Processing Systems (NeurIPS) , 23, 2010. (Cited on page 56.) Ulrich G Haussmann and Etienne Pardoux. Time reversal of di usions. The Annals of Probability , pages 1188 1205, 1986. (Cited on page 26.) Jonathan Ho and Stefano Ermon. Generative adversarial imitation learning. Advances in neural information processing systems (NeurIPS) , 29, 2016. (Cited on page 13.) Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising di usion probabilistic models. Advances in Neural Information Processing Systems (NeurIPS) , 33:6840 6851, 2020. (Cited on pages 3, Aapo Hyv  arinen. Estimation of non-normalized statistical models by score matching. Journal of Machine Learning Research (JMLR) , 6(4), 2005. (Cited on page 10.) Michael Janner, Yilun Du, Joshua Tenenbaum, and Sergey Levine. Planning with di usion for exible behavior synthesis. In International Conference on Machine Learning (ICML) , 2022. (Cited on pages 6 and 12.) Ivan Kapelyukh, Vitalis Vosylius, and Edward Johns. Dall-e-bot: Introducing web-scale di usion models to robotics. In CoRL 2022 Workshop on Pre-training Robot Learning , 2022. Changyeon Kim, Jongjin Park, Jinwoo Shin, Honglak Lee, Pieter Abbeel, and Kimin Lee. Preference transformer: Modeling human preferences using transformers for rl. In The Eleventh International Conference on Learning Representations (ICLR) , 2023. (Cited on Dongjun Kim, Seungjae Shin, Kyungwoo Song, Wanmo Kang, and Il-Chul Moon. Soft truncation: A universal training technique of score-based di usion model for high precision score estimation. In Proceedings of the 39th International Conference on Machine Learning (ICML) , volume 162, pages 11201 11228, 2022. (Cited on page 26.) Diederik P Kingma and Max Welling. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 , 2013. (Cited on pages 12 and 13.) Andrei Kolmogoro .  Uber die analytischen methoden in der wahrscheinlichkeitsrechnung. Mathematische Annalen , 104:415 458, 1931. (Cited on page 27.) Sachin G Konan, Esmaeil Seraj, and Matthew Gombolay. Contrastive decision transformers. InConference on Robot Learning (CORL) , pages 2159 2169. PMLR, 2023. (Cited on page 13.) Yann LeCun, Sumit Chopra, Raia Hadsell, Marc Aurelio Ranzato, and Fu Jie Huang. A tutorial on energy-based learning. Predicting Structured Data , 1:0, 2006. (Cited on page 13.) Holden Lee, Jianfeng Lu, and Yixin Tan. Convergence of score-based generative modeling for general data distributions. In International Conference on Algorithmic Learning Theory (ALT) , pages 946 985. PMLR, 2023. (Cited on page 8.) Sergey Levine, Aviral Kumar, George Tucker, and Justin Fu. O ine reinforcement learning: Tutorial, review, and perspectives on open problems. arXiv preprint arXiv:2005.01643 , 2020. 19Wenhao Li, Baoxiang Wang, Shanchao Yang, and Hongyuan Zha. Diverse policy optimization for structured action space. In Proceedings of the 22th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS) , 2023a. (Cited on page 13.) Yinchuan Li, Shuang Luo, Haozhi Wang, and Hao Jianye. C ownets: Continuous control with generative  ow networks. In The Eleventh International Conference on Learning Representations (ICLR) , 2023b. (Cited on page 13.) Zhixuan Liang, Yao Mu, Mingyu Ding, Fei Ni, Masayoshi Tomizuka, and Ping Luo. Adaptdiffuser: Di usion models as adaptive self-evolving planners. arXiv preprint arXiv:2302.01877 , 2023. (Cited on page 12.) Timothy P Lillicrap, Jonathan J Hunt, Alexander Pritzel, Nicolas Heess, Tom Erez, Yuval Tassa, David Silver, and Daan Wierstra. Continuous control with deep reinforcement learning. In International Conference on Learning Representations (ICLR) , 2016. (Cited on Minghuan Liu, Tairan He, Minkai Xu, and Weinan Zhang. Energy-based imitation learning. InProceedings of the 20th International Conference on Autonomous Agents and MultiAgent Systems (AAMAS) , pages 809 817, 2021. (Cited on page 13.) Weiyu Liu, Tucker Hermans, Sonia Chernova, and Chris Paxton. Structdi usion: Objectcentric di usion for semantic rearrangement of novel objects. In Workshop on Language and Robotics at CoRL , 2022. (Cited on page 12.) Corey Lynch, Mohi Khansari, Ted Xiao, Vikash Kumar, Jonathan Tompson, Sergey Levine, and Pierre Sermanet. Learning latent plans from play. In Conference on robot learning (CORL) , pages 1113 1132. PMLR, 2020. (Cited on pages 12 and 13.) Yi-An Ma, Yuansi Chen, Chi Jin, Nicolas Flammarion, and Michael I Jordan. Sampling can be faster than optimization. Proceedings of the National Academy of Sciences , 116(42): 20881 20885, 2019. (Cited on page 9.) Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, and Yoshua Bengio. Trajectory balance: Improved credit assignment in g ownets. In Advances in Neural Information Processing Systems , 2022. (Cited on page 13.) Ajay Mandlekar, Danfei Xu, Roberto Mart   n-Mart   n, Silvio Savarese, and Li Fei-Fei. Learning to generalize across long-horizon tasks from human demonstrations. In Robotics: Science and Systems (RSS) , 2020. (Cited on page 12.) Oier Mees, Lukas Hermann, and Wolfram Burgard. What matters in language conditioned robotic imitation learning over unstructured data. IEEE Robotics and Automation Letters , 7(4):11205 11212, 2022. (Cited on page 13.) Utkarsh A Mishra and Yongxin Chen. Reorientdi : Di usion model based reorientation for object manipulation. arXiv preprint arXiv:2303.12700 , 2023. (Cited on page 12.) Diganta Misra. Mish: A self regularized non-monotonic activation function. arXiv preprint arXiv:1908.08681 , 2019. (Cited on page 56.) 20Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A Rusu, Joel Veness, Marc G Bellemare, Alex Graves, Martin Riedmiller, Andreas K Fidjeland, Georg Ostrovski, et al. Human-level control through deep reinforcement learning. nature , 518(7540):529 533, 2015. Takayuki Osa, Joni Pajarinen, Gerhard Neumann, J Andrew Bagnell, Pieter Abbeel, Jan Peters, et al. An algorithmic perspective on imitation learning. Foundations and Trends in Robotics , 7(1-2):1 179, 2018. (Cited on page 12.) Ling Pan, Dinghuai Zhang, Moksh Jain, Longbo Huang, and Yoshua Bengio. Stochastic generative  ow networks. arXiv preprint arXiv:2302.09465 , 2023. (Cited on page 13.) Tim Pearce, Tabish Rashid, Anssi Kanervisto, Dave Bignell, Mingfei Sun, Raluca Georgescu, Sergio Valcarcel Macua, Shan Zheng Tan, Ida Momennejad, Katja Hofmann, et al. Imitating human behaviour with di usion models. In International Conference on Learning Representations (ICLR) , 2023. (Cited on page 12.) Karl Pertsch, Youngwoon Lee, and Joseph Lim. Accelerating reinforcement learning with learned skill priors. In Conference on robot learning (CORL) , pages 188 204, 2021. (Cited on Jan Peters, Katharina Mulling, and Yasemin Altun. Relative entropy policy search. In Proceedings of the AAAI Conference on Arti cial Intelligence , volume 24, pages 1607 1612, 2010. (Cited on page 5.) VM Planck.  Uber einen satz der statistischen dynamik und seine erweiterung in der quantentheorie. Sitzungsberichte der Preussischen Akademie der Wissenschaften zu Berlin , 24: 324 341, 1917. (Cited on page 27.) Dean A Pomerleau. Alvinn: An autonomous land vehicle in a neural network. Advances in neural information processing systems (NeurIPS) , 1, 1988. (Cited on page 12.) Rafael Rafailov, Victor Kolev, Kyle Beltran Hatch, John D Martin, Mariano Phielipp, Jiajun Wu, and Chelsea Finn. Model-based adversarial imitation learning as online  ne-tuning. In Workshop on Reincarnating Reinforcement Learning at ICLR 2023 .(Cited on page 13.) Scott Reed, Konrad Zolna, Emilio Parisotto, Sergio G  omez Colmenarejo, Alexander Novikov, Gabriel Barth-maron, Mai Gim  enez, Yury Sulsky, Jackie Kay, Jost Tobias Springenberg, Tom Eccles, Jake Bruce, Ali Razavi, Ashley Edwards, Nicolas Heess, Yutian Chen, Raia Hadsell, Oriol Vinyals, Mahyar Bordbar, and Nando de Freitas. A generalist agent. Transactions on Machine Learning Research (TMLR) , 2022. ISSN 2835-8856. (Cited on page 13.) Moritz Reuss, Maximilian Li, Xiaogang Jia, and Rudolf Lioutikov. Goal-conditioned imitation learning using score-based di usion policies. arXiv preprint arXiv:2304.02532 , 2023. (Cited on pages 3, 6, and 12.) Danilo Rezende and Shakir Mohamed. Variational inference with normalizing  ows. In International conference on machine learning (ICML) , pages 1530 1538, 2015. (Cited on Hannes Risken and Hannes Risken. Fokker-planck equation . Springer, 1996. (Cited on page 27.) 21Erick Rosete-Beas, Oier Mees, Gabriel Kalweit, Joschka Boedecker, and Wolfram Burgard. Latent plans for task-agnostic o ine reinforcement learning. In Conference on Robot Learning (CORL) , pages 1838 1849, 2023. (Cited on page 13.) Gavin A Rummery and Mahesan Niranjan. On-line Q-learning using connectionist systems , volume 37. University of Cambridge, Department of Engineering, 1994. (Cited on page 5.) Brian Sallans and Geo rey E Hinton. Reinforcement learning with factored states and actions. The Journal of Machine Learning Research (JMLR) , 5:1063 1088, 2004. (Cited on page 5.) Simo S  arkk  a and Arno Solin. Applied stochastic di erential equations , volume 10. Cambridge University Press, 2019. (Cited on pages 26 and 27.) John Schulman, Sergey Levine, Pieter Abbeel, Michael Jordan, and Philipp Moritz. Trust region policy optimization. In International conference on machine learningn (ICML) , pages 1889 1897. PMLR, 2015. (Cited on page 5.) John Schulman, Xi Chen, and Pieter Abbeel. Equivalence between policy gradients and soft q-learning. arXiv preprint arXiv:1704.06440 , 2017a. (Cited on page 5.) John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization algorithms. arXiv preprint arXiv:1707.06347 , 2017b. (Cited on page 5.) Nur Muhammad Sha ullah, Zichen Cui, Ariuntuya Arty Altanzaya, and Lerrel Pinto. Behavior transformers: Cloning kmodes with one stone. Advances in neural information processing systems (NeurIPS) , 35:22955 22968, 2022. (Cited on page 13.) David Silver, Guy Lever, Nicolas Heess, Thomas Degris, Daan Wierstra, and Martin Riedmiller. Deterministic policy gradient algorithms. In International conference on machine learning (ICML) , pages 387 395. Pmlr, 2014. (Cited on page 5.) Avi Singh, Huihan Liu, Gaoyue Zhou, Albert Yu, Nicholas Rhinehart, and Sergey Levine. Parrot: Data-driven behavioral priors for reinforcement learning. In International Conference on Learning Representations (ICLR) , 2020. (Cited on page 13.) Jascha Sohl-Dickstein, Eric Weiss, Niru Maheswaranathan, and Surya Ganguli. Deep unsupervised learning using nonequilibrium thermodynamics. In International Conference on Machine Learning (ICML) , pages 2256 2265. PMLR, 2015. (Cited on pages 3 and 26.) Yang Song, Jascha Sohl-Dickstein, Diederik P Kingma, Abhishek Kumar, Stefano Ermon, and Ben Poole. Score-based generative modeling through stochastic di erential equations. In International Conference on Learning Representations (ICLR) , 2021. (Cited on pages 3, 26, Richard S Sutton. Learning to predict by the methods of temporal di erences. Machine learning , 3:9 44, 1988. (Cited on page 13.) Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 1998. (Cited on pages 5 and 13.) Richard S Sutton and Andrew G Barto. Reinforcement learning: An introduction . MIT press, 2018. (Cited on page 5.) 22Richard S Sutton, David McAllester, Satinder Singh, and Yishay Mansour. Policy gradient methods for reinforcement learning with function approximation. Advances in neural information processing systems (NeurIPS) , 12, 1999. (Cited on page 5.) Aleksandar Taranovic, Andras Gabor Kupcsik, Niklas Freymuth, and Gerhard Neumann. Adversarial imitation learning with preferences. In The Eleventh International Conference on Learning Representations (ICLR) , 2023. (Cited on page 13.) Emanuel Todorov, Tom Erez, and Yuval Tassa. Mujoco: A physics engine for model-based control. In 2012 IEEE/RSJ International Conference on Intelligent Robots and Systems , pages 5026 5033. IEEE, 2012. doi: 10.1109/IROS.2012.6386109. (Cited on page 13.) Julen Urain, Niklas Funk, Georgia Chalvatzaki, and Jan Peters. Se(3)-di usion elds: Learning cost functions for joint grasp and motion optimization through di usion. In 2018 IEEE international conference on robotics and automation (ICRA) , 2023. (Cited on page 12.) Aaron Van Den Oord, Oriol Vinyals, et al. Neural discrete representation learning. Advances in neural information processing systems (NeurIPS) , 30, 2017. (Cited on page 12.) Laurens Van der Maaten and Geo rey Hinton. Visualizing data using t-sne. Journal of machine learning research (JMLR) , 9(11), 2008. (Cited on pages 14 and 57.) Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. Advances in neural information processing systems (NeurIPS) , 30, 2017. (Cited on pages 13 and 56.) Santosh Vempala and Andre Wibisono. Rapid convergence of the unadjusted langevin algorithm: Isoperimetry su ces. In Advances in neural information processing systems (NeurIPS) , volume 32, 2019. (Cited on pages 9, 25, and 29.) Pascal Vincent. A connection between score matching and denoising autoencoders. Neural computation , 23(7):1661 1674, 2011. (Cited on page 10.) Lirui Wang, Xiangyun Meng, Yu Xiang, and Dieter Fox. Hierarchical policies for clutteredscene grasping with latent plans. IEEE Robotics and Automation Letters , 7(2):2883 2890, 2022. (Cited on page 13.) Yunke Wang, Chang Xu, Bo Du, and Honglak Lee. Learning to weight imperfect demonstrations. InInternational Conference on Machine Learning (ICML) , pages 10961 10970. PMLR, 2021. Zhendong Wang, Jonathan J Hunt, and Mingyuan Zhou. Di usion policies as an expressive policy class for o ine reinforcement learning. In International Conference on Learning Representations (ICLR) , 2023. (Cited on pages 6 and 12.) Christopher John Cornish Hellaby Watkins. Learning from delayed rewards . PhD thesis, King s College, Cambridge, 1989. (Cited on page 5.) Andre Wibisono and Varun Jog. Convexity of mutual information along the ornstein-uhlenbeck ow. In 2018 International Symposium on Information Theory and Its Applications (ISITA) , pages 55 59. IEEE, 2018. (Cited on page 9.) 23Andre Wibisono and Kaylee Yingxi Yang. Convergence in kl divergence of the inexact langevin algorithm with application to score-based generative models. arXiv preprint arXiv:2211.01512 , 2022. (Cited on pages 9, 28, and 44.) Ling Yang, Zhilong Zhang, Yang Song, Shenda Hong, Runsheng Xu, Yue Zhao, Yingxia Shao, Wentao Zhang, Bin Cui, and Ming-Hsuan Yang. Di usion models: A comprehensive survey of methods and applications. arXiv preprint arXiv:2209.00796 , 2022. (Cited on page 12.) Sherry Yang, O r Nachum, Yilun Du, Jason Wei, Pieter Abbeel, and Dale Schuurmans. Foundation models for decision making: Problems, methods, and opportunities. arXiv preprint arXiv:2303.04129 , 2023. (Cited on page 12.) Chen Yongxin Zhang, Qinsheng. Fast sampling of di usion models with exponential integrator. InInternational Conference on Learning Representations (ICLR) , 2022. (Cited on page 8.) Qinqing Zheng, Amy Zhang, and Aditya Grover. Online decision transformer. In International Conference on Machine Learning (ICML) , pages 27042 27059. PMLR, 2022. (Cited on page 13.) 24A Review on Notations This section reviews some notations and integration by parts formula, using the notations consistent with [Vempala and Wibisono, 2019]. Given a smooth function f:Rn R, itsgradient  f:Rn Rnis the vector of partial TheHessian  2f:Rn Rn nis the matrix of second partial derivatives: TheLaplacian  f:Rn Ris the trace of its Hessian: Given a smooth vector  eld p= (p1,...,pn):Rn Rn, itsdivergence isdiv p:Rn R: When the variable of a function is clear and without causing ambiguity, we also denote the above notation as follows, div (p(x)) =: ( div p)(x) =n In particular, the divergence of the gradient is the Laplacian: Let (x) and (x) be two smooth probability density functions on the space Rp, the Kullback Leibler (KL) divergence and relative Fisher information (FI) from  (x) to (x) are Before we further analyze, we need the integration by parts formula. For any function f:Rp Rand vector  eld v:Rp Rpwith su ciently fast decay at in nity, we have the following integration by parts formula: Rpf(x)(div v)(x)dx. (20) 25B Auxiliary Results B.1 Di usion Probability Model (DPM). This section reviews some basic background about the di usion probability model (DPM). For a given (but unknown) p-dimensional data distribution q(x0), DPM [Sohl-Dickstein et al., 2015, Ho et al., 2020, Song et al., 2021] is a latent variable generative model that learns a parametric model to approximate the distribution q(x0). To simplify the presentation in this section, we only focus on the continuous-time di usion [Song et al., 2021]. The mechanism of DPM contains two processes, forward process and reverse process ; we present them as follows. Forward Process . The forward process produces a sequence {xt}t=0:Tthat perturbs the initial x0 q( ) into a Gaussian noise, which follows the next stochastic di erential equation dxt=f(xt,t)dt+g(t)dwt, (21) where f( , ) is the drift term, g( ) is the di usion term, and wtis the standard Wiener process. Reverse Process. According to Anderson [1982], Haussmann and Pardoux [1986], there exists a corresponding reverse SDE that exactly coincides with the solution of the forward f(xt,t) g2(t) logpt(xt)] where d  tis the backward time di erential, d  wtis a standard Wiener process  owing backward in time, and pt(xt)is the marginal probability distribution of the random variable xtat timet. Once the score function  logpt(xt) is known for each time t, we can derive the reverse di usion process from SDE (22) and simulate it to sample from q(x0) [Song et al., 2021]. B.2 Transition Probability for Ornstein-Uhlenbeck Process Proposition B.1. Consider the next SDEs, where ( )andg( )are real-valued functions. Then, for a given x0, the conditional distribution ofxt|x0is a Gaussian distribution, i.e., Proof. See [Kim et al., 2022, A.1] or [S  arkk  a and Solin, 2019, Chapter 6.1]. 26B.3 Exponential Integrator Discretization The next Proposition B.2 provides a fundamental way for us to derivate the exponential integrator discretization (9). Proposition B.2. For a given state s, we consider the following continuous time process, for Then with It  o integration [Chung and Williams, 1990], xtk+ 2 S(xtk,s,T tk)) wheret [tk,tk+1], andtk=hk. Proof. See [S  arkk  a and Solin, 2019, Chapter 6.1]. Recall SDE (8), according to Proposition B.2, we know the next SDE at+ 2 S( atk,s,T tk)) 2dwt, t [tk,tk+1] (24) formulates the exponential integrator discretization as follows, atk+ 2 S( atk,s,T tk)) atk+ 2 S( atk,s,T tk)) where last equation holds due to Wiener process is a stationary process with independent increments, i.e., the following holds, Then, we rewrite (26) as follows, atk+ 2 S( atk,s,T tk)) which concludes the iteration de ned in (9). B.4 Fokker Planck Equation The Fokker Planck equation is named after Adriaan Fokker and Max Planck, who described it in 1914 and 1917 [Fokker, 1914, Planck, 1917]. It is also known as the Kolmogorov forward equation, after Andrey Kolmogorov, who independently discovered it in 1931 [Kolmogoro , 1931]. For more history and background about Fokker Planck equation, please refer to [Risken and Risken, 1996] or https://en.wikipedia.org/wiki/Fokker%E2%80%93Planck_ equation#cite_note-1 . 27For an It  o process driven by the standard Wiener process wtand described by the stochastic dxt= (xt,t)dt+ (xt,t)dwt, (27) where xtand (xt,t) areN-dimensional random vectors,  (xt,t) is ann mmatrix and wt is anm-dimensional standard Wiener process, the probability density p(x,t) forxtsatis es the Fokker Planck equation xi[ i(x,t)p(x,t)] +N xi xj[Dij(x,t)p(x,t)], (28) with drift vector  = ( 1,..., N) and di usion tensor D(x,t) =1 and ijdenotes the ( i,j)-th element of the matrix  . B.5 Donsker-Varadhan Representation for KL-divergence Proposition B.3 ([Donsker and Varadhan, 1983]) .Let , be two probability distributions on the measure space (X,F), whereX Rp. Then The Donsker-Varadhan representation for KL-divergence implies for any f( ), Rp (x) exp(f(x))dx, (29) which is useful later. B.6 Some Basic Results for Di usion Policy Proposition B.4. Let ( |s)satisfy -Log-Sobolev inequality (LSI) (see Assumption 4.2), the initial random action  a0 ( |s), and  atevolves according to the following Ornstein-Uhlenbeck Let at t( |s)be the evolution along the Ornstein-Uhlenbeck  ow (30), then  t( |s)is t-LSI, Proof. See [Wibisono and Yang, 2022, Lemma 6]. 28Proposition B.5. Under Assumption 4.1, then  log  t( |s)isLpet-Lipschitz on the time interval [0,T0], where the policy  t( |s)is the evolution along the  ow (4), and the time T0is Proof. [Chen et al., 2022, Lemma 13]. The positive scalar T0is well-de ned, i.e., T0always exists. In fact, let 1  e 2t e t then the following holds, Proposition B.6. ([Vempala and Wibisono, 2019, Lemma 10]) Let (x)be a probability distribution function on Rp, and letf(x) = log (x)be aL-smooth, i.e., there exists a positive constant Lsuch that LI 2f(x) LIfor all x Rp. Furthermore, let  (x) satisfy the LSI condition with constant   > 0, i.e., for any probability distribution  (x), 2 FI( ).Then for any distribution  (x), the following equation holds, 29C Implementation Details of DIPO In this section, we provide all the details of our implementation for DIPO. Algorithm 3: (DIPO): Model-Free Learning with Di usion Policy 1:Initialize parameter  , critic networks Q , target networks Q , lengthK; 5: #update experience with diffusion policy 6: datasetDenv ; initial state s0 d0( ); 8: initial  aK N(0,I); 10: zk N(0,I), ifk>1; else zk= 0; 13: at a0;st+1 P( |st,at);Denv D env {st,at,st+1,r(st+1|st,at)}; 15: #update value function 16: foreach mini-batch data do 17: sample mini-batch DfromDenvwith sizeN,D={sj,aj,sj+1,r(sj+1|sj,aj)}N 18: take gradient descent as follows r(sj+1|sj,aj) + Q (sj+1,aj+1) Q (sj,aj))2 20: #improve experience through action 22: replace the action at D envas follows 24: #update diffusion policy 26: sample a pair ( s,a) D envuniformly;k Uniform({1, ,K});z N(0,I); 27: take gradient descent as follows 29: soft update + (1 ) ; 30: soft update + (1 ) ; 31:until the policy performs well in the real environment. 30C.1 DIPO: Model-Free Learning with Di usion Policy Our source code follows the Algorithm 3. C.2 Loss Function of DIPO In this section, we provide the details of #update diffusion policy presented in Algorithm 3. We present the derivation of the loss of score matching (10) and present the details of updating the di usion from samples. First, the next Theorem C.1 shows an equivalent version of the loss de ned in (10), then we present the learning details from samples. C.2.1 Conditional Sampling Version of Score Matching Theorem C.1. For give a partition on the interval [0,T],0 =t0<t1< <tk<tk+1< <tK=T, let 0= e 2T, k= e2( tk+1+tk),and k 1= k k =0 k . Setting (t)according to the next (36), then the objective (10) follows the next expectation version, L( ) =Ek U([K]),zk N(0,I), a0 ( |s)[ zk ( k a0+ 1 kzk,s,k)] where [K] =:{1,2, ,K},U( )denotes uniform distribution, the parametic funciton ( , , ) :A S  [K] Rp shares the parameter  according to: ( , ,k) = 1 k s ( , ,T tk). Proof. According to (3), and Proposition B.1, we know  t( at| a0) =N( log t( at| a0) = at e t a0 1 e 2t, according to (3), we know zt= e t a0+ tzt, (33) Recall (10), we obtain 0 (t)E a0 ( |s)E at| a0[ s ( at,s,t) log t( at| a0) 2 T tE a0 ( |s)E aT t| a0[ T t s ( aT t,s,T t) +zT t 2 Furthermore, we de ne an indicator function It (t) as follows, 31Let the weighting function be de ned as follows, for any t [0,T], where we give a partition on the interval [0 ,T] as follows, 0 =t0<t1< <tk<tk+1< <tK=T. Then, we rewrite (34) as follows, k=1E a0 ( |s)E aT tk| a0[ T tk s ( aT tk,s,T tk) +zT tk 2 We consider the next term contained in (37) s ( aT tk,s,T tk)(33)= s ( e (T tk) a0+ T tkzT tk,s,T tk) 1 e 2(T tk)zT tk,s,T tk) where zT tk N(0,I), then obtain s ( aT tk,s,T tk) +zT tk 2 1 e 2(T tk)zT tk,s,T tk)] Now, we rewrite (37) as the next expectation version, L( ) =Ek U([K]),ztk N(0,I), a0 ( |s)[ 1 e 2(T tk)zT tk,s,T tk)] Fork= 0,1, ,K, and 0= e 2Tand k= e2( tk+1+tk). (39) With those notations, we rewrite (38) as follows, L( ) =Ek U([K]),zk N(0,I), a0 ( |s)[ 1 k s ( k a0+ 1 kzk,s,T tk) Finally, we de ne a function  ( , , ) :S A  [K] Rp, and =: 1 k s ( k a0+ 1 kzk,s,T tk) i,e. we estimate the score function via an estimator  as follows, s ( k a0+ 1 kzk,s,T tk) Then we rewrite (40) as follows, L( ) =Ek U([K]),zk N(0,I), a0 ( |s)[ zk ( k a0+ 1 kzk,s,k)] This concludes the proof. 32Algorithm 4: Di usion Policy (A Backward Version [Ho et al., 2020]) 1:input state s; parameter ; reverse length K; 3:initial  aK N(0,I); 5:zk N(0,I), ifk>1; else zk= 0; C.2.2 Learning from Samples According to the expectation version of loss (31), we know, for each pair ( s,a) sampled from experience memory, let k Uniform ({1, ,K}) and z N(0,I), the following empirical loss d( ) = z ( ka+ 1 kz,s,k) is a unbiased estimator of L( ) de ned in (31). Finally, we learn the parameter  by minimizing the empirical loss  d( ) according to gradient decent method: where is the step-size. For the implementation, see lines 25-28 in Algorithm 3. C.3 Playing Actions of DIPO In this section, we present all the details of #update experience with diffusion policy presented in Algorithm 3. Let k= 1 k, then according to Taylar formualtion, we know Recall the exponential integrator discretization (25), we know ( atk+ 2 s ( atk,s,T tk)) + ( atk+ 2 s ( atk,s,T tk)) + ( atk+ 2 s ( atk,s,T tk)) + (42)=1 k atk 2(1 k 1)1 1 k ( atk,s,k) + =1 k atk k k 1 1 k ( atk,s,k) + 33where ztk N(0,I), Eq.(44) holds since we use the fact (43), which implies To simplify the expression, we rewrite (44) as follows, ak+1=1 k ak k k 1 1 k ( ak,s,k) + ak 1 k 1 k ( ak,s,k)) wherek= 0,1, ,K 1 runs forward in time, zk N(0,I). The agent plays the last action Since we consider the SDE of the reverse process (4) that runs forward in time, while most di usion probability model literature (e.g., [Ho et al., 2020, Song et al., 2021]) consider the backward version for sampling. To coordinate the relationship between the two versions, we also present the backward version in Algorithm 4, which is essentially identical to the iteration (47) but rewritten in the running in backward time version. 34D Time Derivative of KL Divergence Between Difu usion Policy and True Reverse Process In this section, we provide the time derivative of KL divergence between di usion policy (Algorithm 1) and true reverse process (de ned in (4)). D.1 Time Derivative of KL Divergence at Reverse Time k= 0 In this section, we consider the case k= 0 of di usion policy (see Algorithm 1 or the iteration (9)). Ifk= 0, then for 0 t h, the SDE (8) is reduced as follows, where wtis the standard Wiener process starting at w0=0. Let the action  at t( |s) follows the process (48). The next Proposition D.1 considers the distribution di erence between the di usion policy  t( |s) and the true distribution of backward process (4)    t( |s) on the time interval t [0,h]. Proposition D.1. Under Assumption 4.1 and 4.2, let  t( |s)be the distribution at time t with the process (4), and let  t( |s)be the distribution at time twith the process (48). Let exp S(a,s,T hk) log  t(a|s) 2 and0 t h , then the following equation holds, 4KL ( t( |s) t( |s)) +5 Before we show the details of the proof, we need to de ne some notations, which is useful t( a|s) =:p( at= a|s,t) (52) denote the distribution of the action  at= abe played at time talong the process (48), where t [0,h]. For each t>0, let 0,t( a0, at|s) denote the joint distribution of (  a0, at) conditional on the state s, which can be written in terms of the conditionals and marginals as follows, 0|t( a0| at,s) = 0,t( a0, at|s) p( at= a|s,t)= 0,t( a0, at|s) D.2 Auxiliary Results For Reverse Time k= 0 Lemma D.2. Let t( a|s)be the distribution at time talong interpolation SDE (48), where t( a|s)is short for p( at= a|s,t), which is the distribution of the action  at= abe played at timetalongs the process (48) among the time t [0,h]. Then its derivation with respect to t t( a|s) = t( a|s)div ( a+ 2E a0 0|t( | a,s)[ S( a0,s,T) at= a]) 35Before we show the details of the proof, we need to clear the divergence term div. In this section, all the notation is de ned according to (17), and its value is at the point  a. For example, in Eq.(53), the divergence term divis de ned as follows, a+ 2E a0 0|t( | a,s)[ S( a0,s,T) at= a]) p(a) =a+ 2E a0 0|t( |a,s)[ S( a0,s,T) at=a] For example, in Eq.(58), the divergence term divis de ned as follows, Similar de nitions are parallel in Eq.(63), from Eq.(66) to Eq.(69). Proof. First, for a given state s, conditioning on the initial action  a0, we introduce a notation p( | a0,s,t) :Rp [0,1], (56) p( a| a0,s,t) =:p( at= a| a0,s,t) that denotes the conditional probability distribution starting from  a0to the action  at= aat timetunder the state s. Besides, we also know, t( |s) =E a0 N(0,I)[p( | a0,s,t)] = Rp 0( a0)p( | a0,s,t)d a0, (57) where 0( ) =N(0,I) is the initial action distribution for reverse process. For eacht>0, let 0,t( a0, at|s) denote the joint distribution of (  a0, at) conditional on the state s, which can be written in terms of the conditionals and marginals as follows, 0,t( a0, at|s) =p( a0|s) t|0( at| a0,s) =p( at|s) 0|t( a0| at,s). Then we obtain the Fokker Planck equation for the distribution p( | a0,s,t) as follows, tp( a| a0,s,t) = div ( +  p( a| a0,s,t), (58) where the divterm is de ned according to (16) and (17) if p(a) =p(a| a0,s,t)( Furthermore, according to (57), we know Rp 0( a0)p( a| a0,s,t)d a0= tp( a| a0,s,t)d a0 (59) = t( a|s)div a 2div ( t( a|s)E a0 0|t( | a,s)[ S( a0,s,T) at= a]) a+ 2E a0 0|t( | a,s)[ S( a0,s,T) at= a]) 36where Eq.(61) holds since: with the de nition of    t( a|s) =:p( a|s,t), we obtain d a0= t( a|s)div a; (63) t( a|s) =:p( at= a|s,t), (64) 0( a0)p( a| a0,s,t) =p( a, a0|s,t), Bayes  theorem p( a, a0|s,t) =p( a|s,t)p( a0| at= a,s,t) =  t( a|s)p( a0| at= a,s,t), (65) p( a| a0,s,t) S( a0,s,T)) p( a, a0|s,t) S( a0,s,T)) t( a|s)p( a0| at= a,s,t) S( a0,s,T)) Rpp( a0| at= a,s,t) S( a0,s,T)d a0) t( a|s)E a0 0|t( | a,s)[ S( a0,s,T) at= a]) where the last equation holds since Rpp( a0| at= a,s,t) S( a0,s,T)d a0=E a0 0|t( | a,s)[ Finally, consider (60) with (63) and (69), we conclude the Lemma D.2. We consider the time derivative of KL-divergence between the distribution  t( |s) and t( |s), and decompose it as follows. Lemma D.3. The time derivative of KL-divergence between the distribution  t( |s)and t( |s) can be decomposed as follows, Proof. We consider the time derivative of KL-divergence between the distribution  t( |s) and 37where the last equation holds since That concludes the proof. The relative entropy and relative Fisher information FI( Lemma D.4. The relative entropy and relative Fisher information FI( rewritten as the following identity, Proof. We consider the following identity, Rp( t(a|s) t(a|s) t(a|s) t(a|s) t(a|s), log  t(a|s) log  t(a|s), log t(a|s) t(a|s), log  t(a|s) log  t(a|s), log t(a|s) which concludes the proof. Lemma D.4 implies the following identity, which is useful later, t(a|s), log  t(a|s) Lemma D.5. The time derivative of KL-divergence between the distribution  t( |s)and t( |s) can be further decomposed as follows, t(a|s), S( a0,s,T) log  t(a|s) 38Proof. According to Lemma D.3, we need to consider the two terms in (72) correspondingly. First term in (72) .Recall Lemma D.2, we know t t(a|s) = t(a|s)div ( a+ 2E a0 0|t( |a,s)[ S( a0,s,T) at=a]) a+ 2E a0 0|t( |a,s)[ S( a0,s,T) at=a]) To short the expression, we de ne a notation gt( , ) :S A  Rpas follows, gt(s,a) =:a+ 2E a0 0|t( |a,s)[ S( a0,s,T) at=a] then we rewrite the distribution at time talong interpolation SDE (48) as follows, gt(s,a) t(a|s) + t(a|s)) We consider the  rst term in (72), according to integration by parts formula (20), we know gt(s,a) t(a|s) + t(a|s)) gt(s,a) t(a|s) t(a|s), log t(a|s) Second term in (72) .According to the Kolmogorov backward equation, we know t= div ( t(a|s)a) t(a|s) = div ( t(a|s), t(a|s)a+ t(a|s) Time derivative of KL-divergence .We consider the next identity, E a0 0|t( |a,s)[ S( a0,s,T) at=a] then according to (72), and with the results (76), (78), we obtain t(a|s),E a0 0|t( |a,s)[ S( a0,s,T) at=a] Furthermore, we consider t(a|s),E a0 0|t( |a,s)[ S( a0,s,T) at=a] t(a|s), S( a0,s,T) log  t(a|s) where Eq.(80) holds due to  0,t( a0, at|s) denotes the joint distribution of (  a0, at) conditional on the state s, which can be written in terms of the conditionals and marginals as follows, 0|t( a0| at,s) = 0,t( a0, at|s) pt( at|s)= 0,t( a0, at|s) and in Eq.(80), we denote  at=a. Finally, combining (79) and (80), we obtain the following equation, t(a|s), S( a0,s,T) log  t(a|s) which concludes the proof. Lemma D.6. The time derivative of KL-divergence between the distribution  t( |s)and t( |s) is bounded as follows, Rp 0,t( a0,a|s) S( a0,s,T) log  t(a|s) 2 Proof. First, we consider t(a|s), S( a0,s,T) log  t(a|s) 2 S( a0,s,T) log  t(a|s) 2 Rp 0,t( a0,a|s) S( a0,s,T) log  t(a|s) 2 where Eq.(85) holds since we consider  a,b 2 a 2+1 Then, according to Lemma D.5, we obtain t(a|s), S( a0,s,T) log  t(a|s) Rp 0,t( a0,a|s) S( a0,s,T) log  t(a|s) 2 which concludes the proof. Before we provide further analysis to show the boundedness of (87)., we need to consider SDE (8). Let h>0 be the step-size, assume K=T h N, andtk=:hk,k= 0,1, ,K. SDE (8) considers as follows, for t [hk,h (k+ 1)], at+ 2 S( atk,s,T tk)) Recall the SDE (88), in this section, we only consider k= 0, and we obtain the following where wtis the standard Wiener process starting at w0=0, andtis from 0 to h. Integration with (89), we obtain at= et a0+ 2(et 1) S( a0,s,T) + Lemma D.7. Under Assumption 4.1, for all 0 t 1 12Ls, then the following holds, Rp 0,t( a0,a|s) S( a0,s,T) log  t(a|s) 2 Rp t(a|s)( S(a,s,T) log  t(a|s) 2 where  atupdated according to (91). Proof. See Section F.2. 41D.3 Proof for Result at Reverse Time k= 0 Proof. According to the de nition of di usion policy, we know  t( |s) = T t( |s). Then according to Proposition B.4, we know    t( |s) is T t-LSI, where Since we consider the time-step 0  t T, then + (1 )e 2(T t) 1, t [0,T]. (92) According to Proposition B.5, we know under Assumption 4.1,  log  t( |s) isLpet-Lipschitz on the time interval [0 ,T0], where Then according to Proposition B.6, we obtain Rp t(a|s) log  t(a|s) 2 T tKL ( t( |s) t( |s)) + 2pLpet. (93) Furthermore, according to Donsker-Varadhan representation (see Section B.5), let f(a) =: t S(a,s,T) log  t(a|s) 2 the positive constant  twill be special later, see Eq.(98). With the result (29), we know Rp t(a|s) exp(f(a))da, Rp t(a|s) S(a,s,T) log  t(a|s) 2 Rp t(a|s) exp( S(a,s,T) log  t(a|s) 2 exp S(a,s,T) log  t(a|s) 2 Finally, according to Lemma D.6-D.7, Eq.(93)-(94), we obtain Rp 0,t( a0,a|s) S( a0,s,T) log  t(a|s) 2 Rp t(a|s) log  t(a|s) 2 Rp t(a|s) S(a,s,T) log  t(a|s) 2 T tKL ( t( |s) t( |s)) + 2pLpet) exp S(a,s,T) log  t(a|s) 2 exp S(a,s,T) log  t(a|s) 2 KL ( t( |s) t( |s)) +576t2L2 due to Assumption 4.2 KL ( t( |s) t( |s)) +576t2L2 4KL ( t( |s) t( |s)) +576t2L2 4KL ( t( |s) t( |s)) +5 (k,t) [K] [kh,(k+1)h]{ exp S(a,s,T hk) log  t(a|s) 2 Eq.(95) holds since we set  tas follows, we set 576 t2L2 where Eq.(96) holds since Now, we consider the time-step tkeeps the constant  tpositive, it is su cient to consider the next condition due to the property (92), 43Formally, we de ne a notation Then with result of Eq.(100), if 0  t h , we rewrite Eq.(96) as follows, 4KL ( t( |s) t( |s)) +5 4KL ( t( |s) t( |s)) + 12pLs exp S(a,s,T hk) log  t(a|s) 2 which concludes the proof. Remark D.8. The result of Proposition B.5 only depends on Assumption 4.1, thus the result (93) does not depend on additional assumption of the uniform L-smooth of log  ton the time interval [0,T], e.g., Wibisono and Yang [2022]. Instead of the uniform L-smooth of log  t, we consider the Lpet-Lipschitz on the time interval [0,T0], which is one of the di erence between our proof and [Wibisono and Yang, 2022]. Although we obtain a similar convergence rate from the view of Langevin-based algorithms, we need a weak condition. D.4 Proof for Result at Arbitrary Reverse Time k Proposition D.9. Under Assumption 4.1 and 4.2. Let  k( |s)be the distribution at the time t=hkalong the process (4) that starts from  0( |s) = T( |s), then  k( |s) = T hk( |s). Let k( |s)be the distribution of the iteration (9) at the k-the timetk=hk, starting from 0( |s) =N(0,I). Let 0<h , then for all k= 0,1, ,K 1, where is de ned in (102). Proof. Recall Proposition D.1, we know for any 0  t h , the following holds 4KL ( t( |s) t( |s)) +5 where comparing to (51), we use the condition t h. We rewrite (104) as follows, Then, on the interval [0 ,h], we obtain Furthermore, we obtain where last equation holds since we use 1  e x x, ifx 0. Recall  k( |s) is the distribution at the time t=hkalong the process (4) that starts from 0( |s) =  T( |s), then  k( |s) =  T hk( |s). Recall  k( |s) is the distribution of the iteration (9) at the k-the timetk=hk, starting from  0( |s) =N(0,I). According to (105), we rename the  0( |s) with  k( |s), h( |s) with  k+1( |s), 0( |s) with k( |s) and  h( |s) with  k+1( |s), then we obtain which concludes the result. E Proof of Theorem 4.3 Theorem 4.3 (Finite-time Analysis of Di usion Policy). For a given state s, let{ t( |s)}t=0:T and{ t( |s)}t=0:Tbe the distributions along the Ornstein-Uhlenbeck  ow (2) and (4) correspondingly, where { t( |s)}t=0:Tstarts at  0( |s) = ( |s)and{ t( |s)}t=0:Tstarts at  0( |s) = T( |s). Let  k( |s)be the distribution of the exponential integrator discretization iteration (9) at thek-the timetk=hk, i.e.,  atk k( |s)denotes the distribution of the di usion policy (see Algorithms 1) at the time tk=hk. Let{ k( |s)}k=0:Kbe starting at  0( |s) =N(0,I), under Assumption 4.1 and 4.2, let the reverse length Ksatisfy Then the KL-divergence between the di usion policy  aK K( |s)and input policy  ( |s)is upper-bounded as follows, convergence of forward process+ 64pLs errors from discretization exp S(a,s,T hk) log  t(a|s) 2 errors from score matching. 45Proof. Recall  k( |s) =  T hk( |s), then we know K( |s) =  T hK( |s) =  0( |s) = ( |s), (106) then according to Proposition D.9, we know K( |s) ( |s))(106)= KL( where Eq.(107) holds since we consider the and we set the step-size hsatis es the next condition: Let ( ) be standard Gaussian distribution on Rp, i.e., ( ) N (0,I), then we obtain the following result: for a given state s, dtKL ( ( ) t( |s)) =d Fokker Planck Equation t(a|s), t(a|s) log t(a|s) da Integration by Parts t(a|s), t(a|s) log t(a|s) 2 tKL ( ( ) t( |s)) Assumption 4.2 and Proposition B.4 + (1 )e 2tKL ( ( ) t( |s)) 2 KL ( ( ) t( |s)), (110) where the last equation holds since e t 1 witht 0. dtlog KL ( ( ) t( |s)) 2 , integrating both sides of above equation on the interval [0 ,T], we obtain KL ( ( ) T( |s)) e 2 TKL ( ( ) 0( |s)). (111) According to de nition of di usion policy, since:  a0 N(0,I), and  0( |s)(5)=  T( |s), then we 0( |s) 0( |s))(112)= KL( e 2 TKL ( ( ) 0( |s)). (113) Combining (108) and (113), we obtain 4 hK TKL ( ( ) 0( |s)) +20 Recall the following conditions (101), (102), and (109) on the step-size h, which implies the reverse length Ksatisfy the following condition Finally, recall the de nition of  (97), we rewrite (114) as follows exp S(a,s,T hk) log  t(a|s) 2 which concludes the proof. 47F Additional Details F.1 Proof of Lemma F.1 Lemma F.1. Under Assumption 4.1, for all 0 t T, ift 1 12Ls, then for any given state a0,s,t ) 3Lst a0 + 6Lst S( where  atupdated according to (91). Proof. (of Lemma F.1). First, we consider = (et 1) a0+ 2(et 1) S( a0,s,t ) + 2+ 4Lst S( a0,s,t ) + 2Ls where the last equation holds due to et 1 2t. Furthermore, we consider the case with t 1 12Ls, then we obtain the boundedness of the at,s,t ) + 2Lst a0 + 4Lst S( a0,s,t ) + 2Ls at,s,t ) + 2Lst a0 +1 at,s,t ) + 3Lst a0 2 Taking Eq.(117) into Eq.(116), and with t 1 a0,s,t ) 3Lst a0 + 6Lst S( which concludes the proof of Lemma F.1. 48F.2 Proof of Lemma D.7 Proof. (Lemma D.7) Recall the update rule of  at(91), at= et a0+ 2(et 1) S( a0,s,T) + To simplify the expression, in this section, we introduce the following notation z z( ),where z( ) =N(0,I). (119) According to the de nition of  0,t( a0, at|s) (81), we denote  at=a, then we know, Rp 0,t( a0,a|s) S( a0,s,T) log  t(a|s) 2 Rp 0,t( a0,a|s)( S( a0,s,T) S( at,s,T) 2 2+ S( at,s,T) log  t(a|s) 2 Rp 0,t( a0,a|s)( S( a0,s,T) S(a,s,T) 2 2+ S(a,s,T) log  t(a|s) 2 Recall Lemma F.1, we know Rp 0,t( a0,a|s) S( a0,s,T) S(a,s,T) 2 Rp 0,t( a0,a|s) z(z)( Rp 0,t( a0,a|s) z(z)( Rp 0,t( a0,a|s) z(z) z 2 Rp t(a|s)( S(a,s,T) log  t(a|s) 2 where the  rst term in Eq.(120) holds since: Rp 0,t( a0,a|s) z(z)dadz=  0( a0|s); the second term in Eq.(120) holds since: Rp 0,t( a0,a|s) z(z)d a0dz=  t(a|s); 49the third term in Eq.(120) holds since: z N(0,I), then z 2 2 2(p)-distribution with p degrees of freedom, then Rp 0,t( a0,a|s) z(z) z 2 Eq.(121) holds with the same analysis of (123), since  a0 N(0,I), then a0 2 Eq.(122) holds since we use the fact:  + 2 G Details and Discussions for multimodal Experiments In this section, we present all the implementation details and the plots of both 2D and 3D Visualization. Then we provide additional discussions for empirical results of the task of the multimodal environment in Section 3.2. G.1 Multimodal Environment In this section, we clarify the task and reward of the multimodal environment. We design a simple  multi-goal  environment according to the Didactic Example [Haarnoja et al., 2017], in which the agent is a 2D point mass on the 7  7 plane, and the agent tries to reach one of four points (0 ,5), (0, 5), (5,0) and ( 5,0) symmetrically placed goals. The reward is de ned according to the following three parts: 2if agent plays the action a; r2= min{ (x,y) target 2 2},target denotes one of the target points (0 ,5), (0, 5), if the agent reaches one of the targets among {(0,5),(0, 5),(5,0),( 5,0)}, then it receives a reward r3= 10. Since the goal positions are symmetrically distributed at the four points (0 ,5), (0, 5), (5,0) and ( 5,0), a reasonable policy should be able to take actions uniformly to those four goal positions with the same probability, which characters the capacity of exploration of a policy to understand the environment. Furthermore, we know that the shape of the reward curve should be symmetrical with four equal peaks. 0.000.020.04(a) 1E3 iterations 0246Value Function 300 0 (j) 10E3 iterations Figure 11: Policy representation comparison of di usion policy with di erent iterations. G.2 Plots Details of Visualization This section presents all the details of the 2D and 3D visualization for the multi-goal task. At the end of this section, we present the shape of the reward curve. G.2.1 2D Visualization For the 2D visualization, the red arrowheads denote actions learned by the corresponding RL algorithms, where each action starts at one of the totals of 7  7 = 49 points (corresponding to all the states) with horizontal and vertical coordinates ranges among { 3, 2, 1,0,1,2,3} { 3, 2, 1,0,1,2,3}. The length of the red arrowheads denotes the length of the action vector, and the direction of the red arrowheads denotes the direction of actions. This is to say; for each  gure, we plot all the actions starting from the same coordinate points. 0.0(a) 1E3 iterations 0246Value Function400 0 (j) 10E3 iterations Figure 12: Policy representation comparison of SAC with di erent iterations. G.2.2 3D Visualization For the 3D visualization, we provide a decomposition of the the region [  7,7] [ 7,7] into 100 100 = 10000 points, each point ( x,y) [ 7,7] [ 7,7] denotes a state. For each state (x,y), a corresponding action is learned by its corresponding RL algorithms, denoted as a. Then according to the critic neural network, we obtain the state-action value function Qvalue of the corresponding point (( x,y),a). The 3D visualization shows the state-action Q(for PPO, is value function V) with respect to the states. G.2.3 Shape of Reward Curve Since the shape of the reward curve is symmetrical with four equal peaks, the 2D visualization presents the distribution of actions toward those four equal peaks. A good algorithm should take actions with a uniform distribution toward those four points (0 ,5), (0, 5), (5,0), and ( 5,0) on the 2D visualization. The 3D visualization presents the learned shape according to the algorithm during the learning process. A good algorithm should  t the symmetrical reward 0246Value Function150 0246Value Function300 0 (j) 10E3 iterations Figure 13: Policy representation comparison of TD3 with di erent iterations. shape with four equal peaks. A multimodal policy distribution is e cient for exploration, which may lead an agent to learn a good policy and perform better. Thus, both 2D and 3D visualizations character the algorithm s capacity to represent the multimodal policy We have shown all the results in Figure 11 (for di usion policy), 12 (for SAC), 13 (for TD3) and 14 (for PPO), where we train the policy with a total 10000 iterations, and show the 2D and 3D visualization every 1000 iteration. Figure 11 shows that the di usion policy accurately captures a multimodal distribution landscape of reward, while from Figure 12, 13, and 14, we know that both SAC, TD3, and PPO are not well suited to capture such multimodality. Comparing Figure 11 to Figure 12 and 13, we know that although SAC and TD3 share a similar best reward performance, where both di usion policy and SAC and TD3 keep the highest reward around  20, di usion policy matches the real environment and performance shape. Figure 14: Policy representation comparison of PPO with di erent iterations. From Figure 14, we also  nd PPO always runs around at the initial value, and it does not improve the reward performance, which implies PPO fails to  t multimodality. It does not learn any information about multimodality. From the distributions of action directions and lengths, we also know the di usion policy keeps a more gradual and steady action size than the SAC, TD3, and PPO to learn the multimodal reward performance. Thus, the di usion model is a powerful policy representation that leads to a more su cient exploration and better performance, which is our motivation to consider representing policy via the di usion model. 54H Additional Experiments In this section, we provide additional details about the experiments, including Hyper-parameters of all the algorithms; additional tricks for implementation of DIPO; details and additional reports for state-visiting; and ablation study on MLP and VAE. The Python code for our implementation of DIPO is provided along with this submission in the supplementary material. SAC: https://github.com/toshikwa/soft-actor-critic. pytorch PPO: https://github.com/ikostrikov/pytorch-a2c-ppo-acktr-gail TD3: https: //github.com/sfujim/TD3 , which were o cial code library. H.1 Hyper-parameters for MuJoCo Common Hyper-parameters: Hyperparameter DIPO SAC TD3 PPO No. of hidden layers 2 2 2 2 No. of hidden nodes 256 256 256 256 Activation mish relu relu tanh Batch size 256 256 256 256 Discount for reward   0.99 0.99 0.99 0.99 Target smoothing coe cient  0.005 0.005 0.005 0.005 Learning rate for actor 3  10 43 10 43 10 47 10 4 Learning rate for critic 3  10 43 10 43 10 47 10 4 Actor Critic grad norm 2 N/A N/A 0.5 Memeroy size 1  1061 1061 1061 106 Entropy coe cient N/A 0.2 N/A 0.01 Value loss coe cient N/A N/A N/A 0.5 Exploration noise N/A N/A N(0,0.1) N/A Policy noise N/A N/A N(0,0.2) N/A Noise clip N/A N/A 0.5 N/A Use gae N/A N/A N/A True Table 2: Hyper-parameters for algorithms. Additional Hyper-parameters of DIPO: Hyperparameter Hopper-v3 Walker2d-v3 Ant-v3 HalfCheetah-v3 Humanoid-v3 Learning rate for action 0.03 0.03 0.03 0.03 0.03 Actor Critic grad norm 1 2 0.8 2 2 Action grad norm ratio 0.3 0.08 0.1 0.08 0.1 Action gradient steps 20 20 20 40 20 Di usion inference timesteps 100 100 100 100 100 Di usion beta schedule cosine cosine cosine cosine cosine Update actor target every 1 1 1 2 1 Table 3: Hyper-parameters of DIPO. 55DIPO SAC TD3 PPOFigure 15: State-visiting distribution of Humanoid-v3, where states get dimension reduction by t-SNE. The points with di erent colors represent the states visited by the policy with the style. The distance between points represents the di erence between states. H.2 Additional Tricks for Implementation of DIPO We have provided the additional details for the Algorithm 3. H.2.1 Double Q-learning for Estimating Q-Value We consider the double Q-learning [Hasselt, 2010] to update the Qvalue. We consider the two critic networks Q 1,Q 2, two target networks Q 2. Let Bellman residual be as follows, LQ( ) =E(st,at,st+1,at+1)[ ( Then, we update  ias follows, for i {1,2} Furthermore, we consider the following soft update rule for Finally, for the action gradient step, we consider the following update rule: replacing each action at D envas follows H.2.2 Critic and Di usion Model We use a four-layer feedforward neural network of 256 hidden nodes, with activation function Mish [Misra, 2019] between each layer, to design the two critic networks Q 1,Q 2two target 2, and the noise term  . We consider gradient normalization for critic and to stabilize the training process. For each reverse time k [K], we consider the sinusoidal positional encoding [Vaswani et al., 2017] to encode each k [K] into a 32-dimensional vector. 56DIPO SAC TD3 PPOFigure 16: State-visiting distribution of Walker2d-v3, where states get dimension reduction by t-SNE. The points with di erent colors represent the states visited by the policy with the style. The distance between points represents the di erence between states. Ant-v3 DIPO SAC TD3 PPO Figure 17: State-visiting visualization by each algorithm on the Ant-v3 task, where states get dimension reduction by t-SNE. The points with di erent colors represent the states visited by the policy with the style. The distance between points represents the di erence between H.3 Details and Additional Reports for State-Visiting In this section, we provide more details for Section 7.1, including the implementation details (see Appendix H.3.1), more comparisons and more insights for the empirical results. We provide the main discussions cover the following three observations: poor exploration results in poor initial reward performance; good  nal reward performance along with dense state-visiting; a counterexample: PPO violates the above two observations. H.3.1 Implementation Details for 2D State-Visiting We save the parameters for each algorithm during the training for each 1E5 iteration. Then we run the model with an episode with ten random seeds to compare fairly; those ten random seeds are the same among di erent algorithms. Thus, we collect a state set with ten episodes for each algorithm. Finally, we convert high-dimensional state data into two-dimensional state data by t-SNE [Van der Maaten and Hinton, 2008], and we show the visualization according to the open implementation https://scikit-learn.org/stable/auto_examples/manifold/ plot_t_sne_perplexity.html where we set the parameters as follows, perpexity = 50,early exaggeration = 12,random state = 33. We have shown all the results in Figure 15 (for Humanoid); Figure 16 (for Walker2d); Figure 17 (for Ant); Figure 18 (for HalfCheetah); and Figure 19 (for Hopper), where we polt the result after each E5 iterations. 57DIPO SAC TD3 PPOFigure 18: The state-visiting visualization by each algorithm on the HalfCheetah-v3 task, where states get dimension reduction by t-SNE. The points with di erent colors represent the states visited by the policy with the style. The distance between points represents the di erence between states. Figure 19: State-visiting distribution of Hopper-v3, where states get dimension reduction by t-SNE. The points with di erent colors represent the states visited by the policy with the style. The distance between points represents the di erence between states. H.3.2 Observation 1: Poor Exploration Result in Poor Initial Reward Performance From Figure 6, we know TD3 and PPO reach a worse initial reward performance than DIPO and SAC for the Hopper task, which coincides with the results appear in Figure 19. At the initial interaction, TD3 and PPO explore within a very sparse state-visiting region, which decays the reward performance. Such an empirical result also appears in the Walker2d task for PPO (see Figure 16), Humanoid task for TD3 and SAC (see Figure 15), where a spare state-visiting is always accompanied by a worse initial reward performance. Those empirical results once again con rm a common sense: poor exploration results in poor initial reward Conversely, from Figure 6, we know DIPO and SAC obtain a better initial reward performance for the Hopper task, and Figure 19 shows that DIPO and SAC explore a wider range of state-visiting that covers than TD3 and PPO. That implies that a wide state visit leads to better initial reward performance. Such an empirical result also appears in the Walker2d task for DIPO, SAC, and TD3 (see Figure 16), Humanoid task for DIPO (see Figure 15), where the agent runs with a wider range state-visiting, which is helpful to the agent obtains a better initial reward performance. In summary, poor exploration could make the agent make a poor decision and cause a poor initial reward performance. While if the agent explores a wider range of regions to visit more states, which is helpful for the agent to understand the environment and could lead to better initial reward performance. 58/uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013 /uni00000033/uni00000033/uni00000032 /uni00000036/uni00000024/uni00000026 /uni00000037/uni00000027/uni00000016 /uni00000030/uni0000002f/uni00000033 /uni00000039/uni00000024/uni00000028 /uni00000027/uni0000002c/uni00000033/uni00000032(a) Ant-v3 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000019/uni00000013/uni00000013/uni00000013/uni0000001b/uni00000013/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000013/uni00000014/uni00000015/uni00000013/uni00000013/uni00000013 /uni00000033/uni00000033/uni00000032 /uni00000036/uni00000024/uni00000026 /uni00000037/uni00000027/uni00000016 /uni00000030/uni0000002f/uni00000033 /uni00000039/uni00000024/uni00000028 /uni00000027/uni0000002c/uni00000033/uni00000032 (b) HalfCheetah-v3 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000014/uni00000018/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000015/uni00000018/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000016/uni00000018/uni00000013/uni00000013 /uni00000033/uni00000033/uni00000032 /uni00000036/uni00000024/uni00000026 /uni00000037/uni00000027/uni00000016 /uni00000030/uni0000002f/uni00000033 /uni00000039/uni00000024/uni00000028 /uni00000027/uni0000002c/uni00000033/uni00000032 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013/uni00000033/uni00000033/uni00000032 /uni00000036/uni00000024/uni00000026 /uni00000037/uni00000027/uni00000016 /uni00000030/uni0000002f/uni00000033 /uni00000039/uni00000024/uni00000028 /uni00000027/uni0000002c/uni00000033/uni00000032 (d) Humanoid-v3 /uni00000013/uni00000011/uni00000013 /uni00000013/uni00000011/uni00000015 /uni00000013/uni00000011/uni00000017 /uni00000013/uni00000011/uni00000019 /uni00000013/uni00000011/uni0000001b /uni00000014/uni00000011/uni00000013/uni00000013/uni00000014/uni00000013/uni00000013/uni00000013/uni00000015/uni00000013/uni00000013/uni00000013/uni00000016/uni00000013/uni00000013/uni00000013/uni00000017/uni00000013/uni00000013/uni00000013/uni00000018/uni00000013/uni00000013/uni00000013 /uni00000033/uni00000033/uni00000032 /uni00000036/uni00000024/uni00000026 /uni00000037/uni00000027/uni00000016 /uni00000030/uni0000002f/uni00000033 /uni00000039/uni00000024/uni00000028 /uni00000027/uni0000002c/uni00000033/uni00000032 (e) Walker2d-v3 Figure 20: Average performances on MuJoCo Gym environments with  std shaded, where the horizontal axis of coordinate denotes the iterations (  106), the plots smoothed with a H.3.3 Observation 2: Good Final Reward Performance along with Dense StateVisiting From Figure 19, we know DIPO, SAC, and TD3 achieve a more dense state-visiting for the Hopper task at the  nal iterations. Such an empirical result also appears in the Walker2d and Humanoid tasks for DIPO, SAC, and TD3 (see Figure 15 and 16). This is a reasonable result since after su cient training, the agent identi es and avoids the  bad  states, and plays actions to transfer to  good  states. Besides, this observation is also consistent with the result that appears in Figure 6, the better algorithm (e.g., the proposed DIPO) usually visits a more narrow and dense state region at the  nal iterations. On the contrary, PPO shows an aimless exploration among the Ant-v3 task (see Figure 7) and HalfCheetah (see Figure 8), which provides a partial explanation for why PPO is not so good in the Ant-v3 and HalfCheetah task. This is a natural result for RL since a better algorithm should keep a better exploration at the beginning and a more su cient exploitation at the  nal iterations. H.3.4 Observation 3: PPO Violates above Two Observations From all of those 5 tasks (see Figure 15 to 19), we also  nd PPO violates the common sense of RL, where PPO usual with a narrow state-visiting at the beginning and wide state-visiting at the  nal iteration. For example, from Figure 6 and 19, we know PPO achieves an asymptotic reward performance as DIPO for the Hopper-v3, while the state-visiting distribution of PPO is fundamentally di erent from DIPO. DIPO shows a wide state-visiting region gradually turns into a narrow state-visiting region, while PPO shows a narrow state-visiting region gradually turns into a wide state-visiting region. We show the fair visualization with t-SNE by the same setting for all of those 5 tasks, the abnormal empirical results show that PPO may  nd some 59new views di erent from DIPO/TD3/SAC to understand the environment. H.4 Ablation Study on MLP and VAE A fundamental question is why must we consider the di usion model to learn a policy distribution. In fact, Both VAE and MLP are widely used to learn distribution in machine learning, can we replace the di usion model with VAE and MLP in DIPO? In this section, we further analyze the empirical reward performance among DIPO, MLP, and VAE. We show the answer in Figure 9 and Figure 20, where the VAE (or MLP) is the result we replace the di usion policy of DIPO (see Figure 3) with VAE (or MLP), i.e., we consider VAE (or MLP)+action gradient (15) for the tasks. Results of Figure 20 show that the di usion model achieves the best reward performance among all 5 tasks. This implies the di usion model is an expressive and  exible family to model a distribution, which is also consistent with the  eld of the generative model. Additionally, from the results of Figure 20 we know MLP with action gradient also performs well among all 5 tasks, which implies the action gradient is a very promising way to improve reward performance. For example, Humanoid-v3 is the most challenging task among Mujoco tasks, MLP achieves a  nal reward performance near the PPO, SAC, DIPO, and TD3. We all know that these algorithms (PPO, SAC, DIPO, and TD3) are meticulously constructed mathematically, while MLP with action gradient is a simple model, but it achieves so good reward performance, which is a direction worth further in-depth research to search simple but e cient RL algorithm.