MULTI-CHANNEL TARGET SPEAKER EXTRACTION WITH REFINEMENT 
THE WA VLAB SUBMISSION TO THE SECOND CLARITY ENHANCEMENT CHALLENGE
Samuele Cornell1,2, Zhong-Qiu Wang2, Yoshiki Masuyama3,2, Shinji Watanabe2
Manuel Pariente4, Nobutaka Ono3
1Universit  a Politecnica delle Marche, Italy2Carnegie Mellon University, USA
3Tokyo Metropolitan University, Japan4Pulse Audition, France
{cornellsamuele, wang.zhongqiu41 }@gmail.com
ABSTRACT
This paper describes our submission to the Second Clarity Enhancement
 Challenge (CEC2), which consists of target speech enhancement
 for hearing-aid (HA) devices in noisy-reverberant environments
with multiple interferers such as music and competing speakers. Our
approach builds upon the powerful iterative neural/beamforming enhancement
 (iNeuBe) framework introduced in our recent work, and
this paper extends it for target speaker extraction. We therefore name
the proposed approach as iNeuBe-X, where the  X  stands for extraction.
 To address the challenges encountered in the CEC2 setting,
we introduce four major novelties  (1) we extend the state-of-theart
 TF-GridNet model [1], originally designed for monaural speaker
separation, for multi-channel, causal speech enhancement, and large
improvements are observed by replacing the TCNDenseNet used in
iNeuBe [2] with this new architecture  (2) we leverage a recent dual
window size approach with future-frame prediction [3] to ensure that
iNueBe-X satisï¬es the 5ms constraint on algorithmic latency required
by CEC2  (3) we introduce a novel speaker-conditioning branch for
TF-GridNet to achieve target speaker extraction  and (4) we propose
a ï¬ne-tuning step, where we compute an additional loss with respect
to the target speaker signal compensated with the listener audiogram.
Without using external data, on the ofï¬cial development set our best
model reaches a hearing-aid speech perception index (HASPI) score
of0.942and a scale-invariant signal-to-distortion ratio improvement
(SI-SDRi) of 18.8dB. These results are promising given the fact that
the CEC2 data is extremely challenging (e.g., on the development set
the mixture SI-SDR is  12.3dB). A demo of our submitted system
is available at WA VLab CEC2 demo.
1. PROPOSED METHODS
1.1. iNeuBe-X System Overview
Our proposed iNeuBe-X system is depicted in Fig. 1. It is motivated
by the iNeuBe framework [2], which contains two multi-microphone
input single-microphone output (MISO) DNNs [4], with the ï¬rst one
producing an initial target estimate for beamforming and the second
one performing post-ï¬ltering for better enhancement. One key difference
 is that here we condition each MISO DNN with a speakerenrollment
 embedding extracted from a third DNN spkmodule, which
is jointly trained with the rest and takes as input an enrollment utterance
 of the target speaker. Given the complex multi-channel shorttime
 Fourier transform (STFT) spectrogram of the mixture signal, Y,
the ï¬rst DNN (denoted as DNN 1in Fig. 1) estimates the complex
STFT spectrogram Ë†S(n)
1(at iteration n  1) of the anechoic target
speech captured at a reference HA array mic, which is set to  CH1
Left  in CEC2. This estimate can then be reï¬ned by a second step,
which involves a DNN-supported beamformer, and a second MISO
DNN (denoted as DNN 2) with additional inputs from the outputs of
DNN1 ğ‘†ğ‘†beam(n)
DNN2Beamform
ğ’€ğ’€Iterate 
DNNspk ğ´ğ´NAL -R ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘
Dynamic Range
Compressionğ‘›ğ‘›   optional
 ğ‘†ğ‘†1(n)
 ğ‘†ğ‘†out(n) ğ‘†ğ‘†eq(n) ğ‘†ğ‘†2(n) ğ‘†ğ‘†2(n) ğ‘†ğ‘†1(n)Fig. 1   Overview of proposed iNeuBe-X framework. We employ a causal
multi-channel Wiener ï¬lter beamformer between DNN 1and DNN 2.
DNN 1and the beamformer. This reï¬nement can be iteratively performed,
 following [2]. Our system is trained to operate with a 32kHz
sampling rate and with an STFT with 16ms window, 4ms stride, and
the square-root Hann window. This would give such system a theoretical
 latency of 16ms [3]. To overcome this, we train both DNN 1
and DNN 2to predict the current plus future 3frames of the target
anechoic signal, so that a new synthesized frame output can be obtained
 for each 4ms stride. This is done in practice by padding zeros
to the left of the input mixture STFT. The beamforming operation is
performed in a frame-online fashion to comply with the 5 ms constraint
 on algorithmic latency required in CEC2. We employ a causal
multi-channel Wiener ï¬lter as in [2, 3]. Since the listener head can
rotate in CEC2, instead of simply accumulating the statistics, we use
the recursive averaging strategy [5] with a 0.5forgetting factor.
1.2. Speaker Enrollment Embedding Extraction
For DNN spk, we use the encoder and TCN modules (and remove the
decoder) of the TCNDenseNet architecture [2, 6] to extract speaker
embeddings. As the enrollment utterances are available beforehand,
this DNN can be non-causal and its output can be cached before inference
 for each target speaker. Given a complex spectra Aof a monaural
 adaptation utterance, this DNN extracts a ï¬xed-length speaker embedding
 Xadapt R128by performing mean-pooling over time on the
outputs of the TCN module. The speaker embedding is then leveraged
as an additional cue by the iNeuBe framework for target extraction.
Differently from [2,6], we use a smaller DNN, with 3 TCN repeats, 4
TCN blocks, 128 TCN channels, and 16 hidden channels in the downsampling
 Conv2D and DenseNet layers of the encoder. This amounts
to a total of around 0.6million (M) parameters.
1.3. Frame-online Speaker-conditioned MISO-TF-GridNet
For both DNN 1and DNN 2we use TF-GridNet [1], which is a stateof-the-art
 separation model that recently achieved an impressive
23.4 dB SI-SDRi on the WSJ0-2mix dataset [7]. Motivated by this
strong result, we extended this model to multi-channel scenarios asarXiv 2302.07928v1  [eess.AS]  15 Feb 2023ğ’šğ’š STFTConv2D LNIntra -frame Spectral ModuleSub-band Temporal ModuleFull-band Self-attention Module6  Deconv2DiSTFT  ğ‘ ğ‘ 
TF-GridNet blocks
Y ğ‘†ğ‘†
Complex spectral
mapping
FiLM -conditioning
ğ‘‹ğ‘‹ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘ğ‘Fig. 2   Proposed frame-online speaker-conditioned MISO-TF-GridNet.
depicted in Fig. 2. Since it uses complex spectral mapping, we feed
all channels by simply stacking the real and imaginary (RI) components
 of all input channels [2] to TF-GridNet, converting it to a MISO
model named as MISO-TF-GridNet. We use feature-wise linear modulation
 (FiLM) [8] over the speaker embedding extracted by DNN spk
at the beginning of each TF-GridNet block to condition the separation.
 Note that TF-GridNet proposed in [1] is non-causal. Here we
make the following modiï¬cations to achieve frame-online processing.
 Firstly, we use layer normalization (LN) after the ï¬rst Conv2D
layer. Secondly, we use a causal full-band self-attention module by
masking the attention matrix. Thirdly, we use a modiï¬ed sub-band
temporal module, where we employ an unidirectional LSTM (instead
of bidirectional as in [1]) and, in the unfolding operation, we stack
only the previous frames  features with the current. This way, the
sub-band temporal module can output one frame for each frame in
the input if the stride is set to 1.
We use the same hyper-parameters suggested in [1], except that
Eis set to 2here. The resulting TF-GridNet has around 8M parameters
 and the full iNeube-X model (with DNN 1and DNN 2) roughly
doubles that amount.
1.4. STFT-domain NAL-R and Dynamic Range Compression
We found that enhancement alone is not sufï¬cient for obtaining an
high HASPI. Without any compensation to listener s hearing loss, the
HASPI for the oracle target speech is less than 0.7on the development
 set. To perform ï¬tting in a frame-online fashion, we perform
the NAL-R ï¬tting [9] and dynamic range compression in the STFT
domain. We followed the dynamic range compression algorithm implemented
 in [10], and set the knee to -10 dB, threshold -40 dB, ratio
1.2, knee width 4, attack 0.05 s, and release 0.2 s, respectively. As
we use in iNeuBe-X 16 ms STFT windows, the length of the NALR
 equalization ï¬lter was set to 80 taps. When fed the oracle target
anechoic speech, our STFT-domain NAL-R ï¬tting plus compression
algorithms produces an HASPI score of 0.987, which is slightly lower
than the 0.998score obtained by the CEC2 baseline ï¬tting algorithm.
This is likely due to the shorter NAL-R ï¬lter used in our algorithms,
but our oracle score is still acceptable.
1.5. System Training Conï¬gurations and External Data Usage
We trained models on two datasets, the original CEC2 data, which we
scaled up to 16k scenes using the provided scene generation script,
and this 16k scenes plus additional 6k scenes generated using the
same conï¬guration except for external clean speech taken from LibriV
 ox in order to increase speaker variations.
Regarding training, we ï¬rst train DNN 1with DNN spkto convergence,
 and then freeze these two and train DNN 2using the output of
these two plus the MCWF ouput. As explained in Section 1.1, the two
MISO networks are trained to estimate the target anechoic signal atTable 1   Results on development set of CEC2.
Approaches SI-SDRi (dB) STOI PESQ HASPI
Challenge baseline (mixture CH1 Left) [11] 0.0 0.563 1.197 0.245
Challenge baseline (oracle CH1 Left)   1.0 5.0 0.998
BLSTM-WPE MVDR [12] 10.481 0.743 1.931 0.436
iNeuBe DNN 1[2] 16.567 0.887 1.946 0.723
iNeuBe-X DNN 1 17.97 0.92 1.951 0.842
  DNN 2  NAL-R ï¬ne-tune 19.082 0.947 2.269 0.942
  DNN 2  NAL-R ï¬ne-tune   External Data 19.514 0.954 2.336 0.943
CH1 Left from the multi-channel mixture signal using complex spectral
 mapping. We use the Adam optimizer with 0.001learning rate
and batch size 1. We clip gradients with L2norm more than 1. The
MISO DNNs are trained on random 4s segments of the input signal,
which are constrained in such a way that the target and the interfereronly
 beginning part should appear both at least for 1s. We use the
whole development set for validation. The learning rate is halved
if the validation loss is not improved over 5 epochs. We follow the
scale-invariant loss function suggested in [2], which is deï¬ned on the
estimated time-domain signal and its magnitude. Differently from [2],
we use a multi-resolution STFT ï¬lterbanks with 512-,1024 -,2048 -,
256- and128-sample windows to compute multiple magnitude losses.
We observed that the magnitude loss leads to clearly better HASPI.
Since this loss is scale-invariant, to prevent clipping in inference we
re-scale the MISO DNNs  estimates in a frame-online fashion using
the output of the MCWF.
To further improve HASPI, after training DNN 2to convergence,
we ï¬ne-tune DNN 2by adding an additional loss term where the same
multi-resolution loss is computed between NAL-R ï¬tted target anechoic
 speech and NAL-R ï¬tted DNN estimates. Since listener audiograms
 are not available in the training set, we randomly sample
audiograms from the development set listeners to perform this ï¬ttingaware
 training.
2. RESULTS ON DEVELOPMENT SET
We report our results obtained on the CEC2 development set in
Table 1 in terms of SI-SDR, STOI, PESQ and HASPI. Note that
SI-SDR, STOI and PESQ are computed before listener adaptation
and compression while HASPI is computed after. Together with the
proposed iNeuBe-X, we also report the performance for two noncausal
 methods  iNeube (only DNN 1, based on TCNDenseNet) [2]
and a DNN-based masked beamforming approach from the ESPNetSE  
 toolkit [12], which employs a two-layer bidirectional LSTM
(BLSTM) and weighted prediction error (WPE) followed by minimum
 variance distortionless response (MVDR) beamforming. As
another comparison, we report the scores of the unprocessed mixtures
 and the oracle target speech. The latter can be considered as a
reasonable upper bound for our proposed system, since we employ a
similar compensation strategy. Note that the CEC2 baseline system
does not perform enhancement, and only performs listener adaptation
 and dynamic range compression. For our proposed systems, we
report performance for various conï¬gurations and training strategies
in the third panel  iNeube-X with only DNN 1, iNeube-X with DNN 2
trained using the additional loss after listener adaptation (  DNN 2
  NAL-R ï¬ne-tune), and with external data (  DNN 2  NAL-R
ï¬ne-tune   External Data). The two latter systems are the ones we
submitted to CEC2. We found that an additional iteration of beamforming
 followed by post-ï¬ltering led to slightly better HASPI, at a
cost of increased computational overhead. Thus we only submitted
signals obtained after the ï¬rst DNN 2reï¬nement step.
We emphasize that our system enjoys an algorithmic latency of 4
ms, and we conï¬rm that the proposed system satisï¬es the 5ms constraint
 on algorithmic latency using this public code1.
1https //github.com/zqwang7/CausalityCheck3. ACKNOWLEDGEMENTS
S. Cornell was partially supported by Marche Region within the
funded project  Miracle  POR MARCHE FESR 2014-2020. Z.-Q.
Wang used the Extreme Science and Engineering Discovery Environment
 [13], supported by the NSF under grant ACI-1548562,
and the Bridges system [14], supported by the NSF under grant ACI1445606,
 at the Pittsburgh Supercomputing Center. Y . Masuyama and
N. Ono were partially supported by JSPS KAKENHI Grant Numbers
JP21J21371 and JST CREST Grant Number JPMJCR19A3.
4. REFERENCES
[1] Z.-Q. Wang, S. Cornell, S. Cho, Y . Lee, B.-Y . Kim, and S. Watanabe,
 TF-GridNet  Making time-frequency domain models great again for
monaural speaker separation,  in submission to ICASSP 2023 , 2022.
[2] Y .-J. Lu, S. Cornell, X. Chang, W. Zhang, C. Li, Z. Ni, Z.-Q. Wang, and
S. Watanabe,  Towards low-distortion multi-channel speech enhancement 
 The ESPNET-SE submission to the L3DAS22 challenge,  in Proc.
ICASSP , 2022, pp. 9201 9205.
[3] Z.-Q. Wang, G. Wichern, S. Watanabe, and J. L. Roux,  STFT-domain
neural speech enhancement with very low algorithmic latency,  arXiv
preprint arXiv 2204.09911 , 2022.
[4] Z.-Q. Wang, P. Wang, and D. Wang,  Multi-microphone complex spectral
 mapping for utterance-wise and continuous speech separation, 
IEEE/ACM Trans. Audio, Speech, Lang. Process. , vol. 29, pp. 2001 
2014, 2021.
[5] S. Gannot, E. Vincent, S. Markovich-Golan, and A. Ozerov,  A consolidated
 perspective on multi-microphone speech enhancement and source
separation,  IEEE/ACM Trans. Audio, Speech, Lang. Process. , vol. 25,
pp. 692 730, 2017.
[6] Z.-Q. Wang, G. Wichern, and J. Le Roux,  Leveraging low-distortion
target estimates for improved speech enhancement,  arXiv preprint
arXiv 2110.00570 , 2021.
[7] J. R. Hershey, Z. Chen, J. Le Roux, and S. Watanabe,  Deep clustering 
Discriminative embeddings for segmentation and separation,  in Proc.
ICASSP , 2016, pp. 31 35.
[8] E. Perez, F. Strub, H. De Vries, V . Dumoulin, and A. Courville,  FiLM 
Visual reasoning with a general conditioning layer,  in Proc. AAAI ,
vol. 32, no. 1, 2018.
[9] D. Byrne and H. Dillon,  The National Acoustic Laboratories (NAL)
new procedure for selecting the gain and frequency response of a hearing
aid,  Ear and hearing , vol. 7, no. 4, pp. 257 265, 1986.
[10] L. McCormack, V . V  alim akiet al. ,  FFT-based dynamic range compression, 
 in Proc. Sound and Music Computing , 2017, pp. 5 8.
[11] X. Ren, L. Chen, X. Zheng et al. ,  A neural beamforming network for BFormat
 3D speech enhancement and recognition,  in Proc. MLSP , 2021.
[12] Y .-J. Lu, X. Chang, C. Li, W. Zhang, S. Cornell, Z. Ni, Y . Masuyama,
B. Yan, R. Scheibler, Z.-Q. Wang et al. ,  ESPnet-SE    Speech enhancement
 for robust speech recognition, translation, and understanding, 
 Proc. Interspeech , 2022.
[13] J. Towns, T. Cockerill, M. Dahan et al. ,  XSEDE  Accelerating scientiï¬c
discovery,  Computing in Science & Engineering , vol. 16, no. 5, pp. 62 
74, 2014.
[14] N. A. Nystrom, M. J. Levine, R. Z. Roskies, and J. R. Scott,  Bridges  a
uniquely ï¬‚exible HPC resource for new communities and data analytics, 
inProc. XSEDE , 2015, pp. 1 8.