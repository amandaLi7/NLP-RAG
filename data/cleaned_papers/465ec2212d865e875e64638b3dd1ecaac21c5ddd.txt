## PROFNAME
Alexander Rudnicky
## AUTHORID
3156164
## AUTHORNAME
A. Rudnicky
## AUTHORURL
https://www.semanticscholar.org/author/3156164
## AUTHORHINDEX
8
## AUTHORAFFILIATIONS
[]
## AUTHORPAPERCOUNT
16
## AUTHORCITATIONCOUNT
309
## PAPERID
465ec2212d865e875e64638b3dd1ecaac21c5ddd
## EXTERNALIDS
{'DBLP': 'conf/emnlp/ChiFRR23', 'ArXiv': '2305.03796', 'DOI': '10.48550/arXiv.2305.03796', 'CorpusId': 258557586}
## URL
https://www.semanticscholar.org/paper/465ec2212d865e875e64638b3dd1ecaac21c5ddd
## TITLE
Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation
## ABSTRACT
Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.
## VENUE
Conference on Empirical Methods in Natural Language Processing
## YEAR
2023
## REFERENCECOUNT
47
## CITATIONCOUNT
4
## INFLUENTIALCITATIONCOUNT
0
## ISOPENACCESS
True
## OPENACCESSPDF
{'url': 'http://arxiv.org/pdf/2305.03796', 'status': None}
## FIELDSOFSTUDY
['Computer Science']
## JOURNAL
{'pages': '5972-5984'}
## AUTHORS
[{'authorId': '27531332', 'name': 'Ta-Chung Chi'}, {'authorId': '32037089', 'name': 'Ting-Han Fan'}, {'authorId': '3156164', 'name': 'A. Rudnicky'}, {'authorId': '1693135', 'name': 'P. Ramadge'}]
## TLDR
Inspired by the notion of working memory, a new Transformer variant named RegularGPT is proposed, which constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY.
Transformer Working Memory Enables Regular Language Reasoning
And Natural Language Length Extrapolation
Ta-Chung Chi
Carnegie Mellon University
tachungc@andrew.cmu.eduTing-Han Fan
Princeton University
tinghanf@princeton.edu
Alexander I. Rudnicky
Carnegie Mellon University
air@cs.cmu.eduPeter J. Ramadge
Princeton University
ramadge@princeton.edu
Abstract
Unlike recurrent models, conventional wisdom
 has it that Transformers cannot perfectly
 model regular languages. Inspired
by the notion of working memory, we propose
 a new Transformer variant named RegularGPT.
 With its novel combination of WeightSharing,
 Adaptive-Depth, and Sliding-DilatedAttention,
 RegularGPT constructs working
memory along the depth dimension, thereby
enabling efÔ¨Åcient and successful modeling of
regular languages such as PARITY . We further
 test RegularGPT on the task of natural
language length extrapolation and surprisingly
Ô¨Ånd that it rediscovers the local windowed attention
 effect deemed necessary in prior work
for length extrapolation.
1 Introduction
It is long believed that Working Memory (WM),
a term coined in 1960s to liken human minds to
computers, plays an important role in humans  reasoning
 ability and the guidance of decision-making
behavior (Baddeley and Hitch, 1974  Baddeley,
1992  Ericsson and Kintsch, 1995  Cowan, 1998 
Miyake et al., 1999  Oberauer, 2002  Diamond,
2013  Adams et al., 2018). While no single deÔ¨Ånition
 encompasses all applications of WM (Adams
et al., 2018), the following one should be shared by
all of the theories of interest 
Working memory is a system of components
 that holds a limited amount of information
 temporarily in a heightened
state of availability for use in ongoing
processing. - Adams et al. (2018)
WM is instantiated in the two major driving
forces of sequence modeling  Recurrent neural
 networks (RNN) (Elman, 1990  Jordan, 1997 
Hochreiter and Schmidhuber, 1997) short term
memory modulated by their recurrent nature and
gate design (Rae and Razavi, 2020a  Nematzadehet al., 2020  Armeni et al., 2022), and Transformers 
 (Vaswani et al., 2017) salient tokens heightened
 by self-attention.
In reality, self-attention often attends
broadly (Clark et al., 2019), violating the
limited amount of information notion of WM. Our
hypothesis is that such violation is to blame for
Transformers  failure on algorithmic reasoning of
regular languages (Deletang et al., 2023  Liu et al.,
2023) such as PARITY , a seemingly simple task
that checks if the number of 1s in a bit string is
even. Surprisingly, a Transformer can only count
the number of 1s correctly when the sequence
length is held Ô¨Åxed at training sequence length Ttr,
and it fails miserably when the testing sequence
length extrapolates to Tex  T tr(Hahn, 2020 
Bhattamishra et al., 2020  Chiang and Cholak,
2022  Deletang et al., 2023  Liu et al., 2023). In
contrast, an RNN can extrapolate perfectly.
The goal of this work is therefore to enable
Transformers  WM by limiting the amount of accessible
 information at a time. Existing attempt
that uses a combination of scratchpad and recency
biases (Wei et al., 2022  Nye et al., 2022  Anil
et al., 2022  Liu et al., 2023) is not optimal as it
completely foregoes the parallelization property of
a Transformer, making it as computationally inefÔ¨Åcient
 as an RNN.
This begs the question  Does there exist a more
efÔ¨Åcient Transformer working memory design  The
answer is afÔ¨Årmative thanks to the proposed RegularGPT
 , which boils down to the three design
choices  Weight-Sharing, Adaptive-Depth, and
Sliding-Dilated-Attention  Each of them has been
proposed previously but it is the unique combination
 that sparks the successful and efÔ¨Åcient learning
of regular languages. We will further demonstrate
its  1) similar recursive parallel structure as linear
RNN (Orvieto et al., 2023), resulting in a logTtr,ex
number of layers, and 2) generalizability by showing
 strong performances on the task of TransformerarXiv 2305.03796v1  [cs.CL]  5 May 2023natural language length extrapolation (Press et al.,
2022  Chi et al., 2022a,b).
In this work, we use [N]to denote the list of
non-negative integers [0,...,N 1]. The Transformer
 model used in this work is always causal. It
takes in an input sequence of T Ttrunits (can
be tokens or bits) œÉi [T], passes them through a
Ô¨Åxed amount of Ltransformer layers, and Ô¨Ånally
computes the distribution over the vocabulary V
via the prediction head Wo.
2 Background
2.1 Regular Language and Algorithmic
Reasoning
The Chomsky hierarchy (Chomsky, 1956b) classiÔ¨Åes
 formal languages into different hierarchies
based on their increasing complexity. Each hierarchy
 represents a family of formal languages that
can be solved by the corresponding automaton. At
the lowest level resides the family of regular languages,
 which can be expressed using a Ô¨Ånite state
automaton (FSA), a computational model comprising
 a set of states and transitions connecting them.
Our primary objective is to enhance the algorithmic
 reasoning of the Transformer model on regular
languages by testing its language transduction capability
 under the extrapolation setting. Concretely,
the model is trained only to predict desired outputs
on a set of short length- Tsequences with T Ttr.
Still, it must also predict the correct outputs for
longer testing sequences of length Tex Ttr. It
is worth noting that we evaluate our model via
language transduction following recent work (Deletang
 et al., 2023  Liu et al., 2023), instead of the
conventional language recognition protocol. Both
settings are equally hard as they are underpinned
by the same Ô¨Ånite state semiautomaton. Interested
readers may refer to Deletang et al. (2023) for further
 details regarding the two evaluation protocols.
We also reveal the connection between RegularGPT
and Ô¨Ånite state semiautomaton later in  7.
2.2 Failure Mode and An InefÔ¨Åcient Fix
The PARITY task involves a length Tbit string
œÉ1œÉ2   œÉTwhere each bit œÉiis randomly sampled
from a Bernoulli distribution with P(œÉi  1)  
0.5. The goal is to determine whether the sequence
contains an even or odd number of 1s.
It has been observed that a Transformer is incapable
 of performing length extrapolation on PARITY
 , but what could be its potential failure mode Previous work sheds light on this by showing that
a Transformer might settle on the naive-summation
approach (Anil et al., 2022  Deletang et al., 2023 
Liu et al., 2023). Concretely, it sums up all the
bits and outputs the summation modulo 2. This approach
 fails since unseen summations will be produced
 when the model takes sequences of length
Tex T as input or P(Si)deviates from 0.5.
To the best of our knowledge, the existing remedy
 (Liu et al., 2023  Anil et al., 2022) is to use
scratchpad (Wei et al., 2022  Nye et al., 2022) along
with recency biases (Press et al., 2022) to enforce
the correct learning  They create a scratchpad that
interleaves the sequence of input bits and intermediate
 answers (œÉ1,q1,œÉ2,q2,   ,œÉT,qT), where
qi solve (œÉ1   œÉi). The model is trained to predict
 all theœÉi [T]. Recency biases play the role of
limiting a Transformer s receptive Ô¨Åeld to only a
few most recent œÉandqat every timestep i. This is
to prevent self-attention from ignoring qand giving
the same naive-summation solution.
Scratchpad and recency biases jointly create the
notion of WM along the temporal dimension similar
 to RNNs, thereby enabling successful extrapolation
 on regular languages. Nevertheless, we note
that this Ô¨Åx is inefÔ¨Åcient during inference since all
the intermediate answers qihave to be generated
sequentially before reaching the Ô¨Ånal answer qT.
A desirable Ô¨Åx should only take in the input bits
(œÉ1,œÉ2,   ,œÉn)and directly generate the Ô¨Ånal answerqT.
 In other words, our goal is to Ô¨Ånd an
efÔ¨Åcient WM design for a Transformer.
2.3 A Desirable Fix for PARITY (Figure 1)
An alternative solution to the PARITY problem is
based on the spirit of divide-and-conquer, where
we Ô¨Årst divide the sequence into T/C chunks with
each chunk of length C   T , and we compose
the Ô¨Ånal answer by recursively merging the chunk
outputs. This approach does not suffer from the
unseen summation issue as the model was trained
to handle a Ô¨Åxed amount of Cbits at a time in
its WM (chunk). It then recursively applies the
already-seen results to compose the Ô¨Ånal solution
when it encounters longer sequences during inference.
 More importantly, it is more efÔ¨Åcient than
the scratchpad and recency biases approach since
it only requires logCTlayers of parallel computations
 instead of 2Tsteps of sequential decoding.1011000ùêøùëéùë¶ùëíùëü 0
01000011ùêπùêπùëÅùêπùêπùëÅùë°‚Ñéùëñùëêùëòùëõùëíùë†ùë†ùêøùëéùë¶ùëíùëü 1ùêøùëéùë¶ùëíùëü 2ùêπùêπùëÅFigure 1  This is the divide-and-conquer approach that solves the PARITY problem. The lightly shaded blue cells
representM(l)
mnin eq. (1). The darkened blue cells represent the routing path to solve the result for the last bit
speciÔ¨Åcally. As we can see, this approach requires at most log2Tlayers to obtain the result for a length Tinput
sequence, rendering it a more efÔ¨Åcient approach compared to the combination of scratchpad and recency biases.
3 Proposed Architecture of RegularGPT
We present our modiÔ¨Åcations to the vanilla Transformer
 below. Only the related operations will be
expanded, and we follow all the other details of
GPT2 (Radford et al., 2019).
3.1 Sliding-Dilated-Attention
A Transformer layer at layer lconsists of a selfattention
 operation denoted as SA(l)and feedforward
 network denoted as FFN(l). Originally,
SA(l)computes the inter-token relationships across
allTunits. Instead, we set the chunk size to C
and produce T/C non-overlapping chunks 1Only
the units within the same chunk inter-attend with
each other. In practice, this can be achieved by
an attention mask M(l) RT Tat layerl.M(l)
shares the same shape as the self-attention matrix
(see Figure 1) and is deÔ¨Åned as 
M(l)
mn {
r(m n)/C‚Ñì,ifm n
Cl [C]
 inf, otherwise(1)
Note thatMis a lower triangular matrix due to the
causal nature of our model. ri s withi [C]are
learnable relative positional scalars. To be precise,
each attention head has a different set of learnable
biasesri s. Here, we drop the dependency on the
head for notational simplicity.
The use ofri s is similar to the positional scalars
of T5 (Rae and Razavi, 2020a) except that we do
not use the log-binning strategy over m n. It is
to facilitate the extraction of global information
instead of enforcing the windowed-attention effect
 (Raffel et al., 2020  Press et al., 2022  Chi
1WheneverTis not divisible by C, we pad the input sequence
 such that its length is a multiple of C.et al., 2022a,b). Mwill then be added to the
original self-attention matrix, creating the proposed
 Sliding-Dilated-Attention effect. The output
 of SA(l)will be transformed by the positionalindependent
 FFN(l)to produceo(l)
i [T].
The case ofC  2is used as a possible construction
 of Theorem 1 in Liu et al. (2023). However,
their focus is not on length extrapolation, hence
lacking the below two proposed modiÔ¨Åcations.
3.2 Adaptive-Depth and Weight-Sharing
Since our Sliding-Dilated-Attention limits the number
 of accessible tokens at a time, we need an adaptive
 depth  L  logCTso that the Ô¨Ånal output
can utilize every single piece of input information.
However, when Tex Ttr, the depth during inference
 will be higher than that during training. The
simplest way to solve this challenge without further
parameter updating is to perform Weight-Sharing
across layers. To account for the possible performance
 loss due to Weight-Sharing, we Ô¨Årst thicken
the model by Ktimes, resulting in a total number
ofK  Llayers. Next, we share the weights across
theK  Llayers in the following way for k [K] 
SA(l K k) SA(k)forl [ L]
FFN(l K k) FFN(k)forl [ L]
It can be equivalently interpreted as stacking more
SA and FFN components within every Transformer
layer, and the same thickened layer is reused  L
times. This layer thickening design is only used in
the natural language modeling experiments in  6.
3.3 Where is the WM Notion 
Instead of instantiating WM along the temporal
dimension as the combination of scratchpad andùë£ ùë£"ùë£#ùë£$ùê¥ùë£  ùë£"ùê¥ùë£# ùë£$ùê¥"ùê¥ùë£  ùë£" ùê¥ùë£# ùë£$ùê¥#ùë£  ùê¥"ùë£" ùê¥ùë£# ùë£$ùê¥$ ùê¥#ùë£% ùê¥"ùë£& ùê¥ùë£' ùë£(ùë£%ùë£&ùë£'ùë£(ùê¥ùë£% ùë£&ùê¥ùë£' ùë£(ùê¥"ùê¥ùë£% ùë£& ùê¥ùë£' ùë£(Figure 2  This is the parallel scan algorithm that can
accelerate a linear RNN. In this example, we visualize
the routing path for computing x8. Blocks at the same
layer can be computed in parallel on GPUs.
recency biases, RegularGPT limits the amount of
information along the depth dimension. As we
have seen, the idea of breaking Tunits into several
chunks limits the amount of accessible information
 at each layer, thereby enabling the WM notion.
 A similar argument was made by Yogatama
et al. (2021) in a sense that they categorized Longformer
 (Beltagy et al., 2020), a transformer variant
with local attention pattern, as a model of working
memory. Finally, thanks to modern accelerators
such as GPU, all chunks at a layer can be processed
concurrently, and this further makes RegularGPT
more favorable over the scratchpad and recency
biases approach.
3.4 Complexity Analysis
The sparse attention pattern of RegularGPT suggests
 it might enjoy the same speedup provided
by sparsiÔ¨Åed Transformers. The complexity of our
model isO(TCK logCT)whereTCis the complexity
 of each self-attention module and KlogCT
is the total number of layers. On the other hand,
the vanilla Transformer follows O(T2L). To illustrate
 the possible speedup, if T  512 and
C  128 , then 512 128 Klog128512 5122L
whenK  512L
128 log128512 3.11L. Namely, as long
asK   3L, our model is likely to be more efÔ¨Åcient
than a vanilla Transformer.
4 Connection to Prior Work
Sliding-Dilated-Attention This special attention
 pattern dates back to pre-Transformer era such
as Wavenet (van den Oord et al., 2016) with dilated
convolution. It can also be viewed as a special
form of Longformer attention pattern with systematic
 dilation (Beltagy et al., 2020).2Limiting the
2The original Longformer also adopts dilated attention on
a few heads at higher layers but without the systematic patternrange of attention in lower layers of a Transformer
is also corroborated in Rae and Razavi (2020b),
where they Ô¨Ånd such design does not deteriorate
the performance.
Adaptive-Depth and Weight-Sharing ALBERT
 (Lan et al., 2020) and Universal Transformer
 (Dehghani et al., 2019) share the parameters
across layers. The weight sharing design makes
them compatible with the idea of Adaptive
Computation Time (Graves et al., 2014) and
Dynamic Halting (Dehghani et al., 2019  Elbayad
et al., 2020), which allocate different computational
 budget depending on the complexity
of tasks (Simoulin and Crabb√©, 2021  Csord√°s
et al., 2022). However, they lack the special
Sliding-Dilated-Attention design that is necessary
for ruling out naive solutions.
Linear RNN Givenx0  0 RNand the input
 vectorsu1   uT, a linear RNN (Orvieto et al.,
2023) fork [T]can be written as 
xk Axk 1 Buk k 1 
j 0AjBuk j k 1 
j 0Ajvk j,
where we set vk j Buk j. The operation can be
accelerated by the parallel scan algorithm that permits
 efÔ¨Åcient cumulative sum (Ladner and Fischer,
1980  Blelloch, 1990  Lakshmivarahan and Dhall,
1994  Martin and Cundy, 2018  Liu et al., 2023 
Smith et al., 2023). As we can see in Figure 2,
the routing path speciÔ¨Åed by the parallel scan algorithm
 is the same as our Sliding-Dilated-Attention
illustrated in Figure 1.
5 Regular Language Experiments
5.1 Language Transduction and
Extrapolation
First, we want to know if endowing a Transformer
with the notion of WM really improves its length
extrapolation capability on regular languages. We
test RegularGPT and all the baselines on two sets of
regular languages from prior work (Deletang et al.,
2023  Bhattamishra et al., 2020).3Prior work often
reports the maximum score across different hyperparameter
 settings and random seeds because their
used in this work.
3Our implementation is based on the codebase of Deletang
et al. (2023) at  https //github.com/deepmind/
neural_networks_chomsky_hierarchy . We additionally
 implement the regular languages in the second section
of Table 1.Task RNN TransformerRegularGPT
C  2C  3
1) Deletang et al.
Even Pairs 100.0 / 100.0 99.7 / 73.2 100.0 / 89.3 100.0 / 96.6
Modular Arithmetic 100.0 / 100.0 21.9 / 20.3 96.4 / 82.6 21.2 / 20.5
Parity Check 100.0 / 98.9 52.3 / 50.1 100.0 / 100.0 100.0 / 88.7
Cycle Navigation 100.0 / 100.0 21.7 / 20.6 100.0 / 100.0 100.0 / 78.6
2) Bhattamishra et al.
D2 100.0 / 100.0 100.0 / 80.1 100.0 / 100.0 99.8 / 96.5
D3 100.0 / 100.0 100.0 / 77.8 100.0 / 99.7 98.6 / 93.0
D4 100.0 / 100.0 100.0 / 82.6 100.0 / 98.7 97.7 / 91.6
D12 100.0 / 100.0 100.0 / 80.3 100.0 / 99.8 94.1 / 90.4
Tomita 3 100.0 / 100.0 100.0 / 94.4 100.0 / 99.7 100.0 / 99.9
Tomita 4 100.0 / 100.0 100.0 / 70.0 100.0 / 99.8 100.0 / 99.3
Tomita 5 100.0 / 100.0 74.5 / 74.5 100.0 / 99.8 98.2 / 84.1
Tomita 6 100.0 / 100.0 50.0 / 50.0 100.0 / 98.5 100.0 / 65.7
Table 1  Length generalization results on Regular Languages (Max/Avg). All models in the Ô¨Årst section (Deletang
 et al.) are trained on sequences of length 40. The reported numbers are the average of length extrapolation
results from 41 to 500. Each result is an average over 3 seeds. All models in the second section (Bhattamishra et al.)
are trained on sequences of length 50. The reported numbers are the average of length extrapolation results from
51 to 100. Each result is an average over 3 seeds. Please refer to Appendix A for the detailed hyperparameters.
goal is to know if a model can extrapolate at all .
We additionally report the average scores since we
want to know if the model can consistently obtain
good performance. The baseline models we compare
 against are an RNN and vanilla Transformer
with Transformer-XL style relative positional embedding
 (Dai et al., 2019). Table 1 shows that
RegularGPT with C  2acheives similar performance
 as an RNN and substantially outperforms a
vanilla Transformer.
5.2 The Effect of Chunk Size C
We vary the chunk size Cof RegularGPT to see
its impact on the performance. The motivation
for using a larger Cis to reduce the number of
layers (i.e.,  L  logCTdecreases in C) and increase
 the degree of parallelization. However, in
Table 1, a larger Cseems to pose a challenge to
RegularGPT on the Modular Arithmetic task. Modular
 Arithmetic is a hard task with far more states
and complicated state transitions. Increasing Cis
likely to increase the task difÔ¨Åculty by composing
more state transitions at once. We will have an
in-depth discussion of the theoretical reasons in  7.
5.3 Robust to Probability Changes
Other than the length extrapolation experiment, we
alter the probability of sampling 1s of PARITY ,
i.e., set P(œÉi)   0.5. The results in Table 2 showSettingsProbability P(œÉi  1)
0.1 0.3 0.5 0.7 0.9
1) Same Length
RegularGPT 100 100 100 100 100
RNN 100 100 100 100 100
Transformer 98.4 99.8 99.6 97.8 77.2
2) Extrapolation
RegularGPT 100 100 100 100 100
RNN 100 100 100 100 100
Transformer 50.1 49.7 50.3 49.9 50.0
Table 2  We alter the probability P(œÉi  1) used to
sample 1s of PARITY . The same length setting is 40.
The extrapolation setting is from 41 to 500. Each entry
is an average over 3 seeds.
that RegularGPT is robust to different sampling
probabilities, indicating its successful modeling
of the underlying regular language grammar. In
contrast, a vanilla Transformer model struggles to
achieve good performance even for the same length
setting, again validating the fact that it only Ô¨Ånds
the naive-summation solution as discussed in  2.2.
6 Natural Language Experiments
Given that RegularGPT has been battle-tested on
the main experiment of regular languages, we now
shift gear to benchmark its performance in the natural
 language scenario. Given a model trained on
sequences of length Ttr, we test it on much longer
sequences of length Tex Ttrduring inference,Lex KERPLE T5 ALiBiRegularGPT ( C/K )
32 / 6 64 / 6 128 / 6 128 / 12 256 / 6
512 24.71 24.50 24.53 32.06 30.17 28.80 26.37 27.90
1024 24.42 24.38 24.90 32.03 30.30 28.94 26.91 34.38
2048 24.21 25.01 25.08 791.74 30.56 29.14 27.08 34.85
4096 24.53 28.91 25.08 812.00 30.80 29.25 27.28 35.11
8192 24.74 39.08 25.08 818.49 1175.91 29.41 27.39 35.42
Table 3  Natural language extrapolation results on OpenWebText2. The training length is 512. The numbers
are averaged over three random seeds. Please refer to Appendix B for the detailed hyperparameters.
and the goal is to observe similar perplexities. We
ensure only the perplexity of the last token in a sequence
 is used to compute the result so that we do
not suffer from the early token curse (Press et al.,
2022  Chi et al., 2022b). We average over 1,000 sequences
 and report their averaged perplexities. We
compare our model against the existing methods
that are known to demonstrate the ability of length
extrapolation including T5 (Raffel et al., 2020),
ALiBi (Press et al., 2022), and KERPLE (Chi et al.,
2022a).4To counteract the loss of expressive power
due to weight sharing, we thicken each layer of
RegularGPT to Kas detailed in  3.
In Table 3, we Ô¨Årst observe exploding perplexities
 forC  32 afterLex 2048 . RegularGPT
might only learn to model  log32512   2 layers
 during training, hence it fails to recursively
model more than 322  1024 tokens during inference.
 This is validated by C  64 since this time
it is able to extrapolate until 64 log64512   4096 .
While the above argument seems to suggest large
C, settingC  256 also deteriorates the performance.
 This might be due to the limited number
of chunks ( 512/256   2 ) andri s (in Eq. (1)) observed
 at the second layer, making the learning of
ri s harder. Overall, Cis a hyperparameter that
needs to be carefully decided for RegularGPT on
natural languages. We also observe that 128/12 performs
 better than 128/6, implying RegularGPT s
performance could be improved by stacking more
layers to counteract the performance loss due to
Weight-Sharing.
It is worth noting that 128/12 performs relatively
well and is close to previous methods designed
speciÔ¨Åcally for the task of natural language extrapolation.
 We will analyze its inner workings in depth
in Figure 4 and  7, in which we Ô¨Ånd that Regu4We
 use the nanoGPT codebase  https 
//github.com/karpathy/nanoGPT , and the OpenWebText2
 dataset  https //huggingface.co/
datasets/the_pile_openwebtext2.larGPT learns the similar local receptive Ô¨Åeld as
prior work, which is likely the key to its successful
natural language extrapolation performance.
7 Discussion and Analysis
7.1 Regular Language and Finite State
Semiautomaton
Regular language is the type of formal language
recognized by an FSA (Chomsky, 1956a), which
is a 5-tuple (Q,Œ£,Œ¥,q 0,F), whereQis a Ô¨Ånite
non-empty set of states, Œ£is a Ô¨Ånite non-empty
set of symbols, q0 Qis an initial state, Œ¥ Q 
Œ£ Qis a transition function  F Qis a set
of Ô¨Ånal states. However, some of our tasks are
better modeled by a Ô¨Ånite-state transducer (FST)
as discussed in  2.1. To underpin both FSA and
FST, we consider a semiautomation A  (Q,Œ£,Œ¥)
(i.e., an FSA without q0andF) and establish its
connection to a Transformer model.
LetœÉa bbe the sequence from position a(inclusive)
 tob(exclusive) out of a length Tinput
sequence (i.e., 0 a   b T). We deÔ¨Åne
A(œÉa b)  Q Qas the (b a)-step state transition
 relation after receiving œÉa b.
A(œÉa b)  Œ¥(  œÉb 1)     Œ¥(  œÉa),
wheref( ) g( )  f(g( ))denotes function
composition. With abuse of notation, we deÔ¨Åne
Aq(œÉa b) Qas the state after receiving œÉa bif
starting atq Q.
Aq(œÉa b)  Œ¥(  œÉb 1)     Œ¥(  œÉa) q.
7.2 Modeling Transition Composition
We want to show that the layers of RegularGPT
with chunk size C  2can model the composition
of two transition functions 
A(œÉa b)  A(œÉi b) A(œÉa i)fori [a 1,...,b ).
This way, the regular language problem can be
solved recursively using the construction outlinedin  3 and Figure 1. To formalize the statement,
we Ô¨Årst observe that A(œÉa b),A(œÉa i), andA(œÉi b)
can be represented in R Q 2 
A(œÉa b)   
   OneHot Q (Aq0(œÉa b))
OneHot Q (Aq1(œÉa b))
   
OneHot Q (Aq Q  1(œÉa b)) 
    R Q 2,
(2)
where OneHot Q (i)is a one-hot vector of length
 Q with thei-th index being 1.
The next step is to mix A(œÉa i)andA(œÉi b)together
 and getA(œÉa b). We show in Lemma 1 that
a 2-layer ReLU network can learn (and so can a
transformer layer) the composition. The proof of
Lemma 1 is deferred to Appendix C.
Lemma 1 (Approximation for Binary Matrix Product).LetA,B {0,1}n nbe
 binary matrices of
dimensionn n. Then, there exists a two-layer
ReLU network such that
fmlp([Flat(A),Flat(B)])   Flat(AB),
where Flat(X)(i 1)n j Xi,jfori,j [n]is
the operation that Ô¨Çattens a matrix into a vector.
Now, we can relate Lemma 1 to the FFN layers
 in RegularGPT. Following  3, when chuck size
C  2and thickness K  1, the output vector o(l)
i
depends on input sequence œÉi 2l 1 1 i 1. Also,
o(l)
iis computed from o(l 1)
i 2lando(l 1)
i, which
depend on input sequences œÉi 2l 1 1 i 2l 1and
œÉi 2l 1 i 1, respectively. This observation implies
 thato(l)
ilikely models the transition functionA(œÉi 2l 1 1 i 1),
 which we denote as o(l)
i 
A(œÉi 2l 1 1 i 1). We will verify this assumption
in  7.3.
Ifo(l)
i  A (œÉi 2l 1 1 i 1)is true, Lemma 1
implies that RegularGPT s FFN models the transition
 function composition. This is immediate
by settingo(l 1)
i 2l Flat(A(œÉi 2l 1 1 i 2l 1)),
o(l 1)
i Flat(A(œÉi 2l 1 i 1))and recognizing
the fact that function composition is a matrix product
 under the representation of Eq. (2).
The next step is to explain the use of selfattention
 layers in RegularGPT. Although Lemma 1
has established a composition, it is unclear how
the transitions are concatenated in the Ô¨Årst place
(i.e., [Flat(A),Flat(B)]). With a two-head selfattention
 and the learnable relative positional
scalars, it is possible to adjust them so that theattention output contains the concatenated information[Flat(A),Flat(B)].

Recall in Eq. (1), each head has a different set of
scalarsri s. One concrete construction for concatenation
 is setting r0  0and the remaining   for
the Ô¨Årst head  r1  0 and the remaining    for
the second head. In other words, each head is only
responsible for capturing one state transition. After
the multi-head self-attention operation, we obtain
the concatenation of two state transitions.
Finally, when the prediction head reads out
the answer, the operation is equivalent to a mapping
 fromA(œÉ0 T) R Q   Q toAq0(œÉ0 T)  
A(œÉ0 T) q0 R Q . Since we assume that o(l)
T 1
modelsA(œÉ0 T), the transduction readout is performed
 by a linear map on o(l)
T 1asWoo(l)
T 1.
7.3 VeriÔ¨Åcation of Transition Modeling
To verify whether our model learns the dynamics
of a semiautomaton, we perform a clustering experiment
 to demystify the FFN output representations
on the tasks of PARITY and Cycle Navigation. The
two tasks are chosen as we can easily derive their
state transition functions. For example, there are
only two state transitions in PARITY 
[1 0
0 1]
or[0 1
1 0]
and Ô¨Åve state transitions in Cycle Navigation 
 
     OneHot 5((0  k)mod5)
OneHot 5((1  k)mod5)
OneHot 5((2  k)mod5)
OneHot 5((3  k)mod5)
OneHot 5((4  k)mod5) 
     ,fork [0,...,4].
e.g.,k  2gives 
     0 0 1 0 0
0 0 0 1 0
0 0 0 0 1
1 0 0 0 0
0 1 0 0 0 
     .
Given a testing input sequence of length 500
that is much longer than the training length 40, we
extract the output o(l)
iof all layersl, perform dimension
 reduction using PCA, and plot the dimensionreduced
 points on a 2D plane. Ideally, we want to
see a limited number of clusters across all layers,
indicating the model learns to capture the state transition
 function. As we can see in Figure 3, PARITY
has 2 clusters and Cycle Navigation has 5 clusters.
 The clear clustering effect demonstrates RegularGPT s
 correct learning of state transition func-15
 10
 5
 0 5 10 15
PCA115
10
5
051015PCA20
1(a) PARITY .
15
 10
 5
 0 5 10 15
PCA115
10
5
051015PCA20
1
2
3
4 (b) Cycle Navigation.
Figure 3  Clustering of FFN output vectors across all layers via PCA on the task of PARITY and Cycle Navigation.
0 100 200 300 400 500
Position0.20.40.60.81.0
(a) Regular Language - PARITY
0 500 1000 1500 2000
Position0.20.40.60.81.0 (b) Natural Language - OpenWebText2
Figure 4  Receptive Ô¨Åeld of RegularGPT via the cumulative gradient analysis tool (Chi et al., 2022b).
tions. This is in contrast to the naive-summation approach
 learned by a vanilla Transformer as shown
in Figure B.4 of Deletang et al. (2023).
7.4 Receptive Field Analysis
We resort to the gradient analysis tool (Chi et al.,
2022b) to inspect the receptive Ô¨Åeld of RegularGPT
on regular and natural languages. It computes a cumulative
 sum of the gradient norms starting from
the most recent token to the earliest one. A large
magnitude of slope at a position means the most recent
 token has a high dependency on that position.
Ideally, we would like to see the receptive Ô¨Åeld
covering the whole input sequence for the case of
regular languages because every single bit in the input
 sequence is important for the Ô¨Ånal results. This
is equivalent to a slanted line going from the lower
right to the upper left, which is validated in Figure
 4a. As for natural language, we discover something
 interesting in Figure 4b in that RegularGPT
settles on the local windowed-attention pattern as
those enforced manually in prior work (Press et al.,2022  Chi et al., 2022a,b). This suggests the task of
natural language modeling mostly needs only local
context to achieve good performance, which aligns
with the common belief.
8 Conclusion
This paper introduces RegularGPT, a novel variant
of the Transformer architecture inspired by the notion
 of working memory that can effectively model
regular languages with high efÔ¨Åciency. Theoretical
explanations and accompanying clustering visualizations
 are presented to illustrate how RegularGPT
captures the essence of regular languages. Moreover,
 RegularGPT is evaluated on the task of natural
 language length extrapolation, revealing its
intriguing rediscovery of the local windowed attention
 effect previously observed in related research.
Notably, RegularGPT establishes profound connections
 with various existing architectures, thereby
laying the groundwork for the development of future
 Transformer models that facilitate efÔ¨Åcient algorithmic
 reasoning and length extrapolation.Limitations
Currently we set the chunk size Cof RegularGPT
to a constant. Can we make the chunk size more
Ô¨Çexible  A Ô¨Çexible and data-driven Cmight further
 boost its performance on natural languages as
they often demonstrate diverse patterns unlike regular
 languages underpinned by simple grammars.
This might also improve the performance of RegularGPT
 when C   128 .
References
Eryn J Adams, Anh T Nguyen, and Nelson Cowan.
2018. Theories of working memory  Differences
in deÔ¨Ånition, degree of modularity, role of attention,
and purpose. Language, speech, and hearing services
 in schools , 49(3) 340 355.
Cem Anil, Yuhuai Wu, Anders Johan Andreassen,
Aitor Lewkowycz, Vedant Misra, Vinay Venkatesh
Ramasesh, Ambrose Slone, Guy Gur-Ari, Ethan
Dyer, and Behnam Neyshabur. 2022. Exploring
length generalization in large language models. In
Advances in Neural Information Processing Systems.

Kristijan Armeni, Christopher Honey, and Tal Linzen.
2022. Characterizing verbatim short-term memory
 in neural language models. In Proceedings
of the 26th Conference on Computational Natural
Language Learning (CoNLL) , pages 405 424, Abu
Dhabi, United Arab Emirates (Hybrid). Association
for Computational Linguistics.
Alan Baddeley. 1992. Working memory. Science ,
255(5044) 556 559.
Alan D Baddeley and Graham Hitch. 1974. Working
memory. In Psychology of learning and motivation ,
volume 8, pages 47 89. Elsevier.
Iz Beltagy, Matthew E. Peters, and Arman Cohan.
2020. Longformer  The long-document transformer.
arXiv 2004.05150 .
Satwik Bhattamishra, Kabir Ahuja, and Navin Goyal.
2020. On the Ability and Limitations of Transformers
 to Recognize Formal Languages. In Proceedings
 of the 2020 Conference on Empirical Methods
in Natural Language Processing (EMNLP) , pages
7096 7116, Online. Association for Computational
Linguistics.
Guy E Blelloch. 1990. PreÔ¨Åx sums and their applications.
 School of Computer Science, Carnegie Mellon
 University Pittsburgh, PA, USA .
Ta-Chung Chi, Ting-Han Fan, Peter Ramadge, and
Alexander Rudnicky. 2022a. KERPLE  Kernelized
relative positional embedding for length extrapolation.
 In Advances in Neural Information Processing
Systems .Ta-Chung Chi, Ting-Han Fan, and Alexander I. Rudnicky.
 2022b. Receptive Ô¨Åeld alignment enables
transformer length extrapolation.
David Chiang and Peter Cholak. 2022. Overcoming a
theoretical limitation of self-attention. In Proceedings
 of the 60th Annual Meeting of the Association
for Computational Linguistics (Volume 1  Long Papers)
 , pages 7654 7664, Dublin, Ireland. Association
 for Computational Linguistics.
N. Chomsky. 1956a. Three models for the description
of language. IRE Transactions on Information Theory,
 2(3) 113 124.
Noam Chomsky. 1956b. Three models for the description
 of language. IRE Transactions on information
theory , 2(3) 113 124.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and
Christopher D. Manning. 2019. What does BERT
look at  an analysis of BERT s attention. In Proceedings
 of the 2019 ACL Workshop BlackboxNLP 
Analyzing and Interpreting Neural Networks for
NLP, pages 276 286, Florence, Italy. Association
for Computational Linguistics.
Nelson Cowan. 1998. Attention and memory  An integrated
 framework . Oxford University Press.
R√≥bert Csord√°s, Kazuki Irie, and J√ºrgen Schmidhuber.
2022. The neural data router  Adaptive control Ô¨Çow
in transformers improves systematic generalization.
InInternational Conference on Learning Representations
 .
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Carbonell,
 Quoc Le, and Ruslan Salakhutdinov. 2019.
Transformer-XL  Attentive language models beyond
a Ô¨Åxed-length context. In Proceedings of the 57th
Annual Meeting of the Association for Computational
 Linguistics , pages 2978 2988, Florence, Italy.
Association for Computational Linguistics.
Mostafa Dehghani, Stephan Gouws, Oriol Vinyals,
Jakob Uszkoreit, and Lukasz Kaiser. 2019. Universal
 transformers. In International Conference on
Learning Representations .
Gregoire Deletang, Anian Ruoss, Jordi Grau-Moya,
Tim Genewein, Li Kevin Wenliang, Elliot Catt,
Chris Cundy, Marcus Hutter, Shane Legg, Joel Veness,
 and Pedro A Ortega. 2023. Neural networks
and the chomsky hierarchy. In International Conference
 on Learning Representations .
Adele Diamond. 2013. Executive functions. Annual
review of psychology , 64 135 168.
Maha Elbayad, Jiatao Gu, Edouard Grave, and Michael
Auli. 2020. Depth-adaptive transformer. In International
 Conference on Learning Representations .
Jeffrey L Elman. 1990. Finding structure in time. Cognitive
 science , 14(2) 179 211.K Anders Ericsson and Walter Kintsch. 1995. Longterm
 working memory. Psychological review ,
102(2) 211.
Alex Graves, Greg Wayne, and Ivo Danihelka.
2014. Neural turing machines. arXiv preprint
arXiv 1410.5401 .
Michael Hahn. 2020. Theoretical limitations of selfattention
 in neural sequence models. Transactions
of the Association for Computational Linguistics ,
8 156 171.
Sepp Hochreiter and J√ºrgen Schmidhuber. 1997.
Long short-term memory. Neural computation ,
9(8) 1735 1780.
Michael I Jordan. 1997. Serial order  A parallel distributed
 processing approach. In Advances in psychology
 , volume 121, pages 471 495. Elsevier.
Richard E Ladner and Michael J Fischer. 1980. Parallel
preÔ¨Åx computation. Journal of the ACM (JACM) ,
27(4) 831 838.
Sivaramakrishnan Lakshmivarahan and Sudarshan K
Dhall. 1994. Parallel computing using the preÔ¨Åx
problem . Oxford University Press.
Zhenzhong Lan, Mingda Chen, Sebastian Goodman,
Kevin Gimpel, Piyush Sharma, and Radu Soricut.
2020. Albert  A lite bert for self-supervised learning
of language representations. In International Conference
 on Learning Representations .
Bingbin Liu, Jordan T. Ash, Surbhi Goel, Akshay Krishnamurthy,
 and Cyril Zhang. 2023. Transformers
learn shortcuts to automata. In International Conference
 on Learning Representations .
Eric Martin and Chris Cundy. 2018. Parallelizing linear
 recurrent neural nets over sequence length. In
International Conference on Learning Representations
 .
Akira Miyake, Priti Shah, et al. 1999. Models of working
 memory . Cambridge  Cambridge University
Press.
Aida Nematzadeh, Sebastian Ruder, and Dani Yogatama.
 2020. On memory in human and artiÔ¨Åcial
language processing systems. In Proceedings of
ICLR Workshop on Bridging AI and Cognitive Science.

Maxwell Nye, Anders Johan Andreassen, Guy Gur-Ari,
Henryk Michalewski, Jacob Austin, David Bieber,
David Dohan, Aitor Lewkowycz, Maarten Bosma,
David Luan, Charles Sutton, and Augustus Odena.
2022. Show your work  Scratchpads for intermediate
 computation with language models.
Klaus Oberauer. 2002. Access to information in working
 memory  exploring the focus of attention. Journal
 of Experimental Psychology  Learning, Memory,
and Cognition , 28(3) 411.Antonio Orvieto, Samuel L Smith, Albert Gu, Anushan
Fernando, Caglar Gulcehre, Razvan Pascanu, and
Soham De. 2023. Resurrecting recurrent neural
 networks for long sequences. arXiv preprint
arXiv 2303.06349 .
OÔ¨År Press, Noah Smith, and Mike Lewis. 2022. Train
short, test long  Attention with linear biases enables
input length extrapolation. In International Conference
 on Learning Representations .
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
 models are unsupervised multitask learners.
OpenAI blog , 1(8) 9.
Jack Rae and Ali Razavi. 2020a. Do transformers need
deep long-range memory  In Proceedings of the
58th Annual Meeting of the Association for Computational
 Linguistics , pages 7524 7529.
Jack Rae and Ali Razavi. 2020b. Do transformers need
deep long-range memory  In Proceedings of the
58th Annual Meeting of the Association for Computational
 Linguistics , pages 7524 7529, Online. Association
 for Computational Linguistics.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2020. Exploring the limits
of transfer learning with a uniÔ¨Åed text-to-text transformer.
 The Journal of Machine Learning Research ,
21(1) 5485 5551.
Antoine Simoulin and Benoit Crabb√©. 2021. How
many layers and why  An analysis of the model
depth in transformers. In Proceedings of the 59th
Annual Meeting of the Association for Computational
 Linguistics and the 11th International Joint
Conference on Natural Language Processing  Student
 Research Workshop , pages 221 228, Online.
Association for Computational Linguistics.
Jimmy T.H. Smith, Andrew Warrington, and Scott Linderman.
 2023. SimpliÔ¨Åed state space layers for sequence
 modeling. In The Eleventh International
Conference on Learning Representations .
A√§ron van den Oord, Sander Dieleman, Heiga Zen,
Karen Simonyan, Oriol Vinyals, Alexander Graves,
Nal Kalchbrenner, Andrew Senior, and Koray
Kavukcuoglu. 2016. Wavenet  A generative model
for raw audio. In Arxiv .
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, ≈Å ukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Processing
 Systems , volume 30. Curran Associates, Inc.
Jason Wei, Xuezhi Wang, Dale Schuurmans, Maarten
Bosma, brian ichter, Fei Xia, Ed H. Chi, Quoc V Le,
and Denny Zhou. 2022. Chain of thought prompting
elicits reasoning in large language models. In Advances
 in Neural Information Processing Systems .Dani Yogatama, Cyprien de Masson d Autume, and
Lingpeng Kong. 2021. Adaptive Semiparametric
Language Models. Transactions of the Association
for Computational Linguistics , 9 362 373.A Hyperparameters for the Regular
Language Experiments
We report the hyperpamaters used in the regular
language experiments (Table 1) in Table 4.
B Hyperparameters for the Natural
Language Experiments
We report the hyperpamaters used in the natural
language experiments (Table 3) in Table 5.
C Proof of Lemma 1
Lemma 1 (Approximation for Binary Matrix Product).LetA,B {0,1}n nbe
 binary matrices of
dimensionn n. Then, there exists a two-layer
ReLU network such that
fmlp([Flat(A),Flat(B)])   Flat(AB),
where Flat(X)(i 1)n j Xi,j fori,j 
[1,...,n ]is the operation that Ô¨Çattens a matrix into
a vector.
Proof. Observe that a ReLU operation can perfectly
 approximate the multiplication of two binary
scalars 
ReLU (a b 1)  a b,fora,b {0,1}.
The binary matrix product ABis composed of n3
binary scalar products of the form 
AikBkj x(i 1)n kx(n k 1)n j
fori,j,k [1,..,n],
wherex  [Flat(A),Flat(B)]is the concatenated
Ô¨Çattened input. Our goal is to construct two neural
network layers. The Ô¨Årst layer computes all n3
binary scalar products. The second layer sums
these products into the form of matrix product  i.e., n
k 1AikBkj.
The Ô¨Årst layer s binary weight matrix W(1) 
{0,1}2n2 n3is constructed as 
Forz [1,...,2n2], i,j,k [1,...,n ],
W(1)
z,(i 1)n2 (j 1)n k 
{
1ifz  (i 1)n kor(n k 1)n j
0otherwise.
(3)
Then, the Ô¨Årst layer computes all n3binary scalar
products as follows 
ReLU(
[Flat(A),Flat(B)]W(1)  1 
n3)
(i 1)n2 (j 1)n k
 AikBkjfori,j,k [1,...,n ].To sum these n3products into n2results, the
second layer s binary weight matrix W(2) 
{0,1}n3 n2is constructed as 
W(2) In2  1n  
    1n0n0n... 0n
0n 1n0n... 0n
......
0n... 0n 1n 
    
 {0,1}n3 n2,
whereIn2is ann2 n2identity matrix, is the
Kronecker product, 0nis an n-dimensional column
vector of all zeros, and 1nis an n-dimensional
column vector of all ones. We arrive at a twolayer
 ReLU network that perfectly approximates
the multiplication of two binary matrices 
fmlp([Flat(A),Flat(B)])
 ReLU(
[Flat(A),Flat(B)]W(1)  1 
n3)
W(2)
 Flat(AB).
D Illustration of Lemma 1
D.1 Illustration of the Binary Weight
Matrices
We illustrate W(1)andW(2)of Lemma 1 as follows 

import numpy as np
def get_W1 ( n )  
n2   n *n
W1   np . z e r o s ( ( 2 *n*n , n *n*n ) , d t y p e   i n t)
f o r iin range ( n )  
f o r jin range ( n )  
f o r kin range ( n )  
W1[ i *n k , i *n2  j *n k ]   1
W1[ n2 k *n  j , i *n2  j *n k ]   1
return W1
def get_W2 ( n )  
eye   np . eye ( n *n , d t y p e   i n t)
ones   np . ones ( ( n , 1 ) , d t y p e   i n t)
W2   np . kron ( eye , ones )
return W2
get_W1(2) gives 
[ [ 1 0 1 0 0 0 0 0]
[0 1 0 1 0 0 0 0]
[0 0 0 0 1 0 1 0]
[0 0 0 0 0 1 0 1]
[1 0 0 0 1 0 0 0]
[0 0 1 0 0 0 1 0]
[0 1 0 0 0 1 0 0]
[0 0 0 1 0 0 0 1 ] ]# Layers Hidden Size # Attention Heads Train Seq. Len. # Trainable Params.
logCT 256 8 40 or 50 4.3 M
Optimizer Batch Size Train Steps Precision Dataset
Adam (lr 1e-4, 3e-4, 5e-4) 128 100,000 Ô¨Çoat32 Regular Languages
Table 4  Hyperparameters for the regular language experiments.
# Layers Hidden Size # Attention Heads Train Seq. Len. # Trainable Params.
KlogCT 768 12 512 81M ( K  6) or 123M (K  12 )
Optimizer Batch Size Train Steps Precision Dataset
Adam (lr 6e-4) 32 50,000 bÔ¨Çoat16 OpenWebText2
Table 5  Hyperparameters for the natural language experiments.
get_W2(2) gives 
[ [ 1 0 0 0]
[1 0 0 0]
[0 1 0 0]
[0 1 0 0]
[0 0 1 0]
[0 0 1 0]
[0 0 0 1]
[0 0 0 1 ] ]
D.2 An Illustrative Example for n  2
Suppose the input matrices are 
A [1 0
1 0]
, B  [0 1
1 0]
.
The concatenated Ô¨Çattened input becomes 
x  [Flat(A),Flat(B)]   [1 0 1 0 0 1 1 0] .
Then, Lemma 1 is veriÔ¨Åed as follows 
ReLU(
xW(1)  1 
n3)
W(2)
 ReLU ([1 1 2 0 1 1 2 0] 1)W(2)
 [0 0 1 0 0 0 1 0] W(2)
 [0 1 0 1]
 Flat([0 1
0 1])
 Flat(AB).
Here is the Python code for the above example 
A   np . a r r a y ( [ [ 1 , 0 ] , [ 1 , 0 ] ] ) . r e s h a p e (  1)
B   np . a r r a y ( [ [ 0 , 1 ] , [ 1 , 0 ] ] ) . r e s h a p e (  1)
x   np . c o n c a t e n a t e ( [ A, B ] ) . r e s h a p e (1 ,  1 )
W1   get_W1 ( 2 )
W2   get_W2 ( 2 )
flat_AB   np . maximum ( x @ W1  1 ,0) @ W2