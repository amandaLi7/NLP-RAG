Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining Takaaki Saeki1,Soumi Maiti2,Xinjian Li2,Shinji Watanabe2, Shinnosuke Takamichi1andHiroshi Saruwatari1 1The University of Tokyo, Japan 2Carnegie Mellon University, USA takaaki saeki@ipc.i.u-tokyo.ac.jp, {smaiti, swatanab }@andrew.cmu.edu While neural text-to-speech (TTS) has achieved human-like natural synthetic speech, multilingual TTS systems are limited to resource-rich languages due to the need for paired text and studio-quality audio data. This paper proposes a method for zeroshot multilingual TTS using text-only data for the target language. The use of text-only data allows the development of TTS systems for low-resource languages for which only textual resources are available, making TTS accessible to thousands of languages. Inspired by the strong cross-lingual transferability of multilingual language models, our framework first performs masked language model pretraining with multilingual text-only data. Then we train this model with a paired data in a supervised manner, while freezing a language-aware embedding layer. This allows inference even for languages not included in the paired data but present in the text-only data. Evaluation results demonstrate highly intelligible zero-shot TTS with a character error rate of less than 12% for an unseen language. Recent advances in end-to-end neural text-to-speech synthesis (TTS) [Liet al. , 2019b; Kim et al. , 2021 ]have yielded significant improvements in naturalness and speech quality. However, the data-intensive nature and the requirement of paired text and studio-quality audio data have limited multilingual TTS systems to resource-rich languages, which are small portions of the more than 6,000 languages in the world [Jr, 2005 ]. To address the limitation, current research in multilingual TTS aims not only to exploit resource-rich languages [Zenet al. , 2012; Li and Zen, 2016 ]but also to build models for low-resource languages [Prakash et al. , 2019 ]. Previous work has addressed low-resource TTS by using untranscribed speech data with vector-quantized variational autoencoder (VQ-V AE) [Zhang and Lin, 2020 ]or automatic speech recognition (ASR) models [Niet al. , 2022 ]. Another study [Saeki et al. , 2022b ]has built a massively multilingual TTS model jointly using paired TTS, paired ASR, unpaired speech, and unpaired text data. However, these approaches still rely on speech data for the target languages and language AText data for language CMultilingual TTS Figure 1: Our concept. We aim to build TTS model on languages for which only text data is available, to support low-resource languages. face the challenge of data collection, when audio recordings for these languages are hard to obtain. In this study, we focus on the use of a text-only data for multilingual TTS as shown in Fig. 1. Previous research [Wu and Dredze, 2019; Pires et al. , 2019 ]has shown the strong cross-lingual transferability of multilingual language models such as multilingual BERT [Devlin et al. , 2019 ]in natural language processing (NLP) tasks. By leveraging multilingual pretraining, the model can generalize to other languages, even if it has never seen the target data in those languages. Our work applies the framework of multilingual masked language model (MLM) pretraining to TTS, with the goal of achieving zero-shot cross-lingual transfer of pronunciation and prosody. Zero-shot TTS using text data enables the development of TTS systems for languages where only textual resources are available, which potentially opens up TTS to thousands of languages [Ebrahimi and Kann, 2021; Li et al. , 2022 ]. In this paper, we propose a multilingual TTS framework that leverages unsupervised text pretraining. Fig. 2 illustrates the proposed framework. We use a typical end-to-end TTS architecture consisting of token embedding, encoder, and decoder. Our model also has a language-aware embedding layer, which includes the token embedding layer, a language embedding layer, and a bottleneck layer. As shown in Fig. 2(a), we first pretrain the language-aware embedding layer and the encoder of the TTS model with multilingual text data. We then fine-tune the encoder and decoder of the TTS model with paired data, while the language-aware embedding layer is frozen, as illustrated in Fig. 2(b). This allows zeroshot TTS for a language not included in the paired data but present in the text data, as shown on the right in Fig. 2(c). Our contributions are as follows. 1) We propose a zero-shotarXiv:2301.12596v3  [eess.AS]  27 May 2023Bytes or IP A<Mask>Language-aware embedding layerEncoder Bytes or IP ALanguage-aware embedding layerEncoderDecoder Bytes or IP ALanguage-aware embedding layerEncoderDecoder(a) Unsupervised multilingual text pretraining (b) Supervised learning with paired data Bytes or IP ALanguage-aware embedding layerEncoderDecoder Multilingual TTS for seen languages Zero-shot TTS for unseen languageFreeze Text TextFigure 2: Proposed framework. (a) We perform MLM pretraining on multilingual text data and then (b) train TTS model on paired data with frozen language-aware embedding layer. (c) Zero-shot TTS is performed with language IDs that are not included in paired data. multilingual TTS framework that achieves highly intelligible TTS for an unseen language, resulting in a character error rate of less than 12%. 2) Our method also improves TTS for seen languages, resulting in byte-based models without graphemeto-phoneme (G2P) modules that outperform the phonemebased baselines. 3) Our ablation studies provide additional insights, including the effectiveness of the frozen languageaware embedding layer. The experiments were conducted on public datasets and the implementation is available1. We encourage readers to listen to our audio samples2. Our model has a typical neural TTS model architecture consisting of token embedding, encoder, and decoder. First, we use MLM pretraining with multilingual text data to learn cross-lingual representations. Then we perform supervised learning with paired data to learn the mapping from linguistic features to speech features. The model performs inference even for languages that are not present in the paired data. 2.1 Unsupervised Multilingual Text Pretraining Fig. 2(a) illustrates the unsupervised pretraining method. It uses multilingual text data consisting of languages that are not included in the paired data. Let X= (xn V|n= 1, , N)denote the input text token sequence of length N, where Vdenotes a vocabulary constructed for pretraining. 1https://github.com/Takaaki-Saeki/zm-text-tts 2https://takaaki-saeki.github.io/zm-tts-text demoWe define Dtextas the text dataset. Let Ltextdenote the set of language IDs included in Dtext. First, the masked token sequence Xmand a language ID ltext Ltextare fed to the model. Let the token embedding sequence and language embedding n Rd|n= 1, , N)andel Rd, respectively. The embedding layers output Zmandelas: Zm=Embed (Xm; T), el=Embed (ltext; L),(1) where  Tand Ldenote the model parameters of the token embedding and language embedding layers, respectively. Then the token and language embeddings obtained in Eq. (1) are added and fed to a bottleneck layer to project them into a hidden input vector. Let Hin= (hin,n Rd|n= 1, , N) andHout= (hout,n Rd|n= 1, , N)denote hidden vectors in the encoder input and output, respectively. Then the conditional probability p(X|X )is computed as: Hin=Bottleneck (Zm+el; B), (2) Hout=Encoder (Hin; E), (3) p(X|X ) =Softmax (PredictionNet (Hout; P)),(4) where  B, E, Pdenote the model parameters of the bottleneck layer, the encoder and a prediction network, respectively. In Eq. (4), Softmax ( )denotes a softmax function. We define the network with the model parameters { B,  T,  L} aslanguage-aware embedding layer , which jointly embeds the token sequence Xand the language ID ltextas in Eq. (1) and (2). Let   = (  k N|k= 1, , K)be the indexes of the masked tokens of length K. With the probability computed in Eq. (4), the training objective can be defined as: { E, B, T, L}= arg min We use UTF-8 bytes or International Phonetic Alphabet (IPA) symbols for the input token sequence X. For each token type, the vocabulary Vis constructed from Dtext, which includes a start/end of sentence token ([ SOS/EOS ]). We extracted International IPA sequences using an open-source toolkit3. To obtain the masked token Xm, we use the same masking ratio and category as in the original BERT pretraining [Devlin et al. , 2019 ]for each token type. Randomly, 12 % of the tokens are replaced with the [ MASK ] token, and 1.5 % of them are replaced with random tokens. Also, 1.5 % of the tokens are left unchanged and Lmlmis computed as in Eq. (5) for those 15 % of tokens that have indices  . 2.2 Supervised Learning with Paired Data Fig. 2(b) illustrates the supervised learning of the TTS model with paired data. We define the paired data and the set of language IDs as Dpaired andLpaired , respectively. Note that we assume Lpaired Ltext. Let Y= (yt RD|t= 1, , T) denote the speech feature sequence with the length of T. We first initialize the model parameters { E,  B,  T,  L}with 3https://github.com/espeak-ng/espeak-ngthose obtained in the pretraining described in   2.1. Let  D denote the model parameter of the decoder. The speech features are predicted with teacher forcing as: Hout=Encoder (Bottleneck (Z+el)), (6) Y=Decoder (Hout, Y; D), (7) where Zis the unmasked token embedding sequence. Note that the unmasked token sequence is used in Eq. (6), while the masked token sequence is used in Eq. (2) Let Ltts( Y , Y) denote the training objective of the TTS model. Then we consider two types of schemes. Updating language-aware embedding layer We only freeze the parameter of the language embedding layer  L while updating the rest of the parameters. Therefore the trainable model parameters can be written as { D, E, B, T}= arg min D, E, B, TLtts( Y , Y). (8) Previous work has confirmed that multilingual BERT has high cross-lingual transferability for various NLP tasks [Wu and Dredze, 2019 ]. This scheme corresponds to a simple finetuning of BERT [Wu and Dredze, 2019 ], which updates all the parameters during training for the downstream tasks4. Freezing language-aware embedding layer We freeze the bottleneck layer and the token embedding layer along with the language embedding, updating the encoder and decoder. The training process can be written as D, ELtts( Y , Y). (9) In contrast to the scheme represented in Eq. (8), the scheme in Eq. (9) preserves the parameters of the language-aware embedding layer to facilitate cross-lingual transfer. In the evaluation, we use the scheme formulated in Eq. (9), except for the ablation study in   3.4. LetLsyndenote the set of language IDs used for inference. The text token sequence Xand the language ID lsyn Lsyn are fed to the model as in Eq. (1), and the encoder output is predicted as in Eq. (6). Unlike Eq. (7), the speech features are Y=Decoder (Hout; D). (10) The output waveform is obtained by feeding the predicted features  Yto a pretrained neural vocoder. Fig. 2(c) illustrates the inference process. The left and right sides of the figure show the typical multilingual TTS and our zero-shot TTS. Previous work [Liet al. , 2019a ]has typically assumed seen languages, and the inference is performed with the language IDs Lseen Lpaired . However, it is challenging to perform TTS for unseen languages Lunseen Lpaired = . While other work [Saeki et al. , 2022b ]has built a massively multilingual TTS model that even achieves zero-shot TTS from ASR data, it uses paired data for the target languages. 4We freeze the language embedding layer to address the mismatch between language embedding of seen and unseen languages.Our work attempts to only use the linguistic knowledge to improve the zero-shot TTS. Thus, the inference process is written unseen  Lpaired = andL unseen  Ltext. In the evaluation, we denote the inference with Lunseen andL asFully zero-shot TTS andText-seen zero-shot TTS , respectively. Fully zero-shot TTS performs zero-shot TTS without pretraining as in the IPA-based previous method [Staib et al. , 2020 ], which is the baseline method in our evaluations. 2.4 Model Architecture Our model is an autoregressive TTS model based on Transformer TTS [Liet al. , 2019b ], which has also been used in the previous work on byte-based multilingual TTS [Heet al. , 2021 ]. During the supervised learning described in   2.2 and inference described in   2, we use x-vector [Snyder et al. , 2018 ]for the speaker embedding and add it to the encoder output through a projection layer. During supervised learning, we use the average x-vectors computed from the training data. For evaluation purposes, we perform zero-shot synthesis with the average x-vector from the test data of the target language and feed it to the model. Note that we also conduct the evaluation with x-vectors from seen languages. For the bottleneck layer with  B, we use a residual network consisting of Layer Normalization [Baet al. , 2016 ], down projection, ReLU [Nair and Hinton, 2010 ], and up projection with the residual connection, which is used in previous work on language adaptation [Bapna and Firat, 2019 ]. 3 Experimental Evaluations 3.1 Experimental Setting We carried out all the evaluations with publicly available datasets. Table 1 shows the sizes of the data for each language. For the unsupervised text pretraining described in 2.1, we used transcripts from V oxPopuli [Wang et al. , 2021 ], M-AILABS [Munich Artificial Intelligence Laboratories GmbH, 2017 ], and CSS10 [Park and Mulc, 2019 ], resulting in a total of about 2.8 GB of spoken text across 19 languages. We used CSS10 for the supervised learning described in   2.2, and we selected seven European languages as the seen languages, with Spanish as the unseen language. The paired data consisted of one speaker per language. It should be noted that Spanish is not actually a low-resource language, but we chose to use it for evaluation purposes in order to 1) compare our zero-shot TTS methods with the oracle methods using the paired data for the target language and 2) ensure a sufficient number of evaluators for the subjective evaluation. We used 5 and 100 utterances as dev and test sets, respectively, with the remaining data used for training. The sampling rate was set to 16 kHz. An 80-dimension of mel filter bank, 1024 samples of FFT length, and 256 samples of frame shit were used for speech analysis. For the pretraining described in   2.1, we trained the model for 1.2M iterations using the Noam optimizer [Vaswani et al. , 2017 ]with the learning rate and warm-up step set to 1.0 and 10000, respectively. For the TTS model described inLanguages Code Text-only dataPaired data Seen languages for evaluation Lseen German de 359MB 0.73MB 16.13h French fr 372MB 0.94MB 19.15h Dutch nl 336MB 0.75MB 14.10h Finnish fi 308MB 0.47MB 21.36h Hungarian hu 104MB 0.51MB 10.53h Russian ru 4.9MB 1.5MB 10.00h Greek el 0.39MB 0.39MB 4.13h Unseen language for evaluation Lunseen Spanish es 345MB 0.0MB (1.2MB) 0.00h (23.81h) Languages not included in CSS10 Table 1: Amount of text-only and paired data for each language. Parentheses indicate amount of original data in CSS10. 2.4, we used a 6-block Transformer encoder [Vaswani et al., 2017 ]and a 6-block Transformer decoder, with a postnet consisting of five convolutional layers with a kernel size of five. The attention dimension and the number of attention heads were set to 512 and 8, respectively. For the bottleneck layer described in   2.4, we set the hidden dimension after the down projection to 256. The PredictionNet in Eq. (4) consisted of a linear layer, a GELU activation function [Hendrycks and Gimpel, 2016 ], Layer Normalization, and a linear layer with the hidden dimension of 512. We also used guided attention loss [Tachibana et al. , 2018 ]to improve the training efficiency. For the supervised learning described in   2.2, we trained the models for 2.47M iterations (200 epochs). The Noam optimizer was used with the warm-up step of 50000. For the neural vocoder, we trained HiFi-GAN [Kong et al. , 2020 ]for 2M iterations with LibriTTS [Zen et al. , 2019 ], VCTK [Veaux et al. , 2017 ], and CSS10. For the x-vector described in   2.4, we used a model trained on V oxCeleb1 and V oxCeleb2 [Nagrani et al. , 2017 ]published in SpeechBrain [Ravanelli et al., 2021 ]. We used ESPnet2-TTS [Watanabe et al. , 2018; Hayashi et al. , 2021 ]for the implementation. We developed baseline models without the pretraining. Seen language Monolingual: We trained a model for each language independently. Our preliminary study found that Transformer TTS was unstable5and could not synthesize intelligible speech in the monolingual condition due to the lack of training data. Therefore, we used Tacotron2 [Shen et al. , 2018 ]only for the monolingual models, as in the original paper of the dataset [Park and Mulc, 2019 ].Multilingual w/o LIDs: We trained a multilingual Transformer TTS model using the paired data shown in Table 1 without language IDs 5The original paper [Liet al. , 2019b ]also reports the instability.(LIDs). Multilingual w/ LIDs: We trained a multilingual Transformer TTS model with the paired data of the unseen language. It also used the language IDs. Unseen language We compared Fully zero-shot TTS and Text-seen zero-shot TTS defined in   2.3. In Oracle , we used theMonolingual andMultilingual w/ LIDs , which used the paired data of the unseen language. In Fully zero-shot TTS , we used Multilingual w/o LIDs to synthesize speech from text tokens in the unseen language. This method corresponds to the conventional multilingual TTS model using bytes [Heet al., 2021 ]or IPA symbols [Staib et al. , 2020 ]. To objectively measure the synthetic speech quality, we used mel cepstral distortion (MCD) [Fukada et al. , 1992 ]with the mel cepstrum dimension set to 25. We also evaluated the intelligibility using CERs computed with a multilingual ASR model [Radford et al. , 2022 ]. We used a pretrained large model that is publicly available6. To evaluate the naturalness, we carried out listening tests to calculate five-scale mean opinion scores (MOS) of synthesized speech for each method. Forty native speakers were recruited through Amazon Mechanical Turk [Paolacci et al. , 2010 ]for each of the tests. Furthermore, we leveraged a publicly available automatic MOS (AMOS) prediction model [Saeki et al. , 2022a ] to evaluate the naturalness. Note that the model was trained on English and Chinese datasets, but previous work [Seki et al., 2022 ]has reported that it also showed a correlation coefficient higher than 0.8 for another language (Japanese). 3.2 Evaluation Results on Seen Languages We evaluated our framework on the seen languages included in the paired data, as defined in   2.3. Table 2 lists the results in MCD and CER. Lower values are better for both metrics. As we can see, the byte-based or IPA-based models with the proposed multilingual pretraining performed the best across all languages and metrics. Among the baselines, byte-based monolingual and multilingual models tended to have higher MCD and CER than IPA-based models, and failed to synthesize intelligible speech in some languages. For example, the baseline byte-based models showed the high CER values for French, which has a deep orthography, meaning that a single character has different pronunciations depending on the context. We observed that our method improved the byte-based models and they outperformed the IPA-based baseline models for all the metrics and languages. It is worth noting that the proposed byte-based models even outperformed the proposed IPA-based models except for el and ru. These results suggest that our framework is effective in building a TTS model for languages without G2P modules. 3.3 Evaluation Results on Unseen Language We evaluated our method on zero-shot TTS for the unseen language defined in   2.3. As described in   2.4, we first used the x-vector from the es speaker to compute the MCD. Table 3 lists the results. The baseline models showed the CERs of over 40% and MCDs of over 10.0. However, our 6https://github.com/openai/whisperMethodde fr ru fi hu nl el MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Natural - 2.75 - 4.52 - 2.12 - 4.73 - 4.86 - 6.22 - 7.14 Baseline (Monolingual) Bytes monolingual 7.70 8.61 11.76 91.82 11.43 >100 8.33 56.03 10.22 93.05 7.49 15.33 10.20 85.98 IPA monolingual 7.38 4.07 8.96 17.86 11.89 25.30 7.23 27.62 7.59 24.62 7.80 19.20 8.16 21.79 Baseline (Multilingual) Bytes multilingual w/o LIDs 7.68 37.46 8.71 41.35 9.38 45.92 6.26 29.19 6.48 33.82 8.46 46.33 7.64 36.24 Bytes multilingual w/ LIDs 6.51 13.19 10.84 55.79 12.89 >100 6.78 27.22 9.09 42.97 8.47 39.37 7.25 23.56 IPA multilingual w/o LIDs 6.31 10.64 7.44 20.86 8.10 35.32 5.53 19.56 5.59 14.03 7.76 34.49 6.90 19.33 IPA multilingual w/ LIDs 6.16 9.76 6.88 14.97 7.63 23.54 5.17 10.63 5.28 9.11 6.95 19.48 6.90 16.97 Proposed (Unsupervised text pretraining) Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 5.01 6.05 6.52 13.74 6.57 11.75 IPA multilingual 5.88 5.52 6.61 7.72 7.25 15.85 5.18 8.62 5.30 7.37 7.00 14.42 6.53 11.06 Table 2: Evaluation results for seen languages. Bold indicates best scores in baseline and proposed methods. es x-vector fr x-vector Bytes monolingual 8.65 10.70 IPA monolingual 8.47 5.28 IPA multilingual 6.20 5.32 6.99 Baseline (Fully zero-shot TTS) Bytes multilingual 11.22 64.07 66.45 IPA multilingual 10.75 44.75 44.37 Proposed (Text-seen zero-shot TTS) Bytes multilingual 9.05 18.27 13.74 IPA multilingual 9.44 11.69 13.33 Table 3: Evaluation results for unseen language. proposed text preraining improved the metrics, resulting in CERs of less than half for both byte and IPA-based methods. Also, in contrast to the results for the seen languages, the IPA-based model outperformed the byte-based one in terms of CER. Compared with the oracle case with the paired data of the unseen language, our proposed zero-shot TTS showed higher MCD and CER but achieved only 1% difference in CER compared to the oracle byte-based monolingual model. These results demonstrate the effectiveness of our method in achieving intelligible zero-shot TTS for the unseen language. To investigate the case where the target speaker information is completely unavailable, we also used the x-vector from a seen language. We chose the fr speaker because es and fr are both categorized as Western Romance in Glottolog [Hammarstr omet al. , 2021 ]. Table 3 lists the results. Note that this case does not have the MCD results, since a different speaker than the ground-truth speech was used. We can see that the unsupervised text pretraining also improved the zeroshot performance when using the x-vector from the fr speaker. In the proposed byte-based model, the cross-lingual x-vector showed the lower CER. This might result from that the es x-vector was not present in the training data whereas the fr x-vector was present in the training data. (a) Token embedding  (b) Encoder inputs  !"Figure 3: Visualization of token and language embedding. Pairs of similar languages (es fr and de nl) are overlapping in token embedding space, while output of bottleneck layer separates them. To further evaluate our method, we conducted several ablation studies. Table 4 lists the results. Bytes multilingual represents the byte-based proposed method in the evaluation of 3.2 and 3.3. Note that it used the frozen language-aware embedding layer as formulated in Eq. (9). Some additional studies of our method are also presented in the Appendix. InW/o bottleneck layer , we excluded the bottleneck layer and simply added the token and language embedding to obtain the encoder input in Eq. (2). We found that removing the bottleneck layer led to a performance drop in all the languages and metrics, with an average increase of 0.53 in MCD and 4.16% in CER. The largest increase was observed in the unseen language, with an increase of 1.21 in MCD. This suggests that the bottleneck layer, which projects the token and language embedding into the hidden input text representation with nonlinear dimensionality reduction, is effective in improving the generalization for zero-shot TTS. We also evaluated the effect of including language IDs in the proposed method by comparing it with a version that excluded language IDs, referred to as W/o language ID . It corresponds to a simple multilingual BERT pretraining [Wu and Dredze, 2019 ]that uses only text tokens across different languages. We observed that the use of language IDs led to anMethodSeen UnseenAvg.de fr ru fi es MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Bytes multilingual 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 W/o bottleneck layer 6.06 5.01 7.15 9.09 7.71 28.52 5.33 6.47 10.26 24.01 6.99 13.74 W/o language ID 6.07 5.09 7.09 9.99 7.77 22.58 5.23 6.99 10.45 32.70 6.96 14.06 W/o initializing encoder 5.59 3.75 6.52 9.31 7.12 16.47 4.86 5.03 9.02 21.91 6.42 11.85 Updating language-aware embedding layer 6.05 6.22 6.75 6.93 7.46 11.42 5.16 8.00 9.48 17.21 6.75 10.62 Table 4: Ablation studies on training and model configurations. Bold indicates best metrics on average (Avg.). Avg (AMOS)11.522.533.544.5 defrNaturalBaseline (IPA monolingual)Baseline (Bytes multilingual w/o LIDs)Baseline (IPA multilingual w/o LIDs)Baseline (IPA multilingual w/ LIDs)Proposed (Bytes multilingual)Proposed (IPA multilingual) Figure 4: MOS and AMOS results for seen languages. Error bars in MOS results represent 95% confidence intervals. Natural - 2.75 - 2.12 IPA monolingual 7.38 4.07 7.59 24.62 IPA multilingual 6.16 9.76 5.28 9.11 Baseline (Fully zero-shot TTS) IPA multilingual 10.31 38.75 9.93 52.62 Proposed (Text-seen zero-shot TTS) Bytes multilingual 10.00 28.01 9.40 50.11 Table 5: Analysis on different unseen languages. average improvement of 0.5 MCD and 4.48% CER, indicating the effectiveness of our approach in using language IDs. InW/o initializing encoder , we did not initialize the encoder Ebefore the supervised leaning described in   2.2. Instead, we only initialized the parameters  T, L, and  B with the parameters pretrained in   2.1. Through this evaluation, we investigated whether the performance gain with our method resulted from the initialization of the languageaware embedding layer or the encoder. We observed that W/o initializing encoder resulted in an improvement of 0.04 in MCD and only a 2.27% increase in CER on average, suggesting that our method benefits more from the pretraining of the language-aware embedding layer than from the encoder. InUpdating language-aware embedding layer , we updated the language-aware embedding layer during supervised learning, as formulated in Eq. (8). We observed that freezing the language-aware embedding layer led to better performance for most languages and metrics, resulting in an average difference of 0.29 in MCD and 1.04% in CER. NaturalOracle (IPA monolingual)Oracle (IPA multilingual)Baseline (IPA multilingual)Proposed (Bytes multilingual)Proposed (IPA multilingual) es (MOS)es (AMOS)11.522.533.544.5 0.5560.444es (AB test)p-value: 0.011Figure 5: MOS, AMOS, and AB test results for unseen language. Error bars in MOS results represent 95% confidence intervals. 3.5 Dependency on Unseen Languages We conducted evaluations on the zero-shot TTS for different unseen languages. The eight European languages included in the paired data are composed of Indo-European and Uralic language families defined in Glottolog [Hammarstr  omet al. , 2021 ]. In this evaluation, we selected de and hu from each of the families. During supervised learning in   2.2, we excluded the paired data for each of de and hu and instead included the paired data for es. Table 5 lists the results. We chose the IPA-based baseline method, which had shown better results in   3.3. We observed that the pretraining improved the CER by around 10% and MCD by around 0.3 for de. However, the improvement in CER for hu was limited to 2%, while the MCD was improved by around 0.5. These results suggest that the performance of our zero-shot TTS is language dependent, as observed in previous work on crosslingual transfer for NLP tasks [Wu and Dredze, 2019 ]. Fig. 3 visualize the token embedding Zand encoder inputsHinaveraged on each utterance. We used a t-distributedstochastic neighbor embeddings (t-SNE) [der Maaten and Hinton, 2008 ]. We observed overlaps in the token embedding for (es, fr) and (de, nl), which are classified as Western Romance and West Germanic in Glottolog, respectively. The encoder inputs are separated in the embedding space for each language. The results in Table 5 and the visualization suggest that the cross-lingual transfer works better when similar languages sharing the token embedding space are present during supervised learning. However, for languages with distinct token and language embeddings, the cross-lingual transferability might be limited. We leave the further analysis on language dependencies as a topic for future research. 3.6 Subjective Evaluations on Naturalness We conducted evaluations on naturalness as described in 3.1. Fig. 4 shows the results for seen languages. Note that we conducted the listening tests for de and fr. For each language, either of the proposed methods showed the highest MOS, while we did not observe any significant difference between the proposed methods and the best baseline method, which was the IPA-based multilingual model with LIDs. To further validate our results, we also evaluated the naturalness with an AMOS prediction model, as shown in Fig. 4. We observed that the either of the proposed methods showed the highest scores in all the languages. On average, the byte-based and IPA-based proposed models showed 2.89 and 2.84, respectively, while the best baseline method obtained 2.837. Additionally, we observed that the byte-based proposed model often scored higher than the IPA-based proposed models, which is consistent with the results in Table 2. Fig. 5 shows the results for unseen languages. The oracle methods had the highest MOS of 3.76 and 3.96, and the baseline zero-shot method had the lowest MOS of 3.29. The proposed methods outperformed the baseline method, and the byte- and IPA-based models had the MOS of 3.44 and 3.32, respectively. The AMOS results were consistent with the listening test results, with the proposed zero-shot TTS methods outperforming the baseline method. In this evaluation, the proposed byte-based model scored 3.21 on the AMOS, while the oracle IPA-based model scored 3.20. To further validate the results, we conducted a preference AB test on naturalness with 25 rators. As shown in Fig. 5, our byte-based model significantly outperformed the baseline IPA-based model. Multilingual TTS While previous work on multilingual TTS has primarily focused on resource-rich languages [Zen et al. , 2012; Li and Zen, 2016 ], there is growing interest in developing TTS models on low-resource languages. Several studies have explored the input tokens shared across languages such as bytes [Liet al. , 2019a; He et al. , 2021 ], IPA symbols [Gutkin, 2017 ], and articulatory features [Lux and Vu, 2022 ], to transfer knowledge from resource-rich to lowresource languages. Grapheme tokens can eliminate the per7The AMOS tended to be lower than the MOS. While the MOS prediction model has a high correlation, it may produce errors in predicting absolute values, as reported in previous work [Saeki et al. , 2022a ]. The relative relationships are more reliable in the AMOS.language G2P knowledge, and previous work has built a bytebased TTS model for around 40 languages [Heet al. , 2021 ]. There has been work using the phonological features derived from IPA to achieve the zero-shot TTS [Staib et al. , 2020 ]. Our framework achieves the zero-shot cross-lingual transfer with bytes by leveraging multilingual text pretraining. There have been studies on using untranscribed speech data for lowresource scenarios by leveraging VQ-V AE [Zhang and Lin, 2020 ]or an ASR model [Ren et al. , 2019; Ni et al. , 2022 ]. Other work [Saeki et al. , 2022b ]has trained a massively multilingual TTS using paired TTS, paired ASR, unpaired speech, and unpaired text data. While it also performs textonly training as in our work, it still uses the paired speech-text data of the target languages. Our framework is simple and scalable, while pioneering a novel paradigm with the zeroshot TTS approach that relies only on text data. Cross-lingual representation learning for NLP There have been studies on learning cross-lingual representations that can be applied to various NLP tasks in different languages [Gouws et al. , 2015; Ruder et al. , 2019 ]. Recent work has highlighted the strong cross-lingual transferability of multilingual BERT [Devlin et al. , 2019 ], which has been observed to perform surprisingly well when transferred to other languages [Wu and Dredze, 2019; Conneau and Lample, 2019 ]. Building on this, our work leverages multilingual MLM pretraining for TTS, which improves byte-based TTS models without G2P knowledge and achieves zero-shot TTS. Language model pretraining for TTS Previous research has explored self-supervised text pretraining techniques for TTS. BERT models have been used to extract contextual embeddings and enhance the prosody of TTS [Hayashi et al., 2019; Xu et al. , 2021 ]. Other studies have used phonemes jointly with graphemes [Jiaet al. , 2021 ]or subphonemes [Zhang et al. , 2022 ]as the inputs of the MLM pretraining. Our work proposes multilingual MLM pretraining for TTS using text tokens shared across languages, rather than focusing on monolingual pretraining. We presented a multilingual TTS framework that leverages unsupervised text pretraining. Our framework achieved highly intelligible zero-shot TTS for an unseen language, resulting in a CER of less than 12%. It also improved the TTS for seen languages, with byte-based models without G2P modules outperforming the IPA-based baselines. Our ablation studies provided additional insights, including the effectiveness of the frozen language embedding layer. Limitations and future work Our proposed framework has limitations. The performance gap remains between the oracle models and our zero-shot TTS models in terms of intelligibility, speech quality, and naturalness, as seen in the evaluation in   3.3 and   3.6. Further studies are needed to improve our zero-shot TTS. Our framework also has a limitation with language dependency, as the results in   3.5 suggest that this dependency is caused by the presence of similar languages during supervised learning. Our future work will focus on studying this language dependency further and developing a method that performs better for various languages.Acknowledgments Part of this work was supported by JSPS KAKENHI Grant Number 21H05054, 22H03639, and 22J12040. This work used the Bridges system [Nystrom et al. , 2015 ], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center. We would like to thank the research teams at Google through the internship of the first author for providing various insights on this topic. [Baet al. , 2016 ]J. L. Ba, J. R. Kiros, and G. E Hinton. Layer normalization. arXiv preprint arXiv:1607.06450 , 2016. [Ba n onet al. , 2020 ]M. Ba  n on, P. Chen, B. Haddow, K. Heafield, H. Hoang, M. Espl `a-Gomis, M. L Forcada, A. Kamran, F. Kirefu, P. Koehn, et al. Paracrawl: Webscale acquisition of parallel corpora. In Proc. ACL , pages [Bapna and Firat, 2019 ]A. Bapna and O. Firat. Simple, scalable adaptation for neural machine translation. In Proc. EMNLP-IJCNLP , pages 1538 1548, 2019. [Conneau and Lample, 2019 ]A. Conneau and G. Lample. Cross-lingual language model pretraining. In Proc. NeurIPS , pages 7059 7069, 2019. [der Maaten and Hinton, 2008 ]L. Van der Maaten and G. Hinton. Visualizing data using t-sne. JMLR , 9(11):2579 2605, 2008. [Devlin et al. , 2019 ]J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova. BERT: Pre-training of deep bidirectional transformers for language understanding. In Proc. NAACL , pages 4171 4186, 2019. [Ebrahimi and Kann, 2021 ]A. Ebrahimi and K. Kann. How to adapt your pretrained multilingual model to 1600 languages. In Proc. ACL-IJCNLP , pages 4555 4567, 2021. [Fukada et al. , 1992 ]T. Fukada, K. Tokuda, T. Kobayashi, and S. Imai. An adaptive algorithm for mel-cepstral analysis of speech. In Proc. ICASSP , pages 137 140, 1992. [Gouws et al. , 2015 ]S. Gouws, Y . Bengio, and G. Corrado. Bilbowa: Fast bilingual distributed representations without word alignments. In Proc. ICML , pages 748 756, [Gutkin, 2017 ]A. Gutkin. Uniform multilingual multispeaker acoustic model for statistical parametric speech synthesis of low-resourced languages. In Proc. Interspeech , pages 2183 2187, 2017. [Hammarstr  omet al. , 2021 ]H. Hammarstr  om, R. Forkel, M. Haspelmath, and S. Bank. Glottolog 4.5. Max Planck Institute for the Science of Human History , 2021. [Hayashi et al. , 2019 ]T. Hayashi, S. Watanabe, T. Toda, K. Takeda, S. Toshniwal, and K. Livescu. Pre-trained text embeddings for enhanced text-to-speech synthesis. In Proc. Interspeech , pages 4430 4434, 2019. [Hayashi et al. , 2021 ]T. Hayashi, R. Yamamoto, T. Yoshimura, P. Wu, J. Shi, T. Saeki, Y . Ju, Y . Yasuda, S. Takamichi, and S. Watanabe. Espnet2-tts:Extending the edge of tts research. arXiv preprint arXiv:2110.07840 , 2021. [Heet al. , 2021 ]M. He, J. Yang, L. He, and F. K Soong. Multilingual byte2speech models for scalable low-resource speech synthesis. arXiv preprint arXiv:2103.03541 , 2021. [Hendrycks and Gimpel, 2016 ]D. Hendrycks and K. Gimpel. Gaussian error linear units (gelus). arXiv preprint arXiv:1606.08415 , 2016. [Jiaet al. , 2021 ]Y . Jia, H. Zen, J. Shen, Y . Zhang, and Y . Wu. PnG BERT: Augmented BERT on phonemes and graphemes for neural TTS. arXiv preprint arXiv:2103.15060 , 2021. [Jr, 2005 ]R. G Gordon Jr. Ethnologue, languages of the world. https://www.ethnologue.com/, 2005. Accessed: [Kim et al. , 2021 ]J. Kim, J. Kong, and J. Son. Conditional variational autoencoder with adversarial learning for endto-end text-to-speech. In Proc. ICML , pages 5530 5540, [Kong et al. , 2020 ]J. Kong, J. Kim, and J. Bae. HiFi-GAN: Generative adversarial networks for efficient and high fidelity speech synthesis. Proc. NeurIPS , 33:17022 17033, [Li and Zen, 2016 ]B. Li and H. Zen. Multi-language multispeaker acoustic modeling for LSTM-RNN based statistical parametric speech synthesis. In Proc. Interspeech , pages 2468 2472, 2016. [Liet al. , 2019a ]B. Li, Y . Zhang, T. Sainath, Y . Wu, and W. Chan. Bytes are all you need: End-to-end multilingual speech recognition and synthesis with bytes. In Proc. ICASSP , pages 5621 5625, 2019. [Liet al. , 2019b ]N. Li, S. Liu, Y . Liu, S. Zhao, and M. Liu. Neural speech synthesis with Transformer network. In Proc. AAAI , pages 6706 6713, 2019. [Liet al. , 2022 ]X. Li, F. Metze, D. R Mortensen, A. W Black, and S. Watanabe. ASR2K: Speech recognition for around 2000 languages without audio. arXiv preprint arXiv:2209.02842 , 2022. [Lux and Vu, 2022 ]F. Lux and T. Vu. Language-agnostic meta-learning for low-resource text-to-speech with articulatory features. In Proc. ACL , pages 6858 6868, 2022. [Munich Artificial Intelligence Laboratories GmbH, 2017 ] Munich Artificial Intelligence Laboratories GmbH. The M-AILABS speech dataset. https://www.caito.de/ 2019/01/the-m-ailabs-speech-dataset/, 2017. Accessed: [Nagrani et al. , 2017 ]A. Nagrani, J. S. Chung, and A. Zisserman. V oxCeleb: A large-scale speaker identification dataset. In Proc. Interspeech , pages 2616 2620, 2017. [Nair and Hinton, 2010 ]V . Nair and G. E Hinton. Rectified linear units improve restricted boltzmann machines. In Proc. ICML , 2010.[Niet al. , 2022 ]J. Ni, L. Wang, H. Gao, K. Qian, Y . Zhang, S. Chang, and M. Hasegawa-Johnson. Unsupervised text-to-speech synthesis by unsupervised automatic speech recognition. In Proc. Interspeech , pages 461 465, 2022. [Nystrom et al. , 2015 ]N. A Nystrom, M. J Levine, R. Z Roskies, and J Ray Scott. Bridges: a uniquely flexible hpc resource for new communities and data analytics. In Proc. XSEDE , pages 1 8, 2015. [Paolacci et al. , 2010 ]G. Paolacci, J. Chandler, and P. G Ipeirotis. Running experiments on amazon mechanical turk. Judgment and Decision making , 5(5):411 419, 2010. [Park and Mulc, 2019 ]K. Park and T. Mulc. CSS10: A collection of single speaker speech datasets for 10 languages. Proc. Interspeech , pages 1566 1570, 2019. [Pires et al. , 2019 ]T. Pires, E. Schlinger, and D. Garrette. How multilingual is multilingual BERT? In Proc. ACL , pages 4996 5001, 2019. [Prakash et al. , 2019 ]A. Prakash, A L. Thomas, S Umesh, and H. A Murthy. Building multilingual end-to-end speech synthesisers for Indian languages. In Proc. SSW , pages [Radford et al. , 2022 ]A. Radford, J. W Kim, T. Xu, G. Brockman, C. McLeavey, and I. Sutskever. Robust speech recognition via large-scale weak supervision. arXiv preprint arXiv:2212.04356 , 2022. [Ravanelli et al. , 2021 ]M. Ravanelli, T. Parcollet, P. Plantinga, A. Rouhe, S. Cornell, L. Lugosch, C. Subakan, N. Dawalatabad, A. Heba, J. Zhong, et al. SpeechBrain: A general-purpose speech toolkit. arXiv preprint arXiv:2106.04624 , 2021. [Renet al. , 2019 ]Y . Ren, X. Tan, T. Qin, S. Zhao, Z. Zhao, and T.-Y . Liu. Almost unsupervised text to speech and automatic speech recognition. In Proc. ICML , pages 5410 [Ruder et al. , 2019 ]S. Ruder, I. Vuli  c, and A. S gaard. A survey of cross-lingual word embedding models. JAIR , [Saeki et al. , 2022a ]T. Saeki, D. Xin, W. Nakata, T. Koriyama, S. Takamichi, and H. Saruwatari. UTMOS: UTokyo-SaruLab system for V oiceMOS Challenge 2022. InProc. Interspeech , pages 4521 4525, 2022. [Saeki et al. , 2022b ]T. Saeki, H. Zen, Z. Chen, N. Morioka, G. Wang, Y . Zhang, A. Bapna, A. Rosenberg, and B. Ramabhadran. Virtuoso: Massive multilingual speech-text joint semi-supervised learning for text-to-speech. arXiv preprint arXiv:2210.15447 , 2022. [Seki et al. , 2022 ]K. Seki, S. Takamichi, T. Saeki, and H. Saruwatari. Text-to-speech synthesis from dark data with evaluation-in-the-loop data selection. arXiv preprint arXiv:2210.14850 , 2022. [Shen et al. , 2018 ]J. Shen, R. Pang, R. J Weiss, M. Schuster, N. Jaitly, Z. Yang, Z. Chen, Y . Zhang, Y . Wang, RJ Skerrv-Ryan, et al. Natural TTS synthesis by conditioning WaveNet on mel spectrogram predictions. In Proc. ICASSP , pages 4779 4783, 2018.[Snyder et al. , 2018 ]D. Snyder, D. Garcia-Romero, G. Sell, D. Povey, and S. Khudanpur. X-vectors: Robust dnn embeddings for speaker recognition. In Proc. ICASSP , pages [Staib et al. , 2020 ]M. Staib, T. H. Teh, A. Torresquintero, D. S R. Mohan, L. Foglianti, R. Lenain, and J. Gao. Phonological features for 0-shot multilingual speech synthesis. In Proc. Interspeech , pages 2942 2946, 2020. [Tachibana et al. , 2018 ]H. Tachibana, K. Uenoyama, and S. Aihara. Efficiently trainable text-to-speech system based on deep convolutional networks with guided attention. In Proc. ICASSP , pages 4784 4788, 2018. [Vaswani et al. , 2017 ]A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N Gomez,  . Kaiser, and I. Polosukhin. Attention is all you need. In Proc. NeurIPS , volume [Veaux et al. , 2017 ]C. Veaux, J. Yamagishi, K. MacDonald, et al. CSTR VCTK corpus: English multi-speaker corpus for CSTR voice cloning toolkit. University of Edinburgh. The Centre for Speech Technology Research (CSTR) , 2017. [Wang et al. , 2021 ]C. Wang, M. Riviere, A. Lee, A. Wu, C. Talnikar, D. Haziza, M. Williamson, J. Pino, and E. Dupoux. V oxPopuli: A large-scale multilingual speech corpus for representation learning, semi-supervised learning and interpretation. In Proc. ACL , pages 993 1003, [Watanabe et al. , 2018 ]S. Watanabe, T. Hori, S. Karita, T. Hayashi, J. Nishitoba, Y . Unno, N.-E. Y . Soplin, J. Heymann, M. Wiesner, N. Chen, et al. ESPnet: End-to-end speech processing toolkit. Proc. Interspeech , pages 2207 [Wu and Dredze, 2019 ]S. Wu and M. Dredze. Beto, Bentz, Becas: The surprising cross-lingual effectiveness of BERT. In Proc. EMNLP-IJCNLP , pages 833 844, 2019. [Xuet al. , 2021 ]G. Xu, W. Song, Z. Zhang, C. Zhang, X. He, and B. Zhou. Improving prosody modelling with cross-utterance BERT embeddings for end-to-end speech synthesis. In Proc. ICASSP , pages 6079 6083, 2021. [Zenet al. , 2012 ]H. Zen, N. Braunschweiler, S. Buchholz, M. JF Gales, K. Knill, S. Krstulovic, and J. Latorre. Statistical parametric speech synthesis based on speaker and language factorization. TASLP , 20(6):1713 1724, 2012. [Zenet al. , 2019 ]H. Zen, V . Dang, R. Clark, Y . Zhang, R. J Weiss, Y . Jia, Z. Chen, and Y . Wu. LibriTTS: A corpus derived from LibriSpeech for text-to-speech. In Proc. Interspeech , pages 1526 1530, 2019. [Zhang and Lin, 2020 ]H. Zhang and Y . Lin. Unsupervised learning for sequence-to-sequence text-to-speech for lowresource languages. Proc. Interspeech , pages 3161 3165, [Zhang et al. , 2022 ]G. Zhang, K. Song, X. Tan, D. Tan, Y . Yan, Y . Liu, G. Wang, W. Zhou, T. Qin, T. Lee, et al. Mixed-Phoneme BERT: Improving BERT with mixed phoneme and sup-phoneme representations for text to speech. In Proc. Interspeech , pages 456 460, 2022.MethodSeen UnseenAvg.de fr ru fi es MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Spoken Text 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 Written Text 5.81 4.55 6.94 9.10 7.61 21.24 5.22 12.73 9.50 18.44 6.76 12.52 Spoken+Written Text 5.54 3.72 6.34 7.51 7.07 15.33 4.96 5.44 8.82 17.48 6.35 10.04 Table 6: Comparison of text data domain for unsupervised text pretraining. MethodSeen UnseenAvg.de fr ru fi es MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Residual layer 5.65 3.79 6.48 7.15 7.38 10.62 4.99 5.28 9.05 18.27 6.46 9.58 Transformer encoder 5.77 4.63 6.36 6.61 7.17 11.25 4.90 6.50 8.89 14.15 6.44 9.97 Table 7: Comparison of bottleneck layer architecture. A Text data for pretraining We investigated the effect of different types of text data used in pretraining Dtexton the performance of our method. As described in   3.1, we used spoken texts from V oxPopuli, M-AILABS, and CSS10 in the evaluations presented in 3. However, we can acquire a large amount of text data from datasets designed for NLP or from web-crawled text resources. These data typically consist of written texts and cover a wide range of domains. To investigate the effectiveness of using written texts for our multilingual TTS, we used ParaCrawl [Ba n onet al. , 2020 ], a web-crawled text dataset built for machine translation, for Dtextduring the unsupervised pretraining described in   2.1. For the investigation, we randomly sampled the same amount of texts as in Table 1 for each language. We then trained our model using the following three different cases. 1) Spoken Text : Only using the spoken text for pretraining as in the previous evaluations, 2) Written Text : Only using the text data from ParaCrawl, and 3)Spoken+Written Text : We combined the text data in Spoken Text andWritten Text . We used the byte-based proposed model presented in   3.2 and   3.3. Table 6 lists the results. We observed that Spoken Text outperformed Written Text in all the metrics and languages, resulting in an average difference of 0.3 in MCD and 2.94% in CER. These results demonstrate the effectiveness of using spoken text for pretraining. Spoken+Written Text showed on average 0.11 lower MCD and 0.46% higher CER compared to Spoken Text . However, for the unseen language, Spoken+Written Text outperformed Spoken Text in MCD and CER. These results suggest that adding written text data can improve the generalization of our TTS models for the zero-shot scenarios. B Architecture of bottleneck layer As described in   2.4, we used the residual layer for our bottleneck layer. Also, we demonstrated the effectiveness of the residula bottleneck layer for both seen and unseen languages in the evaluation presented in   3.4. In this section, we explored an alternative architecture for the bottleneck layer. We conducted experiments using a single-layer Transformerencoder as the bottleneck layer (referred to as Transformerencoder ), comparing it with the original residual layer detailed in   2.4 (referred to as Residual layer ). Table 7 lists the For the seen languages, the superior performance between the residual layer and the transformer encoder varied, depending on the specific language and evaluation metrics. However, for the unseen language, the Transformer encoder showed higher performance, achieving an improvement of 4.12 in CER. Looking at the average scores across all languages, the Transformer encoder had a slightly lower MCD, while the CER was reduced by 0.39 when using the residual layer. These results suggest that the use of a deeper layer can improve the generalizability of the proposed model. Nevertheless, the overall performance of both models remains comparable in terms of average metrics. C Effect of excluding some languages from In this section, we have deliberately excluded several languages from the paired data used for the supervised learning described in   2.2 in order to study their impact. As shown in Table 1, the paired data originally included the languages de, fr, nl, fi, hu, ru, and el. As a comparison case, we first excluded fr, which belongs to Italic languages as es according to Glottolog [Hammarstr  omet al. , 2021 ]. We also removed de and nl, which belong to Germanic languages, from the paired data. Consequently, in the comparison case, supervised learning was performed only with fi, ru, hu, and el. Table 8 lists the results. Original corresponds to the case shown in Table 1, while Excluded denotes the case where only fi, ru, hu, The three languages listed on the left-hand side of Table 8 (ru, hu, fi) represent the seen languages in both cases. Interestingly, the Original scenario generally outperformed the Excluded scenario for these languages. These results indicate that in the context of multilingual TTS training, performance can potentially be improved by including a wider variety of languages rather than restricting to similar languages.Methodru hu fi de fr es MCD CER MCD CER MCD CER MCD CER MCD CER MCD CER Original (de, fr, nl, fi, hu, ru, el) 7.38 10.62 5.01 6.05 4.99 5.28 5.65 3.79 6.48 7.15 9.05 18.27 Excluded (fi, hu, ru, el) 7.00 11.11 5.32 6.92 4.98 5.46 10.39 34.11 10.90 49.65 10.00 24.80 Table 8: Effects of excluding languages from paired data. (a) Baseline (Bytes)(b) Proposed(Bytes) Figure 6: Cross-attention maps obtained from byte-based baseline and proposed methods, which correspond to first attention head in fifth and sixth layers of decoder. This suggests the effectiveness of massively multilingual TTS training, as supported by previous work [Saeki et al. , 2022b ]. The two languages (de, fr) were included in the paired data forOriginal , but were excluded from the paired data for Excluded . We observed that the zero-shot setting resulted in a decrease in performance. Comparing the de results in Table 5 with those in Table 8, we confirmed that excluding fr, nl, and es resulted in a 6.1% performance degradation in CER. Also, es was absent from the paired data in both the Original and Excluded scenarios. An examination of the es results revealed a 6.53% increase in CER for the Excluded scenario. These results demonstrate the importance of including linguistically similar languages in the paired data. D Observation of cross-attention map. In this section, we show the cross-attention maps obtained during inference from both the byte-based proposed and the baseline methods defined in   3.1. In both cases, an utterance sampled from an unseen language (Spanish) was used. Fig. 6 shows the cross-attention maps corresponding to the first attention head in the fifth and sixth layers of the decoder. It should be noted that we have cross-attention maps for other heads and layers. The results shown in Fig. 6 are derived from attention maps that have a diagonal shape in higher layers forboth the baseline and the proposed methods. The top part of Fig. 6 represents results from the fifth layer, while the bottom part corresponds to results from the sixth layer. The left side of the figure shows the results of the baseline method, while the right side shows the results of the proposed method. We observe that the fifth layer of the baseline model shows a discontinuity in the attention map, which leads to instability of the linguistic content. Conversely, in the fifth layer of the proposed model, the attention map is significantly more continuous than in the baseline method. These results suggest that our unsupervised text pretraining can improve crossattention in the absence of paired speech-text data. The results are also reflected in the intelligibility difference between the baseline and the proposed methods presented in   3.3.