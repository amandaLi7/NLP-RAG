## PROFNAME
Malihe Alikhani
## AUTHORID
2715920
## AUTHORNAME
Malihe Alikhani
## AUTHORURL
https://www.semanticscholar.org/author/2715920
## AUTHORHINDEX
12
## AUTHORAFFILIATIONS
['Rutgers University']
## AUTHORPAPERCOUNT
58
## AUTHORCITATIONCOUNT
520
## PAPERID
e2be92ff1ae2abdc05c1929d6d02623c58f37097
## EXTERNALIDS
{'PubMedCentral': '10226667', 'DOI': '10.3389/frai.2023.1048874', 'CorpusId': 258678310, 'PubMed': '37255946'}
## URL
https://www.semanticscholar.org/paper/e2be92ff1ae2abdc05c1929d6d02623c58f37097
## TITLE
Image–text coherence and its implications for multimodal AI
## ABSTRACT
Human communication often combines imagery and text into integrated presentations, especially online. In this paper, we show how image–text coherence relations can be used to model the pragmatics of image–text presentations in AI systems. In contrast to alternative frameworks that characterize image–text presentations in terms of the priority, relevance, or overlap of information across modalities, coherence theory postulates that each unit of a discourse stands in specific pragmatic relations to other parts of the discourse, with each relation involving its own information goals and inferential connections. Text accompanying an image may, for example, characterize what's visible in the image, explain how the image was obtained, offer the author's appraisal of or reaction to the depicted situation, and so forth. The advantage of coherence theory is that it provides a simple, robust, and effective abstraction of communicative goals for practical applications. To argue this, we review case studies describing coherence in image–text data sets, predicting coherence from few-shot annotations, and coherence models of image–text tasks such as caption generation and caption evaluation.
## VENUE
Frontiers in Artificial Intelligence
## YEAR
2023
## REFERENCECOUNT
57
## CITATIONCOUNT
2
## INFLUENTIALCITATIONCOUNT
0
## ISOPENACCESS
True
## OPENACCESSPDF
{'url': 'https://www.frontiersin.org/articles/10.3389/frai.2023.1048874/pdf', 'status': None}
## FIELDSOFSTUDY
['Medicine']
## JOURNAL
{'volume': '6', 'name': 'Frontiers in Artificial Intelligence'}
## AUTHORS
[{'authorId': '2715920', 'name': 'Malihe Alikhani'}, {'authorId': '1468726581', 'name': 'Baber Khalid'}, {'authorId': '144884556', 'name': 'Matthew Stone'}]
## TLDR
This paper shows how image–text coherence relations can be used to model the pragmatics of image-text presentations in AI systems, and reviews case studies describing coherence in image– Text data sets, predicting coherence from few-shot annotations, and coherence models of image– text tasks such as caption generation and caption evaluation.
TYPEOriginal Research
PUBLISHED /one.tnum/five.tnum May /two.tnum/zero.tnum/two.tnum/three.tnum
DOI/one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
OPENACCESS
EDITEDBY
Andy Lücking,
Université Paris Cité, France
REVIEWEDBY
Casey Kennington,
Boise State University, United States
Christian Otto,
L/three.tnumS Research Center, Germany
*CORRESPONDENCE
Malihe Alikhani
malihe@pitt.edu
SPECIALTYSECTION
This article was submitted to
Language and Computation,
a section of the journal
Frontiers in Artiﬁcial Intelligence
RECEIVED /two.tnum/zero.tnum September /two.tnum/zero.tnum/two.tnum/two.tnum
ACCEPTED /one.tnum/zero.tnum March /two.tnum/zero.tnum/two.tnum/three.tnum
PUBLISHED /one.tnum/five.tnum May /two.tnum/zero.tnum/two.tnum/three.tnum
CITATION
Alikhani M, Khalid B and Stone M (/two.tnum/zero.tnum/two.tnum/three.tnum)
Image text coherence and its implications for
multimodal AI. Front. Artif. Intell. /six.tnum /one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum.
doi  /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
COPYRIGHT
 /two.tnum/zero.tnum/two.tnum/three.tnum Alikhani, Khalid and Stone. This is an
open-access article distributed under the terms
of theCreative Commons Attribution License
(CC BY). The use, distribution or reproduction
in other forums is permitted, provided the
original author(s) and the copyright owner(s)
are credited and that the original publication in
this journal is cited, in accordance with
accepted academic practice. No use,
distribution or reproduction is permitted which
does not comply with these terms.Image text coherence and its
implications for multimodal AI
Malihe Alikhani/one.tnum*, Baber Khalid/two.tnumand Matthew Stone/two.tnum
/one.tnumDepartment of Computer Science, University of Pittsburgh, Pit tsburgh, PA, United States,/two.tnumDepartment
of Computer Science, Rutgers University, Piscataway, NJ, Unite d States
Human communication often combines imagery and text into integra ted
presentations, especially online. In this paper, we show how im age text
coherence relations can be used to model the pragmatics of image te xt
presentationsinAIsystems.Incontrasttoalternativeframe worksthatcharacterize
image text presentations in terms of the priority, relevance , or overlap of
information across modalities, coherence theory postulates t hat each unit of a
discourse stands in speciﬁc pragmatic relations to other parts of the discourse,
witheachrelationinvolvingitsowninformationgoalsandinfere ntialconnections.
Text accompanying an image may, for example, characterize what s vi sible in
the image, explain how the image was obtained, oﬀer the author  s appraisal
of or reaction to the depicted situation, and so forth. The advant age of
coherence theory is that it provides a simple, robust, and eﬀecti ve abstraction
of communicative goals for practical applications. To argue this, we review case
studiesdescribingcoherenceinimage textdatasets,predicti ngcoherencefrom
few-shotannotations,andcoherencemodelsofimage texttaskss uchascaption
generation and caption evaluation.
KEYWORDS
coherence,discourse,multimodality,machinelearning,eval uation
/one.tnum. Introduction
The internet has become a multimodal information ecosystem, where units of
content news articles, web pages, posts to social media regularly tie together written
words,emojiandothericons,staticanddynamicimagery,andlinkstoyetmoremultimodal
content. Faced with the heterogeneity of online information, Artiﬁcial Intelligence (AI)
researchers have increasingly characterized problems of information access from the
perspective of multimodality  for example, producing text captions that make visual
information more accessible (e.g., Lin et al., 2014  Young et al., 2014 )  or taking both
text and image content into account in information retrieval (e.g., Funaki and Nakayama,
2015 Chowdhury et al., 2019 ). At the same time, this heterogeneity has empowered AI
researchers to compile vast multimodal datasets (e.g., Sharma et al., 2018 ) and to build
largescale foundation models(e.g., Luetal.,2019  Radfordetal.,2021 )trainedtocapture
cross-modalpatternsandmakecross-modalpredictions.
Applicationsofsuchmodels,liketheDALL-Esystemforsynthesizingimageryfromtext
(Rameshetal.,2021 ),havecapturedtheimaginationofresearchersandthepublicalikeand
serve as high-proﬁle examples of the ability of representation learning to drive surprisingly
richAIcapabilities.
The rapid progress in multimodal AI brings new urgency to the challenge of better
understanding the data, tasks, model architectures, and performance metrics in the ﬁeld.
In fact, even the basic data points,  image text pairs  scraped from the web, as in Radford
et al.(2021), involve diverse and surprising juxtapositions. In Figure1, for example, we see
Frontiersin ArtiﬁcialIntelligence /zero.tnum/one.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
FIGURE/one.tnum
People combine text and images creatively to communicate. In the se two unrelated image text pairs, the images provide explanat ions for what is
described in the text. Image credits  Malihe Alikhani.
image text pairs where the image depicts the reasonfor the text
contribution the depicted traﬃc is why the author will be late 
the possibility of encountering bears in nature is why repellent
is needed. Though such text is  grounded  in imagery only in
a very abstract way, such inferences seem like common-sense to
human readers. How good then are large vision language models
at capturing the varied implicit generalizations and relationships
that connect text and imagery  How robust to this variation
are machine learning approaches to using natural language as a
supervisionsignalformultimodalinference Canwedesignmodels
that better understand and reason about these inferential links 
More generally, what concepts and methods are needed for AI
researcherstoexploresuchquestionsinpreciseandeﬀectiveways 
In this paper, we address these challenges through the lens
of theories of discourse coherence ( Phillips, 1977  Hobbs, 1979 ,
1985 Asher and Lascarides, 2003 ). Our focus is on image text
coherence,wherewearguethatcoherencerelationsthatresolvethe
interpretation of text segments against juxtaposed imagery oﬀer a
broad and powerful framework to improve AI datasets, models,
and systems so that they can better account for the structural,
logical and purposeful organization of authors  communicative
contributionstoonlinediscourse.
Coherence theory originates in the detailed analysis of the
inferences needed to support text interpretation in knowledgebased
 approaches to natural language processing. For example,
this discourse from Hobbs(1985ex 3) depends on common-sense
knowledgeaboutbooksareformedandhandled 
(1) Johntookabookfromtheshelf.Heturnedtotheindex.
Coherence here consists of the fact that the ﬁrst event brings
about the situation in which the second event takes place a
relationship referred to as Occasion (Hobbs, 1985 ) orNarration
(Asher and Lascarides, 2003 ). For coherence theory, establishing
thisOccasion relationship guides and prompts key inferences,
including the inference that John s turning involves opening the
bookhemustbeholdinginhishandtoanewpageandtheinference
thattheindexreferstothesectionofthisbookJohnexhibits.Whilecurrent AI rarely approaches such inferences explicitly, coherence
theoryneverthelessremainsaninﬂuentialparadigmthatinformsa
widerangeofAIworkontextdiscourse,aswesurveyinSection2.
Interpreting image text presentations requires analogous
inferences across modalities, including inferences that locate the
viewpoint of imagery ( Cumming et al., 2017 ), identify depicted
objects (Abusch, 2013 ), and place the ongoing scene in the arc of
thenarrative( Cohn,2013 ).Somecoherencerelationslinkimagery
together ( McCloud, 1993 ). Others guide inferences that enrich the
joint interpretation of communicative actions across modalities
(e.g.,Lascarides and Stone, 2009a  Stone and Stojnic, 2015 ). The
speciﬁc case ofimage textcoherenceisthefocusofourworkhere
and underpins the contribution of our research. Section 2 builds
onourreviewofbroaderworkoncoherenceinAItomotivateand
characterizeimage textcoherence.
Having laid out the principles of coherence, we go on to
demonstrate the signiﬁcance of image text coherence for stateof-the-art
 multimodal AI. Section 3 explores how coherence can
be used to annotate and to analyze image text datasets. Section 4
illustrates how coherence can be used to make sense of the
representations and learning of diﬀerent model architectures for
multimodal AI. Section 5 reviews how coherence-aware tasks and
metrics enhance researchers  ability to build more useful tools
and measure performance in more meaningful ways. We close by
suggesting some key directions for building on these successes in
futureresearch.
/two.tnum. Coherence in image text
presentations
We begin with an overview of coherence theory. We
have two aims. Our ﬁrst aim is to present the motivations,
analyses, and principles of coherence theory as an approach to
multimodal discourse. Like text discourse, we argue, multimodal
discourse recruits numerous ﬁne-grained inferences to enrich
the interpretation of communicative contributions in context 
Frontiersin ArtiﬁcialIntelligence /zero.tnum/two.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
what uniﬁes these inferences is the need to establish coherence
relations that organize the contributions of parts of the discourse
into an integrated whole. It may seem counterintuitive that such
an approach a theory devised to address abstract foundational
questions about meaning should pave the way for concrete
progress in AI. Our second aim, then, is to explain why
the constructs of coherence theory have such direct, practical
implications for AI methodology in general, and for multimodal
AIinparticular.
/two.tnum./one.tnum. Coherence  The key ideas
Authorshavediﬀerentpurposesinpresentinginformation.The
writerofExample(2),forexample,usesthesecondsentencetooﬀer
anexplanationofwhytheﬁrsteventcameabout.
(2) Maxspiltabucketofwater.Hetrippedonhisshoelace.
Such relationships are central to making sense of the author s
message. The second sentence of Example (2) works as an
explanation, for example, only because we understand Heas
Max,hisas Max s, the shoelace as that of one of the shoes
Max must be wearing, and the event where He tripped as
located temporally immediately prior to the spilling. Coherence
theory counsels that we take a fundamentally relational view
of actions in discourse. Successive actions in discourse don t
expressindependentpropositionsoractunassistedtoinﬂuencethe
audience discoursecontributionsbuildononeanother.Coherence
relationsspecifyhowtheydothis.(OurdiscussionofExamples2 4
followsKehler, 2002  Asher and Lascarides, 2003 ). We counsel AI
researchers also to take a relational approach to discourse actions.
One lesson ofourexperiments in Section4isthat AIarchitectures
should not assume that discourse actions stand on their own.
Instead, AI architectures should learn about discourse actions via
theirlatentrelationtootherdiscourseactionsinthecontext.
After all, relationships vary. Here are variants of Example (2)
where the followup sentences make contributions of very diﬀerent
kinds.
(3) Maxspiltabucketofwater.Hespiltitallovertherug.
(4) Maxspiltabucketofwater.Johndroppedajarofcookies.
InExample(3),weﬁndwhat Hobbs(1985)callsExpansion .We
learn more about the initial event, its context and consequences.
In Example (4), we ﬁnd what Kehler(2002) callsResemblance .
The author presents another event as notable for its similarity
and diﬀerence to the ﬁrst. Coherence theorists have elegant ways
to systematize the diversity of such relational interpretations in
a rational taxonomy. For Kehler(2002), for example, Examples
(2 4)illustrate Cause-Eﬀect ,Contiguity ,andResemblance relations
(respectively), each a manifestation of the relationality and ﬁtness
of human thought. At the same time, AI research is increasingly
successful at standardizingguidelines for annotators to classify the
relationsfoundinspeciﬁcexamples.
Strikingly, these diﬀerent relations are not encoded directly in
the linguistic forms and structures that make up Examples (2 
4). Of course, the forms of referring expressions in 3 are thepronouns that you d expect if this was a straightforward extended
description of a single scene  the structure of Example (4) also
exhibits the syntactic and semantic parallels you d expect if the
author wanted to facilitate a comparison. In that sense, linguistic
formcorroboratescoherence.Butformdoesnotsignalcoherence 
there are no words or constructions that give decisive evidence
about what the author has in mind. Ambiguity is pervasive. A
consonant lesson of Section 5 is that AI researchers should be
skeptical that models and metrics learn and respect coherence. If
coherence matters, rather than hoping learning methods capture
it automatically, we recommend designers solve coherence-aware
tasksandassesssystemsoncoherence-awaremetrics.
Alternative coherence relations provide broad organizing
frameworks for interpretation, rather than clearly demarcated
elements of form or content. Nevertheless, ambiguities in
coherence often correlate transparently with clear interpretive
diﬀerences. Smyth(1994) uses Example (5), to illustrate the
connectionbetweencoherenceandcoreference.
(5) PhiltickledStanley,andLizpokedhim.
Coherence could organize (Example 5) in two ways. The
second sentence could describe Liz s contingent reaction to the
tickling. This suggests that Liz poked Phil, perhaps expressing
disapproval of his action. Alternatively, the two sentences could
describe similar events. That suggests that Stanley is the object
of both tickling and poking. Asking how himis interpreted
indirectly answers questions about coherence. Similarly, we can
easily appreciate the diﬀerent coherence relations in Examples
(2 4) by considering when we understand the second event to
have occurred  before the ﬁrst, simultaneous to it, or at any time
on the same relevant occasion. As we explore in Section 3, such
interpretive eﬀects enable AI researchers to approach coherence
through a surprisingly diverse set of methodologies for data
annotationandanalysis.
Imagery, like language, must be understood by inference,
and scholars have long argued that the interpretation of visual
communicationalsoaimstoestablishcoherence. McCloud (1993),
for example, argues that coherence relations between panels
underpin storytelling in comics. Abusch(2013) makes an analogy
between the persistence of individuals across panels in comics and
the phenomenon of coreference in text discourse. Cumming et al.
(2017) show how ﬁlm viewers rely on coherence relations to infer
the spatial relationships implicitly connecting the viewpoints of
successive shots. In all cases, viewers must draw inferences about
what objects imagery is depicting, where, and when just as in
they must draw inferences to understand linguistic content. In
all cases, they use coherence to do it. Koby Leﬀ s video essay
presentsaparticularlyclearvisualexplanationofthephenomenon 
Alikhani and Stone (2018) analyze a case study of coherent visual
communicationincomputationalterms.
Text communication and visual communication thus share
common principles of coherence, but we see the overarching
role of coherence especially vividly when we consider the
coherentrelationshipsbetweentextandimagery.Authorsregularly
juxtapose text and imagery to present their ideas. When we
establish the coherence of such presentations, we can ﬁnd that
text relates to imagery  the text may describe how the imagery
was obtained, what it shows, what its context is, or what its
Frontiersin ArtiﬁcialIntelligence /zero.tnum/three.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
implications are. Conversely, we can use coherent links to text to
establish what imagery depicts  perhaps a representative moment
during the event described by the text, or perhaps a telling
moment as the event got underway, or achieved its results.
Such interpretations attest to the importance of the cross-modal
coherence relations which we illustrated already with Figure1in
the introduction. Coherence relations can provide a framework
for ways that we pose questions about commonsense inferences in
image textpresentations.
In fact, cross-modal coherence relations have a long history in
theanalysisofface-to-facecommunication( Engle,2001  Lascarides
and Stone, 2009a  Stone and Stojnic, 2015  Hunter et al., 2018  
Hunter,2019 ),asaformaltooltooperationalizecognitivescientists 
view that speakers use diverse modalities, including speech and
coverbal gesture, to present integrated messages ( McNeill, 1992  
Bavelas and Chovil, 2000  Kendon, 2004 ). Image text coherence
is less well studied (the work of Feiner and McKeown, 1991 , who
used a taxonomy of cross-modal coherence relations to inform
the automated synthesis of multimodal documents, is a prescient
exception), but it provides new illustrations of the key principles
ofcoherence.
Considertheimagesof Figure2forexample.
Both images are associated with the summary  looking out the
window butthetextgetstwoqualitativelydiﬀerentinterpretations.
Atleftwehaveviewofacatthroughglass.It sthecatthat slooking
out the window. This is a characteristic example of a coherence
relation we have called Visible(Alikhani et al., 2020 )  as we review
in more detail in Section 3, we attribute many unique features of
common image caption corpora in AI research to the distinctive
inferentialcharacteroftextthatsupplies Visibleinformation.
At right of Figure2, meanwhile, we have a view of a window,
framed to draw attention to the landscaped scene outside. The
image features no gazing subject. It s the camera that s looking
out the window. This is an example of a diﬀerent coherence
relation we have called Meta(Alikhani et al., 2020 )  utterances
often get their coherence through Meta-talk by characterizing
related communicative actions rather than by amplifying the
communicated content of related segments ( Hobbs, 1985  Sanders
et al., 1992  Asher and Lascarides, 2003 ).Metatext doesn t
summarize the visual information in the image like a typical
caption would, but in some genres authors often supply Meta
text to accompany their imagery. Such ambiguities in image text
coherence mirror the ambiguities found in text text coherence.
Text that accompanies imagery can provide qualitatively diﬀerent
kindsofinformationabouttheimageitampliﬁes theremaybelittle
surface-level information that reveals the contribution that text is
making. Thus, there is a crucial but implicit role for coherence in
interpreting image text presentations, one that we argue impacts
thedata,models,tasks,andmetricsofmultimodalAIsystems.
/two.tnum./two.tnum. The methodology of coherence
Because of the ambiguity of coherence, annotated data is
indispensable for AI experiments, whether in training models of
coherence by supervised learning or in evaluating the predictions
of unsupervised methods. Our work on text image coherence isinspired by the success of analogous approaches to text discourse,
particularly the theoretical work of Asher and Lascarides (2003)
andtheempiricalworkof Prasadetal. (2008).
To start, we need a framework that systematically organizes
coherence relations based on their implications for the structure,
content, and purpose of the discourse. Asher and Lascarides
(2003) introduce such a taxonomy of coherence relations between
discourse segments, as part of their Segmented Discourse
Representation Theory, or SDRT. The simplest relations are based
on reference to shared entities  Examples include Expansion (as
in Example 3) when a second discourse unit ampliﬁes and
expands on what s described in the ﬁrst unit, and Narration (as
in one interpretation of Example 5), when a second discourse
segment describes an event that follows the one described in the
ﬁrstsegment.
SDRT also involves relations at proposition level, such as the
Parallelrelationthatconnectstwodiscoursesegmentsthatexpress
propositions that make similar claims about similar entities (as in
oneinterpretationofExample5).
Finally, SDRT includes relations that describe the intents
and goals of the utterances these are particularly important
in interactive relationships such as Correction andClariﬁcation
Requestthatconnectutterancesbydiﬀerentspeakers.
To annotate these relations, researchers have mapped out
structured, multifaceted, hierarchical annotation guidelines
(Prasad et al., 2008  Rohde et al., 2018  Alikhani et al., 2019 ). In
general, specifying coherence relations ﬁrst requires deciding how
discourse elements attach into an ongoing discourse structure 
see especially Webber et al. (2003). For each discourse unit, we
need to describe what other units it s related to by coherent links.
Connecting a discourse segment to a related segment can create
a sibling (coordination) or a child (subordination), generating a
hierarchical structure. Diﬀerent discourse frameworks model the
structure of the discourse diﬀerently. Some of them only capture
shallow relations, while others, like SDRT, use more complex and
hierarchical graphs./one.tnumIn extended multimodal presentations, like
blog posts involving multiple images interleaved with extended
textualdescriptions( AlikhaniandStone,2018 )orcontributionsto
spoken conversation including multiple utterances and co-verbal
gestures performed in synchrony ( Lascarides and Stone, 2009a ),
it s routine for each contribution to attach both to a synchronous
contribution across modalities and a previous contribution in the
samemodality.
Our work on image text coherence relations highlights
cases whose discourse structure is relatively clear for example,
images together with their ALT-TEXT, an auxiliary record that is
understood as providing subordinate, supplementary information
to accompany the image. In more complex presentations, the
question of multimodal discourse structure is a challenging issue
ripeforfurtherresearch.
Given an attachment, researchers ﬁrst decide which of the
top-level categories describes the relationship between the two
segments ( Prasad et al., 2008  Hunter et al., 2018 ). We have found
/one.tnum Researchers in text discourse often focus on tree discourse stru ctures
(Hobbs,/one.tnum/nine.tnum/eight.tnum/five.tnum  Kehler,/two.tnum/zero.tnum/zero.tnum/two.tnum )buttheanalysisofmultimodaldiscoursebeneﬁts
from appealing to more ﬂexible structures (Hunter et al., /two.tnum/zero.tnum/one.tnum/eight.tnum ).
Frontiersin ArtiﬁcialIntelligence /zero.tnum/four.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
FIGURE/two.tnum
Two diﬀerent ways of interpreting  looking out the window  as a spe ciﬁcation of an image. (Left)Visual information describing the pose and activity
of the subject in a photograph of a cat by Cristie Guevara (CC Publ ic Domain /one.tnum./zero.tnum viapublicdomainpictures.net). (Right)Meta-level information
describing the camera viewpoint in a photograph of the Phoenix Art Museum by Chanel Wheeler (CC BY-SA /two.tnum./zero.tnum viaWikimedia Commons).
it helpful to give annotators the option of specifying multiple
top-level relations. Given a relation, we can then  drill down 
to characterize the relationships in more detail. Webber et al.
(2019) describes the hierarchical decisions that annotators must
make to resolve the coherence relations deﬁned for the Penn
DiscourseTreebank.
We can sometimes extend coherence annotation protocols to
multimodal discourse by building on relationships that have been
studied in text. The Narration relation ( Hobbs, 1985  Asher and
Lascarides, 2003 ) is a case in point. In multimodal discourse,
two images are connected by the Narration relation when the
second image shows the subsequent event of what s depicted
in the ﬁrst image. We can even ﬁnd Narration from text to
images and vice versa. An image can show what happened right
after the text it elaborates, and text can report what happened
right after the image it modiﬁes. More generally, Narration is
one of a family of coherence relations expressing contingent
temporal connections that link an event to its preparatory process ,
its culmination , orits result state . These relationships can be
found between when clauses and main clauses in discourse
(Moens and Steedman, 1988 ), between successive clauses in
discourse ( Webber, 1987 ), or between related text and imagery
(AlikhaniandStone,2018 ).
In other cases, we need new relations to describe inferences
between text and images. In postulating such relations, we can
draw valuable insights from research that explores how linguistic
content can be related to other kinds of visual content. Engle
(2001), for example, present a number of semantic relationships
between speech and coverbal gesture, showing not only that
gesture has a characteristic ability to relate to accompanying
speech by Depiction , but that such relations combine with familiar
relationsfromtextualdiscoursesuchas Exempliﬁcation .Stoneand
Stojnic(2015), meanwhile, study how physical demonstrations are
connected to speech and gesture, and appeal to relations such
asSummary  where an utterance is understood to characterize a
visible situation and Compliance  where an event is understood
to meet an expectation for action established by a previous
utterance. As our empirical results in Section 3 make clear,
presentations with diﬀerent purposes and genres naturally feature
distinctive relations, so we should expect future research to lead tobroaderperspectivesontherangeofpossiblecoherencerelationsin
multimodalcommunication.
Despite the many open questions about the structure
and relations exhibited by coherent multimodal discourse,
AI researchers can nevertheless make practical progress with
coherence-based approaches. For text image coherence, we have
used a restricted list of relations to annotate data at scale
(Figure3), analyse large multimodal datasets, design coherenceawaremodelsandevaluateourframework.Ourtaxonomyincludes

ﬁve relations  Visible,Action,Subjective ,StoryandMeta any
subset of which can deﬁne the coherent use of text to amplify on
imagecontent.
TheVisiblerelation holds when the text presents information
that is depicted in the image. This is similar to the Restatement
relationsintext( Prasadetal.,2008 )intext,butherethecontentof
animageoverlapswiththecontentofitsaccompanyingtext.When
the image depicts a moment or a snapshot of an action described
in text, the pairs are connected with Actionrelation. The Action
relation is analogous to Elaboration relations described in Prasad
etal.(2008)fortext.
Thetextandimagearerelatedwiththe Subjective relationwhen
the overlapping information described in text sometimes includes
an evaluative statement or a reaction to the content of the image.
Thisissimilarto Evaluation relationsintext( Hobbs,1985 ).
Similar to the Occasion relation of Hobbs(1985) that holds
when a discourse unit provides the background for another
discourse segment, sometimes the text provides a free-standing
description of the occasion depicted in the image. We call this
aStorycoherence relation. Sometimes the text goes beyond just
providing information about what s depicted in the image or the
occasion. It describes how, when, or where the image was taken
byexplainingthepresentationandproductionprocedures.Insuch
cases, we argue the text and the image are connected with a Meta
coherencerelation.
/two.tnum./three.tnum. Why coherence 
Thecommunicativefunctionsofimagesandtextinmultimodal
communicationcanbeanalyzedfromseveraldiﬀerentperspectives.
Frontiersin ArtiﬁcialIntelligence /zero.tnum/five.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
FIGURE/three.tnum
Captions from left to right  A man sitting in front of a bunch of fruits. A woman is traveling on a tr ain. The new manager of the team. The view from
the bridge. Text and images are linked together using a constrained set of cohe rence relations, which can summarize the structural, logical a nd
purposeful relationships between the contributions of text and t he contributions of images. Examples from the Conceptual Captions d ataset
(Sharma, /two.tnum/zero.tnum/two.tnum/zero.tnum ) that include Creative Commons Licensed image text pairs.
For example, Kruk et al. (2019) andShuster et al. (2019) focus
on the emotions evoked by presentations, Guo et al. (2019) study
genre and style, and Otto et al. (2019) andVempala and Preo  tiucPietro(2019)assesshowtextandimagesmightbecomplementary

orredundant.
The main contrast with our approach is that none of these
frameworks try to model information-level inferences between
text and images. An image might be a uniquely eﬀective way to
prompt emotion for example, but it would be surprising if our
cognitive mechanisms could resolve ambiguity in the image (or
in the accompanying text) to foster such aﬀective engagement.
Similarly, regardless of how we resolve their ambiguities, we will
beabletoclassifyrelatedtextandimageryaseithercomplementary
orredundant.
In text discourse, information structure is an alternative
to coherence theory that provides yet another perspective to
relate meaning to communicative goals. Information structure
is a dimension of pragmatic meaning in language that helps
explain variation in word order, intonation, and other linguistic
cues that mark the relationship between utterances and their
context ( van Kuppevelt, 1995  Roberts, 2012 ). A key construct
in theories of information structure is the  question under
discussion,  or QUD  utterances relate to the context in part by
addressing the QUD. Information structure describes how an
utterance is partitioned into material that evokes the question
under discussion and the material that supplies the answer.
Although information structure is signaled grammatically, the
point of emphasis of the speaker often has to be inferred
(Bolinger, 1972 ). This makes information structure and coherence
complementary  in particular, Hunter and Abrusán (2017) argue
that coherence provides a ﬂexible and perspicuous way of
mapping the inferences involved in disambiguating information
structure and reconstructing the QUD. Coherence is especially
natural when we consdier how approaches should generalize to
multimodal discourse, because there isn t anything analogous to
information structure in a photograph, map, or diagram. Even
the emphasis you ﬁnd in gesture is very diﬀerent structurally
and compositionally from information structure in language
(Kendon, 2004 ). Neither QUD nor the frameworks that prioritize
ways that images serve complementary roles for text would
give insights into inferences that connect the content of text
and imagery. They do not provide a framework that cansupport data creation and model designs that we describe in the
nextsection.
/three.tnum. Coherence as a framework for
analyzing image text datasets
Our ﬁrst argument for the utility of the coherence framework
comes in characterizing AI datasets. Coherence relations
can provide information about the inferential and linguistic
characteristics of image text corpora. In particular, coherence
sheds light on how genres vary and how linguistic form is likely to
changeacrossdatasets.Inadditiontocharacterizingthechallenges
of image text inference, such results can also inform AI research
byhelpingtodeﬁnethedomainadaptationthatwillbenecessaryto
generalizemachinelearningresultsfromoneimage textcollection
tootheres.
In this section, we ﬁrst describe the Clue dataset, which is
the largest image-text dataset annotated with coherence relations.
Then we discuss empirical studies that support the importance
of coherence relations and their associated linguistic constructs in
supporting commonsense inference. Finally, we discuss results on
the correlations of coherence relations and genre, and the use of
coherence relations to diagnose mismatches between image text
corporaandmachinelearningmodels.
/three.tnum./one.tnum. Clue  A dataset of image text
coherence relations
Alikhani et al. (2020) presented Clue that provides a protocol
for annotating coherence relations in text and imagery. They used
the protocol to annotate from the 10,000 image-text pairs from
the Conceptual Captions ( Sharma et al., 2018 ) and Open Images
(Kuznetsova et al., 2020 ) datasets. Half of these pairs include
captions generated by models and half of them are captions that
users have written for the images. Guidelines are described in
detailsin Alikhanietal. (2020).
Figure4 shows the statistics over the resulting annotations
presented in Alikhani et al. (2020). We can see that models
struggle to generate subjective or story-like captions. The rate of
captions and images with the Metarelation however is higher
Frontiersin ArtiﬁcialIntelligence /zero.tnum/six.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
FIGURE/four.tnum
The distribution of coherence relations in our dataset.
in text generated by models that signals the potential context
hallucinationproblem.
/three.tnum./two.tnum. Coherence predicts linguistic form
Alikhani and Stone (2019) present an empirical investigation
and argue that we can learn new perspectives on commonsense
inference by correlating coherence relations with linguistic
constructs. In particular, they report that visible descriptions are
very distinctive. They only describe what s depicted in the image
inarestrictedway.
They observed that the rate of captions that describe ongoing
events (atelicevents) is drastically higher than the rate of captions
that describe events with end points ( telicevents). This is the
diﬀerence between arriving at an event (telic) and standing
somewhere (atelic).stativedescriptions are also very common in
captions. Many captions describe quality, condition or the state
of what s depicted in the image. Examples include the kids are
happyorgreen bananas are on the table .Alikhani and Stone
(2019) argue that these captions are connected to images by
theillustration orvisiblerelation. They study the following
datasets with diﬀerent types of textual descriptions with images 
(1) Google s Conceptual Captions (CC) ( Sharma et al., 2018 ) (2)
Flickr30K (Flickr) ( Young et al., 2014 ) (3) Visual Storytelling
(VIST) (Huang et al., 2016 ) (4) the Recipe dataset ( Yagcioglu
etal.,2018 ).WhileFlickrandCOCOarecaptainingcorpora,VIST
includes story-like descriptions for connecting ﬁve images and CC
pairs web images with relevant text from associated ALT-TEXT
HTMLattributes.
They studied 1,000 random image-text pairs from each of the
discusseddatasets.Over94%ofthetextincaptioncorporasuchas
Flicker and COCO include atelic events. However, the rate drops
to 40% in other image-text corpora, such as the multimodal recipe
dataset.Alikhani and Stone (2019) includes the experiments and
statisticsdetails./three.tnum./three.tnum. Coherence relations indicate Genre
AI models do not, by default, exhibit the behavior of the texts
that they are trained on. As we can see in Figure4, machinegenerated
 text includes more captions with the meta relation in
comparison with text written by humans. This is one of the main
sources of the hallucination problem when the model includes
information in generated text that may not necessarily be true.
Information about the background or context of the image that is
not depicted in the image. The coherence framework can help us
identify,characterizeandaddresstheseissues.
Coherence relations indicate discourse goals. Figure5shows
that the labels that our dataset presents correlate with the genre
under which the captions have been produced, which means that
text and images from diﬀerent types of publications have diﬀerent
distributions of relations. For example, captions from a news
website such as daily mail are more story-like, whereas Getty
Imagespostsofteninclude Visiblecaptions./two.tnum
/four.tnum. Using coherence to critique
image text models
In the previous Section, we discussed how coherence
deﬁnes what linguistic forms and inferences go into text image
interpretation.Wealsodiscussedhowtheframeworkcouldidentify
weaknesses in machine learning models trained on image-text
corpora.InthisSection,westudyhowdiﬀerentmodelarchitectures
andtrainingmechanismscancapturecoherencetovariousdegrees.
We ﬁrst present the details of our computational experiments
then move on to describe the results and discussions. Our
computational investigations reveal that large multimodal models
cannotaccuratelyrepresentcoherencerelations.Theyfailtoreason
/two.tnum Getty Images  http //www.overleaf.com .
Daily Mail  https //www.dailymail.co.uk/ushome/index.html .
Frontiersin ArtiﬁcialIntelligence /zero.tnum/seven.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
FIGURE/five.tnum
Diﬀerent websites have diﬀerent types of image caption pairs
(Alikhani et al., /two.tnum/zero.tnum/two.tnum/zero.tnum ).
ﬂexibly about the links that connect text image pairs in the same
waywedescribedinourdata.
/four.tnum./one.tnum. Experiment structure
Inthissection,wedescribeourexperimentalsetup,theproblem
formulation and then go on to describe the insights we draw from
ourresults.
We investigate whether the two most recent and successful
versions of these models, VilBERT ( Lu et al., 2019 ) and CLIP
(Radford et al., 2021 ) can implicitly learn coherence relations in
image textpresentationsduringthepre-trainingprocess.
VilBERT is a transfomer based model pre-trained on the
Conceptual Captions dataset. It is inspired from the BERT
architecture and takes as input a sequence of image blocks and
text tokens belonging to the image caption. Image blocks and
text sequence are separated by a special token. It is trained using
two proxy tasks  masked multi-modal modeling and multi-modal
alignment prediction and then the learned weights. It relies on the
self-attention mechanism of transformer architecture to learn the
relationsbetweendiﬀerentpartsofanimageandthecorresponding
textualtokens.
CLIPis trained using contrastive learning to maximize
similaritytextandimagepairs.UnlikeVilBERT,ithastwoseparate
encoders for the text and visual data. Visual encoder is either
a variation of ResNet architecture or a visual transformer while
the text encoder is based on the transformer architecture. A dot
product between the text and image representations is used to
compute the similarity and then loss is computed using the binary
crossentropyobjective.
/four.tnum./one.tnum./one.tnum. Problem formulation
The problem is structured in the form of a classiﬁcation task
where goal of the model is to classify the representation hias a
coherence relation. We measure the model performance using theF1metricasitisawell-knownmetricformeasuringaclassiﬁcation
modelperformance.
Formally, speaking, we deﬁne a function fθpas a pre-trained
model which maps an input pair ( xi,vi) to a vector hiin
addimensional representation space Rd, where θprepresents
the pre-trained weights, xiandvirepresent the text sample
and its corresponding image respectively. To check whether the
representation hihas a certain information, we can deﬁne a
linear probe flinwhich takes hias input and outputs a probability
distribution P(li hi)overagivensetof nlabelsli L.Bytuningthe
weights associated with the linear probe W, we can learn whether
therepresentation hitoidentifydiﬀerentclasses li L.
P(li hi) flin(fθp(xi,vi))
flin(hi) σ(Whi b)
Li 1
n 
lj L log(P(lj hi))(1)
WhereLiis the cross entropy loss signal associated with the
image-text pairs and their corresponding labels, and σrepresents
the sigmoid function. Since the parameters θpassociated with the
pre-trainedmodelarefrozen,thelinearprobecanonlymakeuseof
informationencodedintherepresentation hi.
To discern if the representations hiencode the discourse
relation information, we compare probes for the above mentioned
models with the probes trained using representations from
ResNet and BERT. In addition, we also ﬁne-tune the pretrained
 model weights θto observe the improvements when the
representations hiare also ﬁne-tuned. Our hypothesis is that pretrainedrepresentationscontainingsignalsforidentifyingdiscourse

relations should outperform the baselines and show competitive
performance when compared to the models obtained using ﬁnetunedrepresentations.

/four.tnum./one.tnum./two.tnum. Experiment parameters
AsdescribedintheSection4.2,weﬁne-tunethemodelsintwo
ways. When the pre-trained weights are frozen and only the probe
weights are trained, we rely on a batch size of 64 and a learning
rate of 5  10 3. However, when the we run the experiments to
ﬁne-tunethepre-trainedweightswerelyonabatchsizeof8anda
learningrateof10 5.Inbothcases,theweightsareoptimizedusing
the Adam optimizer and a small regularization weight of 10 5is
utilizedtopreventmodelfromoverﬁttingthedata.
/four.tnum./two.tnum. Results
To conduct our experiments we split the 5760 image-text
pairs into train, validation and test ratios of 0.85, 0.075, 0.075
respectively. The idea of our evaluation setup is to investigate
whether the representations learned by the pre-trained models
encode the discourse relations between image-text pairs. We do
so by comparing the performance of ﬁne-tuned models with
their respective linear probes. If the ﬁne-tuned models perform
better than their respective linear probes it shows that pre-trained
Frontiersin ArtiﬁcialIntelligence /zero.tnum/eight.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
TABLE/one.tnum ThistableshowstheF/one.tnumscoreaveragesandtheirbreakdownsob tainedbythelinearprobesfordiﬀerentvisual-linguisticarchite ctures.
Metrics
Macro average Micro average
Model F/one.tnum Precision Recall F/one.tnum Precision Recall
CLIP 0.466 0.468 0.571 0.762 0.744 0.781
VilBERTVil 0.582 0.545 0.712 0.780 0.758 0.804
VilBERTLinguistic 0.635 0.601 0.713 0.808 0.796 0.820
Resnet 0.333 0.337 0.345 0.682 0.617 0.763
BERT 0.623 0.590 0.776 0.798 0.791 0.804
BERT Resnet 0.637 0.614 0.712 0.794 0.798 0.789
Macro average gives equal weights to the model performance in eac h class and hence represents the eﬀect of imbalance while micro ave rage represents the overall average performance.
The diﬀerence in performance highlights that representations lear ned by some model architecture are better suited for the purpose of ide ntifying discourse relations  VilBERTLinguistic
representationsinthiscase.
FIGURE/six.tnum
This ﬁgure highlights the change in macro F/one.tnum scores when the model we ights are ﬁne-tuned during the training process. In all cases, the
performance for the ﬁne-tuned models improves by a signiﬁcant mar gin reﬂecting that pre-trained representations do not encode in formation
necessary to identify discourse relations.
representations lack key information which would have allowed
themtoidentifycoherencerelationsandviceversa.
As stated earlier we build probes for two pre-trained models 
CLIP and Vilbert. For the pre-trained Vilbert weights, we
build two probes  for the visual-linguistic representations and
linguisticrepresentationsoutputtedbythemodel.Visual-linguistic
representationsareobtainedbycombiningthevisualandlinguistic
representations, and linguistic representations use co-attention
mechanism with image representations to represent relations
betweenimageandtextpairs.FortheCLIPmodel,weconcatenate
the visual and text representations to obtain the representation for
image-textpair.
We present the linear probe performance based on the F1
average scores (macro and micro) and their breakdown in the
Table1. These results highlight the capability of representations
learned by diﬀerent architectures in encoding information
necessary to identify discourse relations. Our results show that
the probe for linguistic representations learned by the VilBERT
usingco-attentionwithimagerepresentations(VilBERTLinguistic)
shows better performance when compared to the probes for otherarchitecture including visual-linguistic variation of VilBERT. This
could be a signal that architectures which try to learn ﬁne-grained
relations between image-text pairs are more suited to learning
discourserelations.
When compared with the baseline probes, speciﬁcally BERT
probe, the performance of the best pre-trained visual-linguisitic
probes is similar in terms of the F1 score achieved as shown in
theTable1. This highlights that pre-trained models are unable to
make use of the visual information in a meaningful manner to
successfully encode the relations between the image-text pairs in
thehigherdimensionalembeddingspace.
When the pre-trained model weights are ﬁne-tuned we see a
signiﬁcantincreaseinthemacroF1scoreasshowninthe Figure6.
Even though the best performance after ﬁne-tuning is achieved
by the VilBERTLinguistic model, it only lags behind the BERT
  Resnet performance by a slight margin of 3%. This provides
evidence that pre-training with large corpora of image-text pairs
doesnotimplicitlyallowmodelslikeVilBERTandCLIPtoencode
discourse relations in the higher dimensional embedding space
and calls for the exploration of techniques which explicitly utilize
Frontiersin ArtiﬁcialIntelligence /zero.tnum/nine.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
discourse relations to learn models whose predictions are in-line
withhumanjudgements.
/five.tnum. Using coherence to deﬁne and
evaluate AI systems
Inthissection,wediscusswaystousethedatasetandmodelswe
discussedintheprevioussectionsindesigningmultimodalsystems.
Previous works have shown the utility of using the coherence
framework in design image retrieval models ( Alikhani et al.,
2022),captiongenerationmodels( Alikhanietal.,2020 ),automatic
evaluation( Inanetal.,2021 ),anddiagramunderstanding( Hiippala
etal.,2021 ).
Can a coherence-aware model present information that is
aligned with the goal of the discourse  Can a coherence-aware
model signiﬁcantly improve caption quality  Can we design
automatic learned generation metrics that can evaluate the output
ofcoherence-awaregenerationmodels 
/five.tnum./one.tnum. Generating coherent captions
Can we design controllable image description generation
models that can generate captions with respect to diﬀerent
coherence relations  Alikhani et al. (2020) introduced such
controllable model using the Clue dataset. They used Transformer
Networks ( Vaswani et al., 2017 ) and designed a generation model
that can output captions using a sequence-generation approach.
The result is a sequence of sub-tokens that create the desired
caption.Theinputincludesdiﬀerentimagesfeaturesandthetarget
coherence relation label. The relation label is the start token of the
Transformerdecoder.
The proposed model is able to reduce noise by around 30%
from the generated captions overall. It includes substantially
fewer irrelevant captions, and it can respect the discourse goals
by generating captions connected to the image by the desired
coherence relations. The success rates of the model when it was
asked to generate visible, meta, story and subjective captions ware
respectively79.85%,46.49%,58.8%and45.00%.Thedetailscanbe
foundinAlikhanietal. (2020).
/five.tnum./two.tnum. Coherence-aware learned evaluation
metrics
As we observed in the previous section, image captioning
metrics have struggled to give accurate learned estimates of the
semantic and pragmatic success of output text. Inan et al. (2021)
proposed the ﬁrst discourse-aware learned metric for evaluating
such properties in image descriptions. The goal of the metric is
to output a score that reﬂects the quality of the generated caption
given the image, coherence relation and the reference caption. In
whatfollowswereviewtheproposedmodelandresults.
They worked with 1,000 image text pairs from the Conceptual
Captions (CC) training dataset ( Ng et al., 2020 ) and collected
ratings for them. They use the cc imges as inputs to caption
generation model presented by Alikhani et al. (2020). Themodel generates coherence-aware descriptions for these images in
diﬀerentcoherenceclasses.Then,theyaskedtheannotatorstowrite
Visiblecaptions for 1,000 images from the OpenImages dataset
(Kuznetsova et al., 2020 ) and called the dataset COIN ( Corpus of
OpenImageswith Naturaldescriptions).
The proposed approach has two versions a baseline Vanilla
version and a ViLBERT-based model. Both are trained on
RaCCoon training data with normalized human annotated rating
toobtainthemodel stargetscore.Detailsofthedatasetandablation
studies are available here ( Inan et al., 2021 ).Table2presents the
results of the COIN-based study. The last row shows the Kendall
correlation coeﬃcient between the scores assigned by users and
the metric. The N-gram based metrics cannot adapt to the outof-domainground-truthcaptionsfromCOINwhichresultsinlow

correlationcoeﬃcients.TheCIDErscoreshavenegativecorrelation
coeﬃcients which indicate negative association with user ratings.
BLEURTandBERTScoredoamuchbetterjobincomparisonwith
CIDEr and N-gram based metrics but they are still agnostic to the
coherence relation label. Our proposed model which is coherenceawarehasthehighestcorrelationscoreswithuserjudgments.

/six.tnum. Conclusion
When authors combine text and imagery, they use the
diﬀerent modalities of communication in concert  common
principles of coherence relate communicative actions together,
guideinterpretiveinferences,andresolveambiguities.Inthispaper,
we have described how the well-known theory of coherence in
text discourse extends to image text presentations and can guide
AI research on mulitmodal communication. While we have thus
far oﬀered a range of ﬁndings to show the potential beneﬁts of
coherenceinmultimodalAI,weareoptimisticforfurtherresearch
progressinalloftheseareas.
In particular, we have seen that coherence relations oﬀer
important tools to analyze data sets  coherence relations can allow
us to quantify diﬀerences in language use across diﬀerent corpora
and even to explain the distribution of linguistic phenomena in
corpora as a function of the distinctive character of coherence
in corpora. Work on taxonomies of coherence relations for
multimodal discourse is in its infancy. New genres and tasks
could highlight the importance of additional relations or further
distinctionsinhowtextrelatestoimagery.Conversely,wehavesaid
littleaboutimagerythatgetsitscoherencefromaccompanyingtext.
Alikhanietal. (2019)annotatestheinferencesthatgroundimagery
in a speciﬁc domain through particular temporal, spatial, and
logicalconnections,ratherthanthroughatraditionaltaxonomyof
coherence relations. It is an open question whether coherence can
be systematized more generally across images and text. Another
challenge is accounting for the structure of multimodal discourse,
particularly for presentations that involve relations within and
across modalities. Multimodal analyses of situated conversations
have revealed many complexities ( Lascarides and Stone, 2009b  
Hunteretal.,2018 ).
Wehavealsoseenthatcoherencerelationsalsooﬀeravaluable
lens to critique and improve the architecture of machine learning
models. Models that build in an assumption that text and imagery
relate in simple, uniform ways, are less eﬀective in capturing
coherence than models that allow for more ﬂexibility. Research
Frontiersin ArtiﬁcialIntelligence /one.tnum/zero.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnumTABLE/two.tnum Scoresfordiﬀerentimagecaptioningmodelsasevaluated byusersanddiﬀerentcaptioningmetrics (Inanetal.,/two.tnum/zero.tnum/two.tnum/one.tnum ).
System Metrics
Model Coh. Label Avg.Hum.
RatingB1B2MRLCSBR BS-F COSMic
VanillaCOSMic
VBERTCOSMic
Vanilla COSMic
VBERT 
BUTD Visible 2.191 0.163 0.077 0.049 0.160 0.092 0.030  0.877 0.863 0.706 0.796 0.522 0.641
BaseVisible 30.532 0.050 0.025 0.019 0.066 0.020 0.002  1.114 0.862 0.696 0.777 0.516 0.614
Meta 3.213 0.041 0.000 0.012 0.063 0.012 0.000  1.059 0.863 0.548 0.727 0.505 0.602
Subj. 2.830 0.033 0.012 0.011 0.057 0.017 0.000  1.197 0.849 0.323 0.421 0.358 0.403
Story 2.915 0.029 0.000 0.017 0.058 0.013 0.000  1.304 0.842 0.533 0.629 0.482 0.527
LiteVisible 3.298 0.028 0.011 0.013 0.053 0.011 0.000  1.101 0.863 0.684 0.784 0.515 0.604
Meta 2.830 0.026 0.010 0.008 0.055 0.015 0.000  1.084 0.859 0.548 0.748 0.511 0.565
Subj. 2.298 0.039 0.012 0.019 0.066 0.024 0.003  1.217 0.849 0.364 0.451 0.379 0.419
Story 2.426 0.036 0.000 0.018 0.062 0.021 0.000  1.362 0.842 0.568 0.666 0.499 0.519
Kendall s Correlation( τ) 1.000 0.071 0.154 0.036  
0.036 
0.571 
0.0520.286 0.445 0.571 0.546 0.667 0.764
TheCOSMicViLBERT modelhasthehighestcorrelationscoreswithhu manjudgments.in large vision language models, however, has overwhelmingly
designed around capturing the relationships between image
content and Visibletext. Relaxing this assumption oﬀers exciting
prospects for learning more powerful representations of the
meaningsofimage textpresentations.
Finally, we have oﬀered an example of coherence-aware tasks
andevaluationmetrics.SincecurrentAItechnologystruggleswith
coherence, we need to take coherence into account from the start
as we design and test AI systems. AI researchers still face many
challenges in extending information and interaction tasks from
their origins in text processing to multimodal communication.
All of these domains, we believe, oﬀer fruitful settings to pursue
coherence-awaremethodologies.
Data availability statement
The original contributions presented in the study are included
in the article/supplementary material, further inquiries can be
directedtothecorrespondingauthor.
Ethics statement
The studies involving human participants were reviewed
and approved by Institutional Review Board, Rutgers
University. The patients/participants provided their written
informed consent to participate in this study. Written
informed consent was obtained from the individual(s), and
minor(s)  legal guardian/next of kin, for the publication
of any potentially identiﬁable images or data included in
thisarticle.
Author contributions
MA led data annotation, empirical analysis, and evaluation.
BK led model training and implementation. MS led theoretical
analysis,modeldesign,andexplanation.Allauthorscontributedto
thearticleandapprovedthesubmittedversion.
Funding
The research presented in this paper has been supported
by NSF awards IIS-1526723, IIS-1703883, CCF-1934924, IIS1955404,
 IIS-1955365, DGE-2021628, RETTL-2119265, and
EAGER-2122119 and by Air Force Research Lab Contract
FA8650-22-P-6415.
Acknowledgments
Thanks to Mert Inan, Piyush Sharma, Radu Soricut,
Shengjie Li, Gerard de Melo, Sreyasi Nag Chowdhury,
Fangda Han, Hareesh Ravi, Mubbasir Kapadia, and Vladimir
Pavlovic for their contributions to the Clue, CITE, COSMic,
and the coherence-aware image retrieval models and
datasets, and to the reviewers and editors of the special
Frontiersin ArtiﬁcialIntelligence /one.tnum/one.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
issue for their valuable comments on previous versions of
thepaper.
Conﬂict of interest
The authors declare that the research was conducted in the
absence of any commercial or ﬁnancial relationships that could be
construedasapotentialconﬂictofinterest.Publisher s note
All claims expressed in this article are solely those of the
authors and do not necessarily represent those of their aﬃliated
organizations, or those of the publisher, the editors and the
reviewers. Any product that may be evaluated in this article, or
claim that may be made by its manufacturer, is not guaranteed or
endorsedbythepublisher.
References
Abusch,D.(2013). Applyingdiscoursesemanticsandpragmatic stoco-referencein
picturesequences, in ProceedingsofSinnundBedeutung17 ,edsE.Chemla,V.Homer,
andG.Winterstein(Paris),9 25.
Alikhani, M., Chowdhury, S. N., de Melo, G., and Stone, M. (2019) .
 Cite  a corpus of image-text discourse relations,  in Proceedings of the 2019
Conference of the North American Chapter of the Association for Computati onal
Linguistics  Human Language Technologies, Volume 1 (Long and Short Pa pers),
570 575.
Alikhani, M., Han, F., Ravi, H., Kapadia, M., Pavlovic, V., and S tone, M. (2022).
 Cross-modal coherence for text-to-image retrieval,  in Proceedings of the 36th AAAI
ConferenceonArtiﬁcialIntelligence,Vol.10 ,10427 10435.
Alikhani, M., Sharma, P., Li, S., Soricut, R., and Stone, M. (2 020).  Cross-modal
coherencemodelingforcaptiongeneration, in Proceedingsofthe58thAnnualMeeting
of the Association for Computational Linguistics (Association for Computational
Linguistics),6525 6535.
Alikhani, M., and Stone, M. (2018).  Exploring coherence in vis ual explanations, 
in2018 IEEE Conference on Multimedia Information Processing and Retrie val (MIPR)
(Miami,FL IEEE),272 277.
Alikhani, M., and Stone, M. (2019).   caption  as a coherence r elation  evidence
and implications,  in Proceedings of the Second Workshop on Shortcomings in Vision
andLanguage (Minneapolis,MN AssociationforComputationalLinguistics ),58 67.
Asher,N.,andLascarides,A.(2003). LogicsofConversation .Cambridge Cambridge
UniversityPress.
Bavelas, J. B., and Chovil, N. (2000). Visible acts of meaning  a n integrated
messagemodeloflanguageinface-to-facedialogue. J.Lang.Soc.Psychol .19,163 194.
doi 10.1177/0261927X00019002001
Bolinger, D. (1972). Accent is predictable (if you re a mind-re ader).Language . 48,
633 644.doi 10.2307/412039
Chowdhury, S. N., Tandon, N., and Weikum, G. (2019). Know2look  
commonsense knowledge for visual search. arXiv preprint arXiv 1909.00749.
doi 10.48550/arXiv.1909.00749
Cohn, N. (2013). Visual narrative structure. Cogn. Sci . 37, 413 452.
doi 10.1111/cogs.12016
Cumming, S., Greenberg, G., and Kelly, R. (2017). Conventions of viewpoint
coherenceinﬁlm. Philos.Imprint 17,1 29.
Engle, R. A. (2001). Toward a Theory of Multimodal Communication  Combining
Speech, Gestures, Diagrams and Demonstrations in Instructional E xplanations (Ph.D.
thesis).StanfordUniversity.
Feiner, S. K., and McKeown, K. R. (1991). Automating the gene ration of
coordinated multimedia explanations. IEEE Comput . 24, 33 41. doi  10.1109/2.
97249
Funaki, R., and Nakayama, H. (2015).  Image-mediated learni ng for zero-shot
cross-lingual document retrieval,  in Proceedings of the 2015 Conference on Empirical
MethodsinNaturalLanguageProcessing ,585 590.
Guo, L., Liu, J., Yao, P., Li, J., and Lu, H. (2019).  Mscap  multi -style
image captioning with unpaired stylized text,  in Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (Long Beach, CA  IEEE),
4204 4213.
Hiippala, T., Alikhani, M., Haverinen, J., Kalliokoski, T., Logac heva, E., Orekhova,
S.,etal.(2021).Ai2d-rst amultimodalcorpusof1000primarys choolsciencediagrams.
Lang.Resour.Evaluat .55,661 688.doi 10.1007/s10579-020-09517-1
Hobbs, J. R. (1979). Coherence and coreference. Cogn. Sci . 3, 67 90.
doi 10.1207/s15516709cog0301_4
Hobbs, J. R. (1985). On the coherence and structure of discourse . Technical report,
CenterfortheStudyofLanguageandInformation,StanfordU niversity.Huang, T.-H. K., Ferraro, F., Mostafazadeh, N., Misra, I., A grawal, A., Devlin, J.,
et al. (2016).  Visual storytelling,  in Proceedings of the 2016 Conference of the North
American Chapter of the Association for Computational Linguistics   Human Language
Technologies ,1233 1239.
Hunter, J. (2019). Relating gesture to speech  reﬂections on th e role of conditional
presuppositions. Linguist.Philos .42,317 332.doi 10.1007/s10988-018-9244-0
Hunter, J., and Abrusán, M. (2017).  Rhetorical relations an d quds.  in New
Frontiers in Artiﬁcial Intelligence  JSAI-isAI Workshops LENLS, J URISIN, KCSD, LLLL
RevisedSelectedPapers (Springer),371 380.
Hunter, J., Asher, N., and Lascarides, A. (2018). A formal se mantics for situated
conversation. SemantPragmat .doi 10.3765/sp.11.10
Inan, M., Sharma, P., Khalid, B., Soricut, R., Stone, M., and A likhani, M. (2021).
 COSMic acoherence-awaregenerationmetricforimagedes criptions, in Findingsof
theAssociationforComputationalLinguistics EMNLP2021 ,3419 3430.
Kehler,A.(2002). Coherence,Reference,andtheTheoryofGrammar .Stanford,CA 
CSLIPublications.
Kendon, A. (2004). Gesture  Visible Action as Utterance . Cambridge  Cambridge
UniversityPress.
Kruk, J., Lubin, J., Sikka, K., Lin, X., Jurafsky, D., and Div akaran, A. (2019).
 Integrating text and image  Determining multimodal docume nt intent in Instagram
posts,  in Proceedings of the 2019 Conference on Empirical Methods in Natural
Language Processing and the 9th International Joint Conference on Nat ural Language
Processing(EMNLP-IJCNLP) (HongKong AssociationforComputationalLinguistics),
4622 4632.
Kuznetsova, A., Rom, H., Alldrin, N., Uijlings, J., Krasin, I., Pont-Tuset, J.,
et al. (2020). The open images dataset v4. Int. J. Comput. Vis . 128, 1956 1981.
doi 10.1007/s11263-020-01316-z
Lascarides, A., and Stone, M. (2009a). Discourse coherence and gesture
interpretation. Gesture9,147 180.doi 10.1075/gest.9.2.01las
Lascarides, A., and Stone, M. (2009b). A formal semantic ana lysis of gesture. J.
Semant.26,393 449.doi 10.1093/jos/ﬀp004
Lin, T.-Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ram anan, D., et al. (2014).
 Microsoft coco  common objects in context,  in European Conference on Computer
Vision(Springer),740 755.
Lu, J., Batra, D., Parikh, D., and Lee, S. (2019).  Vilbert  pre training task-agnostic
visiolinguisticrepresentationsforvision-and-languageta sks, inProceedingsofthe33rd
InternationalConferenceonNeuralInformationProcessingSystems ,13 23.
McCloud,S.(1993). UnderstandingComics TheInvisibleArt .WilliamMorrow.
McNeill,D.(1992). HandandMind WhatGesturesRevealAboutThought .Chicago 
UniversityofChicagoPress.
Moens, M., and Steedman, M. (1988). Temporal ontology and tempo ral reference.
Comput.Linguist .14,15 28.
Ng, E. G., Pang, B., Sharma, P., and Soricut, R. (2020). Unders tanding guided
image captioning performance across domains. arXiv preprint arXiv 2012.02339.
doi 10.18653/v1/2021.conll-1.14
Otto, C., Springstein, M., Anand, A., and Ewerth, R. (2019).   Understanding,
categorizing and predicting semantic image-text relations,   inProceedings of the 2019
onInternationalConferenceonMultimediaRetrieval (ACM),168 176.
Phillips, B. (1977).  A calculus of cohesion,  in Fourth LACUS Forum (Montreal,
QC),627 637.
Prasad, R., Dinesh, N., Lee, A., Miltsakaki, E., Robaldo, L., J oshi, A., et al. (2008).
 ThepenndiscourseTreeBank2.0, in ProceedingsoftheSixthInternationalConference
on Language Resources and Evaluation (LREC 08) (Marrakech  European Language
ResourcesAssociation,ELRA).
Frontiersin ArtiﬁcialIntelligence /one.tnum/two.tnum frontiersin.orgAlikhani et al. /one.tnum/zero.tnum./three.tnum/three.tnum/eight.tnum/nine.tnum/frai./two.tnum/zero.tnum/two.tnum/three.tnum./one.tnum/zero.tnum/four.tnum/eight.tnum/eight.tnum/seven.tnum/four.tnum
Radford, A., Kim, J. W., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., et al.
(2021).  Learning transferable visual models from natural lan guage supervision,  in
Proceedingsofthe38thInternationalConferenceonMachineLearning, ICML2021,1824
 July 2021, Virtual Event, volume 139 of Proceedings of Machine Lea rning Research ,
edsM.MeilaandT.Zhang(PMLR),8748 8763.
Ramesh,A.,Pavlov,M.,Goh,G.,Gray,S.,Voss,C.,Radford,A.,e tal.(2021). Zeroshot
 text-to-image generation,  in Proceedings of the 38th International Conference
on Machine Learning, ICML 2021, 18-24 July 2021, Virtual Event, v olume 139 of
Proceedings of Machine Learning Research , eds M. Meila and T. Zhang (PMLR),
8821 8831.
Roberts, C. (2012). Information structure  towards an inte grated formal theory of
pragmatics. SemantPragmat .5,6 1.doi 10.3765/sp.5.6
Rohde, H., Johnson, A., Schneider, N., and Webber, B. (2018) .  Discourse
coherence  Concurrent explicit and implicit relations,  in Proceedings of the 56th
Annual Meeting of the Association for Computational Linguistics (Vo lume 1  Long
Papers)(Melbourne,VIC AssociationforComputationalLinguistics ),2257 2267.
Sanders, T. J. M., Spooren, W. P. M., and Noordman, L. G. M. (1992 ).
Toward a taxonomy of coherence relations. Discour. Process . 15, 1 35.
doi 10.1080/01638539209544800
Sharma, A. (2020).  Improving intent classiﬁcation in an E-co mmerce voice
assistant by using inter-utterance context,  in Proceedings of The 3rd Workshop on
e-CommerceandNLP (Seattle,WA AssociationforComputationalLinguistics),4 0 45.
Sharma, P., Ding, N., Goodman, S., and Soricut, R. (2018).  Co nceptual captions 
a cleaned, hypernymed, image alt-text dataset for automatic im age captioning,  in
Proceedingsofthe56thAnnualMeetingoftheAssociationforComputati onalLinguistics
(Volume1 LongPapers) (Melbourne,VIC AssociationforComputationalLinguistics ),
2556 2565.
Shuster, K., Humeau, S., Hu, H., Bordes, A., and Weston, J. (2 019).  Engaging
imagecaptioning viapersonality, in IEEEConferenceonComputerVisionandPattern
Recognition,CVPR2019 (LongBeach,CA IEEE),12516 12526.Smyth, R. (1994). Grammatical determinants of ambiguous pron oun resolution. J.
Psycholinguist.Res .23,197 229.doi 10.1007/BF02139085
Stone,M.,andStojnic,U.(2015).Meaninganddemonstratio n.Rev.Philos.Psychol .
6,69 97.doi 10.1007/s13164-014-0213-4
vanKuppevelt,J.(1995).Discoursestructure,topicalityandque stioning.J.Linguist .
31,109 147.doi 10.1017/S002222670000058X
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., et al.
(2017).  Attention is all you need,  in Advances in Neural Information Processing
Systems,5998 6008.
Vempala, A., and Preo  tiuc-Pietro, D. (2019).  Categorizing and inferring the
relationship between the text and image of Twitter posts,  in Proceedings of the 57th
AnnualMeetingoftheAssociationforComputationalLinguistics (Florence Association
forComputationalLinguistics),2830 2840.
Webber,B.,Prasad,R.,Lee,A.,andJoshi,A.(2019). Thepenndiscoursetreebank3.0
annotationmanual .Technicalreport,LinguisticDataConsortium.
Webber, B., Stone, M., Joshi, A., and Knott, A. (2003). Anapho ra and
discourse structure. Comput. Linguist . 29, 545 588. doi  10.1162/0891201033227
53347
Webber, B. L. (1987).  The interpretation of tense in discour se,  inProceedings of
the25thAnnualMeetingonAssociationforComputationalLinguisti cs(Associationfor
ComputationalLinguistics),147 154.
Yagcioglu, S., Erdem, A., Erdem, E., and Ikizler-Cinbis, N. (2 018). Recipeqa  a
challenge dataset for multimodal comprehension of cooking reci pes.arXiv preprint
arXiv 1809.00812.doi 10.18653/v1/D18-1166
Young, P., Lai, A., Hodosh, M., and Hockenmaier, J. (2014). F rom image
descriptions to visual denotations  New similarity metrics f or semantic
inference over event descriptions. TACL2, 67 78. doi  10.1162/tacl_a
_00166
Frontiersin ArtiﬁcialIntelligence /one.tnum/three.tnum frontiersin.org