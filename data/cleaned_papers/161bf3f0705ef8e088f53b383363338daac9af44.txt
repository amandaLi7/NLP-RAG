Latent Positional Information is in the Self-Attention Variance of
Transformer Language Models Without Positional Embeddings
Carnegie Mellon UniversityTing-Han Fan
Princeton UniversityLi-Wei Chen
Carnegie Mellon University
Alexander I. Rudnicky
Carnegie Mellon UniversityPeter J. Ramadge
The use of positional embeddings in transformer
 language models is widely accepted.
However, recent research has called into question
 the necessity of such embeddings. We
further extend this inquiry by demonstrating
that a randomly initialized and frozen transformer
 language model, devoid of positional
embeddings, inherently encodes strong positional
 information through the shrinkage of selfattention
 variance. To quantify this variance,
we derive the underlying distribution of each
step within a transformer layer. Through empirical
 validation using a fully pretrained model,
we show that the variance shrinkage effect still
persists after extensive gradient updates. Our
findings serve to justify the decision to discard
positional embeddings and thus facilitate more
efficient pretraining of transformer language
1 Introduction & Related Work
Transformer models have become the backbone of
natural language processing applications (Vaswani
et al., 2017  Devlin et al., 2019  Radford et al.,
2019). Within the transformer architecture, there
are two main categories  1) bidirectional models,
such as BERT (Devlin et al., 2019), that are trained
using the masked language modeling objective, and
2) (causal) language models, such as GPT (Radford
et al., 2019), that are trained using the traditional
language modeling objective. Both of these categories
 share the common feature of using positional
embeddings for encoding token distance.
Whether positional embeddings are truly essential
 has been a subject of ongoing research. While
they have been considered necessary for bidirectional
 transformer models (Lee et al., 2019  Luo
et al., 2021  Sinha et al., 2021  Haviv et al., 2022),
the situation is different for transformer language
models (Irie et al., 2019  Yang et al., 2019  Tsai
 Correspondence to  tachungc@andrew.cmu.edu
Layer 1Figure 1  The architecture of a Pre-LN transformer language
 model. All the parameters are randomly initialized
 and randomly sampled input is used in this work.
et al., 2019  Scao et al., 2022  Haviv et al., 2022).
In transformer language models, the removal of
positional embeddings results in only a marginal
decline in performance, while enabling more efficient
 training (Haviv et al., 2022). In addition
to empirical evidence, it has been proven (Bhattamishra
 et al., 2020) that transformer language
models without positional embeddings are Turingcomplete
 and able to model sequences akin to recurrent
 neural networks (Rumelhart and McClelland,
 1987  Jordan, 1986). Despite this, it remains
an open question where positional information is
stored in the absence of positional embeddings.
This motivates further investigation into individual
operations within a transformer layer.
The example architecture of a pre-LN (Xiong
et al., 2020) multi-layer transformer language
model with no positional embeddings used in thisarXiv 2305.13571v1  [cs.CL]  23 May 2023405060708090100
0510152025MAENumber of LayersFigure 2  We plot the positions w.r.t their mean absolute
error (MAE) for input sequence length L  512 . A
naive baseline of predicting the middle point of L  256
gives an MAE of 128. The numbers are the average of
work is shown in Figure 1.1We hereinafter refer
to this configuration as TLM. Our primary focus
is on the multi-head attention (MHA) module of a
randomly initialized TLM, as it is the only module
that allows inter-token information exchange. To
gain a deeper understanding, we compute the mean
and variance of MHA outputs. To our surprise, we
discover that the variance already encodes latent
positional information, with later tokens in a sequence
 displaying smaller variance. This motivates
us to quantify the variance by deriving the output
distribution after MHA operations. Finally, through
empirical validation using a fully pre-trained TLM,
we confirm thatthe same variance shrinkage effect
persists after extensive gradient updates.
To the best of our knowledge, we are the first
to identify and quantify the latent positional information
 in TLMs. Our results provide theoretical
insights into the removal of positional embeddings,
enabling more efficient pretraining of future TLMs.
2 Probing Experiments
Given BERT and TLM (GPT) with positional embeddings
 removed, prior work (Haviv et al., 2022)
shows that only TLM is able to maintain the same
language modeling performance as its original version
 with positional embeddings. The discrepancy
might be explained by the fact that only TLM encodes
 positional information within its layers, as
shown by the position probing experiment in Haviv
et al. (2022). Since both BERT and TLM have
access to the same semantic input and the only
difference is the use of causal attention masks in
TLM, we hypothesize that the positional informa1Post-LN
 places layer norm at different positions. It is the
configuration used in BERT (Devlin et al., 2019).tion may be attributed to the interaction between
causal attention masks and the TLM architecture.
To further explore this hypothesis, we use a randomly
 initialized and frozen TLM to eliminate any
semantic influence and focus solely on the architectural
 design. Additionally, to prevent the model
from memorizing the order of input sequences, we
do not perform embedding lookups and feed the
model with randomly sampled input vectors. A
trainable two-layer linear classifier with ReLU activation
 in between was appended to the TLM to
probe the position of each token (further details can
be found in Appendix B). We plot the mean absolute
 error (MAE) w.r.t the number of transformer
layers in Figure 2. The plot indicates a randomly
initialized and frozen TLM with randomly sampled
input vectors inherently provides positional information,
 with an increase in the number of layers
resulting in higher probing performance. This surprising
 outcome prompts further investigation into
the encoding of latent positional information inside
the TLM architecture.
3 Theoretical Analysis
We dissect the inner workings of a TLM by deriving
 the distribution of TLM operations in the hope
that they elucidate where the latent positional information
 is stored. The derivation is made possible
thanks to the usage of a randomly initialized and
frozen TLM. We adopt the initialization settings
in accordance with those employed in GPT (Radford
 et al., 2019). WLOG, our derivation is limited
to the operations of the first layer in a TLM and
the FFN component is omitted (justified in  3.4).
The hyperparameters utilized in the simulations
are  hidden dimension d  768 , number of attention
 heads H  12 , head dimension d/H   64 ,
sequence length L  512 , standard deviation for
initialization σ  0.02. All proofs of lemmas are
deferred to Appendix A.
Given a sequence of randomly sampled input
m 1, where each element of
xm Rdis sampled i.i.d from N(0, σ2), a TLM
consists of the following operations 
3.1 Layer Normalization
For each input embedding xm, it computes the
sample mean and (biased) sample variance 
d0 100 200 300 400 500
Positions0.00.20.40.60.81.0Cumulative Attention ProbabilityFigure 3  We plot the positions w.r.t their cumulative
attention score for L  512 averaged over 500 samples.
Then each entry iofxm, denoted as xmi, is normalized
 by mean and variance to emi 
where V[x]denotes the variance of x. Since the
initialization scheme sets γ  1 andβ  0,( )
holds with sufficiently large dby the Law of large
numbers and the continuous mapping theorem.
Each attention head computes query, key, and value
qm Wqem,km Wkem,vm Wvem,
each element sampled i.i.d from N(0, σ2).
To be precise, most matrices ( W(h)
m), and scalars ( l(h)
mn) are associated with a head number h. For
notation simplicity, we only show the dependency
Lemma 1. qm,km, andvmhave zero mean and
(dσ2) Icovariance matrix.
The resulting vectors are processed by the selfattention
 module for pre-Softmax logits 
Log VarianceTheoretical@Layer 0
Simulation@Layer 11Figure 4  We plot the log positions (up to L  512 ) w.r.t
their log variance under the assumption of Property 1.
The simulation aligns with the theoretical curve posited
by Lemma 3 at the 0thlayer averaged over 500 samples.
followed by the scaled softmax normalization 
Lemma 2. lmnhas zero mean andd3σ4
The numerical variance of lmn/p
12 0.0079 . Lemma 2 suggests the
following approximation 
Property 1. When σ4 H
d2,lm, has small variance,
 making the attention weights am, almost
evenly distributed among all positions.2
In Figure 3, we verify Property 1 by showing that
amnis almost evenly distributed in simulation.
Observe that the output vector omat position m
where denotes the concatenation of vectors from
allHattention heads. Assume that Property 1
is valid and that Wo Rd dhas elements i.i.d
sampled from N(0, σ2), we derive the distribution
Lemma 3. omhas zero mean andd2σ4
2This approximation was also used in Xiong et al. (2020)
except that they made a stronger assumption that WqandWk
have to be initialized as zero matrices.0 1 2 3 4 5 6
05Log VarianceSimulation@ 0.2
Figure 5  We vary the value of σand show its effect at the 0thlayer. As we can see, a smaller value of σbrings
Lemma 3 into alignment with the corresponding simulation more closely. Note that the two lines overlap completely
when σ  0.002. Average of 500 samples.
Figure 4 is a simulation that verifies Lemma 3 under
 the assumption of Property 1. We can see
thatthe variance of omalready encodes the positional
3.3 Residual Connection
As denoted by the Addition block of Figure 1, the
residual connection sets the output as ym xm 
om. It allows the model to pass the first MHA
output to later MHA modules as well as the final
classifier. As the positional information has been
passed by the residual connection, we omit the FFN
part in our analysis.
3.4 The Final Layer Normalization
Layer normalization is an operation that might
eliminate the positional information derived in
Lemma 3, which happens before the MHA modules
 and position classifier. As mentioned in  3.1,
E[ymi]   0,V[ymi]  V[xmi]  V[omi]
Lemma 4. The variance of the j-th dimension of
mσ2 d2σ4,where Wo,j  R1 dis the j-th row of Wo.
Wv, i Rd 1is the i-th column of Wv. As long
i(Wo,j Wv, i)2  d2σ4, the classifier should
be able to exploit the discrepancy to derive m.
Readers might wonder why Wo,j andWv, iin
the numerator cannot be treated as random variables.
 The reason is that we only focus on one
dimension ( j-th) at a time. This means we cannot
use the law of large numbers to approximate the
sample variance of ymjas we did for the denominator.
3.5 Relaxing the Assumptions
We discuss possible relaxation of the assumptions
What if Property 1 does not hold  Or equivalently,
d2. This prompts us to vary the
value of σ. In Figure 5, we see that smaller σbetter
 aligns Lemma 3 with the simulations, which is
unsurprising as Lemma 3 assumes small σ. Even
when σis not too small (i.e., σ  0.2,0.02), the
variance still encodes the positional information as
the variance of omis negatively correlated with its
Other Initialization Schemes So far we assume
the weight matrices ( Wq,Wk,Wv,Wo) are initialized
 i.i.d from N(0, σ2). However, we can relax
the assumption to i.i.d. samples from a distribution
with zero mean and finite variance. This is because
the proof in Appendix A calculates the covariance.
The variance calculation relies on E[rir 
iis the i-th row vector of a weight matrix.This property holds for any distribution with zero
Why are the positions of later tokens in a sequence
 harder to be predicted in Figure 3 of Haviv
 et al. (2022)  Lemma 3 states the variance
is inversely proportional to the position m, so the
variance of later tokens (large m) plateaus, resulting
 in a harder numerical optimization problem.
This also suggests a potential downside of removing
 positional embeddings  It might be challenging
for the model to infer positional information of the
later tokens in extremely long input sequences.
Why do lower layers (closer to input) give worse
probing performances in both Figure 2 and Haviv
 et al. (2022)  This can be explained by Figure
 4. Most of the positions at the 0thlayer have
tiny variance ( exp( 10)   4 .5e 5), which again
poses a difficult numerical optimization problem.
Why does BERT fail to converge without positional
 embeddings  In a BERT model (Devlin
et al., 2019), each token has access to all the other
tokens, making the variance at all positionsd2σ4
Therefore, a BERT model cannot utilize variance
differences as its positional indicator.
5 Post-Training Results
Our derivations only apply to the initial stage where
the TLM and input embeddings are randomly initialized,
 which may not hold true after gradient updates.
 It is essential to verify the existence of variance
 properties and lemmas on a fully pre-trained
TLM on OpenWebText2 (details in Appendix C).
We expect that the properties of lower layers of
a pre-trained TLM should align more closely with
the theoretical results for two reasons  1) There
are more steps between the lower layers and the final
 language modeling loss, resulting in smaller
gradients and thereby fewer parameter updates,
and 2) Lower layers typically encode more lowlevel
 information dependent on positional information
 (Vuli  c et al., 2020  de Vries et al., 2020).
Figures 6 and 7 demonstrate that the 0th(lowest)
layer exhibits highly similar cumulative attention
probability and decay-with-position variance as the
theoretical results. In contrast, higher layers deviate
from the analyses in   3. We posit that the model
learns to rely more heavily on semantic rather than
positional information. This also explains why
0 100 200 300 400 500
Positions0.20.40.60.81.0Cumulative Attention ProbabilityLayer 0
Layer 11Figure 6  We plot the positions w.r.t their cumulative
attention probability for L  512 of a pre-trained TLM.
We average over all heads in a layer and 500 samples.
0123Log VarianceLayer 0
Figure 7  We plot the log positions w.r.t their log variance
 for L  512 of a pre-trained TLM. We average
predicting positions using outputs of higher transformer
 layers is more challenging as demonstrated
in Figure 2 of Haviv et al. (2022).
We mathematically analyzed a randomly initialized
transformer language model without positional embeddings.
 We showed that the variance of the selfattention
 output decreases as the position increases,
which serves as an indicator for positional information.
 We validated that, after extensive gradient
updates, the low layers of a pretrained language
model still exhibit highly similar variance reduction
behaviors. Our results pave the way for the pretraining
 of more efficient and positional embedding-free
transformer language models.Limitations
The limitations of this work mostly come from
our assumptions  1) A randomly initialized and
frozen TLM, and 2) Input tokens are all different
and randomly sampled. These two assumptions
obviously do not hold true for human languages
and pre-trained TLMs. Therefore, we attempted
to empirically verify the existence of lemmas and
properties on a pre-trained TLM without positional
That being said, several methods could be attempted
 to remove these assumptions. Firstly, we
can analyze the training dynamics of a TLM to shed
light on the model parameter distribution after pretraining.
 Secondly, Zipf s law or a simple n-gram
language model could be used to quantify the degree
 of input token duplication in human languages.
This might give us a more accurate estimate of the
variance at different positions. We leave these ideas
Our work provides a deeper understanding of why
a transformer language model can still perform
well without positional embeddings, potentially
enabling the application of developing future transformers
 that are greener and more cost-efficient.
Inappropriate usage of our technique might have
negative societal impacts though. These include
the ethical challenges of improper text generation
and privacy issues inherent in the data collection
process. These implications apply to any natural
language processing research and are not unique to
The authors acknowledge the support from Boeing
(2019-STU-PA-259), Amazon (CC ADV 00474341
2021 TR), NSF MRI Award 1919452, and Princeton
Alex Andonian, Quentin Anthony, Stella Biderman, Sid
Black, Preetham Gali, Leo Gao, Eric Hallahan, Josh
Levy-Kramer, Connor Leahy, Lucas Nestler, Kip
Parker, Michael Pieler, Shivanshu Purohit, Tri Songz,
Wang Phil, and Samuel Weinbach. 2021. GPT-NeoX 
Large Scale Autoregressive Language Modeling in
Satwik Bhattamishra, Arkil Patel, and Navin Goyal.
2020. On the computational power of transformersand its implications in sequence modeling. In Proceedings
 of the 24th Conference on Computational
Natural Language Learning , pages 455 475, Online.
Association for Computational Linguistics.
Wietse de Vries, Andreas van Cranenburgh, and Malvina
 Nissim. 2020. What s so special about BERT s
layers  a closer look at the NLP pipeline in monolingual
 and multilingual models. In Findings of the
Association for Computational Linguistics  EMNLP
2020 , pages 4339 4350, Online. Association for
Computational Linguistics.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2019. BERT  Pre-training of
deep bidirectional transformers for language understanding.
 In Proceedings of the 2019 Conference of
the North American Chapter of the Association for
Computational Linguistics  Human Language Technologies,
 Volume 1 (Long and Short Papers) , pages
4171 4186, Minneapolis, Minnesota. Association for
Computational Linguistics.
Leo Gao, Stella Biderman, Sid Black, Laurence Golding,
 Travis Hoppe, Charles Foster, Jason Phang,
Horace He, Anish Thite, Noa Nabeshima, Shawn
Presser, and Connor Leahy. 2020. The Pile  An
800gb dataset of diverse text for language modeling.
arXiv preprint arXiv 2101.00027 .
Adi Haviv, Ori Ram, Ofir Press, Peter Izsak, and Omer
Levy. 2022. Transformer language models without
positional encodings still learn positional information.
arXiv preprint arXiv 2203.16634 .
Kazuki Irie, Albert Zeyer, Ralf Schlüter, and Hermann
Ney. 2019. Language modeling with deep transformers.
M I Jordan. 1986. Serial order  a parallel distributed processing
 approach. technical report, june 1985-march
Diederik P. Kingma and Jimmy Ba. 2014. Adam 
A method for stochastic optimization. Cite
arxiv 1412.6980Comment  Published as a conference
 paper at the 3rd International Conference for
Learning Representations, San Diego, 2015.
Juho Lee, Yoonho Lee, Jungtaek Kim, Adam Kosiorek,
 Seungjin Choi, and Yee Whye Teh. 2019.
Set transformer  A framework for attention-based
permutation-invariant neural networks. In Proceedings
 of the 36th International Conference on Machine
 Learning , volume 97 of Proceedings of Machine
 Learning Research , pages 3744 3753. PMLR.
Ziyang Luo, Artur Kulmizev, and Xiaoxi Mao. 2021.
Positional artefacts propagate through masked language
 model embeddings. In Proceedings of the 59th
Annual Meeting of the Association for Computational
Linguistics and the 11th International Joint Conference
 on Natural Language Processing (Volume 1 
Long Papers) , pages 5312 5327, Online. Association
for Computational Linguistics.Adam Paszke, Sam Gross, Francisco Massa, Adam
Lerer, James Bradbury, Gregory Chanan, Trevor
Killeen, Zeming Lin, Natalia Gimelshein, Luca
Antiga, Alban Desmaison, Andreas Kopf, Edward
Yang, Zachary DeVito, Martin Raison, Alykhan Tejani,
 Sasank Chilamkurthy, Benoit Steiner, Lu Fang,
Junjie Bai, and Soumith Chintala. 2019. Pytorch 
An imperative style, high-performance deep learning
library. In Advances in Neural Information Processing
 Systems 32 , pages 8024 8035. Curran Associates,
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, Ilya Sutskever, et al. 2019. Language
models are unsupervised multitask learners. OpenAI
David E. Rumelhart and James L. McClelland. 1987.
Learning Internal Representations by Error Propagation
Teven Le Scao, Thomas Wang, Daniel Hesslow, Lucile
Saulnier, Stas Bekman, M Saiful Bari, Stella Biderman,
 Hady Elsahar, Jason Phang, Ofir Press, Colin
Raffel, Victor Sanh, Sheng Shen, Lintang Sutawika,
Jaesung Tae, Zheng Xin Yong, Julien Launay, and
Iz Beltagy. 2022. What language model to train if
you have one million GPU hours  In Challenges &
Perspectives in Creating Large Language Models .
Koustuv Sinha, Robin Jia, Dieuwke Hupkes, Joelle
Pineau, Adina Williams, and Douwe Kiela. 2021.
Masked language modeling and the distributional hypothesis 
 Order word matters pre-training for little.
InProceedings of the 2021 Conference on Empirical
 Methods in Natural Language Processing , pages
2888 2913, Online and Punta Cana, Dominican Republic.
 Association for Computational Linguistics.
Yao-Hung Hubert Tsai, Shaojie Bai, Makoto Yamada,
Louis-Philippe Morency, and Ruslan Salakhutdinov.
2019. Transformer dissection  An unified understanding
 for transformer s attention via the lens of
kernel. In Proceedings of the 2019 Conference on
Empirical Methods in Natural Language Processing
and the 9th International Joint Conference on Natural
 Language Processing (EMNLP-IJCNLP) , pages
4344 4353, Hong Kong, China. Association for Computational
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob
Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz
Kaiser, and Illia Polosukhin. 2017. Attention is all
you need. In Advances in Neural Information Processing
 Systems , pages 5998 6008.
Ivan Vuli  c, Edoardo Maria Ponti, Robert Litschko,
Goran Glavaš, and Anna Korhonen. 2020. Probing
pretrained language models for lexical semantics. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 7222 7240, Online. Association for Computational
Ruibin Xiong, Yunchang Yang, Di He, Kai Zheng,
Shuxin Zheng, Chen Xing, Huishuai Zhang, YanyanLan, Liwei Wang, and Tie-Yan Liu. 2020. On layer
normalization in the transformer architecture. In International
 Conference on Machine Learning .
Baosong Yang, Longyue Wang, Derek F. Wong, Lidia S.
Chao, and Zhaopeng Tu. 2019. Assessing the ability
of self-attention networks to learn word order. In
Proceedings of the 57th Annual Meeting of the Association
 for Computational Linguistics , pages 3635 
3644, Florence, Italy. Association for Computational
The proof of Lemma 1 and 2 are head-dependent
while that of Lemma 3 is head-independent. For
notation simplicity, at Lemma 1 and 2, we drop
the head dependency on matrices ( W(h)
m), and scalars ( l(h)
Proof of Lemma 1 Here, we use r 
thei-th row vector of Wv.
Tr(( 1i jσ2) Id  1m n Id) d
( )holds because riandrjare independent when
i  j(similarly for emanden) and the covariance
of a Gaussian random vector is an identity matrix.
IdandId/H denote d dandd
Proof of Lemma 2 Here, we use r 
thei-th row vector of WqandWk.
Proof of Lemma 3 Because Wo Rd dis applied
 on a concatenation of vectors at all heads, we
i.vihere is head-independent
while viat Lemma 1 is head-dependent. Here, we
ito denote the i-th row vector of Wo.
Tr(( 1k lσ2) I ( 1i jdσ2) I) d
( )follows from Lemma 1  because
j)   ( 1i jdσ2) Id/H, a concatenation
 for all h HgivesE[viv 
B Probing Experiment Details
We train a randomly initialized and frozen TLM
with 12layers, d  768 ,H  12 ,L  512 , and
σ  0.02. We use the Adam optimizer (Kingma
and Ba, 2014) with learning rate 1e 3and5000
gradient updates. The batch size is set to 32. We
implement our model using PyTorch (Paszke et al.,
2019).# Layers Hidden Size # Attention Heads Train Seq. Len. # Trainable Params.
Optimizer Batch Size Train Steps Precision Dataset
Adam (lr 6e-4) 32 50,000 bfloat16 OpenWebText2
Table 1  Pre-trained Model Configurations.
C Pre-trained Transformer Language
We use the gpt-neox library (Andonian et al., 2021)
to train a TLM with no positional embeddings. Detailed
 hyperparameters are listed in Table 1. The
pretraining takes 5 hours on one NVIDIA A10040GB.
D Scientific Artifacts
We use the gpt-neox library (Andonian et al., 2021)
under Apache-2.0 license. OpenWebText2 (Gao
et al., 2020) is released by the authors of gpt-neox.
The codebase and dataset are publicly released for
research purposes. The steps taken to protect privacy
 and anonymization are discussed in Section
6 and 7 of Gao et al. (2020). The distribution
and statistics of OpenWebext2 are also discussed
in Gao et al. (2020).