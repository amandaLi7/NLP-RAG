arXiv 2302.06448 1  [cs.CL]  13 2023Joint Span Segmentation and Rhetorical Role
Labeling with Data Augmentation for Legal
Documents
Santosh T.Y.S.S, Philipp Bock, and Matthias Grabmair
School of Computation, Information, and Technology 
Technical University of Munich, Germany
{santosh.tokala, philipp.bock, matthias.grabmair }@tum.de
Abstract. Segmentation and Rhetorical Role Labeling of legal judgements
 play a crucial role in retrieval and adjacent tasks, in cluding case
summarization, semanticsearch,argumentminingetc.Prev iousapproaches
have formulated this task either as independentclassiﬁcat ion or sequence
labeling of sentences. In this work, we reformulate the task at span level
as identifyingspansof multipleconsecutivesentences tha tshare thesame
rhetorical role label to be assigned via classiﬁcation. We e mploy semiMarkov
 Conditional Random Fields (CRF) to jointly learn spa n segmentation
 and span label assignment. We further explore three d ata augmentation
 strategies to mitigate the data scarcity in the speci alized domain
of law where individual documents tend to be very long and ann otation
cost is high. Our experiments demonstrate improvement of sp an-level
prediction metrics with a semi-Markov CRF model over a CRF ba seline.
This beneﬁt is contingent on the presence of multi sentence s pans in the
document.
Keywords  Rhetorical Role Labeling  semi-Markov CRF  Data Augmentation

1 Introduction
Rhetorical Role Labeling (RRL) of legal documents involves segment ing a document
 into semantically coherent chunks and assigning a label to the chunk
that reﬂects its function in the legal discourse (e.g., preamble, fac t, evidence,
reasoning). RRL for long legal case documents is a precursor task to several
downstream tasks, such as case summarization [9,22,12,5] , fact-b ased semantic
case search [21], argument mining [25] and judgement prediction [12 ].
Prior works in RRL on legal judgements have regarded the task eith er as
straightforward classiﬁcation of sentences without modeling any c ontextual dependency
 between them [1,25] or as sequence labeling [27,3,8,12]. Init ial works
[22,5,9] performed RRL using hand-crafted features as part of a s ummarization
 pipeline. Savelka et al. [24] employed a CRF on hand-crafted feat ures to
segment US court decisions into functional and issue speciﬁc parts . Similarly,
Walker et al. [25] used engineered features for RRL on US Board of V eterans 2 Santosh et al.
Appeals (BVA) decisions. With the rise of deep learning, Yamada et al. [27] ,
Ghosh et al. [8], Paheli et al. [3] and Ahmad et al. [1] employed deep learn ing
based BiLSTM-CRF models for RRL on Japanese civil rights judgemen ts, Indian
 Supreme Court opinions, UK supreme court judgements and th e US BVA
corpus respectively. More recently, Kalamkar et al. [12] benchmar ked RRL on
Indian legal documents using a Hierarchical Sequential Labeling Net work model
(HSLN). The corpus they used claims to be the largest available corp us of legal
documents annotated with rhetorical sentence roles.
In this work we approach RRL on legal documents with the observat ion that
the texts of judgement are not only very long, but also often cont ain large sections
 of the same sentence type (e.g. explanations of case facts) . We hence build
models that segment the document into thematically coherent sets of contiguous
 sequence of sentences (which we refer to as spans) and assign them labels.
We also hypothesize that modeling documents at this span level can a lso help
to capture certain types of contexts eﬀectively that may be spre ad across long
sequences of sentences that can be collapsed into a much smaller nu mber of thematically
 coherent spans. For example, when case documents are t o be retrieved
according to certain types of information, then aggregating that content from a
small number of topical blocks across a long document is intuitive. At the same
type, we explore how this assumption of topical continuity in the law c an help
RRL models learn better from small amounts of training data.
Totacklethisproblemassequentialspanclassiﬁcation,weapplysem i-Markov
Conditional Random Field (CRF) [23], which have been proposed to join tly handle
 span segmentation and labeling. Semi-Markov CRFs have been use d in various
 tasks such as Chinese word segmentation [17,16], named entity r ecognition
[31,32,2], character-level parts of speech labelling [13], phone recog nition [19],
chord recognition [20], biomedical abstract segmentation [28] and piano transcription
 [29]. Most previous works dealt with shorter input sequenc es and thus
contained smaller span lengths, which allows for a convenient upper b ound on
the maximum length of a span. In this work, we assess the performa nce of semiMarkov
 CRFs on legal judgements, which are usually very long and als o possess
a potentially large range of labels, making this setup even more challen ging.
Obtaining suﬃciently large amounts of annotated data for deep lear ning
models in specialized domains like the law is very expensive as it requires e xpert
 annotators. To mitigate this data scarcity, we explore three strategies of
data augmentation (DA) such as random deletion of words, back tr anslation
and swapping of sentences within a span. DA techniques which are co mmon
in computer vision ﬁeld, has witnessed growing interest in NLP tasks d ue to
the twin challenge of large annotated data for neural networks an d expensive
data annotation in low-resource domains [6]. In sum, this paper cont ributes the
casting RRL of legal judgments as a sequential span classiﬁcation t ask and associated
 experiments with semi-MarkovCRFs on existing public dataset s. We also
explore three data augmentation strategies to assess their impac t on the task.
Our experiments demonstrate that our semi-Markov CRF model pe rforms bet-Rhetorical Role Labeling for Legal Documents 3
ter compared to a CRF baseline on documents characterized by mult i-sentence
spans.1 2 Method
Our hierarchical semi-Markov CRF model takes the judgement doc umentx 
{x1,x2,...,x m}as input, where xi {xi1,xi2,...,x in}and outputs the rhetorical
 role label sequence l {l1,l2,...,lm}withli L.xiandxjpdenoteith
sentence and pthtoken of jthsentence, respectively. mandndenote the number
of sentences and tokens in the ithsentence respectively. liis the rhetorical role
corresponding to sentence xiandLdenotes set of pre-deﬁned rhetorical role
labels.
2.1 Hierarchical semi-Markov CRF model
Our model contains a semi-Markov CRF component [23] built on top o f a Hierarchical
 Sequential Labeling Network model [11] with the following lay ers 
Encoding layers  Following [12], we encode each sentence with BERT-BASE
[14]toobtaintokenlevelrepresentations zi {zi1,zi2,...,z in}.Thesearepassed
through a Bi-LSTM layer [10] followed by an attention pooling layer [30] to
obtain sentence representations s {s1,s2,...,s m}.
uit  tanh(Wwzit bw) &αit exp(uituw) 
sexp(uisuw)&si n 
t 1αituit(1)
whereWw,bw,uware trainable parameters.
Context enrichment layer  Thesentencerepresentations sarepassedthrough
aBi-LSTMtoobtaincontextualizedsentencerepresentations c {c1,c2,...,c m},
which encode contextual information from surrounding sentence s.
Classiﬁcation layer  A semi-Markov CRF takes the sequence of sentence representations
 cand segments it into labeled spans k {k1,...,k s }withkj 
(aj,bj,yj) whereajandbjare the starting and ending position of the sentences
in thejthspan, and yjis the corresponding rhetorical role label of the jthspan.
 s denotes the total number of spans where  s 
l 1(bj aj 1)  m.
We model the conditional probability through a semi-MarkovCRF whic h jointly
tackles the span segmentation and label assignment for a span as f ollows 
p(y c)  1
Z(c)exp( s  
j 1F(kj,c) A(yj 1,yj)) (2)
whereZ(c)   
k  Kexp( 
jF(k 
j,c) A(yj 1,yj)) (3)
whereF(kj,c) is the scoreassignedfor span kj(i.e., for interval [ aj,bj] belonging
to labelyjbased on span input c) andA(yj 1,yj) is the transition score of the
1Our code is available at https //github.com/TUMLegalTech /Span-RRL-ECIR234 Santosh et al.
labels of two adjacent spans. Z(c) denotes the normalization factor computed
as the sum over the set of all possible spans Kagainstc. The score F(kj,c) is
computed using a learnable weight and bias matrix.
F(kj,c)  WT.f(kj,c) b (4)
where W and b denote trainable parameters and f(kj,c) represents span representation
 of jthspan derived from c.
To obtain the span representations f(kj,c), we pass the sentence-level representations
 cfor the sentences in the given span kjthrough a BiLSTM layer
initially to capture the context of the span. Then we obtain the span representationf(kj,c)
 as the concatenation of the ﬁrst two and ﬁnal two sentences ve ctors,
and the mean of the sentences in the span. In case of shorter spa ns, we repeat
the same sentence to match the dimension.
We maximize the above deﬁned conditional log-likelihood to estimate th e
parameters and train the model end-to-end. We perform inferen ce using the
Viterbi decoding algorithm [7] to obtain the best possible span seque nce along
with its label assignment. These computations are done in logarithmic space to
avoid numerical instability. In traditional semi-Markov CRF which are applied
to relatively shorter sequences in the previous works, the assump tion is that
that there exists no transition between the same rhetorical labels . However, due
to the long input data and a larger range of potential label spans, w e relax
this assumption as we can deal with a certain maximum span length due to
computational constraints as it involves quadratic complexity.
2.2 Data Augmentation
The main goal of Data Augmentation in low resource settings is to incr ease the
diversity of training data which in turn helps the model to generalize b etter on
test data. In this regard, we implement the following three Data Aug mentation
techniques as preliminary analysis and leave the exploration of more a dvanced
techniques as a future work.
Word deletion [26]isanoisebasedmethodthatdeleteswordswithinasentence
at random. The augmented data diﬀers from the original without aﬀ ecting the
rhetoricalroleofthe sentenceasthe rhetoricalroleofthe sent ence canbe derived
from the other words present in the sentence. This helps the mode l to derive
better contextual understanding of the sentence rather than relying on wordlevel
 surface features.
Inback-translation [18], we translate the original text at sentence level into
other languages and then back to the original language to obtain au gmented
data. Unlike word level methods, this method does not not directly d eal with
individual words but rewrites the whole sentence. This makes the mo del robust
to any writing style based spuriously correlated features and learn the semantic
information conveyed by the text.
Sentence swapping [4] is based on the notion that a minor change in order of
sentences is still readable for humans. We restrict swapping of sen tences to thoseRhetorical Role Labeling for Legal Documents 5
within a single span, which preserves the overall discourse ﬂow of th e document.
While somediscontinuities will be introduced,the text remainsconten t complete
and rhetorical roles do not change. This helps the model to learn th e discourse
ﬂow of the document and makes the model overcome the limitation of having
transition between same spans as described in the previous sub-se ction.
3 Experiments & Discussion
Datasets   We experiment on two datasets - (i) BUILDNyAI dataset [12] consisting
 of judgement documents from the Indian supreme court, h igh court and
district courts. It consists of publicly available train and validation sp lits with
184 30 documents, respectively, annotated with 12 diﬀerent r hetorical role
labels along with  None . As test dataset is not publicly available, we split a nd
use training dataset for both training and validation and test it on th e validation
partition  (ii) the BVA PTSD dataset [25] consists of 25 decisions by the U.S.
Board of Veterans  Appeals (BVA) from appealed disability claims by v eterans
for service-related post-traumatic stress disorder (PTSD). We use 19 documents
for training and validation, and 6 as test. They are annotated with 5 rhetorical
roles along with  None .
Baselines   We compare our method, HSLN-spanCRF DA (data augmentation)against
 the following variants   HSLN-CRF (normal CRF, no DA), HSLNspanCRF
 (spanCRF, no DA) and HSLN-CRF DA (normal CRF with DA).
Metrics   We use both span-macro-F1 and span-micro-F1, which is computed
based on match of span-by-span labels3(i.e., it encompasses both segmentation
into exact spans as well their labeling). We also report span-segmen tation-F1
which only evaluates on segmentation of spans ignoring the label. We f urther
evaluate at the sentence level using micro-F1 and macro-F1 followin g previous
works [12].
Implementation Details   We use the hyperparameters of [12] for the HSLN
model. For the semi-Markov CRF, we obtain the the maximum segment length
using validation set and set it to 30 4 for BUILDNyAI and BVA data sets
respectively. We used a batch size of 1 and trained our model end-t o-end using
Adam [15] optimizer with a learning rate of 1e-5. For data augmentat ion, we
employed a maximum word deletion rate of 20%. For back-translation , we used
English, German and Spanish as the sequence of languages. We augm ented the
datasetonce using eachDAtechnique and thus models with DAcompo nent were
trained with four times the size of training dataset.
2 75 decisions, out of which onl y 25 3We post-process and merge the same consecutive labels to obt ain the span labels.6 Santosh et al.
Table 1  Model performance on BUILDNyAI and BVA datasets
BUILDNyAI BVA PTSD
Span Sentence Span Sentence
Model s-mic.s-mac.s-segmic.mac.s-mic.s-mac.s-segmic.mac.
CRF 0.310.280.330.800.600.670.580.710.810.74 0.380.350.390.760.560.670.560.690.780.72 0.320.320.340.820.630.720.640.750.850.81 0.400.360.410.810.580.730.650.750.830.80 1 reports the performance of our model and
its variants on the two datasets. On BUILDNyAI, we observe that spanCRF
performs better compared to a normal CRF in span-level metrics ( statistically
signiﬁcant (p  0.05) using McNemar Test), with a drop at the sentence-level.
With the addition of data augmentation (DA), both CRF and spanCRF performance
 improves. However, the increase is larger for spanCRF s sentence level
metrics (statistically signiﬁcant (p  0.05) using McNemar Test). This can be
attributed to spanCRF having to compute the optimal segmentatio n path over
all the possible paths, which requires enough data to learn and gene ralize better.
 On the other hand, on the BVA PTSD dataset, spanCRF did not show a
signiﬁcant impromavement compared to normal CRF. This is because 73.8% of
the spans in BVA dataset (BUILDNyAI  31%) have length 1 1.85 (BUILDNyAI  6.81) which does not allow spanCRF to show its
potential. However, the trend towards a beneﬁcial eﬀect of data augmentation
persists.
Eﬀect of Maximum Span Length   We create variants of spanCRF by varying
 the maximum span length. First section in Table 2 shows that increa sing the
span length improved the performance on span-level metrics with a marginal
drop at the sentence-level. We choose 30 as the maximum span lengt h due to
the computational resource constraints and our very long judgm ent documents.
Eﬀect of Span representation   We experiment with various span representations
 such as grConv[13] (Gated Recursive Convolutional Neural Networks),
simple[28] involving concatenation of ﬁrst and last sentence representa tion in
span. We also create a variant of our proposed span representat ion by removing
the BiLSTM ( ours w/o BiLSTM ). From second section in Table 2, we observe a
performance drop without the BiLSTM layer (both at span- and sen tence-level)
indicating the importance of capturing context speciﬁcally at the sp an level to
obtain good representations. We notice less improvement with grConv, which
can also be attributed to its high number of parameters for our low d ata condition.
 Though simpleachieves an improvement in span-level metrics, it shows a
huge drop in sentence-level performance.Rhetorical Role Labeling for Legal Documents 7 2  First and second section indicates the eﬀect of max span len gth (w/o
DA) and diﬀerent span feature representations (w/o DA) on BUIL DNyAI
Span Sentence
Model s-mic.s-mac.s-segmic.mac.
CRF (len   1) 0.310.280.330.800.60
spanCRF (len 5) 0.330.300.340.680.45
spanCRF (len 10) 0.340.320.360.710.48
spanCRF (len 20) 0.360.330.370.730.52
spanCRF (len 30) 0.380.350.390.760.56
CRF (no span) 0.310.280.330.800.60
Span CRF (ours) 0.380.350.390.760.56
Span CRF (ours w/o BiLSTM) 0.360.320.370.750.55
Span CRF (grConv) 0.320.300.340.740.51
Span CRF (simple) 0.340.330.360.720.52
Ablation on Data Augmentation Strategies   We observethe eﬀect of each
data augmentation strategy in isolation. From Table 3, we observe t hat, in the
case of CRF, each of the augmentation strategies boosted perfo rmance at the
sentence-level by a considerable margin. With all three augmentat ion strategies
combined, CRF witnessed a considerable jump, indicating the complem entarity
between the strategies. Similarly, we observe an improvement with e ach data
augmentation strategy in case of spanCRF, and the greatest incr easewhen using
all three strategies combined.
Table 3  Diﬀerent data augmentations on CRF and spanCRF on BUILD NyAI
Span Sentence
s-mic. s-mac. s-seg mic. mac.
Model CRFsp.CRF CRFsp.CRF CRFsp.CRF CRFsp.CRF CRFsp.CRF
No Augmentation 0.310.380.280.350.330.390.800.760.600.56 0.320.390.300.360.340.400.820.800.620.58 0.320.390.300.360.340.400.810.780.610.58 0.320.400.310.360.340.400.810.770.620.57 0.320.400.320.360.340.410.820.810.630.58 4 Conclusion
Our experiments demonstrate that while semi-Markov CRFs help to b oost the
predictions at the span level, data augmentation strategies can mit igate data
scarcity and improve the performance both at sentence- and spa n-levels, albeit
conditioned on the documents exhibiting patterns of longer passag es of the same
rhetorical type. While this is typical for legal judgments, it is not un iversal. In
the future, we hence would like to combine the complimentary senten ce- and
span-level methods. We would also like to explore diﬀerent data augm entation
strategies to alleviate the bottle neck of limited annotated data and expensive
data annotation, especially in these specialized domains.8 Santosh et al.
References
1. Ahmad, S.R., Harris, D., Sahibzada, I.  Understanding le gal documents  classiﬁcation
 of rhetorical role of sentences using deep learning and natural language
processing. In  2020 14th International Conference on Semantic Computing
(ICSC). pp. 464 467. IEEE (2020)
2. Arora, R., Tsai, C.T., Tsereteli, K., Kambadur, P., Yang, Y.  A semi-markov structured
 support vector machine model for high-precision name d entity recognition.
In  Proceedings of the 57th Annual Meeting of the Associatio n for Computational
Linguistics. pp. 5862 5866 (2019)
3. Bhattacharya, P., Paul, S., Ghosh, K., Ghosh, S., Wyner, A .  Deeprhole  deep
learning for rhetorical role labeling of sentences in legal case documents. Artiﬁcial
Intelligence and Law pp. 1 38 (2021)
4. Dai, X., Adel, H.  An analysis of simple data augmentation for named entity recognition.
 In  Proceedings of the 28th International Conferen ce on Computational
Linguistics. pp. 3861 3867 (2020)
5. Farzindar, A., Lapalme, G.  Letsum, an automatic legal te xt summarizing. In 
Legal knowledge and information systems  JURIX 2004, the se venteenth annual
conference. vol. 120, p. 11. IOS Press (2004)
6. Feng, S.Y., Gangal, V., Wei, J., Chandar, S., Vosoughi, S. , Mitamura, T., Hovy, E. 
A survey of data augmentation approaches for nlp. In  Findin gs of the Association
for Computational Linguistics  ACL-IJCNLP 2021. pp. 968 9 88 (2021)
7. Forney, G.D.  The viterbi algorithm. Proceedings of the I EEE61(3), 268 278
(1973)
8. Ghosh, S., Wyner, A.  Identiﬁcation of rhetorical roles o f sentences in indian legal
 judgments. In  Legal Knowledge and Information Systems   JURIX 2019  The
Thirty-second Annual Conference. vol. 322, p. 3. IOS Press ( 2019)
9. Hachey, B., Grover, C.  Extractive summarisation of lega l texts. Artiﬁcial Intelligence
 and Law 14(4), 305 345 (2006)
10. Hochreiter, S., Schmidhuber, J.  Long short-term memor y. Neural computation
9(8), 1735 1780 (1997)
11. Jin, D., Szolovits, P.  Hierarchical neural networks fo r sequential sentence classiﬁcation
 in medical scientiﬁc abstracts. In  Proceedings of the 2018 Conference on
Empirical Methods in Natural Language Processing. pp. 3100 3109 (2018)
12. Kalamkar, P., Tiwari, A., Agarwal, A., Karn, S.M., Gupta , S., Raghavan, V., Modi,
A.  Corpus for automatic structuring of legal documents. In   LREC (2022)
13. Kemos, A., Adel, H.  Neural semi-markov conditional ran dom ﬁelds for robust
character-based part-of-speech tagging. In Proceedings ofNAACL-HLT.pp.2736 2743 (2019)
14. Kenton, J.D.M.W.C., Toutanova, L.K.  Bert  Pre-traini ng of deep bidirectional
transformers for language understanding. In  Proceedings of NAACL-HLT. pp.
4171 4186 (2019)
15. Kingma, D.P., Ba, J.  Adam  A method for stochastic optim ization. In  Bengio,
Y., LeCun, Y. (eds.) 3rd International Conference on Learni ng Representations,
ICLR 2015, San Diego, CA, USA, May 7-9, 2015, Conference Trac k Proceedings
(2015)
16. Kong, L., Dyer, C., Smith, N.A.  Segmental recurrent neu ral networks. In  Bengio,
Y., LeCun, Y. (eds.) 4th International Conference on Learni ng Representations,
ICLR 2016, San Juan, Puerto Rico, May 2-4, 2016, Conference T rack Proceedings
(2016)Rhetorical Role Labeling for Legal Documents 9 17. Liu, Y., Che, W., Guo, J., Qin, B., Liu, T.  Exploring segm ent representations
for neural segmentation models. In  Proceedings of the Twen ty-Fifth International
Joint Conference on Artiﬁcial Intelligence. pp. 2880 2886 (2016)
18. Lowell, D., Howard, B., Lipton, Z.C., Wallace, B.C.  Uns upervised data augmentation
 with naive augmentation and without unlabeled data. In  Proceedings of
the 2021 Conference on Empirical Methods in Natural Languag e Processing. pp.
4992 5001 (2021)
19. Lu, L., Kong, L., Dyer, C., Smith, N.A., Renals, S.  Segme ntal recurrent neural
networks for end-to-end speech recognition. Interspeech 2 016 pp. 385 389 (2016)
20. Masada, K., Bunescu, R.C.  Chord recognition in symboli c music using semimarkov
 conditional random ﬁelds.
21. Nejadgholi, I., Bougueng, R., Witherspoon, S.  A semi-s upervised training method
for semantic search of legal facts in canadian immigration c ases. In  JURIX. pp.
125 134 (2017)
22. Saravanan, M., Ravindran, B., Raman, S.  Automatic iden tiﬁcation of rhetorical
roles using conditional random ﬁelds for legal document sum marization. In  Proceedings
 of the Third International Joint Conference on Nat ural Language Processing 
 Volume-I (2008)
23. Sarawagi, S., Cohen, W.W.  Semi-markovconditional ran domﬁeldsfor information
extraction. Advances in neural information processing sys tems17(2004)
24. Savelka, J., Ashley, K.D.  Segmenting us court decision s into functional and issue
speciﬁc parts. In  JURIX. pp. 111 120 (2018)
25. Walker, V.R., Pillaipakkamnatt, K., Davidson, A.M., Li nares, M., Pesce, D.J. 
Automatic classiﬁcation of rhetorical roles for sentences   Comparing rule-based
scripts with machine learning. In  ASAIL@ ICAIL (2019)
26. Wei,J., Zou,K. Eda Easydataaugmentationtechniques forboostingperformance
on text classiﬁcation tasks. In  Proceedings of the 2019 9th Internat ional Joint Conference
 on Natural Language Processing (EMNLP-IJCNLP). pp. 63 82 6388 (2019)
27. Yamada, H., Teufel, S., Tokunaga, T.  Neural network bas ed rhetorical status classiﬁcation
 for japanese judgment documents. In  Legal Knowl edge and Information
Systems, pp. 133 142. IOS Press (2019)
28. Yamada, K., Hirao, T., Sasano, R., Takeda, K., Nagata, M.   Sequential span classiﬁcation
 with neural semi-markov crfs for biomedical abst racts. In  Findings of
the Association for Computational Linguistics  EMNLP 2020 . pp. 871 877 (2020)
29. Yan, Y., Cwitkowitz, F., Duan, Z.  Skipping the frame-le vel  Event-based piano
transcription with neural semi-crfs. Advances in Neural In formation Processing
Systems 34, 20583 20595 (2021)
30. Yang, Z., Yang, D., Dyer, C., He, X., Smola, A., Hovy, E.  H ierarchical attention
networks for document classiﬁcation. In  Proceedings of th e 2016 conference of the
North American chapter of the association for computationa l linguistics  human
language technologies. pp. 1480 1489 (2016)
31. Ye, Z., Ling, Z.H.  Hybrid semi-markov crf for neural seq uence labeling. In  Proceedings
 of the 56th Annual Meeting of the Association for Co mputational Linguistics
 (Volume 2  Short Papers). pp. 235 240 (2018)
32. Zhuo, J., Cao, Y., Zhu, J., Zhang, B., Nie, Z.  Segment-le vel sequence modeling
usinggated recursive semi-markov conditional random ﬁeld s. In Proceedings ofthe
54th Annual Meeting of the Association for Computational Li nguistics (Volume 1 
Long Papers). pp. 1413 1423 (2016)