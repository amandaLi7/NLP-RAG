Answering Ambiguous Questions with a Database of
Questions, Answers, and Revisions
Haitian Sun1 2William W. Cohen1Ruslan Salakutinov2
Abstract
Many open-domain questions are under-specified
and thus have multiple possible answers, each of
which is correct under a different interpretation
of the question. Answering such ambiguous questions
 is challenging, as it requires retrieving and
then reasoning about diverse information from
multiple passages. We present a new state-ofthe-art
 for answering ambiguous questions that
exploits a database of unambiguous questions generated
 from Wikipedia. On the challenging ASQA
benchmark, which requires generating long-form
answers that summarize the multiple answers to
an ambiguous question, our method improves performance
 by 15% (relative improvement) on recall
 measures and 10% on measures which evaluate
 disambiguating questions from predicted outputs.
 Retrieving from the database of generated
questions also gives large improvements in diverse
 passage retrieval (by matching user questionsqto
 passages pindirectly, via questions q 
generated from p).
1. Introduction
Most previous research in open-domain QA focuses on finding
 the most probable answer to a question (Joshi et al.,
2017  Kwiatkowski et al., 2019). However, many questions
are ambiguous, and hence have multiple possible answers,
each of which is correct under some interpretation of the
question. For example,  Where is the home stadium of the
Michigan Wolverines   has different answers depending
on whether one interprets the question as being about  football 
 or  basketball . Several datasets have been proposed
recently that test models  ability to predict all possible answers
 (Zhang & Choi, 2021  Dhingra et al., 2022  Sun et al.,
2022) and explain the differences between those answers
(Min et al., 2020  Stelmakh et al., 2022).
1Google DeepMind2Carnegie Mellon University. Correspondence
 to  Haitian Sun  haitiansun@google.com  .
Preprint.The tasks of finding all possible answers, and finding all unambiguous
 interpretations of a question, have been shown to
be very challenging for existing models, because these tasks
require retrieving and aggregating diverse information (Min
et al., 2020  Stelmakh et al., 2022). The best-performing
open-domain QA models typically use a retrieve-and-read
pipeline, which first retrieves several passages from a corpus,
 and then applies reading comprehension models to
extract (or generate) answers from the retrieved passages.
However, passages found by retrieval models such as BM25
and DPR (Karpukhin et al., 2020) often lack diversity, i.e.
the top-ranked passages commonly mention the same answers.
 While increasing the number of retrieved passages
can increase recall of answers, it makes the answer prediction
 process more expensive often quadratically more
expensive.1
Rather than retrieving passages, an alternative way to implement
 a retrieve-and-read question-answering system is to
extract information from a corpus, and retrieve the extracted
information. Several recent systems have proposed extracting
 information as question and answer (QA) pairs (Lewis
et al., 2021  Chen et al., 2022a b  Wu et al., 2022). To
answer open-domain questions, models retrieve generated
QA pairs as evidence, rather than retrieving passages. Compared
 to passages, questions easier to retrieve, and are much
shorter (usually 10 to 20 tokens, while passages commonly
contain hundreds of tokens). Therefore, more questions can
be retrieved, to increase the recall of answers.
In this paper, we study the use of QA databases in answering
 ambiguous questions. First, we describe methods for
producing a large set of questions from Wikipedia. In contrast
 to most previous question generation (QG) systems,
our QG model is trained on questions from AmbigQA (Min
et al., 2020), which contains NQ (Kwiatkowski et al., 2019)
questions that have been revised by adding additional information
 from passages to questions to reduce ambiguity. This
results in a very large set of questions, most of which have
only one answer  we generate 127 million questions, with
only 5.8 million that have more than one answer. Second,
1The complexity of a Transformer-based reading comprehension
 model has the complexity of O(N2)whereNis the number
of tokens.arXiv 2308.08661v1  [cs.CL]  16 Aug 2023we show this database of more-specific, better-grounded
generated questions can be used to indirectly retrieve passages
 relevant to a question, by finding passages that generate
 questions similar to a query question x. This retrieval
approach leads to more diverse passages and higher answer
 recall on two benchmark QA datasets with ambiguous
questions, AmbigQA (Min et al., 2020) and WebQuestionsSP
 (Yih et al., 2016). Third, we show that retrieving from
generated questions, and incorporating these generated questions
 as context, leads to improvement on the ASQA task
(Stelmakh et al., 2022), a very challenging task of longform
 question answering for summarizing and comparing
answers of ambiguous questions. Overall we improve the
baselines by 1.9 points in DR and achieve a state-of-the-art
on ASQA.
2. Related Work
In previous work, Lewis et al. (2021) constructed a database
of 67M probably asked questions (PAQ) from Wikipedia
and proposed to answer open-domain questions by retrieving
 similar questions from PAQ. Later work Chen et al.
(2022b)  Wu et al. (2022) proposed alternative approaches
to using databases of QA pairs in QA systems, proposing
 pre-training methods that are more statistically-efficient
(Chen et al., 2022b) or more computationally efficient (Wu
et al., 2022), and developing multi-hop QA models (Chen
et al., 2022b) that retrieve multiple times from a QA memory.
 However, prior QA-memory based models (Lewis
et al., 2021  Chen et al., 2022b  Wu et al., 2022) focused on
the traditional QA setting of unambiguous questions with
unique answers. This leads to many differences in emphasis 
for example, the PAQ dataset purposely removes generated
questions with multiple answers.
Another efficient way to store world knowledge is to build
knowledge bases (KB) or knowledge graphs (KG), such as
Freebase and Wikidata, where information is stored with
entities and relations as triples, e.g. ( Charles Darwin ,
 author of ,  On the Origin of Species ). Knowledge bases
have been commonly used in many knowledge intensive
tasks due to its structured nature (Sun et al., 2018  Min et al.,
2019). Knowledge bases, however, lack the expressiveness
in representing complex relationships that involve multiple
pieces of information, and often do not contain information
in a format that naturally reflects users  questions.
To resolve this problem, Dhingra et al. (2020)  Sun et al.
(2021) proposed to construct virtual knowledge bases that
are not restricted to pre-defined vocabularies of entities and
relations. Dhingra et al. (2020) proposed to store entitycentric
 information as vectors and build a large database
of vectors by iterating through passages in Wikipedia. Sun
et al. (2021) encoded pair-wise relationship between entities
as vectors. Both methods support a similar reasoning pro-cess as regular knowledge bases. Others have argued for use
of entity-linked QA pairs as a formalism for storing knowledge,
 as a representation that is more aligned with users 
information needs, but still are closely related to traditional
AI representations like KBs and KGs (Chen et al., 2022a).
Recent interest in QA for ambiguous questions poses new
challenges for retrieving and representing knowledge. In
such datasets, models are required to not only find one of
the correct answers, but also comply with additional requirements
 associated with the need to choose between multiple
answers. For example, Temp-LAMA Dhingra et al. (2022)
requires models to answer time-sensitive questions under
given time constraints  (Zhang & Choi, 2021) contains questions
 with geographic and temporal constraints  ConditionalQA
 Sun et al. (2022) contains constraints based on user
scenarios  and ROMQA (Zhong et al., 2022) requires QA
subject to different combinations of constraints.
This work builds especially on the AmbigQA dataset (Min
et al., 2020), which contains NQ questions that have multiple
 interpretations, and the ASQA dataset (Stelmakh et al.,
2022). The ASQA dataset contains ambiguous questions
with long-form answers, where ther answers explain in text
what the alternative interpretations of the original question
are, and what the answer is for each interpretation.
3. Method
In this section, we first discuss our approach to constructing
a database of questions from Wikipedia, and then propose
methods which use the generated questions for two important
 tasks in answering ambiguous questions  retrieving
passages with diverse answers, and generating long-form
answers to ambiguous questions.
3.1. Question Generation from Wikipedia
We generate questions and answers from Wikipedia passages.
 The construction process involves three stages  answer
 detection, question generation, and answer verification.
We discuss each stage in detail in this section and compare
each stage to another popular database of questions, PAQ
(Lewis et al., 2021). We name our database of generated
questions SIXPAQ ( Synthesized question Interpretations to
eXtendProbably Asked Questions).
3.1.1. S OURCE
We use the dump of Wikipedia preprocessed by DPR
(Karpukhin et al., 2020) as inputs to generate questions.
In DPR s dump, Wikipedia articles are chunked into passages
 which contain 100 tokens. The preprocessed dump
of Wikipedia contains 21 million passages. Since many of
the question interpretations in ASQA involve less popular
entities, we generate questions from all 21 million passages.In contrast PAQ generates from only 9.1 million passages
(filtered by a learned question selection model).
3.1.2. S TAGE 1  A NSWER DETECTION
A Wikipedia passage usually contains multiple pieces of
information and thus questions can be asked from different
perspectives. We let the question generation process to
be answer-conditioned to reflect this observation. During
generation, possible answers are first detected and then
questions are generated for every detected answer (Lewis
et al., 2021  Chen et al., 2022b).
We model the answer detection step as a sequence-tosequence
 (seq2seq) generation problem with a T5 model
(Raffel et al., 2020).2We do not use a Named Entity Recognition
 (NER) model for answer prediction, as used in PAQ
(Lewis et al., 2021). The input of the generation task is
a Wikipedia passage and the output is a text span that is
likely to be the answer to some questions. The answer detection
 model (with a pretrained T5) is finetuned on NQ
(Kwiatkowski et al., 2019). We use beam search with beam
size of 32 to generate multiple outputs, but filter these outputs
 with two heuristics. First, we require the generated
spans to be sub-strings of the Wikipedia passage. Second,
we merge spans which are identical after removing articles
 and punctuation. We end up with 283 million answers
detected from 21 million Wikipedia passages.
3.1.3. S TAGE 2  Q UESTION GENERATION
Given answers detected from a Wikipedia passage, we then
train a model to generate questions for the specific answers.
Again, we finetune a T5 model for the question generation
task. An input for question generation contains a passage
and a target answer, e.g.   answer   Michigan Stadium
context   The Michigan Stadium is the home stadium ... .
An expected output should first repeat the target answer and
then generate a question, e.g.   answer   Michigan Stadium
question  Where is the home stadium ... . In preliminary
 experiments, encouraging the model to first repeat
the answers generally improves the quality of questions by
making generated questions more specific to target answers.
We use question and answer (QA) pairs from AmbigQA
(Min et al., 2020) to train the question generation task.
Questions in AmbigQA originate from NQ but are revised
by adding additional answer-specific information from
passages to questions to remove ambiguity. This departs
from most prior QG work, which generally trains models on
SQuAD (Rajpurkar et al., 2016) or NQ (Kwiatkowski et al.,
2019). While AmbigQA is smaller than either of these
datasets, the questions are more natural than SQuAD (where
2We use the pretrained T5-11B model for all subtasks in constructing
 SIXPAQ to ensure the high quality of data.questions were formulated by crowdworkers looking at the
passage) and better-grounded than NQ (since questions are
revised by crowdworkers looking at the passaege), which
seems to be a happy medium in producing natural questions
with minimal hallucination(Bandyopadhyay et al., 2022).
We use greedy search at inference time to generate one question
 per answer. While PAQ used beam search (with a beam
size of 4) to increase the number of generated questions
(Lewis et al., 2021), we find that questions generated from
beam search are often very similar to each other. Having
near-duplicate questions makes the database larger but does
not increase the utility of the database for most downstream
tasks.
3.1.4. S TAGE 3  A NSWER VERIFICATION
Questions generated from the previous step are sometimes
invalid   i.e. some questions may not be answerable from
the provided passages, or the correct answers to the generated
 questions are different from the answers from which
the questions are generated. Therefore, an additional answer
verification step is needed.
We train a question answering (QA) model in the reading
comprehension setting to perform the answer verification
task. In particular, the model takes a passage and a generated
 question to predict an answer. If an answer does not
exist in the passage, the model should predict  not answerable .
 We finetune a T5 model on SQuAD v2 (Rajpurkar
et al., 2018), a reading comprehension dataset which contains
 unanswerable questions. During verification, we drop
questions if their predicted answers are  not answerable  or
different from their original answers.3After the verification
step, 156 million questions are left.
The question generation process often produces questions
that are ambiguous in an open-book setting, i.e. they have
multiple answers. This is expected since many NQ questions
 are themselves ambiguous (Min et al., 2020) when
considered carefully. In PAQ, questions that have multiple
open-book answers are filtered by running an open-book
QA system and discarding questions with an open-book
answer different from the one used for generation. This has
several disadvantages  it is expensive, since open-book QA
is expensive to run  it is relatively noisier than our proposed
QA-based filter, since open-book QA is less accurate than
machine-reading style QA  and it filters out more than 76%
of the generated questions, and it is not actually appropriate
for some downstream applications (such as the ones considered
 in  3.3), where questions are used in conjunction with
the passages from which they were generated.
3We normalize the original and predicted answers before comparison
 using scripts provided by Rajpurkar et al. (2016).3.1.5. S TATISTICS
We merge the question and answer pairs by merging pars
with identical questions and end up with 127 million unique
questions, among which 14.3 million questions have more
than one answer mention, and 5.8 million questions have
more than one unique answer.4
3.2. Retrieval of Diverse Passages
One common problem with existing retrieval models
(Karpukhin et al., 2020  Ni et al., 2021) for open-domain
QA is the lack of diversity of the retrieved results (Min et al.,
2021), i.e. only a subset of correct answers are obtained
from the top-retrieved passages. This restricts models  performance
 in predicting multiple answers and in comparing
different answers. We show that we can get more diverse
passages indirectly , by first retrieving similar generated
questions q given a input question x, and then using as
the final retrievals the passages from which the q  s here
generated.
Retrieving questions q given a question xis analogous
to retrieving passages from text corpora, soany existing
retrieval method can be applied. In this paper, we use a
sparse retrieval model, BM25, and a state-of-the-art dense
retrieval model, GTR (Ni et al., 2021). GTR was originally
designed for passage retrieval but the query encoder and
passage encoder in GTR share parameters, so, we can directly
 use it to encode and retrieve questions as well. We use
GTR-large and finetune the checkpoint of GTR on NQ-open
(Kwiatkowski et al., 2019) in our experiments.
Questions retrieved from SIXPAQ are then mapped to passages
 where those questions were generated. With an input
question x, the score for a passage piis
s(x, pi)  maxq  GEN (pi)f(x, q ) (1)
where GEN (pi)is the set of questions generated from the
passage piandf(x, q )is the retrieval score of the question
q  GEN (p)from BM25 or GTR. We denote this method
as  max  in the our experiments ( 4.1).
In addition, we propose another simple heuristic to map
questions to passages. It returns passages from which the
most top- kretrieved questions are generated. We use k 
50in our experiments. This method is denoted as  count 
in our experiments ( 4.1).
sc(x, pi)   {GEN (pi) argmaxk,q f(x, q )}  (2)
3.3. Ambiguous QA with Long Outputs
In the second task, we investigate the challenging task of
answering ambiguous questions with long outputs summa4Merging
 is performed by word matching, even though many
questions are semantically same.rizing multiple answers to the questions. For ambiguous
questions that have different answers, one practical way
to answer such questions is to specify under what conditions
 answers are correct. For example, for the question
 Where is the home stadium of Michigan Wolverines  , in
addition to predicting a list of answers, { Crisler Center ,
 Michigan Stadium , ... }, a QA system should clarify that
 Crisler Center  is the home stadium of the Michigan basketball
 team while the  Michigan Stadium  is the home
of the football team. The ASQA task proposed to answer
ambiguous questions by summarizing the multiple answers
into short paragraphs, e.g.  The home stadium of Michigan
Wolverines men s football is the Michigan Stadium, while
the stadium of its men s basketball team is the Crisler Center.
 Crisler Center is also the home stadium for Michigan
Wolverines women s basketball .
Previous models simply retrieve passages from a text corpus
 and generate answers from the retrieved results. However,
 the retrieved passages are usually long and contain
information irrelevant to the answers. We propose to retrieve
 questions from SIXPAQ as a concise representation
of question-specific information from passages. We additionally
 propose a question revision step which operates on
the retrieved questions to include more detailed information
for the disambiguation task.
3.3.1. Q UESTION REVISION
While the questions in SIXPAQ are fairly unambiguous,
we also explored approaches to make the questions include
more information from the passages from which they were
generated. We trained a sequence-to-sequence model to
extract answer-specific information from passages where
SIXPAQ questions are generated and rewrite the questions
to include such information. Examples of questions before
and after revision are shown in Table 1  e.g., the model
locates the information  men s football  from the context
 ... is the home stadium for the University of Michigan
men s football team (Michigan Wolverines) in Ann Arbor
...  and adds it to the initial question. The revised question,
 Where is the home stadium of the Michigan Wolverines
men s football team built in 1927  , contains information
{ men s football ,  built in 1927  }that is specific to the
answer  Michigan Stadium . Compared to passages with
hundreds of tokens, the revised questions are more concise
in capturing information that is specific to answers.
We finetune a T5 model to perform the question revision
task. The T5 model is trained with data provided as auxiliary
information in the ASQA dataset (Stelmakh et al., 2022),
which contains revised questions for different answers ai
and passages piprovided to human annotators to write the
revised questions q 
i.5The question revision model takes an
5The revised questions originate from the AmbigQA (MinAnswers Context Revision 1 Revision2
Michigan StadiumMichigan Stadium, nicknamed  The
Big House , is the home stadium for
the University of Michigan men s
football team (Michigan Wolverines)
in Ann Arbor, Michigan. Michigan
Stadium was built in 1927 , and it is
the largest stadium in the US...Where is the home stadium
of Michigan Wolverines
men s football team  Where is the home stadium
of Michigan Wolverines
men s football team
built in 1927  
Crisler CenterCrisler Center (formerly known as the
University Events Building and Crisler
Arena) is an indoor arena located in
Ann Arbor, Michigan. It is the home
arena for the Michigan Wolverines
men s and women s basketball teams ...Where is the indoor home
stadium of Michigan Wolverines Where is the indoor home
stadium of Michigan Wolverines
men s and women s basketball 
Table 1  Examples of revisions of a question  Where is the home stadium of Michigan Wolverines   (not an exclusive list).
Depending on different answers, questions are revised at each revision step to add additional answer-specific information.
We consider question revision as a question expansion step which moves information from passages to questions.
ambiguous question q, an answer ai, and a passage pito
generate a revised question q 
i. The input and output of the
model are shown below.
input  question  q answer  ai passage  pi
output  answer  ai revised  q 
i
At inference time, we repeat the revision process ktimes
to increase the amount of information added to original
questions. In the experiments, we use k  2 because we
observe the model tends to generate identical questions if
k   2. The revised questions have an average length of 14.5
compared to original questions, which average 9.0 words
long.
3.3.2. L ONG -FORM ANSWER GENERATION
After revision of the top-retrieved SIXPAQ questions, we
perform a generation task, to summarize the differences
between multiple answers of the ambiguous questions. In
addition to the revised questions from SIXPAQ, we also
retrieve a few passages from Wikipedia for generating longform
 answers. We find retrieving passages is necessary
for ASQA perhaps because during annotation annotators
were encouraged to include background information in the
long-form answers. Such information is not specific to any
answer, so merely retrieving from SIXPAQ does not provide
 the necessary information. To mitigate this problem,
we follow the baseline to also include top npassages retrieved
 by JPR (Min et al., 2021) from Wikipedia (Stelmakh
et al., 2022).6The inputs to the generation model are thus
et al., 2020) dataset. ASQA conducted additional annotation and
included more auxiliary information, such as passages for question
revision.
6JPR is an auto-regressive reranking model aiming for increasing
 the diversity of retrieved passages.a concatenation of the original question q, answers and retrieved
 questions {(ai, q 
i)}, and retrieved passages {pj}.
The target outputs are the long answers provided in ASQA.
We finetune a T5-large model (Raffel et al., 2020) for this
generation task.
input  question  q  conditions  a1,q 
1, ...   passages  p1, ...
4. Experiments
In this section, we discuss the experimental results for retrieving
 diverse passages and generating long-form answers
for ambiguous questions.
4.1. Retrieval of Diverse Passages
4.1.1. D ATASET
We use AmbigQA (Min et al., 2020) and WebQuestionsSP
(Yih et al., 2016) in our experiments. AmbigQA is an opendomain
 QA dataset derived from NQ (Kwiatkowski et al.,
2019) which contains questions that are ambiguous and
thus have multiple possible answers. WebQuestionsSP (WebQSP)
 (Yih et al., 2016) is another dataset which contains
open-domain questions asked by web users, and a subset of
the questions have multiple answers. We only evaluate on
multi-answer questions (in both datasets) in this experiment 
1172 questions in the AmbigQA dev set and 809 questions
in the WebQuestionsSP test set have multiple answers.7
4.1.2. E VALUATION
To measure the diversity of retrieval, we evaluate models 
performance as the recall of answers. Similar to traditional
passage-level retrieval models (Karpukhin et al., 2020), the
7We consider questions have multiple answers if at least one of
the annotators find multiple answers.AmbigQA WebQSP
k 5 10 5 10
passage - based retrieval
Wikipedia   BM25 35.5 44.4 23.4 32.0
Wikipedia   DPR 50.7 57.7 36.2 43.0
Wikipedia   GTR 55.0 61.9 38.8 46.3
question - based retrieval
PAQ   BM25 (max) 36.9 43.6 26.7 32.7
PAQ   GTR (max) 43.7 51.3 33.2 39.6
SIXPAQ   BM25 (max) 35.5 45.8 24.8 34.4
SIXPAQ   GTR (max) 53.6 60.4 45.3 51.8
SIXPAQ   BM25 (count) 36.4 47.0 25.7 35.8
SIXPAQ   GTR (count) 55.9 63.4 46.7 53.0
Table 2  Recall@k of retrieving diverse answers for multianswer
 questions in AmbigQA and WebQuestionsSP. Experiments
 with  max  (Eq. 1) and  count  (Eq. 2) use different
methods to map top-retrieved questions to passages. We use
GTR-large in the baselines and our method.
recall is measured as the percentage of correct answers that
are mentioned in the retrieved passages.
4.1.3. R ESULTS
Experimental results are presented in Table 2. Numbers
in the first block (passage-based retrieval) show the performance
 of baseline models, BM25, DPR (Karpukhin et al.,
2020) and GTR (Ni et al., 2021), in directly retrieving passages
 from Wikipedia. DPR is another popular dense retrieval
 method with separate query and candidate encoders,
and trained with hard negatives. We re-run the DPR opensourced
 code and evaluate the retrieved results. We also
run GTR-large for passage retrieval on both datasets. For
question-based retrieval, we apply the proposed method on
both PAQ (Lewis et al., 2021) and our SIXPAQ dataset.
Again, we use GTR-large in our method. Experiments with
(max) refer to the method of directly mapping top-retrieved
questions to passages where they are generated (Eq. 1),
while ones with (count) refer to returning passages where
most top-retrieved questions are generated (Eq. 2). Compared
 to passage-based retrieval methods, indirect retrieval
with SIXPAQ yields better performance than using BM25
or GTR. In particular, the recall@10 with BM25 improves
from 44.4 to 45.8 on AmbigQA and from 32.0 to 34.4 on
WebQuestionsSP. The performance with GTR is also better
with SIXPAQ. On AmbigQA, the recall@10 improves 61.9
to 63.4. More improvement comes on WebQuestionsSP
with an increase from 46.3 to 53.0. We conjecture that the
improvement with GTR is less significant on AmbigQA
because GTR is pretrained on NQ, which is a superset of
AmbigQA.4.2. Ambiguous QA with Long Outputs
4.2.1. D ATASET
The ASQA dataset contains 4353 train and 948 dev examples.
 Each example contains an ambiguous question, a list
of disambiguated questions and answers (short text spans),
and a long-form answer which discusses the difference between
 short answers. Due to the high variance of long-form
answers, each example in ASQA was annotated by two
human annotators and the better score among the two annotations
 is recorded. The average length of answers is 65.0
white-space split tokens. Each question has an average of
3.4 different short answers.
4.2.2. E VALUATION
In ASQA, predicted outputs are evaluated from a few different
 perspectives. First, as a long output prediction task, it
evaluates the similarity of predicted outputs with reference
outputs with ROUGE-L scores. Second, it measures the
recall of answers in the predicted outputs (named STR-EM) 
all possible answers must be mentioned in the predicted
output in order to receive full STR-EM scores. Third, it
introduces a learned metric DISAMBIG-F1, with the goal
of measuring whether the disambiguating information about
answers in the outputs is accurate. To compute DISAMBIGF1,
 the ASQA dataset uses a learned a QA model to find the
answers of a sequence of disambiguated questions (provided
by annotators) from the generate output. The output will
receive a full DISAMBIG-F1 score if all predicted answers
from the QA model match the oracle answers of the disambiguated
 questions. Finally, they compute an overall score,
DR, as the geometric mean of ROUGE-L and DISAMBIGF1.
 In addition, LEN (words) measures the average length
of outputs in terms of words. Shorter outputs with higher
DR scores are preferred.
4.2.3. R ESULTS
We evaluate the finetuned T5-large model on the quality
of predicted long-form answers. To show the effectiveness
of the retrieved questions and answers from SIXPAQ, we
compare to the outputs generated from retrieved passages
only.
Results are presented in Table 3. The first group of results
(DPR@1 and JPR@1) means we directly return the top 1
passage retrieved by DPR and JPR. The second group of
results, e.g. T5 (5 passages), shows the performance of
directly generating outputs with the top 5 retrieved passages
with T5-large. Both groups of numbers are copied from the
original paper by (Stelmakh et al., 2022).
To check whether higher recall from more retrieved passages
 leads to better outputs, we re-implement the baseline
model to run it on more retrieved passages. The resultsLEN (words) ROUGE-L STR-EM DISAMBIG-F1 DR
DPR @ 1 99.9 31.1 30.1 16.7 22.8
JPR @ 1 196.8 27.9 45.0 25.8 26.9
T5 (1 passage) 63.0 40.3 33.6 21.2 29.2
T5 (3 passages) 71.1 42.7 39.9 25.1 32.7
T5 (5 passages) 71.6 43.0 41.0 26.4 33.7
T5 (5 passages) * 68.1 43.0 40.1 26.4 33.7
T5 (7 passages) * 69.3 43.0 39.5 25.5 33.1
T5 (10 passages) * 68.9 43.0 39.2 25.9 33.2
ours
T5 (1 passage   10 questions) 58.3 41.6 39.4 26.5 33.2
T5 (2 passages   10 questions) 62.0 42.9 41.8 28.0 34.6
T5 (3 passages   10 questions) 63.3 42.9 41.5 28.2 34.8
T5 (5 passages   10 questions) 63.5 43.8 42.4 28.9 35.6
T5 (oracle) 82.6 46.6 88.7 59.2 52.5
Human 64.8 49.4 98.4 77.4 61.8
Table 3  Performance of long-form answer generation with retrieved answers and passages. All models are finetuned on
T5-large. Numbers copied from the baseline (Stelmakh et al., 2022). * Numbers obtained by re-implementing the baseline
models.
with 5 passages from our implementation matches the numbers
 reported in the original paper (Stelmakh et al., 2022).
However, as shown in Table 3, as the number of passages
increases, both STR-EM and DISAMBIG-F1 drops (40.1 to
39.2, 26.4 to 25.9).
In the third group of experiments, we retrieve questions from
SIXPAQ and add top 10 answers with their conditions to the
input. Without changing the model, the performance of longanswer
 generation with information retrieved from SIXPAQ
increases by 0.8 in ROUGE-L and 2.5 in DISAMBIG-F1.
In addition, the output length of the model with information
from SIXPAQ is also  10% shorter than the baseline but
covers more answers in its outputs.
4.2.4. A BLATIONS
Recall of Answers In the first ablation experiment, we justify
 the claim that retrieving questions from SIXPAQ can
improve the diversity of answers. We report the recall of
answers with and without questions retrieved from SIXPAQ
 in terms of numbers of tokens (see Figure 1). With 10
questions from SIXPAQ added to 5 passages, the recall of
answers improve from 65.5 to 71.1, which leads to around
2 additional points in the final DR metric. From another
perspective, with as few as 10 questions and 2 passages, the
recall becomes comparable to 5 passages (66.1 vs. 66.5).
Furthermore, the total length of 10 answers plus 2 passages
is 43% less than 5 passages (574.5 vs. 1008.8), since information
 from the revised questions are more concise. This
eventually leads to 1 point of increase in the final DR metric
(34.6 vs. 33.7) as shown in Table 3.
Accuracy vs. Input Length We additionally compare the
Figure 1  Left  Recall of answers from the retrieved results with
varying number of passages. Right  DISAMBIG-F1 of predicted
long answers with varying number passages. Both figures show
that inputs with more questions yield better outputs under certain
number of tokens, in both answer recall and DISAMBIG-F1.
performance of models in DISAMBIG-F1 under different
numbers of tokens. Results are shown in Figure 1 (right).
With SIXPAQ questions, models get better DISAMBIG-F1
performance with shorter inputs. The DISAMBIG-F1 with
10 questions and 3 passages (775.6 tokens) is 28.2, better
than the DISAMBIG-F1 of 27.2 with 5 questions and 4
passages (899.9 tokens) and DISAMBIG-F1 of 26.4 with 0
question and 5 passages (1008.8 tokens).
Revised vs. Unrevised Questions We further ablate our
model to investigate the importance of question revision
step proposed in  3.3.1. The results are shown in Table 1
(right). The model s performance with 10 revised question
is consistently better than with unrevised questions.Question Disambiguation In the next ablation experiment,
we experiment with a more straightforward task to investigate
 whether the additional information retrieved from
SIXPAQ help disambiguating questions. Here, we study a
question revision sub-task, using auxiliary data provided in
ASQA.8In this sub-task, the model should revise an ambiguous
 question using information provided a passage such
that it can differentiate the provided answer with others. The
task is similar to revising questions when constructing SIXPAQ
 (  3.3.1), except that the additional information added
to the revised question should be contrastive, i.e. it should
differentiate its answer with others possible answers to the
ambiguous questions. For example, to differentiate the answer
  the Michigan Stadium  and  Crisler Center , one
should provide the additional information   basketball team  
vs.  football team  , but not   built in 1927   vs.   basketball
team  .
A naive model simply takes an ambiguous question, an
answer, and the provided passage as input to predict the revised
 question. We instead retrieve similar questions, along
with their answers and conditions from SIXPAQ, and augment
 the provided passage with the retrieved information,
similar to  3.3. The additional information should provide
background knowledge for models to determine how to revise
 the question. Again, we finetune a T5 model for this
question revision task. The model s output is measured by a
metric,  EDIT-F1 , proposed by Min et al. (2020), to only
evaluate the edits made in the revised questions.9
The results are shown in Table 4. In addition to the
naive baseline of only taking the provided passage as input
(passage-only), we experiment with a few other options that
can potentially improve the coverage of information for the
question revision task. First, we retrieve the top 1 passage
from Wikipedia (top-1 Wikipedia). Second, we expand the
provided passage with preceding and following tokens to
double the length of inputs (adjacent context). Third, we
deliberately add a passage which contains different answers
of the same question (contrasting passage).10Results in
Table 4 shows that adding questions retrieved from SIXPAQ
 is the most effective method in revising questions, and
therefore justify our claim that questions from SIXPAQ are
concise and can provide sufficient background information
for models to differentiate answers.
5. Conclusion
In this paper, we proposed to use a database of questions
constructed from Wikipedia to answer ambiguous questions.
8This information is also available in the question revision
sub-task in AmbigQA (Min et al., 2020).
9Please refer to Min et al. (2020) for more information on
 EDIT-F1 .
10Also available as auxiliary information in ASQA.EDIT-F1
oracle passage 22.4
  adjacent context 22.6
  top-1 Wikipedia 21.6
  contrasting passage 23.0
  top-5 SIXPAQ questions 24.8
  top-10 SIXPAQ questions 25.6
Table 4  Results on the question revision task of AmbigQA.
We compare different approaches to increase the coverage
of information in the inputs.
We experiment on two different tasks to show its efficacy in
solving questions with multiple answers. In the first task, we
show that retrieving from generated questions can increase
the diversity of retrieval results as an increase in the recall
of answers. In the second task, we show that the increase
in recall, along with the concise information contained in
the revised questions, improves the performance of models
on a challenging long-form QA task in summarizing and
comparing different answers of ambiguous questions.
References
Bandyopadhyay, S., Pal, S., Zou, H., Chandra, A., and BoydGraber,
 J. Improving question answering with generation
of nq-like questions. arXiv preprint arXiv 2210.06599 ,
2022.
Chen, W., Cohen, W. W., De Jong, M., Gupta, N.,
Presta, A., Verga, P., and Wieting, J. Qa is the new
kr  Question-answer pairs as knowledge bases. arXiv
preprint arXiv 2207.00630 , 2022a.
Chen, W., Verga, P., de Jong, M., Wieting, J., and Cohen,
W. Augmenting pre-trained language models with qamemory
 for open-domain question answering. arXiv
preprint arXiv 2204.04581 , 2022b.
Dhingra, B., Zaheer, M., Balachandran, V ., Neubig, G.,
Salakhutdinov, R., and Cohen, W. W. Differentiable
reasoning over a virtual knowledge base. arXiv preprint
arXiv 2002.10640 , 2020.
Dhingra, B., Cole, J., Eisenschlos, J., Gillick, D., Eisenstein,
J., and Cohen, W. Time-aware language models as temporal
 knowledge bases. Transactions of the Association
for Computational Linguistics , 10 257 273, 2022.
Joshi, M., Choi, E., Weld, D. S., and Zettlemoyer, L. Triviaqa 
 A large scale distantly supervised challenge dataset
for reading comprehension. In Proceedings of the 55th
Annual Meeting of the Association for Computational
Linguistics (Volume 1  Long Papers) , pp. 1601 1611,
2017.Karpukhin, V ., Oguz, B., Min, S., Lewis, P., Wu, L., Edunov,
S., Chen, D., and Yih, W.-t. Dense passage retrieval
for open-domain question answering. In Proceedings of
the 2020 Conference on Empirical Methods in Natural
Language Processing (EMNLP) , pp. 6769 6781, 2020.
Kwiatkowski, T., Palomaki, J., Redfield, O., Collins, M.,
Parikh, A., Alberti, C., Epstein, D., Polosukhin, I., Devlin,
J., Lee, K., et al. Natural questions  a benchmark for question
 answering research. Transactions of the Association
for Computational Linguistics , 7 453 466, 2019.
Lewis, P., Wu, Y ., Liu, L., Minervini, P., K  uttler, H., Piktus,
 A., Stenetorp, P., and Riedel, S. PAQ  65 million
 probably-asked questions and what you can do
with them. Transactions of the Association for Computational
 Linguistics , 9 1098 1115, 2021. doi  10.
1162/tacl a00415. URL https //aclanthology.
org/2021.tacl-1.65 .
Min, S., Chen, D., Zettlemoyer, L., and Hajishirzi,
H. Knowledge guided text retrieval and reading for
open domain question answering. arXiv preprint
arXiv 1911.03868 , 2019.
Min, S., Michael, J., Hajishirzi, H., and Zettlemoyer, L.
Ambigqa  Answering ambiguous open-domain questions.
InProceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) , pp.
5783 5797, 2020.
Min, S., Lee, K., Chang, M.-W., Toutanova, K., and Hajishirzi,
 H. Joint passage ranking for diverse multi-answer
retrieval. arXiv preprint arXiv 2104.08445 , 2021.
Ni, J., Qu, C., Lu, J., Dai, Z.,  Abrego, G. H., Ma, J., Zhao,
V . Y ., Luan, Y ., Hall, K. B., Chang, M.-W., et al. Large
dual encoders are generalizable retrievers. arXiv preprint
arXiv 2112.07899 , 2021.
Raffel, C., Shazeer, N., Roberts, A., Lee, K., Narang, S.,
Matena, M., Zhou, Y ., Li, W., Liu, P. J., et al. Exploring
the limits of transfer learning with a unified text-to-text
transformer. J. Mach. Learn. Res. , 21(140) 1 67, 2020.
Rajpurkar, P., Zhang, J., Lopyrev, K., and Liang, P. Squad 
100,000  questions for machine comprehension of text.
InProceedings of the 2016 Conference on Empirical
Methods in Natural Language Processing , pp. 2383 
2392, 2016.
Rajpurkar, P., Jia, R., and Liang, P. Know what you don t
know  Unanswerable questions for squad. arXiv preprint
arXiv 1806.03822 , 2018.
Stelmakh, I., Luan, Y ., Dhingra, B., and Chang, M.-W.
Asqa  Factoid questions meet long-form answers. arXiv
preprint arXiv 2204.06092 , 2022.Sun, H., Dhingra, B., Zaheer, M., Mazaitis, K., Salakhutdinov,
 R., and Cohen, W. Open domain question answering
using early fusion of knowledge bases and text. In Proceedings
 of the 2018 Conference on Empirical Methods
in Natural Language Processing , pp. 4231 4242, 2018.
Sun, H., Verga, P., Dhingra, B., Salakhutdinov, R., and
Cohen, W. W. Reasoning over virtual knowledge bases
with open predicate relations. In International Conference
on Machine Learning , pp. 9966 9977. PMLR, 2021.
Sun, H., Cohen, W., and Salakhutdinov, R. Conditionalqa  A
complex reading comprehension dataset with conditional
answers. In Proceedings of the 60th Annual Meeting of
the Association for Computational Linguistics (Volume 1 
Long Papers) , pp. 3627 3637, 2022.
Wu, Y ., Zhao, Y ., Hu, B., Minervini, P., Stenetorp, P.,
and Riedel, S. An efficient memory-augmented transformer
 for knowledge-intensive nlp tasks. arXiv preprint
arXiv 2210.16773 , 2022.
Yih, W.-t., Richardson, M., Meek, C., Chang, M.-W., and
Suh, J. The value of semantic parse labeling for knowledge
 base question answering. In Proceedings of the
54th Annual Meeting of the Association for Computational
 Linguistics (Volume 2  Short Papers) , pp. 201 206,
Berlin, Germany, August 2016. Association for Computational
 Linguistics. doi  10.18653/v1/P16-2033. URL
https //aclanthology.org/P16-2033 .
Zhang, M. and Choi, E. Situatedqa  Incorporating extralinguistic
 contexts into qa. In Proceedings of the 2021
Conference on Empirical Methods in Natural Language
Processing , pp. 7371 7387, 2021.
Zhong, V ., Shi, W., Yih, W.-t., and Zettlemoyer, L. Romqa 
A benchmark for robust, multi-evidence, multi-answer
question answering. arXiv preprint arXiv 2210.14353 ,
2022.