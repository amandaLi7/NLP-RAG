Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research
Ji-Ung Lee1,2, Haritz Puerto1,2, Betty van Aken3, Yuki Arase4,
Jessica Zosa Forde5, Leon Derczynski6,7, Andreas RÃ¼cklÃ©10, ,
Iryna Gurevych1,2, Roy Schwartz8, Emma Strubell9,11, Jesse Dodge11
1Technical University of Darmstadt,2Hessian AI,3Berliner Hochschule fÃ¼r Technik,
4Osaka University,5Brown University,6University of Washington,7IT University of Copenhagen,
8The Hebrew University of Jerusalem,9Carnegie Mellon University,10Amazon,11Allen Institute for AI
Many recent improvements in NLP stem
from the development and use of large pretrained
 language models (PLMs) with billionsofparameters.
 Largemodelsizesmakes
computational cost one of the main limiting
factors for training and evaluating such models 
 and has raised severe concerns about
the sustainability, reproducibility, and inclusiveness
 for researching PLMs. These concernsareoftenbasedonpersonalexperiences
and observations. However, there had not
been any large-scale surveys that investigate
them. In this work, we provide a first attempt
 to quantify these concerns regarding
threetopics,namely, environmentalimpact ,
equity, andimpact on peer reviewing . By
conducting a survey with 312 participants
from theNLP community, wecapture existing(dis)paritiesbetweendifferentandwithin
groups with respect to seniority, academia,
andindustry  andtheirimpacton thepeerreviewing
 process. For each topic, we provide
ananalysisanddeviserecommendationsto
mitigate found disparities, some of which already
 successfully implemented. Finally, we
discussadditionalconcernsraisedbymany
participants in free-text responses.
Recentadvancesinhardwareandalgorithmshave
transformed the field of NLP. Whereas NLP practitionersandresearchersusedtobeabletodevelop
and use cutting-edge NLP technology on relatively
affordable hardware such as a laptop or a commodity
 server, modern state-of-the-art approaches have
evolvedtorequiremoresubstantialcomputational
power,typicallyachievedbyspecializedtensorprocessinghardwaresuchasaGPUorTPU.Thisshift
hasraisedatleasttwoconcernsamongmembersof
theNLPcommunity(Strubelletal.,2019 Schwartz
 This work does not relate to AR s position at Amazon.050100
% ParticipantsActual distribution
Figure1  DistributionofavailableGPUsacrossour
participants (in %). As can be seen, 87.8% of our
survey participants have access to less than 10% of
the total number of GPUs.
et al., 2020  Arase et al., 2021) and the AI community
 (Patterson et al., 2022  Wu et al., 2022)  (1)
Understanding and mitigating the environmental
cost of NLP research and use, in terms of greenhousegas(GHG)emissions,and(2)equityofac-
 the extent to which increasing computational
requirements restricts who has access to develop
Inresponsetotheseconcerns,weformedaworkinggroupwithinACLwiththegoalofbetterunder-
 the challenges surrounding efficient NLP
and establishing policies to address them. In order
to quantify views and impacts in the NLP communityrelatedtotheseconcerns,weconducteda
survey of the ACL community in July 2021, the
resultsofwhichwereporthere. Besidesconcerns
about the (1) environmental impact and (2) equity,
we further solicit answers about their (3) impact
onthewholepeerreviewingprocess,asthisisan
important matter for inclusiveness. Overall, we
elicited312responsesfromadistributedrangeof
junior and senior researchers hailing from industry
and academia. Some of our key findings include 
 More than 50% of the survey participants are
moderately or very concerned about the envi-arXiv 2306.16900v2  [cs.CL]  9 Nov 2023ronmentalfootprintofNLPresearch mostly
with respect to trainingandmodel selection .
 Overall,  62%ofourrespondentshaveaccess
to less than eight GPUs and moreover, over
90%haveaccessto lessthan 10%of thetotal
GPU power (Fig. 1). As a frame of reference,
recent work (Izsak et al., 2021) showed that
a clever set of techniques can be used to train
BERT in 24 hours on 8 GPUs, and it takes
about7minutestofine-tuneaRoBERTaLARGE
modelontheMNLInaturallanguageinference
dataset(about400ktrainingsentences)onone
GPU (GTX 2080 Ti) to an accuracy of 85%
 Amajority(76%)ourrespondentsbelievethat
itwouldbebeneficialtohavesmallerversions
ofpre-trainedmodelsreleasedtogetherwith
larger ones. In fact, 33% of our free-text respondents
 emphasised the importance of sharing
 artifacts (such as code, models, training
 The group that suffers most from lack of resources
 are students, who struggle to reproduce
 previous results when compared to researchers
 from large industry.
 While we find disparities between different
groups especially regarding the job sector 
ouranalysisshowsthatmostofthemarenot
statistically significant . Instead, we find outliersacrossallgroupsshowingthatthereexist
disparities within. We find no evidence in
our survey responses that  industry  has accesstosignificantlymorecomputepowerthan
 academia . Instead, this mostly seems to be
the case for very few extreme outliers (6%).
Withthissurvey,wehopetoprovideamoresolid
foundationtoback-uptheongoingdiscussioninthe
community and for devising concrete actions to
make research more inclusive.
The survey was open over a period of 17 days,
fromMonday,July12,2021toThursday,July29,
2021. It was conducted via Microsoft Forms and
distributed across the *CL community by mass
mailingtoACLmembership,andsharedonTwitter.
During that time, we collected 312 responses. The
creation of the survey indicated that,  input willremainanonymousandtheresponseswillalsobe
summarized in aggregate form .1Therefore the
data will be made available on request with a statement
 of intended purpose, due to privacy and ethical
The questions were divided into four categories.
First, we collected some general information about
ourparticipants, liketheircurrentpositionand seniority
 ( 2.2). Second, we asked our participants
abouttheirconcernsregardingtheenvironmental
impact of NLP experiments ( 3) and their access
to computational resources ( 4). Finally, we asked
abouttheimpactofcompute-intensiveexperiments
onthereviewingprocessaswellasaboutspecific
measurements to alleviate them ( 5).
To keep a low-effort for our participants,
we crafted most of the ( ğ‘„)uestions as simple
yes/no/unsurequestions. Forsubjectsthatrequire
a more fine-grained analysis (e.g., environmental
concerns)weusedfivepointscales(eithernumeric
ortext-based). Overall,weaskedatotalof19questionsfromwhich15weremultiple-choicequestions
(13 with a single answer possibility and two with
multiple possible answers). ğ‘„4 (available compute
resources) and ğ‘„11 (number of times reviewers
asked for expensive experiments) required a numeric
 answer. Finally, ğ‘„9,ğ‘„18, andğ‘„19 allowed
free text answers. Participants were asked to provide
 answers to 13 questions, while six questions
(ğ‘„4,ğ‘„9,ğ‘„11,ğ‘„12,ğ‘„14,ğ‘„19)wereoptional. All
questions are provided in Table 1.
2.2 Demographic Overview
In our first three questions, we asked the participants
 about their seniority, job sector, and geographic
Seniority. We asked our participants about the
numberofactiveyearsinthe*CLcommunityasan
author, reviewer, or in a related role ( ğ‘„1). Overall,
a little over half (53.5%) indicated they were junior
members of the community, while the remainder
were fairly evenly split across mid- and late-career.
Job Sector. We further asked our participants
about the current position they are holding ( ğ‘„2).
Possibleresponseswerestudent,academicpost-doc
(Aca. PD), researcher from small (s) and large (l)
1https //www.aclweb.org/portal/
content/efficient-nlp-surveyDemographics
Q1. Yearsactive. HowmanyyearshaveyoubeenactiveintheACLcommunity(asanauthor/reviewer/area
chair/etc.)  Answer  [1-5], [6-10], [11-15], [16 ].
Q2. Current Role. Answer   Student, Academic Postdoc, Academic PI, Researcher in large industry, Research
in small industry, other.
Q3. Geographic Location. Answer   Americas, Europe/Middle East, Africa, Asia/Oceania.
Q4. Availablecomputeresources . PleaseprovidearoughestimateoftheaveragenumberofGPUsorequivalent
accelerators that are available to you (for students / researchers) or to each researcher in your lab/group (for PIs /
managers). Ifyoucannotquantifytheamountofcomputeresources,leavethisfieldempty. Answer  Numeric
Q5. Unable to run experiments . In the last year, have you been unable to run experiments important for one of
due to lack of computational resources  Answer  Yes, No, Unsure.
Q6. More resources would make your work more valuable . How often do you feel like your work would have
been valued more by the community (e.g., accepted instead of rejected to some venue) if you had access to more
computational resources  Answer  Five point scale.
Environmental Concern
Q7. Concernaboutenvironmentalfootprint. Howconcernedareyoubytheenvironmentalfootprintofthe
field of NLP  Answer  Five point scale.
Q8. Mostpressingfactor. Whichofthefollowingdoyoufeelisthemostpressingfactorwithrespecttothe
environmental impact of NLP  Answer  Choose all that apply  Training, Inference, Model selection, None,
Q9. Why  Optionally explain the reasons for your choices above. Answer  Free text (optional).
Q10. Did reviewers ask for too expensive experiments  In the past 3 years, have you received feedback from
reviewerswhorequestedexperimentsthatweretooexpensiveforyourbudgetforaparticularpaper  Answer 
Q11. If yes, how many times 
Q12. Was the critique justified  If yes, do you feel the critique was justified  I.e., that the main scientific
claims in your paper (e.g., that your approach was better than some baseline) were not sufficiently supported by
smaller-budget experiments  Answer  Yes, No, Not sure.
Q13. Lackofresourcespreventsreproductionofpreviousresults. Howoftendoyoufindyourselfunsuccessful
inreproducingapreviousresultduetolackofcomputationalresources  Answer  Never,Rarely,Sometimes,
Q14. EfficiencyTrack . Ifyou have work onefficient methodsand/or enhancedreporting, would youconsider
submitting it to a dedicated track  Answer  Yes, No, N/A.
Q15. Justifyallocationofbudgetforexperiments . Asareader,wouldyoupreferauthorstoberequestedto
justifythewaytheyallocatetheirbudgettorunexperimentswhichadequatelysupporttheirscientificclaims 
Answer  Yes, No, Not sure.
Q16. Reviewersshouldjustifythepetitionforadditionalexperiments . Asanauthor,wouldyoupreferitif
reviewers took up space in their review to justify their suggestions for additional experiments in terms of the
evidence that those additional experiments would provide  I.e., what is currently missing in terms of lack of
evidencetosupportthemainclaimsofthepaper,andhowtheadditionalexperimentswouldprovideevidencefor
the paper s research questions  Answer  Yes, No, Not sure.
Q17. Releasing small versions of pretrained models . Would your work benefit from smaller versions of
pretrained models released alongside larger ones  Answer  Yes, No, Not sure.
Q18. Howtoencouragethereleaseofmodels . Whichofthesesolutionswouldyouendorseforencouraging
the releaseof trained models  Answer  Choose allthat apply  Bestartifact award, Instructreviewers to reward
papers who share/promise to share models, Visible branding of the paper in conference proceedings, None of the
Q19. Any other thoughts or suggestions  Answer   Free text (optional).
Table 1  List of questions in the survey. Summaries of the questions in bold. Only full questions were
shown to the participants.industries (Ind.), and academic PI (Aca. PI). The
largestgroupofparticipantswerestudents(38.5%),
followed by academic postdocs and PIs (34.3%),
and industry researchers (24.7%). Eight participants
 (2.5%) responded with  other  from which
sevenwereaffiliatedwithacademia(e.g.,lecturers)
and one with industry (consultant). For the finegrainedanalysis,wemergeeachresponseof other 
into the most fitting group in the survey (one student,fiveacademicPIs,oneacademicpost-doc,and
one small industry researcher). For our analysis,
we do not merge the academic and industry subgroups,asthismayobfuscateexistingdisparities 
e.g., between small and large industry.
Geographiclocation. Wefurtheraskedrespondentstosharetheirgeographiclocation(
 45.8% of responses came from the Americas
(AM), 40.4% from Europe (EU) and the Middle
East(ME),and13.8%fromAsia(AS)andAustralia
(Aus). Wereceivednoresponsesfromresearchers
in Africa (AF). The heavily skewed responses in
terms of geographic location limits the expressiveness
 ofthis factor andthus, will notbe considered
In the following sections, we analyse and discuss
the participants  responses with respect to the remaining
 three categories ( environmental concerns ,
equity,andimpactonthereviewingprocess ). For
each section, we first provide an overview of the
distributionintheresponsesandthenprovideafinegrained
 analysis with respect to the seniority and
job sector . The goal of the fine-grained analysis
istoinvestigateifwecanobserveanystatistically
significant differences across different groups.
Statistical tests. Due to the explorative nature of
oursurvey,thecollecteddataviolatesthenecessary
conditionsonhomoscedasticity(Levene,1960)and
normality (Shaphiro and Wilk, 1965) that are requiredtoconductananalysisofvariances(ANOVA,
Fisher1921). Instead,weperformaKruskal-Wallis
test (Kruskal and Wallis, 1952) as an indicator
for any statistically significant differences2and if
so, perform pairwise Welch s t-tests (Welch, 1951)
against a Bonferroni corrected ğ›¼ 0.05
isthenumberofpairwisecomparisons i.e.,for ğ‘›
2This is the case when ğ»   ğ»ğ‘›
groups. For ğ›¼  0.05, we getğ»5
0  9.488(job sector) and
0  7.815(seniority) (Abramowitz, 1974).groups,ğ‘š ğ‘› (ğ‘› 1)
2(Bonferroni, 1936). This results
 in corrected ğ›¼  0.0083for seniority with
ğ‘›  4andğ›¼  0.005forthejobsectorwith ğ‘›  5
(notmergingacademiaandindustrysectors). For
the numerical questions ( ğ‘„4 andğ‘„11), we further
analyze if there exist disparities within each group,
using interquartile ranges with ğ‘˜  1.5to detect
outliers (Tukey, 1977).
3 Environmental Footprint
We quantified existing concerns about the environmental
 impact of NLP experiments using a five
pointLikert scale( ğ‘„7)and askedourparticipants
toselectthemostpressingissueinthetypicallife
cycle of an NLP model ( ğ‘„8) between (Train)ing,
model (Select)tion, and (Infer)ence. Participants
wereallowedtoselect allapplicableanswersand
could select (None) or provide (Other) pressing issues.
 Theycouldalsoprovideatextualjustification
of their answer(s) ( ğ‘„9).
Figure 3a shows that more than 50% of our participantsweremoderately(28.2%)orvery(27.9%)con-
cernedabouttheenvironmentalfootprintofNLP,
while around 33% of them were slightly (14.7%)
or somewhat (18.6%) concerned. 10.6% of participants
 were not concerned at all. Our participants
 further agreed that training(75.3%) and
model selection (59.9%) are the most pressing issues
 (Fig. 3b).3Inference took third place with
20.2%,while6.1%ofourparticipantsselected none.
Thesmallestnumberresponseswasgivenfor other
withhyperparameter tuning andtravelling (6 mentionseach)beingthemostfrequentones.
 storageconsumption ,hardware ,expectationsaboutlargedataexperiments
 ,andscale. Interestingly,
 many respondents considered inference
less pressing than training and model selection.
Job Sector. Although wedo not findsignificant
differences by seniority, we see larger (although
notstatisticallysignificant)differences whenlooking
 at the responses grouped by researchers from
different job sectors (Fig. 4a). We find that respondents
 from the large industry sector were mostly
somewhat concerned,whilethemedianforallother
groupsliesat oftenconcerned. Similarly,wealso
see larger differences in the most pressing issues
3Notethat39.7%ofourparticipantsselectedexactlythese
two as the only pressing factors.(a)ğ‘„1  Years at ACL% Participants
(b)ğ‘„2  Job sector% Participants
StudentAca. PDAca. PIInd. (s)Ind. (l)Other
(c)ğ‘„3  Located in% Participants
Figure2  Demographicstatistics  (a)describestheseniority, (b)thejobsector, and(c) thegeographic
location of our participants (in %).
betweendifferentgroups. Forinstance,smalland
large industries were substantially more concerned
with respect toinference and muchless concerned
with respect to model selection than academia.
3.2 Discussion and Recommendations
Ananalysisofthe81(26%)free-textresponses( ğ‘„9)
reveals diverse opinions about the environmental
impact of NLP and the reasons behind the most
pressingfactors. Forinstance,amongrespondents
thatstatedtobe notatall concernedaboutNLP s
environmental footprint, a majority considered the
impact of NLP research on climate change to be
negligible compared to other factors. Factors mentioned
 as being more relevant to climate change
include air travel (also mentioned twice in the general
 responses ğ‘„19), cars, and more cost intensive
computations from other areas (of science). Another
 argument brought up multiple times in this
groupofrespondentsisthattheACLisnottheright
institution to tackle challenges of climate change.
Some responses alternatively suggested to push for
regulatorychanges,sincebigtechcompaniesmight
not be affected by decisions made by the ACL.
Regardingthemostpressingfactors,oneofthe
main arguments provided for inference was that industry
 spends most time on inference, hence it is
themostexpensiveone. However,participantsalso
arguedthatthereexistvariousmethodsforefficient
inference(see,e.g.,Trevisoetal.,2023). Prominent
argumentswithrespecttotrainingandmodelselectionwerethatthepressuretoachievestate-of-the-
 performance leads to extensive hyperparameter
tuning and that a large variety of models are being
trained during research and development (even if
just for debugging) without being ever deployed.4 Equity
The(in)equityoftheavailablecomputeresources
across groups (e.g., academia and industry) is an
increasinglybroughtuptopicindiscussions. While
thegeneralgistseemstobethatmanyresearchers
feelexcluded bynot havingaccess tosubstantially
large compute power (e.g., thousands of GPUs),
it often remains unclear whether this is really the
case. Oneofthemainobjectivesofthissurveywas
therefore to quantify such potential disparities.
Forğ‘„4,229participantsresponded(73.4%)with
the number of GPUs they have access to. Fig. 1
showsthedistributionofthetotalnumberofGPUs
across our participants (in %). Overall, we observe
a high disparity across our participants in terms of
access to GPUs. For instance, 62% of the participants
 had access to less than eight GPUs(Fig. 6a),
thenumberusedfortrainingacademicBERT(Izsak
etal.,2021),and87.8%oftheparticipantshadaccess
 to only 9.7% of the total number of GPUs.
15 participants (6.6%) had access to more than 100
GPUs, up to 3000 GPUs, representing 85.6% of
thetotalGPU count(  11.2k). An outlieranalysis
shows that 13.1% of the respondents had access
to a substantially higher number of GPUs (more
than 22 GPUs) than the rest. We further find that
57.4% of our participants were unable to run experiments
 dueto the lackof computational resources,
and36.2%hadnolackofresources( ğ‘„5). Finally,
Fig.5showsthat31.4%ofourrespondents never
orrarelythoughtthatmoreresourcescouldmake
their work valuable, while 34.3% of respondents
answered sometimes , and 34.3% answered oftenor
always(ğ‘„6).(a)ğ‘„7  How concerning is the env. footprint % Participants
SomewhatModeratelyVery
(b)ğ‘„8  What are the most pressing issues % Participants
Train SelectInferOtherNone
Figure 3  Environmental concerns and pressing issues (in % of participant answers).
(a)ğ‘„7  Concerns by job sector
StudentAca. PDAca. PIInd. (s)Ind. (l)
(b)ğ‘„8  Pressing issues by job sector% Participants
TrainSelectInferOther None
Figure 4  Concerns and pressing issues, grouped by positions.
Q6  Would more resources make work more valuable % Participants
Figure5  Lackofresourcesformorevaluablework.
JobSector. Asin 3,ouranalysisshowsnosignificant
 differences w.r.t. the seniority, and we find
larger disparities by job sector. As we can observe
inFig.6c,respondentsinindustry(large)hadaccess
toahighernumberofGPUsthanindustry(small)
andacademia. Thisisoneofthefewcaseswhere
wehavetoresorttopairwisetesting,astheKruskal-WallistestindicatesthattheNullhypothesiscannot
be rejected with ğ»5  16.976  ğ»5
Whilewedonotfind significantdifferencesinour
pairwise comparisons, there are still substantial
differences between Ind. (l) and Aca. PI (p-value
  0.0827),aswellasInd.(s)andInd.(l)(p-value
  0.0850). Even though students reported the lowestnumberofavailableGPUs,thedifferencesseem
less substantial compared to researchers at small
industry (p-value   0.110). Additionally, we find
that large industry has the highest percentage of
outliers and the largest in-group disparity. Interestingly,researchersfromsmallindustryseemtohave
the least issues when running experiments  a stark
contrast considering they are among those who reported
 the fewest GPU resources ( ğ‘„5). Regardingğ‘„6,
 researchers from large industry responded
slightly less often than other groups that their researchcouldbemorevaluableiftheyhadaccessto
more compute power.(a)ğ‘„4  #GPUs per participant% Participants01020304050
(b)ğ‘„4  GPUs by seniority#GPUs
(c)ğ‘„4  GPUs by job sector#GPUs
StudentAca. PDAca. PIInd. (s)Ind. (l)
Figure 6  Distribution of GPUs among participants  (a) overall, (b) by seniority, (c) by job sector.
4.2 Discussion and Recommendations
While our survey highlights existing disparities,
particularly between small industry or academic
researchers and large industry, we also find that
thereexistsubstantialdisparities withineachgroup.
Most surprising might be the general disparity
we find across the field, as 87.8% of our participants
 had access to less than 10% of the total number
 of GPUs, and 62% had access to less then 8
GPUs. Only a very small faction of researchers
(2.2% of our respondents) had access to GPU compute
 (1000 or more) to train models with several
hundredsofbillionparametersforseveraldaysor
weeks. Many researchers, hence, could only finetune
 models which requires far fewer resources
thanpre-training(Zhouetal.,2021) whichisonly
possible when pre-trained model weights are available.
 Unfortunately, many recent models are being
kept private, which has intensified the discussion
about equity in the field (Togelius and Yannakakis,
5 Impact on Reviewing
Finally,wequantifiedhowtheconcernsaboutthe
environmentalimpactanddifferencesintermsof
availablecomputeresourcesaffectpeerreviewing
(ğ‘„10 ğ‘„13). Wefurtheraskedourparticipantsfour
questions(ğ‘„14 ğ‘„17)whichrelatetoconcreteideas
that would change the reviewing process and encourage
 model release ( ğ‘„18).
Figure7ashowsthat30.1%oftheparticipantsexperienced
 being asked (during peer-review) to conductadditionalexperimentsthatweretooexpensive
for them (ğ‘„10) with 77 respondents having experi-encedthismorethanonce( ğ‘„11)and19.2%having
a substantiallyhigher number(five ormore times)
accordingtoouroutlieranalysis. Mostparticipants
(65.9%)furtherthoughtthatthiscriticismwasunjustified
 (ğ‘„12). Figure 7b ( ğ‘„13) shows that 34.3%
oftherespondents oftenoralwayslackedresources
toreproducepreviousexperimentsand41.4% sometimes.
 Only7.7% neveror16.7%rarelyfacedalack
of resources to reproduce experiments.
With respect to the concrete reviewing actions,
Fig.9ashowsthatalargemajority(89.8%)ofour
participantswouldconsidersubmittingtheirwork
to a dedicated track on efficient methods ( ğ‘„14).
Followingupontheresultsfromthesurvey,such
an efficiency track was implemented at EMNLP
2022. 35.9% of our participants were unsure about
requestingauthorstojustifytheallocationofbudget
 for experiments ( ğ‘„15), with 41% voting for
yes. Also, even though 52.6% of the participants
had not been asked for experiments that were too
expensiveforthem,aclearmajorityoftheparticipants
 (83.7%) would like to require reviewers to
justifytheirpetitionsformoreexperiments( ğ‘„16).
Lastly,wealsoseealargemajority(75.6%)thatbelievedthattheirworkcouldbenefitfromtherelease
of small versions of pretrained models alongside
large ones (ğ‘„17). To promote this, a majority of
our respondents thought that venues should have
a visible branding of papers to release a model
(59.3%) and that reviewers should be instructed to
reward model release (50.6%). 42.6% of respondents
 thought that the venues should grant a best
artifactaward. 11.5% of respondents supported
noneof the options. A first step towards increasing
the reproducibility and ensuring the submission of
experimental code was implemented at NAACL(a) Reviewer critique
Q10  Did reviewers ask for too expensive experiments 
Q12  Was the critique justified 
Possible answers  yes (  ), not sure (  ), no ( ).
(b) Reproducing results
Q13  Lack of resources to reproduce results % Participants
Figure 7  Analysis on how of a lack of resources can affect research. In (a), we show what percentage of
participantshadbeenaskedbyreviewersfortooexpensiveexperiments( ğ‘„10)andifso,iftheyfeltthe
critique was justified ( ğ‘„12). In (b), we show how often our participants could not reproduce previous
results due to a lack of computational resources ( ğ‘„13).
2022 by introducing a badge system at the reproducibility
 track.4Upon acceptance, the authors
couldfollowspecificprocedurestoearnthreetypes
ofbadges  1)open-sourcecode,2)trainedmodel,
and 3) reproducible results.
Seniority. We find no significant differences
w.r.t. the seniority of our participants regarding
ğ‘„10 ğ‘„18. However,juniorresearchers(1 5years)
showed a substantially higher tendency towards
requesting authors to justify their compute budget
 (ğ‘„15) against all other age groups (p-values
 0.035). WealsoobserveinFigure9cdiverging
preferences between junior and senior groups in
terms of ideas to improve the reviewing process
(ğ‘„18). Juniorresearchers(1 5years)seemedtobe
more inclined towards a visual branding as well as
instructingreviewersthanseniorresearchers(11 
15yearswithap-valueof 0.089and16 yearswith
JobSector. Intermsofthejobsector,weagain
find no significant differences with respect to reviewersaskingfortooexpensiveexperiments(
orcritiquebeingjustified( ğ‘„12). Interestingly,respondents
 from small industry received fewer such
requests (ğ‘„11) compared to post-docs (p-value
  0.024),PIs(p-value   0.061),andlargerindustry
(p-value   0.087). The most concerning trend can
beobservedwhencomparingthedifferentgroups
withrespecttotheirlackofcomputeresourcesto
4https //2022.naacl.org/blog/
reproducibility-track/reproduce experiments ( ğ‘„13, Fig. 8)  where we
find significant differences and conduct pairwise
analyses.5In general, students suffered most, with
a significant difference compared to the large industrysector
 witha p-valueof 0.002 0.005  ğ›¼
(Bonferroni-corrected). We further find substantial
differencesbetweenstudentsandacademicPIs (pvalue
   0.026) and between academic post-docs
and large industry labs (p-value   0.088).
Wefindnosubstantialdifferenceswhenitcomes
to actionable items for the *CL community ( ğ‘„14 
ğ‘„17),indicatingthatimplementingpopularideas
would be welcomed by all groups. However, we
findsomedifferenceswhenitcomestoencouraging
 the release of models ( ğ‘„18). For instance, Figure
 9d shows that academic post-docs had a higher
preferencetowardsreviewersrewardingpapersthat
promisetoreleasemodelsthanacademicPIs. Also,
participantsfromsmallindustrywouldprefervisual
branding over awards in contrast to large industry.
5.2 Discussion and Recommendations
Our analysis shows that the two most pressing issues
 among our respondents are the lack of resources
 to reproduce results and reviewers requestingfortooexpensiveexperimentswithoutproper
justification. Thisisreflectedinthelargesupport
for both respective counter measures  namely, askingreviewerstoprovidejustificationandtherelease
ofsmallermodelsthatwouldallowresearcherstoat
leastreproducesomeexperiments. Consideringthe
5Kruskal-Wallis test  ğ»5  12.486 ğ»5
StudentAca. PDAca. PIInd. (s)Ind. (l)
Figure 8 ğ‘„13  Lack of resources by job sector.
successofbadgesatNAACL2022with175code,
98modeland20reproducibilitybadges,introducing
 an explicit badge for small model release could
boost inclusiveness and reproducibility.6To improve
 peer reviewing, one immediate action could
betoadapttheARRreviewingguidelinesandinstruct
 reviewers to consider the compute budget
reported in a paper when asking for more experiments.7
Among the 22 additional suggestions for ğ‘„18,
we find a high emphasis (68.2%) towards the releaseofartifacts bothbecausethisfacilitatesfu-
tureresearchandhelpsreproducibility.
22ofthe67generalsuggestion( ğ‘„19)alsotouched
upon issues about model release and reviewing,
highlighting the importance of both topics. The
responses mentioned a remarkably wide variety of
artifacts  code trainedmodels systemoutputs(to
facilitate comparative evaluations without rerunning
 the code)  training checkpoints (to study the
trainingdynamics) andproperdocumentationof
trainingdata(includingcrowdsourcingquestions).
In addition to simply releasing trained models, several
 respondents also wished for a sufficiently high
quality of the released models complemented by
code and documentation. One particular concern
washowthereleaseofartifactsshouldbeintegrated
into the reviewing process. On the one hand, it
seemsusefultosubmitartifactstogetherwiththe
paper before reviewing, so that reviewers can accessthemandtopreventbreakingpromisesoffu-
 code release. On the other hand, this needs
to happen within the constraints of double-blind
6https //naacl2022-reproducibility-track.
7https //aclrollingreview.org/
reviewertutorialreviewing. Finally, 12 of the free-text responses of
ğ‘„18andğ‘„19suggestedthatartifactreleaseshould
be mandatory for acceptance.
6 Further Considerations
Finally,wediscusssuggestions( ğ‘„19)thatdonotfit
intoanyofthepreviouslydiscussedtopics. From
the 67 free-text responses (21.5%), the two most
prominent topics were evaluation (11 respondents)
and emphasizing research over engineering (7 respondents).
Evaluation. 16.4% of the free-text respondents
touched upon the issue of evaluation and model
comparability ascurrentbenchmarksoftenfocus
on improving a single metric. One measure to
counterthistrendwouldbetoreportperformance
basedonParetofrontiersandtoconsiderthecompute
 budget along with the model performance. To
promote such curves, it would also be important
to release metadata including preprocessing and
hyperparameter choices that allows future research
to draw proper comparisons as well as to provide
concrete guidelines for reviewers.
Researchvs.engineering. 10.4%ofthefree-text
respondentsfurthernotedthatthefieldseemedto
havedriftedmoretowardsengineeringbyprimarilychasinghighperformance 
producing meaningful scientific insights. The respondentsbroughtforwardvarioussuggestionsto
combatthis forinstancethatauthorsshouldclearly
statetheirscientifichypothesisandthenreportresearch
 that tests this hypothesis using the lowest
appropriateamountofresources. Othersuggestions
were to actively promote more theoretical, or more
non deep learning work.
Other suggestions. Another suggestion worth
mentioningwasthecreationofaseparatetrack(four
respondents) eitherspecificallyforsmallmodels
orforindustrythatcannotpublishtheirmodels. Finally,
 there was also a call for more shared tasks
with limited resources such as the efficient NMT
challenge (Heafield et al., 2022) or the efficient inference
 task (Moosavi et al., 2020).
Wepresentedafirstattempttocaptureandquantify
existingconcernsabouttheenvironmentalimpact
and equity within the *CL community. We further
investigated the resulting implications on peer reviewing
 consideringthe increasingcomputational(a) Potential changes to reviewing% Participants
Q14 Q15 Q16 Q17Q14  Would consider submitting to efficiency track 
Q15  Authors to justify budget allocation 
Q16  Reviewers to justfy petition for more experiments 
Q17  Benefit from releasing smaller models 
Possible answers  yes (  ), not sure (  ), no ( ).
Q18  How to encourage model release % Participants
for best artifactReviewers
reward releaseOtherNone
(c) Model release (by seniority)
Q18  How to encourage model release % Participants
Brandingin proceedingsAward
for best artifactReviewersreward releaseOtherNone
1 56 1011 1516 (d) Model release (by job sector)
Q18  How to encourage model release % Participants
Brandingin proceedingsAward
for best artifactReviewersreward releaseOtherNone
StudentAca. PDAca. PIInd. (s)Ind. (l)
Figure9  Analysisofresponsesonhowtoimprovethereviewingprocess. In(a),weshowthedistribution
of our participants  responses for ğ‘„14 ğ‘„17(in %). A majority of our participants would submit to an
efficiency track ( ğ‘„14) and would prefer reviewers to justify a request for more experiments ( ğ‘„16). They
further would benefit from a release of smallermodels ( ğ‘„16). In contrast, the responses are more mixed
about the authors justifying the compute budget ( ğ‘„15). In (b d), we show our participants  responses on
howtoencouragethereleaseofmodels(in%)  (b)overall,(c)byseniority,(d)byjobsector. Multiple
responses were allowed for ğ‘„18.
demand. Amajorityofourrespondentswereconcerned
 regarding the environmental footprint of
NLP experiments with model training and model
selectionbeingthemostpressingissues. Wealso
found a high disparity among our respondents with
students and small industry researchers suffering
most from a lack of resources. There was a large
supportformeasurestoimproveequityandaccessibility
 across all respondents  most prominently for
anefficiencytrack,askingreviewerstojustifythe
petition for additional experiments, and the release
of small versions of pretrained models.
Considering the continuous increase of param-eters in PLMs (Zhao et al., 2023), one danger we
faceisthatexistingdisparitiesmayintensifyeven
further. However, we find that much can be done
tocombatthis,evenonanindividuallevel. Asaresearcher,
 by making our model weights, code, and
data publicly available  and as a reviewer, by being
consideratetowardstheavailablecomputebudget.
To receive a large number of responses, this survey
wasadvertisedthroughoutvariouschannels. Hence,
thisisbynomeansarepresentativestudywithinthe
whole*CLcommunity. Thisispartiallyreflectedintheevaluationofthegeographiclocations,e.g.,they
were too coarse to capture a more precise picture
about existing geographic inequalities. Nonetheless,
 the fact that we did not receive any responses
from bodies located in Africa indicates that there
mayexisthighdisparitiesintermsofgeographiclocation.
 Forthesamereason,thedisparitiesfoundin
this survey are more indicative than representative.
Consequently,anyactionthatisbeingimplemented
shouldnotbesolelyderivedfromthesurveydata
and carefully considered beforehand.
This work was initiated at and benefited substantiallyfromtheDagstuhlSeminar22232 
and Equitable Natural Language Processing in the
Age of Deep Learning . We further thank Niranjan
 Balasubramanian, Jonathan Frankle, Michael
Hassid, Kenneth Heafield, Sara Hooker, AlexanderKoller,AlexandraSashaLuccioni,Alexander
LÃ¶ser, AndrÃ© F. T. Martins, Colin Raffel, Nils
Reimers, Leonardo Riberio, Anna Rogers, EdwinSimpson,NoamSlonim,NoahA.Smith,and
ThomasWolfforafruitfuldiscussionandhelpful
feedback at the seminar. We further thank Leshem
Choshen for helpful feedback on this work.
Milton Abramowitz. 1974. Handbook of Mathematical
 Functions, With Formulas, Graphs, and
Mathematical Tables, . Dover Publications, Inc.,
Yuki Arase, Phil Blunsom, Mona Diab, Jesse
Dodge, Iryna Gurevych, Percy Liang, Colin Raffel,
 Andreas RÃ¼cklÃ©, Roy Schwartz, Noah A.
Smith, Emma Strubell, and Yue Zhang. 2021.
Efficient NLP policy document.
Carlo Bonferroni. 1936. Teoria statistica delle
classi e calcolo delle probabilita. Pubblicazioni
delRIstitutoSuperiorediScienzeEconomiche
e Commericiali di Firenze , 8 3 62.
RolandA.Fisher.1921. Onthe"probableerror"of
acoefficientofcorrelationdeducedfromasmall
sample.Metron, 1 3 32.
Kenneth Heafield, Biao Zhang, Graeme Nail,
JelmerVanDerLinde,andNikolayBogoychev.
2022. FindingsoftheWMT2022sharedtaskonefficienttranslation. In ProceedingsoftheSeventhConferenceonMachineTranslation(WMT)
pages 100 108, Abu Dhabi, United Arab Emirates
 (Hybrid). Association for Computational
Peter Izsak, Moshe Berchansky, and Omer Levy.
2021. HowtotrainBERTwithanacademicbudget.
 InProceedingsofthe2021Conferenceon
EmpiricalMethodsinNaturalLanguageProcessing,pages10644 10652,OnlineandPuntaCana,
Dominican Republic. Association for Computational
WilliamH.KruskalandW.AllenWallis.1952. Use
of Ranks in One-Criterion Variance Analysis.
JournaloftheAmericanStatisticalAssociation ,
Howard Levene. 1960. Robust tests for equality
of variances. Contributions to probability and
statistics, pages 278 292.
Nafise Sadat Moosavi, Angela Fan, Vered Shwartz,
Goran GlavaÅ¡, Shafiq Joty, Alex Wang, and
Thomas Wolf, editors. 2020. Proceedings of
SustaiNLP  Workshop on Simple and Efficient
Natural Language Processing . Association for
Computational Linguistics, Online.
David Patterson, Joseph Gonzalez, Urs HÃ¶lzle,
Quoc Le, Chen Liang, Lluis-Miquel Munguia,
DanielRothchild,DavidRSo,MaudTexier,and
Jeff Dean. 2022. The carbon footprint of machine
 learning training will plateau, then shrink.
Computer , 55(7) 18 28.
Roy Schwartz, Jesse Dodge, Noah A Smith, and
OrenEtzioni.2020. Greenai. Communications
of the ACM , 63(12) 54 63.
S Shaphiro and MBJB Wilk. 1965. An analysis
 of variance test for normality. Biometrika ,
Emma Strubell, Ananya Ganesh, and Andrew McCallum.2019.
 Energyandpolicyconsiderations
for deep learning in NLP. In Proceedings of the
57thAnnualMeetingoftheAssociationforComputational
 Linguistics , pages 3645 3650, Florence,
 Italy. Associationfor Computational Linguistics.Julian
 Togelius and Georgios N Yannakakis.
2023. Choose your weapon  Survival strategiesfordepressedaiacademics.
Marcos Treviso, Ji-Ung Lee, Tianchu Ji, Betty van
Aken,QingqingCao,ManuelR.Ciosici,Michael
Hassid, Kenneth Heafield, Sara Hooker, Colin
Raffel, Pedro H. Martins, AndrÃ© F. T. Martins,
Jessica Zosa Forde, Peter Milder, EdwinSimpson,NoamSlonim,JesseDodge,EmmaStrubell,
Niranjan Balasubramanian, Leon Derczynski,
IrynaGurevych,andRoySchwartz.2023. Efficient
 Methods for Natural Language Processing 
A Survey. Transactions of the Association for
Computational Linguistics , 11 826 860.
JohnW.Tukey.1977. ExploratoryDataAnalysis .
BernardLewisWelch.1951. OntheComparisonof
Several Mean Values  An Alternative Approach.
Biometrika , 38(3/4) 330 336.
Carole-Jean Wu, Ramya Raghavendra, Udit Gupta,
Bilge Acun, Newsha Ardalani, Kiwan Maeng,
Gloria Chang, Fiona Aga, Jinshi Huang, Charles
Bai, et al. 2022. Sustainable ai  Environmental
 implications, challenges and opportunities.
ProceedingsofMachineLearningandSystems ,
WayneXinZhao,KunZhou,JunyiLi,TianyiTang,
Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen
 Zhang, Junjie Zhang, Zican Dong, et al.
2023. A survey of large language models. arXiv
preprint arXiv 2303.18223 .
Xiyou Zhou, Zhiyu Chen, Xiaoyong Jin, and
WilliamYangWang.2021. HULK Anenergy
efficiency benchmark platform for responsible
naturallanguageprocessing. In Proceedingsof
the 16th Conference of the European Chapter of
the Association for Computational Linguistics 
System Demonstrations , pages 329 336, Online.
Association for Computational Linguistics.