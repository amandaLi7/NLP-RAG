## PROFNAME
Yiming Yang
## AUTHORID
46286308
## AUTHORNAME
Yiming Yang
## AUTHORURL
https://www.semanticscholar.org/author/46286308
## AUTHORHINDEX
17
## AUTHORAFFILIATIONS
[]
## AUTHORPAPERCOUNT
43
## AUTHORCITATIONCOUNT
1532
## PAPERID
1804dc14b1cf7bbae96bf3215997e9f14425d622
## EXTERNALIDS
{'ArXiv': '2304.11872', 'DBLP': 'journals/corr/abs-2304-11872', 'DOI': '10.48550/arXiv.2304.11872', 'CorpusId': 258297998}
## URL
https://www.semanticscholar.org/paper/1804dc14b1cf7bbae96bf3215997e9f14425d622
## TITLE
Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-tuned GPT
## ABSTRACT
Moreover, GPT-based zero-shot classification models tend to make independent predictions over test instances, which can be sub-optimal as the instance correlations and the decision boundaries in the target space are ignored. To address these difficulties and limitations, we propose a new approach to zero-shot text classification, namely \ourmodelshort, which leverages the strong generative power of GPT to assist in training a smaller, more adaptable, and efficient sentence encoder classifier with contrastive self-training. Specifically, GenCo applies GPT in two ways: firstly, it generates multiple augmented texts for each input instance to enhance the semantic embedding of the instance and improve the mapping to relevant labels; secondly, it generates augmented texts conditioned on the predicted label during self-training, which makes the generative process tailored to the decision boundaries in the target space. In our experiments, GenCo outperforms previous state-of-the-art methods on multiple benchmark datasets, even when only limited in-domain text data is available.
## VENUE
arXiv.org
## YEAR
2023
## REFERENCECOUNT
24
## CITATIONCOUNT
4
## INFLUENTIALCITATIONCOUNT
1
## ISOPENACCESS
True
## OPENACCESSPDF
{'url': 'http://arxiv.org/pdf/2304.11872', 'status': None}
## FIELDSOFSTUDY
['Computer Science']
## JOURNAL
{'volume': 'abs/2304.11872', 'name': 'ArXiv'}
## AUTHORS
[{'authorId': '46752970', 'name': 'Ruohong Zhang'}, {'authorId': '46394797', 'name': 'Yau-Shian Wang'}, {'authorId': '46286308', 'name': 'Yiming Yang'}]
## TLDR
This work proposes a new approach to zero-shot text classification, namely \ourmodelshort, which leverages the strong generative power of GPT to assist in training a smaller, more adaptable, and efficient sentence encoder classifier with contrastive self-training.
Generation-driven Contrastive Self-training for Zero-shot Text
Classiﬁcation with Instruction-tuned GPT
Ruohong Zhang Yau-Shian Wang Yiming Yang
Carnegie Mellon University
ruohongz@andrew.cmu.edu king6101@gmail.com
yiming@cs.cmu.edu
Abstract
With the success of large GPT-based models,
 natural language processing (NLP) tasks
have received signiﬁcant performance improvements
 in recent years. However, using
pretrained large GPT models directly for zeroshot
 text classiﬁcation has faced difﬁculties
due to their large sizes and computational requirements.
 Moreover, GPT-based zero-shot
classiﬁcation models tend to make independent
 predictions over test instances, which can
be sub-optimal as the instance correlations and
the decision boundaries in the target space
are ignored. To address these difﬁculties and
limitations, we propose a new approach to
zero-shot text classiﬁcation, namely G ENCO,
which leverages the strong generative power of
GPT to assist in training a smaller, more adaptable,
 and efﬁcient sentence encoder classiﬁer
 with contrastive self-training. Speciﬁcally,
GENCOapplies GPT in two ways  ﬁrstly, it
generates multiple augmented texts for each
input instance to enhance the semantic embedding
 of the instance and improve the mapping
to relevant labels  secondly, it generates augmented
 texts conditioned on the predicted label
 during self-training, which makes the generative
 process tailored to the decision boundaries
 in the target space. In our experiments,
GenCo outperforms previous state-of-the-art
methods on multiple benchmark datasets, even
when only limited in-domain text data is available.1

1 Introduction
Zero-shot text classiﬁcation is a challenging task of
predicting the class labels of text instanced without
requiring labeled instances for supervised training.
Effective solutions for zero-shot classiﬁcation are
crucial for many real-world applications as labeled
data are often difﬁcult to obtain. With the great success
 of large pre-trained language models in recent
years (Brown et al., 2020  Ouyang et al., 2022),
1Code is available at https //github.com/RiﬂeZhang/GenCohow to leverage the generation power of such models
 in zero-shot text classiﬁcation problems has
become an important question for research.
Recent research on zero-shot text classiﬁcation
can be roughly divided into two categories. The
ﬁrst involves using large GPT models for inference.
For instance, the GPT-3 model has demonstrated
exceptional zero-shot performance when the input
text is transformed into prompts (Brown et al.,
2020). InstructGPT and Alpaca are other variants
of GPT have shown performance improvements by
leveraging human instructions in zero-shot classiﬁcation.
 However, those GPT-based models have
certain drawbacks due to their sizes and computational
 requirements, making them less available
or inefﬁcient to use. Additionally, they tend to
predict the labels of test instances independently ,
and thus cannot leverage correlations over test instances
 ordecision boundaries in the target space.
The second category involves ﬁne-tuning smaller
models for zero-shot classiﬁcation. For example,
LOTClass (Meng et al., 2020) uses BERT to extract
 keywords that are semantically related to class
labels and then use those keywords to help label additional
 instances for the ﬁne-tuning of the BERT
classiﬁer. Other attempts convert classiﬁcation
tasks to close test tasks and design prompts (Schick
and Schütze, 2020b  Gera et al., 2022) to generate
training pairs for smaller classiﬁers. While those
smaller models are easier to train and more efﬁcient
at inference, they do not have the same level of language
 modeling power as the large GPT models as
a drawback.
In this paper, we propose a new approach which
combines the strengths of large pretrained GPT
models and the adaptivity/efﬁciency of a smaller,
sentence encoder classiﬁer trained with contrastive
self-learning. Our framework, namely Generationdriven
 Contrastive Self-Training ( GENCO), effectively
 leverages the generative power of GPT in
two novel ways to assist in training a smaller, sen-arXiv 2304.11872v1  [cs.CL]  24 Apr 2023tence encoder classiﬁer. Firstly, it uses the GPTgenerated
 texts to augment each input text, aiming
to reduce the gap between the input-text embedding
and the embeddings of semantically relevant labels
(Section2.2). Secondly, it uses GPT to generate
new training instances conditioned on the systempredicted
 labels (the pseudo labels) in an iterative
self-training loop, which can enhance the training
data quality by leveraging the contrastive learning
with decision-boundary information in the target
space (Section 2.3). These strategies yields signiﬁcant
 performance improvements in zero-shot
classiﬁcation, as evident in our experiments (Section
 3.3).
In summary, our contributions in this paper are
the following 
 We demonstrate the effectiveness of contrastive
 self-learning techniques to improve
a sentence-encoder model for zero-shot text
classiﬁcation.
 We propose the novel and effective ways to
leverage instruction-tuned GPT for generating
augmented text during the self-training loop.
 We conduct extensive experiments on several
 benchmark datasets, where the proposed
method improve the performance of previous
state-of-the-art methods in zero-shot text classiﬁcation.

2 Proposed Method
We ﬁrst introduce the sentence encoder classiﬁer
as our basic design choice, and then focus on the
novel components in our framework in the followup
 sections.
2.1 Zero-shot Text Classiﬁcation as Sentence
Alignment
The task of zero-shot text classiﬁcation involves
predicting the most relevant labels for a given
document without requiring any labeled training
data. Given a set of Nunlabeled documents
X {x1,x2,   ,xN}and a set of Lcategory
descriptions C {c1,c2,   ,cL}, the goal is to
learn a scoring function g(x,c)that takes document
xand label description cas input and produces a
similarity score as the measure of how well the
document and the label match to each other. In the
rest of the paper, we assume each input text as asentence for convenience, which can be easily generalized
 to a multi-sentence passage or document
without jeopardizing the key concepts.
In the absence of labeled training data, the task
of assigning labels to text can be formulated as
a sentence alignment problem. This involves encoding
 both the input sentence and the label descriptions
 using a pre-trained sentence encoder like
SimCSE (Gao et al., 2021). The alignment scores
between the sentence and labels embeddings are
then used to predict related labels. This approach is
particularly suitable for zero-shot classiﬁcation as
it relies on the semantic matching between textual
instances and label descriptions in the embedding
space, instead of relying on the availability of labeled
 training data.
However, as label descriptions are often just a
few words instead of long sentences, they may not
provide enough context for a pre-trained encoder
to grasp the semantic meaning of the labels. To address
 this issue, prompt-based approaches (Schick
and Schütze, 2020a) convert label names into natural
 language sentences, namely label prompts . For
example, the label  sports" can be converted to
 This is an article about sports." Following this,
we denote by p( )as a function that converts label
 namecinto a prompt by placing the label description
 into a predeﬁned template. We design
Ttemplates for each dataset and the label prompt
embedding for category cis deﬁned as 
ec 1
TT 
i 1fθ(
pi(c))
(1)
wherefθ( )is the sentence encoder parameterized
byθ. The scoring function can be implemented as 
g(x,c)   sim (fθ(x),ec) (2)
where sim( , )is a similarity function such as dot
product or cosine similarity.
Given a input text at inference time, the predicted
label is the one with the highest similarity score 
ˆy  arg max
jg(x,cj) (3)
2.2 Input Text Augmentation
In this section, we propose a way to enhance the
semantic embedding of the original input text with
multiple GPT-generated pieces of texts, as shown
in ﬁgure 1. When the input text is relatively short,
such as consisting of only one or a few sentences,Input (Observed) Text   InstructionStarbucks  president, Orin Smith, plans to retire because he wants to focus on philanthropy, family and sports.Label Prompt (Embedding)Category  Politics news.
GPTSample 1 Sample 2Sample KSmith will step down from his CEO role in March 2005  Mr. Smith who has held his job for 10 years  The board will select the successor who  Category  Business news.Category  Sports news.Category  Technology news.Merged EmbeddingSimilarity ComparisonEncode & taking averageMultiple Versions of Generated TextInstruction  Elaborate the text in a few sentences. Figure 1  Input Text Augmentation using GPT Models  The input text and an instruction are fed into the GPT
model to generate multiple pieces of elaborated texts, each of which is concatenated to the original input to obtain
an augmented text. The embeddings of the augmented texts are then averaged to obtain a merged embedding,
which is used for label prediction in the self-training process.
the alignment-based matching to relevant labels
may not be sufﬁciently effective. A natural remedy
 is to elaborate the input with a pre-trained
GPT model to generate multiple pieces of texts.
Speciﬁcally, we use a simple human instruction,
"Elaborate the text with a few sentences," to guide
the instruction-tuned GPT model, such as Alpaca7B
 (Taori et al., 2023), in generating probable expansions
 and continuations of the text. Our system
 treats the concatenation of the input text and
each GPT-generated piece as one augmentation,
and then takes the average of the embeddings of
multiple augmentations as the merged embedding
of the augmented input. Intuitively, such an augmentation
 should enhance the semantic matching
among input text and relevant labels if the meaning
 of the input is underrepresented (too short) and
if the generative model is highly effective in generating
 relevant pieces for the given input. The
ﬁrst assumption is often true in realist textual data,
and the second condition is well-met by large pretrained
 GPT models.
Formally, the input text x P(x)can be viewed
as randomly sampled from an underlying distribution,
 and the augmented texts can be viewed as
the different variants sampled from the conditional
probability distribution induced by the GPT model,
denoted asxaug Pg(  x). We obtain the augmented
 text embedding by averaging the embeddings
 of the multiple versions of the augmented
text 
1
KK 
i 1fθ(x xaug
i), (4)
where is the concatenation operator for text andxaug
iis thei-th sample from Pg(  x). Our augmented
 texts provide different views of the input
text, and the mean of the embedding provides an
ensemble of induced features. We then use the augmented
 text embedding for pseudo-label prediction.
If GPT is available at test time, we can use this
method for inference as well.
2.3 Self-Training with Contrastive Learning
We employ a contrastive self-training process to
enhance a pre-trained classiﬁer s generalization
capability to iteratively augmented training data.
Speciﬁcally, it is an iterative process where the pretrained
 model is used to classify unlabeled data,
and the newly classiﬁed data with high conﬁdence
is then used to further train the model. When the
labels are noisy, previous studies have suggested
using soft labeling (Xie et al., 2016  Meng et al.,
2020) or label smoothing (Müller et al., 2019) to
prevent the model from becoming overly conﬁdent.
 In this work, we propose a loss function
with soft labeling that connects contrastive learning
and entropy regularization (Grandvalet and Bengio,
2004).
We denotefθ(x)as our sentence encoder model.
Given a input text x, the distribution over labels is 
P(ˆyi x θ)  exp(sim(fθ(x),fθ(pi))) 
c Cexp(sim(fθ(x),fθ(pc)))
(5)
Here,piis a shorthand notation for p(ci), a randomly
 sampled label prompt for label ci. The targetdistribution is derived as 
Q(ˆyi x θ)  exp(sim(fθ(x),fθ(pi))/τ) 
c Cexp(sim(fθ(x),fθ(pc))/τ)
(6)
whereτ 1is the temperature. A lower temperature
 implies a sharper distribution and thus greater
conﬁdence in the predicted label. We drop the notation
 ofθfor convenience. The contrastive text to
label (t2l) objective function is deﬁned as 
Lt2l  N 
i 1L 
j 1Q(ˆyj xi) logP(ˆyj xi)(7)
Whenτ 0,Q(ˆy x)becomes categorical distribution
 and the loss reduces to the supervised
contrastive learning loss with pseudo label ˆycas
the target 
Lτ 0
t2l  N 
i 1logP(ˆyc xi) (8)
It encourages the model to predict label cigivenx
with more conﬁdent. On the other hand, when τ 
1, the loss reduces to a minimization of conditional
entropy function H 
Lτ 1
t2l H(C X) (9)
  N 
i 1L 
j 1P(ˆyj xi) logP(ˆyj xi) (10)
We show a theorem such that minimizing the loss
function equation 7 can achieve similar effects Entropy
 Regularization (Grandvalet and Bengio, 2006,
2004), which is a means to enforce the cluster assumption
 such that the decision boundary should
lie in low-density regions to improve generalization
performance (Chapelle and Zien, 2005).
Theorem 1. Consider a binary classiﬁcation problem
 with linearly separable labeled examples.
When 0 τ   1, optimizing equation 7 with gradient
 descend will enforce the larger margin between
classes and achieves max margin classiﬁer under
certain constraint.
We place our formal theorems and proofs in Appendix
 section A. In our experiment, we set τ  0.1
to balance supervised classiﬁcation and low density
separation between classes.
While self-learning was effective in improving
the performance of our model, it is not without its
limitations. One potential issue is overﬁtting tothe pseudo label, which is prone to error. Additionally,
 self-learning requires a large amount of
unlabeled data, which may not always be available.
In the following section, we propose conditional
augmentation methods with generative model in the
training loop to make self-learning more robust.
2.4 Augmentation Conditioned on Prediction
The loss function of equation 7 can effectively enhance
 the separability of class instances by enforcing
 the decision boundary to lie in low density regions
 of the embedding space. In each self-training
iteration, when the sampled instances are labeled
with relatively lower conﬁdence, which lie near
the decision boundary, the contrastive loss pushes
the instances closer to the pseudo label prompt
embedding. However, self-training can lead to an
undesirable bias in the classiﬁer when instances
are mislabeled. To address this issue, we propose
a novel approach to the generation of labeled data
for self-training. That is, we use GPT to generate
labeled training pairs by augmenting each input
text conditioned on the system-predicted (pseudo)
labels, as shown in ﬁgure 2. For example, if a business
 news article discussing the retirement of Starbucks 
 president is misclassiﬁed with the label of
"sports", optimizing the model with this mislabeled
training instance will make the decision boundaries
between business articles less separable from sport
articles. To alleviate such an undesirable effect,
we use GPT to augment the input text conditioned
on the sport category, resulting in a text closer to
the typical ones in the sport category instead of
the original one which lies closely to the decision
boundary between "sports" and "businesses". In
other words, by using GPT to generate augmented
texts conditioned on pseudo labels, we aim to enhance
 the system-produced training pair with better
separation of class labels in the embedding space.
Based on the aforementioned intuition, we propose
 an approach called instruction-based conditional
 generation to generate augmented text conditioned
 on the pseudo label. In this approach, we
incorporate the predicted label information into the
instructions provided to the model. For instance,
we can use the instruction  Discuss the sports aspects
 of the article" to guide the model in generating
 text that is more relevant to the sports category.
Additionally, we propose two loss functions to
enhance the self-training algorithm with the augmented
 text as follows.Instruction  Discuss the sportsaspect of the article. Input (Observed) TextStarbucks  president, Orin Smith, plans to retire because he wants to focus on philanthropy, family and sports.
GPTConditionally Augmented TextSmith is a lifelong sports fan and has been an avid athlete throughout his life. He has long been a supporter of youth sports programs and has worked tirelessly to promote sports to promote health, fitness, and teamwork. Instruction/Prompt
Label prompt  for sportsTrue label  businessPseudo label  sportsConditional AugmentationDecision boundaryLabel prompt for businessMislabeled instanceCorrectly labeled instanceLabel prompt embeddingGenerated  sports-like  instanceConditional augmentationFigure 2  Augmented Text Generation Conditioned on Pseudo Labels  When a pseudo label is incorrect, it can
mislead the training process and decrease classiﬁcation performance. We generate augmented text conditioned on
the pseudo label, aiming to make the generated text closer to the majoriry members in the category of the pseudo
label. This approach aims to improve the quality of the generated instances for self-training.
Algorithm 1  Self-training with GPT model assisted in the loop
Require  Unlabeled texts X, label descriptions C, pre-trained GPT model g( ).
Initialization  Classiﬁerfθ( )initialized with pre-trained sentence encoder. Empty dictionary
GenDict to cache conditional generated text.
Input augmentation   For each observed text, generate Ksamples of augmented text from Pg(  x).
fort  1 Tself-training iterations do
Usefθ( )to generate pseudo-labels ˆy(eq.3) and soft-labels Q(eq.6) for texts with input
augmentation. Sample a balanced subset of pseudo-labeled training pairs of size Staccording
to prediction conﬁdence 
foreach training sample (x,ˆy)do
ifkey(x,ˆy) GenDict then
Fetch generated texts from GenDict  Use cached generated text
else
GenerateMsamples from Pg(  x,ˆy) Conditional augmentation 
Add generated texts to GenDict  Cached generated text 
Use sampled training pairs and the conditionally generated text to update the parameters θof
fθ( )with the objective function L Lg2l Lt2g.
2.4.1 Contrastive Learning for Conditionally
Augmented Text and Label Prompt
To alleviate the problem of erroneous label assignment,
 we use the conditional augmented text
(xc_aug Pg(  x,ˆy)) and the pseudo label prompt
as positive pairs.
Lg2l  N 
i 1L 
j 1Q(ˆyj xc_aug
i) logP(ˆyj xc_aug
i)
(11)
2.4.2 Contrastive Learning for Observed
Text and Augmented Text
For document representation, the contrastive pairs
are usually created by sampling spans of docu-ment (Izacard et al., 2022). In our case, the generative
 model naturally creates different views of data
and we use the contrastive loss between observed
text and generated text for optimization 
Lt2g  
i I 1
 A(i) 
 
xaug A(i)logexp(sim(fθ(xi),fθ(xaug))) 
j Iexp(sim(fθ(xi),fθ(xj))).
(12)
where Iis a training batch and A(i)denotes the set
of augmented texts belonging to the same pseudo
class of input xi.Algorithm 1 shows the self-training of GENCO
with generative model assisting in the self-training
loop. During training, we found that a balanced
sampling that keeps the same number ( Stfor iterationt)
 of training for each category is important
for the stability of self-training. Additionally, we
use a dictionary to cache the conditional generated
text to avoid repeated generation.
3 Experiments
3.1 Datasets and Experimental Settings
We conduct experiments on 4benchmark text classiﬁcation
 datasets  AG News, DBpedia, Yahoo
Answers and Amazon, with the statistics shown in
table 1. In the experiments, we initialize our sentence
 encoder with supervised SimCSE Robertabase
 model (Gao et al., 2021). The designed
prompts for enhanced label description is illustrated
 in table 2. For the generative model, we use
the Alpaca-7B (Taori et al., 2023) model, which
is an open source GPT model ﬁne-tuned with
human instructions (Touvron et al., 2023). The
prompts for instruction-based augmentation (table
 2) is the same as the one used in the Alpaca
model ﬁne-tuning. For the generation parameters,
we usedtemperature  0.8,top_p 0.95, and sampleK 5
 augmented texts for each instance with
min_length   64 andmax _length   128 . For
the self-training of sentence encoder model, we
usedbatch _size 3  C ( C is the number of categories),lr 1e-5,
 the max length is 128for AG
News and DBPedia and 192for Yahoo Answers
and Amazon. All the experiments are performed
on NVIDIA RTX A6000 gpus.
3.2 Baseline Methods
 PET (Schick and Schütze, 2020b) method formulates
 zero-shot text classiﬁcation as a cloze
test tasks, where a pretrained BERT (Devlin
et al., 2018) model is used to predict the output
 label(s) by completing a prompt such as
 This article is about _", which is concatenated
right after an input document.
 iPET (Schick and Schütze, 2020b) uses a selftraining
 algorithm to improve from the PET
model, where multiple generations of models
are trained by gradually increasing the number
 of training instances labeled by a model
trained in the previous generation.
 LOTClass (Meng et al., 2020) ﬁrst applies theBERT model to extract keywords related to
the label names from unlabeled texts and then
assigns pseudo labels for texts based on the
extracted keywords. LOTClass also applies a
self-training algorithm to further improve the
classiﬁcation performance.
 Other Baselines We include prompt-based
GPT model, a sentence-encoder based model
without any self-training and a self-training
baseline without any text augmentation. Additionally,
 a supervised learning baseline is
included for reference.
3.3 Main Results
In table 3, we present a comparison of the test accuracy
 of our model with other baselines on four
benchmark classiﬁcation datasets. Due to the large
number of text instances, it was not feasible to
perform augmentation using the entire dataset. Instead,
 our model was trained on a downsampled
dataset, with uniform sampling resulting in less
than 2% of the original data used (rows 7-8). Despite
 the reduced size of the dataset, we observed
that our proposed model GENCOstill outperforms
the other zero-shot baseline methods and is close
to supervise learning settings (row 1 and 7).
Compared with SOTA Methods Both LOTClass
and iPET use a self-training algorithm for zero-shot
classiﬁcation, but our adaptation of GPT model
can better enhance the self-training performance.
Speciﬁcally, LOTClass uses a BERT model to extract
 keywords for each category, and employs lexical
 matching between input text and the keywords
to assign pseudo labels. While the keywords can
be considered as an augmentation, it is less expressive
 than using a GPT model to generate coherent
human language as augmentation. Our proposed
method uses a sentence encoder with more expressive
 neural features, making it more effective than
using lexical-based features to assign pseudo labels.
 The iPET model requires training multiple
models and ensembling about 15 of them, which is
memory extensive. While ensembling can stabilize
self-training by reducing variance, it does not introduce
 new information about the input text. Our
approach uses a generative model to augment text
data during self-training, leading to improved performance
 and a more memory efﬁcient alternative.
Comparison with GPT   While GPT (row 6) has
demonstrated strong zero-shot performance in various
 tasks, it underperforms compared to ourDataset Classiﬁcation Type #Classes #Train #Test Avg Length
AG News News Topic 4 120,000 7,600 38
DBPedia Wikipedia Topic 14 560,000 70,000 50
Yahoo Answers Quetion Answering 10 1,400,000 60,000 70
Amazon Product Review Sentiment 2 3,600,000 400,000 78
Table 1  Statistics of datasets for multi-class text classiﬁcation.
Label Prompt
(1)Category  [label].
(2)It is about [label].
Instruction-based (Conditional) Augmentation
Below is an instruction that describes a task, paired
with an input that provides further context. Write a
response that appropriately completes the request.
### Instruction 
Elaborate the text in a few sentences.
(Discuss the [pseudo label] aspects of the article.)
### Input 
[text ]
### Response 
Table 2  The designed prompts for enhanced label
description and conditional augmentation based on
pseudo label.
sentence-encoder classiﬁer baseline (row 5), which
is ﬁne-tuned using contrastive learning on the Natural
 Language Inference dataset (Gao et al., 2021),
in the context of text classiﬁcation. Classiﬁcation
involves comparing instances, such as an article
being more likely to belong to the  sports" category
 when compared to articles in the  business"
category. Contrastive learning leverages this comparison
 and our contrastive self-training further
improves it.
In table 4, we present the impact of inference
time augmentation (assuming GPT is available at
test time) and self-training on the performance metric.
 To test inference time augmentation, we performed
 experiments on a downsampling of both
training and testing instances.
Inference Time Augmentation   Our results show
that inference time augmentation (rows with "IA")
leads to a performance gain of 1-2%, with a more
substantial improvement observed for AG News
and Yahoo Answers. This may be attributed to
the fact that AG News has an average text length
of only 38words, and the Yahoo Answers dataset
includes many answers with only one phrase. Inference
 time augmentation effectively enhances the
quality of shorter text inputs.
Self-Training   Our experiments demonstrate that
self-training improves the performance on alldatasets, even in the absence of augmented data
(rows 3-4). The DBpedia dataset exhibits an improvement
 of over 20%. Theoretically, self-training
enhances the separation of text, thereby making
the decision boundary lie in the low-density area,
which is critical for classiﬁcation. Our generativedriven
 approaches, with and without conditioning
on pseudo label, both lead to improved performance.
 However, the conditional augmentation
approach is more effective due to its ability to stabilize
 self-training.
3.4 Analysis of Input Augmentation
In this evaluation, we investigate the effectiveness
of input augmentation for zero-shot inference without
 training . We evaluate the performance of our
model on two datasets, namely AG News and Yahoo
 Answers, using two evaluation metrics  per
class F1 metric and ranking-based precision metric
according to prediction conﬁdence. The per class
F1 metric provides an insight into how well the
model performs on each individual class by balancing
 precision and recall. In the upper part of
ﬁgure 3, our ﬁndings indicate that using GPT augmented
 data leads to improved performance across
all categories for AG News and in eight out of ten
classes for Yahoo Answers.
In the lower part of ﬁgure 3, we employ a
ranking-based precision metric to assess the quality
of the most conﬁdent cases. Our results demonstrate
 that using augmented data yields better precision
 for the most conﬁdent cases. Notably, our
study on the Yahoo Answers dataset indicates that
the predictions are better calibrated with the use
of augmented data, implying that highly conﬁdent
samples exhibit better precision. Conversely, such
a trend was not observed in unaugmented data,
where the top 30 had higher accuracy than the top
10. Better calibration justiﬁes the sampling from
the most conﬁdent pools for self-training, making
it a more reliable method for improving model performance.ID
 Self-train Methods AG News DBpedia Yahoo Answers Amazon
1   Supervised 94.2 99.3 77.3 97.1
2 No PET 79.4 75.2 56.4 87.1
3 Yes iPET 86.0 85.2 68.2 95.2
4 Yes LOTClass 86.4 91.1   91.6
5 No Sentence-enc (SimCSE) 74.5 73.8 55.6 88.8
6 No GPT (Alpaca-7B) 71.2 65.5 52.1 87.2
7   Supervised-downsample* 93.8 98.7 76.5 97.0
8 Yes G ENCO-instruction* 89.2 98.3 68.7 95.4
Table 3  Comparison of classiﬁcation methods on benchmark datasets. The test accuracy of best performing zeroshot
 method is highlighted in bold phase. Row 7-8 (with *) use a downsampled dataset with 4k (3.4%), 11.2k
(2%), 15k ( 1%), 20k ( 1%) unlabeled training instances respectively.
ID Self-train Methods AG News DBpedia Yahoo Answers Amazon
# unlabeled train 4k (3.4%) 11.2k (2%) 15k (  1%) 20k (  1%)
# unlabeled test 7.6k 28k 20k 20k
1 No Sentence-enc 75.6 73.4 55.5 89.6
2 No Sentence-enc  IA 78.2 74.7 57.4 90.2
3 Yes Self-train 83.3 96.3 62.5 91.1
4 Yes Self-train  IA 83.9 96.8 64.3 91.3
5 Yes Self-train  TA 86.9 97.0 66.1 94.4
6 Yes Self-train  TA IA 87.1 97.1 67.2 94.6
7 Yes G ENCO 89.2 98.4 68.6 95.3
8 Yes G ENCO IA 89.7 98.5 70.2 95.4
Table 4  Results of ablation test for our proposed method, showing the effect of speciﬁc components on the
performance metric.  TA" represents input augmentation added during training, while  IA" represents input augmentation
 added during inference.
3.5 Analysis of Conditional Augmentation
In table 5, we present generated texts conditioned
on an sample in AG News dataset and the pseudo
labels. Each example is a cherry-picked sample
out of ﬁve random samples. The generated text expands
 on a speciﬁc aspect regarding the label while
retaining the original meaning of the observed text.
The left ﬁgure in ﬁgure 4 shows a heatmap of
the probability that a conditionally generated text
falls into the corresponding pseudo label category.
The highest probability occurs along the diagonal,
indicating that the conditionally generated data has
a closer meaning to the pseudo label. The right
ﬁgure in ﬁgure 4 shows the distribution of the generated
 text plotted using T-SNE. The embeddings
were obtained by our sentence encoder trained on
the100-th iteration. We selected two instances that
were misclassiﬁed as business and located close to
the decision boundary. The augmented text, conditioned
 on the business category, was found to be
closer to the label prompt embedding of the busi-ness category. This demonstrates the effectiveness
of our method to generate less confusing training
pairs away from the decision boundary.
4 Related Work
4.1 Knowledge Distillation from GPT
To leverage the language modeling power of large
model, previous works(Yoo et al., 2021  Ye et al.,
2022  Meng et al., 2022) use GPT to generate
text and label pairs to train a classiﬁer for downstream
 tasks. However, generating training data
from scratch can lead to low-quality data with unrelated
 or ambiguous generated text (Gao et al.,
2022). Our approach also generates text data, but it
is grounded in the context of the corpus of interest,
and further enhances the quality and semantic diversity
 of the generated text. This approach provides
a practical alternative to generation-based methods
for zero-shot text classiﬁcation.politicssports
business
technology0.600.650.700.750.800.850.900.95F1
72.486.6
70.473.8 74.488.7
74.975.9AG news
Original Text
Augmented Text
society sciencehealth
education computerssports
businessmusic familypolitics0.20.30.40.50.60.70.8F1
27.957.369.0
39.668.577.4
37.955.859.3
56.6
28.160.668.1
42.670.280.6
37.358.260.258.1Yahoo Answers
Observed Text
Augmented Text
top 10 top 20 top 30 top 50 top 70 top 100
Top@k ranked list by confidence0.880.900.920.940.960.981.00Precision95.0
92.5
90.890.591.4 91.397.5
96.2
94.2
92.092.592.8Original Text
Augmented Text
top 10 top 20 top 30 top 50 top 70 top 100
Top@k ranked list by confidence0.700.750.800.850.90Precision80.081.5 81.3
77.8
75.1
71.984.0 84.0
82.7
80.4
76.8
73.9Observed Text
Augmented TextFigure 3  Per class F1 (upper) and ranking-based precision (lower) for classiﬁcation performance with input augmentation.

Observed Text Starbucks Corp s president and chief executive, Orin Smith, said Tuesday he plans
to retire early next year because he wants to slow down and focus on philanthropy,
family and sports.
Politics The announcement of Orin Smith s retirement is likely to have political implications
for Starbucks Corporation... His replacement will have big shoes to ﬁll, and the
decision could have long-term effects on the company s policies and direction.
Sports ...Sports have always been a major part of Smith s life, as he was a college athlete
and later went on to become the CEO of Starbucks. It is clear that sports have had a
major inﬂuence on his life and he wants to make time for them in his retirement.
Business Starbucks Corp has seen great success under the leadership of Orin Smith, with the
company s stock price more than tripling since he became CEO in 2005. This success
has allowed him to retire early and . . .
Technology Orin Smith s plan to retire early next year is an example of how technology has
changed the way we work and live. By utilizing technology, Smith is able to take
advantage of the increasingly popular trend of  work-life balance" ...
Table 5  Examples of generated text conditioned on pseudo labels in the left column.
4.2 Sentence Encoder and Contrastive
Learning
Sentence encoders (Gao et al., 2021) model the
alignment of sentences by their similarity in the
embedding space, which can be adopted to the
zero-shot text classiﬁcation task (Hong et al., 2022 
Shi et al., 2022). Sentence encoders are typically
trained with contrastive learning, which optimizes
representations by pulling inputs with similar semantics
 closer in the embedding space and pushing
 inputs with different semantics further apart.Recent approaches (Izacard et al., 2022) sample
positive pairs from spans of the same document
and negative pairs from different documents in the
training batch. Our model applies GPT to generate
training pairs for contrastive learning.
4.3 Self-training Methods
Self-training methods (Van Engelen and Hoos,
2020) have been proposed as a semi-supervised
approach to improve a classiﬁer from unlabeled
datasets, where predictions on unlabeled data arePolitics (label) Sports (label)Business (label)T echnology (label)Politics (text)
Sports (text)
Business (text)
T echnology (text)0.52 0.065 0.25 0.17
0.17 0.55 0.12 0.16
0.15 0.11 0.5 0.24
0.05 0.06 0.21 0.68
0.10.20.30.40.50.6
30
 25
 20
 15
 10
 5
 0 5 1015
10
5
0510152025 Politics
Business
T echnology
Correctly labeled text
Mislabeled text as business
Conditionally augmented text
Label prompt embeddingFigure 4  The left ﬁgure shows a heatmap of the probability that a conditionally generated text falls into the
corresponding pseudo label category. The right ﬁgure shows the distribution of the generated text plotted using
T-SNE (sports category is out of scope).
used to ﬁne-tune the classiﬁer (Lee et al., 2013).
To improve the pseudo label quality, previous
work (Gera et al., 2022) use a small set of instances
with the most conﬁdent prediction for self-training.
LOTClass (Meng et al., 2020) improves the quality
of pseudo label by an expansion of label vocabulary
using BERT and iPET (Schick and Schütze, 2020b)
ensembles multiple version of model at different
stage of training. Our work improves self-training
by generating augmented text with GPT in the training
 loop.
4.4 Authors  Considerations and Limitations
The main goal of our paper is to promote the usage
of pre-trained GPT model (Alpaca-7B) to assist
in training of a smaller model (Roberta-SimCSE)
on zero-shot classiﬁcation tasks. We are aware
that there are rooms more experiments with selftraining
 algorithms, such as how the temperature
of our loss function can affect the training stability.
 Currently, we only use that as a theoretical
motivation of leveraging decision boundaries between
 classes, but tuning the temperature will be
additional work to do.
Another part is data efﬁciency. We have shown
that using GPT generated data can alleviate the data
hungry issue in deep learning models for text classiﬁcation,
 but more comprehensive study can be
done by applying the baseline models on the down-sampled dataset and analyzing the performance in
those scenarios.
Finally, we realize that more tricks and engineering
 designs are employed in our experiments and
we have released our code on github for reference.
5 Conclusion
In conclusion, our proposed approach, GenCo, effectively
 addresses the difﬁculties and limitations
of using pretrained large GPT models directly for
zero-shot text classiﬁcation. By leveraging the generative
 power of GPT models in a self-training
loop of a smaller, sentence encoder classiﬁer with
contrastive learning, GENCOoutperform state-ofthe-art
 methods on four benchmark datasets. Our
approach is particularly effective when limited indomain
 text data are available. The success of our
approach highlights the potential beneﬁts of incorporating
 the generative power of GPT models into
iterative self-training processes for smaller zeroshot
 classiﬁers. We hope that our work will inspire
further research in this direction, ultimately leading
to more efﬁcient and effective NLP models.
References
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, AmandaAskell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv 2005.14165 .
Olivier Chapelle and Alexander Zien. 2005. Semisupervised
 classiﬁcation by low density separation.
InInternational workshop on artiﬁcial intelligence
and statistics , pages 57 64. PMLR.
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert  Pre-training of deep
bidirectional transformers for language understanding.
 arXiv preprint arXiv 1810.04805 .
Jiahui Gao, Renjie Pi, Yong Lin, Hang Xu, Jiacheng
Ye, Zhiyong Wu, Xiaodan Liang, Zhenguo Li, and
Lingpeng Kong. 2022. Zerogen    Self-guided highquality
 data generation in efﬁcient zero-shot learning.
 arXiv preprint arXiv 2205.12679 .
Tianyu Gao, Xingcheng Yao, and Danqi Chen. 2021.
Simcse  Simple contrastive learning of sentence embeddings.
 arXiv preprint arXiv 2104.08821 .
Ariel Gera, Alon Halfon, Eyal Shnarch, Yotam Perlitz,
Liat Ein-Dor, and Noam Slonim. 2022. Zero-shot
text classiﬁcation with self-training. arXiv preprint
arXiv 2210.17541 .
Yves Grandvalet and Yoshua Bengio. 2004. Semisupervised
 learning by entropy minimization. Advances
 in neural information processing systems , 17.
Yves Grandvalet and Yoshua Bengio. 2006. Entropy
regularization.
Jimin Hong, Jungsoo Park, Daeyoung Kim, Seongjae
 Choi, Bokyung Son, and Jaewook Kang. 2022.
Tess  Zero-shot classiﬁcation via textual similarity
comparison with prompting using sentence encoder.
arXiv preprint arXiv 2212.10391 .
Gautier Izacard, Mathilde Caron, Lucas Hosseini, Sebastian
 Riedel, Piotr Bojanowski, Armand Joulin,
and Edouard Grave. 2022. Unsupervised dense information
 retrieval with contrastive learning.
Dong-Hyun Lee et al. 2013. Pseudo-label  The simple
and efﬁcient semi-supervised learning method for
deep neural networks. In Workshop on challenges
in representation learning, ICML , volume 3, page
896.
Yu Meng, Jiaxin Huang, Yu Zhang, and Jiawei Han.
2022. Generating training data with language
models  Towards zero-shot language understanding.
arXiv preprint arXiv 2202.04538 .
Yu Meng, Yunyi Zhang, Jiaxin Huang, Chenyan
Xiong, Heng Ji, Chao Zhang, and Jiawei Han. 2020.
Text classiﬁcation using label names only  A language
 model self-training approach. arXiv preprint
arXiv 2010.07245 .
Rafael Müller, Simon Kornblith, and Geoffrey E Hinton.
 2019. When does label smoothing help  Advances
 in neural information processing systems , 32.Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll
 L Wainwright, Pamela Mishkin, Chong Zhang,
Sandhini Agarwal, Katarina Slama, Alex Ray, et al.
2022. Training language models to follow instructions
 with human feedback. arXiv preprint
arXiv 2203.02155 .
Timo Schick and Hinrich Schütze. 2020a. Exploiting
 cloze questions for few shot text classiﬁcation
and natural language inference. arXiv preprint
arXiv 2001.07676 .
Timo Schick and Hinrich Schütze. 2020b. It s
not just size that matters  Small language models
 are also few-shot learners. arXiv preprint
arXiv 2009.07118 .
Weijia Shi, Julian Michael, Suchin Gururangan, and
Luke Zettlemoyer. 2022. Nearest neighbor zeroshot
 inference. arXiv preprint arXiv 2205.13792 .
Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann
Dubois, Xuechen Li, Carlos Guestrin, Percy
Liang, and Tatsunori B. Hashimoto. 2023. Stanford
 alpaca  An instruction-following llama
model. https //github.com/tatsu-lab/
stanford_alpaca .
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. Llama  Open and efﬁcient
 foundation language models. arXiv preprint
arXiv 2302.13971 .
Jesper E Van Engelen and Holger H Hoos. 2020. A survey
 on semi-supervised learning. Machine learning ,
109(2) 373 440.
Junyuan Xie, Ross Girshick, and Ali Farhadi. 2016.
Unsupervised deep embedding for clustering analysis.
 In International conference on machine learning,
 pages 478 487. PMLR.
Jiacheng Ye, Jiahui Gao, Qintong Li, Hang Xu,
Jiangtao Feng, Zhiyong Wu, Tao Yu, and Lingpeng
 Kong. 2022. Zerogen  Efﬁcient zero-shot
learning via dataset generation. arXiv preprint
arXiv 2202.07922 .
Kang Min Yoo, Dongju Park, Jaewook Kang, SangWoo
 Lee, and Woomyeong Park. 2021. Gpt3mix 
Leveraging large-scale language models for text augmentation.
 arXiv preprint arXiv 2104.08826 .A Proof of Theorems
Theorem 2. Consider a binary classiﬁcation problem with linearly separable labeled examples, when
0 τ   1, optimizingLt2l   N
i 1 L
j 1Q(ˆyj xi) logP(ˆyj xi)with gradient descend will enforce
the larger margin between classes.
Proof. We use dot product   ,  as implementation of similarity function. Let the embedding of instance i
bexi fθ(xi)and the embedding of label prompt jbeec fθ(pc),c {1,2}for binary classiﬁcation.
Then,
P(ˆy1 xi θ)  exp( xi,e1 )
exp( xi,e1 )   exp( xi,e2 ) 1
1   exp(  xi,e1 e2 )(13)
P(ˆy1 xi θ)   1 P(ˆy0 xi θ) (14)
(15)
Notation-wise, deﬁne di  xi,e1 e2 , then
P(ˆy1 xi θ)  1
1  e di(16)
P(ˆy1 xi θ)   1 1
1  e di(17)
(18)
In binary classiﬁcation, the margin is simply
margin  {
dixiis class 1
 dixiis class 2
For soft-label distribution Q,
Q(ˆy1 xi θ)  1
1  e di/τ(19)
Q(ˆy1 xi θ)   1 1
1  e di/τ(20)
(21)
ThenLt2lis derived as
Lt2l N 
i 1log(1  e di)  die di/τ
1  e di/τ(22)
Calculate the derivative of Lt2lw.r.tdi,
 Lt2l
 di  die di/τ
t(e di/τ  1)2 e di/τ e di
(e di/τ  1)(e di  1)(23)
For the ﬁrst part of equation 23, the sign depends on  di. For the second part, the sign depends on
e di/τ e di. When 0 τ   1,
{
e di/τ e di 0whendi 0
e di/τ e di 0whendi 0
Therefore, { Lt2l
 di 0whendi 0
 Lt2l
 di 0whendi 0(24)
One step of gradient descend optimizes dbyd 
i di η Lt2l
 di. From equation 24, we get the conclusion
that d 
i   di . In other words, the margin becomes larger after optimization, which ﬁnishes the proof.Theorem 3. Under the setting in Theorem 2, let mibe the margin of instance iand consider the constraint
mi Bfor alli, the classiﬁer converges to a max margin classiﬁer, as the bound Bgoes to inﬁnity.
Proof. Using the deﬁnition from Theorem 2,
Lt2l N 
i 1log(1  e di)  die di/τ
1  e di/τ(25)
The marginmifor instance ican be written as mi {
dixiis class 1
 dixiis class 2.
The equation 25 can be written as
Lt2l  
yi 0log(1  e mi)  mie mi/τ
1  e mi/τ  
yj 1log(1  emj) mjemj/τ
1  emj/τ(26)
Letm   min(mi)be the minimal margin, let N1andN2be the number of instances in class 1 and class
2 respectively which reaches the minimal margin. From the gradient analysis in equation 24, the examples
withmi m has loss lower bounded by that with minimal margin. Then
Lt2l N1(log(1  e m )  m e m /τ
1  e m /τ)  N2(log(1  em ) m em /τ
1  em /τ)
 O(log(1  e m )  m e m /τ
1  e m /τ)  O(log(1  em ) m em /τ
1  em /τ)(27)
WhenBapproaches , forN1part in equation 27,
log(1  e m )  m e m /τ
1  e m /τ e m  m e m /τ(28)
Whenm B,limm Be m  0, and limm Bm e m /τ  limm B1
1/τem /τ  0by L Hopital s
rule.
ForN2part in equation 27,
log(1  em ) m em /τ
1  em /τ log(1  em ) m (29)
Whenm B,limm Blog(1  em ) m   limm Blog(1  1
em )   0 .
Therefore, the loss is minimized when the minimal margin is maximized and thus the classiﬁer converges
to a max margin classiﬁer when Bgoes to inﬁnity.