[
  {
    "paperId": "10127fa44054eb985ede206113b96aac3a96fd80",
    "externalIds": {
      "DBLP": "conf/acl/ReiGTCLM23",
      "ACL": "2023.acl-short.94",
      "ArXiv": "2305.11806",
      "DOI": "10.48550/arXiv.2305.11806",
      "CorpusId": 258822885
    },
    "url": "https://www.semanticscholar.org/paper/10127fa44054eb985ede206113b96aac3a96fd80",
    "title": "The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics",
    "abstract": "Neural metrics for machine translation evaluation, such as COMET, exhibit significant improvements in their correlation with human judgments, as compared to traditional metrics based on lexical overlap, such as BLEU. Yet, neural metrics are, to a great extent, \u201cblack boxes\u201d returning a single sentence-level score without transparency about the decision-making process. In this work, we develop and compare several neural explainability methods and demonstrate their effectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our study reveals that these metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and with synthetically-generated critical translation errors. To ease future research, we release our code at: https://github.com/Unbabel/COMET/tree/explainable-metrics",
    "year": 2023,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.11806",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "1089-1105"
    },
    "authors": [
      {
        "authorId": "15631652",
        "name": "Ricardo Rei"
      },
      {
        "authorId": "144726818",
        "name": "Nuno M. Guerreiro"
      },
      {
        "authorId": "145188499",
        "name": "Marcos Vin\u00edcius Treviso"
      },
      {
        "authorId": "147294938",
        "name": "Lu\u00edsa Coheur"
      },
      {
        "authorId": "1784914",
        "name": "A. Lavie"
      },
      {
        "authorId": "2069905347",
        "name": "Andr\u00e9 Martins"
      }
    ]
  },
  {
    "paperId": "5579d38636b898c6a67ad67a16a80dd83be0f8d4",
    "externalIds": {
      "DBLP": "journals/corr/abs-2308-16795",
      "DOI": "10.48550/arXiv.2308.16795",
      "CorpusId": 265476314
    },
    "url": "https://www.semanticscholar.org/paper/5579d38636b898c6a67ad67a16a80dd83be0f8d4",
    "title": "Towards Multilingual Automatic Dialogue Evaluation",
    "abstract": null,
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.16795",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2308.16795",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2007581062",
        "name": "John Mendon\u00e7a"
      },
      {
        "authorId": "1784914",
        "name": "A. Lavie"
      },
      {
        "authorId": "2268558660",
        "name": "Isabel Trancoso"
      }
    ]
  },
  {
    "paperId": "5c920b2326282d93ad2ac3d1cb8f746bd7ab6f50",
    "externalIds": {
      "ACL": "2023.wmt-1.51",
      "DBLP": "conf/wmt/FreitagMLARTKBD23",
      "DOI": "10.18653/v1/2023.wmt-1.51",
      "CorpusId": 265607943
    },
    "url": "https://www.semanticscholar.org/paper/5c920b2326282d93ad2ac3d1cb8f746bd7ab6f50",
    "title": "Results of WMT23 Metrics Shared Task: Metrics Might Be Guilty but References Are Not Innocent",
    "abstract": "This paper presents the results of the WMT23 Metrics Shared Task. Participants submitting automatic MT evaluation metrics were asked to score the outputs of the translation systems competing in the WMT23 News Translation Task. All metrics were evaluated on how well they correlate with human ratings at the system and segment level. Similar to last year, we acquired our own human ratings based on expert-based human evaluation via Multidimensional Quality Metrics (MQM). Following last year\u2019s success, we also included a challenge set subtask, where participants had to create contrastive test suites for evaluating metrics\u2019 ability to capture and penalise specific types of translation errors. Furthermore, we improved our meta-evaluation procedure by considering fewer tasks and calculating a global score by weighted averaging across the various tasks. We present an extensive analysis on how well metrics perform on three language pairs: Chinese-English, Hebrew-English on the sentence-level and English-German on the paragraph-level. The results strongly confirm the results reported last year, that neural-based metrics are significantly better than non-neural metrics in their levels of correlation with human judgments. Further, we investigate the impact of bad reference translations on the correlations of metrics with human judgment. We present a novel approach for generating synthetic reference translations based on the collection of MT system outputs and their corresponding MQM ratings, which has the potential to mitigate bad reference issues we observed this year for some language pairs. Finally, we also study the connections between the magnitude of metric differences and their expected significance in human evaluation, which should help the community to better understand and adopt new metrics.",
    "year": 2023,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.wmt-1.51.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "578-628"
    },
    "authors": [
      {
        "authorId": "2247094600",
        "name": "Markus Freitag"
      },
      {
        "authorId": "2615454",
        "name": "Nitika Mathur"
      },
      {
        "authorId": "2269458984",
        "name": "Chi-kiu Lo"
      },
      {
        "authorId": "2837687",
        "name": "Eleftherios Avramidis"
      },
      {
        "authorId": "2269461024",
        "name": "Ricardo Rei"
      },
      {
        "authorId": "137174569",
        "name": "Brian Thompson"
      },
      {
        "authorId": "3452584",
        "name": "Tom Kocmi"
      },
      {
        "authorId": "2269457763",
        "name": "Frederic Blain"
      },
      {
        "authorId": "2258954267",
        "name": "Daniel Deutsch"
      },
      {
        "authorId": "2269462160",
        "name": "Craig Stewart"
      },
      {
        "authorId": "36259430",
        "name": "Chrysoula Zerva"
      },
      {
        "authorId": "2269457734",
        "name": "Sheila Castilho"
      },
      {
        "authorId": "1784914",
        "name": "A. Lavie"
      },
      {
        "authorId": "2149559186",
        "name": "George F. Foster"
      }
    ]
  },
  {
    "paperId": "9364720b2ab9ac67bc08e2b0b49aadded3d4e2e5",
    "externalIds": {
      "ArXiv": "2304.14553",
      "DBLP": "journals/corr/abs-2304-14553",
      "DOI": "10.48550/arXiv.2304.14553",
      "CorpusId": 258418023
    },
    "url": "https://www.semanticscholar.org/paper/9364720b2ab9ac67bc08e2b0b49aadded3d4e2e5",
    "title": "Appropriateness is all you need!",
    "abstract": "The strive to make AI applications\"safe\"has led to the development of safety-measures as the main or even sole normative requirement of their permissible use. Similar can be attested to the latest version of chatbots, such as chatGPT. In this view, if they are\"safe\", they are supposed to be permissible to deploy. This approach, which we call\"safety-normativity\", is rather limited in solving the emerging issues that chatGPT and other chatbots have caused thus far. In answering this limitation, in this paper we argue for limiting chatbots in the range of topics they can chat about according to the normative concept of appropriateness. We argue that rather than looking for\"safety\"in a chatbot's utterances to determine what they may and may not say, we ought to assess those utterances according to three forms of appropriateness: technical-discursive, social, and moral. We then spell out what requirements for chatbots follow from these forms of appropriateness to avoid the limits of previous accounts: positionality, acceptability, and value alignment (PAVA). With these in mind, we may be able to determine what a chatbot may and may not say. Lastly, one initial suggestion is to use challenge sets, specifically designed for appropriateness, as a validation method.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2304.14553",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2304.14553",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "1729525678",
        "name": "Hendrik Kempt"
      },
      {
        "authorId": "1784914",
        "name": "A. Lavie"
      },
      {
        "authorId": "1855863",
        "name": "S. Nagel"
      }
    ]
  },
  {
    "paperId": "9e8f125ef479af7e95ee5b8949b24e750c7df367",
    "externalIds": {
      "DBLP": "conf/sigdial/MendoncaLT23",
      "ACL": "2023.sigdial-1.11",
      "ArXiv": "2308.16795",
      "DOI": "10.18653/v1/2023.sigdial-1.11",
      "CorpusId": 261394693
    },
    "url": "https://www.semanticscholar.org/paper/9e8f125ef479af7e95ee5b8949b24e750c7df367",
    "title": "Towards Multilingual Automatic Open-Domain Dialogue Evaluation",
    "abstract": "The main limiting factor in the development of robust multilingual open-domain dialogue evaluation metrics is the lack of multilingual data and the limited availability of open-sourced multilingual dialogue systems. In this work, we propose a workaround for this lack of data by leveraging a strong multilingual pretrained encoder-based Language Model and augmenting existing English dialogue data using Machine Translation. We empirically show that the naive approach of finetuning a pretrained multilingual encoder model with translated data is insufficient to outperform the strong baseline of finetuning a multilingual model with only source data. Instead, the best approach consists in the careful curation of translated data using MT Quality Estimation metrics, excluding low quality translations that hinder its performance.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.sigdial-1.11.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "130-141"
    },
    "authors": [
      {
        "authorId": "2007581062",
        "name": "John Mendon\u00e7a"
      },
      {
        "authorId": "1784914",
        "name": "A. Lavie"
      },
      {
        "authorId": "1691021",
        "name": "I. Trancoso"
      }
    ]
  },
  {
    "paperId": "bcefc74b20649fd41ea05d87a3fa512d2559fc8d",
    "externalIds": {
      "ACL": "2023.dstc-1.16",
      "DBLP": "journals/corr/abs-2308-16797",
      "ArXiv": "2308.16797",
      "DOI": "10.48550/arXiv.2308.16797",
      "CorpusId": 261395306
    },
    "url": "https://www.semanticscholar.org/paper/bcefc74b20649fd41ea05d87a3fa512d2559fc8d",
    "title": "Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation",
    "abstract": "Despite significant research effort in the development of automatic dialogue evaluation metrics, little thought is given to evaluating dialogues other than in English. At the same time, ensuring metrics are invariant to semantically similar responses is also an overlooked topic. In order to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics, we propose a novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs). Empirical results show our framework achieves state of the art results in terms of mean Spearman correlation scores across several benchmarks and ranks first place on both the Robust and Multilingual tasks of the DSTC11 Track 4 \u201cAutomatic Evaluation Metrics for Open-Domain Dialogue Systems\u201d, proving the evaluation capabilities of prompted LLMs.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.16797",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2308.16797",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "8026343",
        "name": "J. Mendoncca"
      },
      {
        "authorId": "2058093218",
        "name": "Patr\u00edcia Pereira"
      },
      {
        "authorId": "123739597",
        "name": "Joao Paulo Carvalho"
      },
      {
        "authorId": "1784914",
        "name": "A. Lavie"
      },
      {
        "authorId": "1691021",
        "name": "I. Trancoso"
      }
    ]
  }
]