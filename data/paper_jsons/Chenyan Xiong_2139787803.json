[
  {
    "authorId": "2139787803",
    "authorName": "Chenyan Xiong",
    "authorUrl": "https://www.semanticscholar.org/author/2139787803",
    "authorHIndex": 7,
    "authorAffiliations": [],
    "authorPaperCount": 22,
    "authorCitationCount": 222,
    "paperId": "105759bdb5e3bddc1d3244df2eff2d5c997a1d84",
    "externalIds": {
      "DBLP": "journals/corr/abs-2307-00342",
      "ArXiv": "2307.00342",
      "DOI": "10.1162/tacl_a_00597",
      "CorpusId": 259316561
    },
    "url": "https://www.semanticscholar.org/paper/105759bdb5e3bddc1d3244df2eff2d5c997a1d84",
    "title": "Improving Multitask Retrieval by Promoting Task Specialization",
    "abstract": "Abstract In multitask retrieval, a single retriever is trained to retrieve relevant contexts for multiple tasks. Despite its practical appeal, naive multitask retrieval lags behind task-specific retrieval, in which a separate retriever is trained for each task. We show that it is possible to train a multitask retriever that outperforms task-specific retrievers by promoting task specialization. The main ingredients are: (1) a better choice of pretrained model\u2014one that is explicitly optimized for multitasking\u2014along with compatible prompting, and (2) a novel adaptive learning method that encourages each parameter to specialize in a particular task. The resulting multitask retriever is highly performant on the KILT benchmark. Upon analysis, we find that the model indeed learns parameters that are more task-specialized compared to naive multitasking without prompting or adaptive learning.1",
    "venue": "Transactions of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 36,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00597/2159628/tacl_a_00597.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "11",
      "pages": "1201-1212",
      "name": "Transactions of the Association for Computational Linguistics"
    },
    "authors": [
      {
        "authorId": "2107940644",
        "name": "Wenzheng Zhang"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "1714215",
        "name": "K. Stratos"
      },
      {
        "authorId": "2734525",
        "name": "Arnold Overwijk"
      }
    ]
  },
  {
    "authorId": "2139787803",
    "authorName": "Chenyan Xiong",
    "authorUrl": "https://www.semanticscholar.org/author/2139787803",
    "authorHIndex": 7,
    "authorAffiliations": [],
    "authorPaperCount": 22,
    "authorCitationCount": 222,
    "paperId": "1fe3a802efdc4f1a3e5c8187547f38a3ec65750b",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-05834",
      "ArXiv": "2305.05834",
      "DOI": "10.1145/3539618.3592080",
      "CorpusId": 258588167
    },
    "url": "https://www.semanticscholar.org/paper/1fe3a802efdc4f1a3e5c8187547f38a3ec65750b",
    "title": "Unsupervised Dense Retrieval Training with Web Anchors",
    "abstract": "In this work, we present an unsupervised retrieval method with contrastive learning on web anchors. The anchor text describes the content that is referenced from the linked page. This shows similarities to search queries that aim to retrieve pertinent information from relevant documents. Based on their commonalities, we train an unsupervised dense retriever, Anchor-DR, with a contrastive learning task that matches the anchor text and the linked document. To filter out uninformative anchors (such as \"homepage\" or other functional anchors), we present a novel filtering technique to only select anchors that contain similar types of information as search queries. Experiments show that Anchor-DR outperforms state-of-the-art methods on unsupervised dense retrieval by a large margin (e.g., by 5.3% NDCG@10 on MSMARCO). The gain of our method is especially significant for search and question answering tasks. Our analysis further reveals that the pattern of anchor-document pairs is similar to that of search query-document pairs. Code available at https://github.com/Veronicium/AnchorDR.",
    "venue": "Annual International ACM SIGIR Conference on Research and Development in Information Retrieval",
    "year": 2023,
    "referenceCount": 32,
    "citationCount": 2,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3539618.3592080",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "name": "Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval"
    },
    "authors": [
      {
        "authorId": "1892794261",
        "name": "Yiqing Xie"
      },
      {
        "authorId": "2111312954",
        "name": "X. Liu"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      }
    ]
  },
  {
    "authorId": "2139787803",
    "authorName": "Chenyan Xiong",
    "authorUrl": "https://www.semanticscholar.org/author/2139787803",
    "authorHIndex": 7,
    "authorAffiliations": [],
    "authorPaperCount": 22,
    "authorCitationCount": 222,
    "paperId": "24811cadf16519910f643b6084107164e6ca4219",
    "externalIds": {
      "DBLP": "conf/acl/YuXY023",
      "ACL": "2023.acl-long.136",
      "ArXiv": "2305.17331",
      "DOI": "10.48550/arXiv.2305.17331",
      "CorpusId": 258960666
    },
    "url": "https://www.semanticscholar.org/paper/24811cadf16519910f643b6084107164e6ca4219",
    "title": "Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In",
    "abstract": "Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM\u2019s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 52,
    "citationCount": 9,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.17331",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "2421-2436"
    },
    "authors": [
      {
        "authorId": "103985655",
        "name": "Zichun Yu"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "150311558",
        "name": "S. Yu"
      },
      {
        "authorId": "2109232579",
        "name": "Zhiyuan Liu"
      }
    ]
  },
  {
    "authorId": "2139787803",
    "authorName": "Chenyan Xiong",
    "authorUrl": "https://www.semanticscholar.org/author/2139787803",
    "authorHIndex": 7,
    "authorAffiliations": [],
    "authorPaperCount": 22,
    "authorCitationCount": 222,
    "paperId": "38aaf8a29df6deeff0bf64cc835d242a25b10337",
    "externalIds": {
      "ArXiv": "2305.12567",
      "DBLP": "journals/corr/abs-2305-12567",
      "ACL": "2023.acl-long.724",
      "DOI": "10.18653/v1/2023.acl-long.724",
      "CorpusId": 258833130
    },
    "url": "https://www.semanticscholar.org/paper/38aaf8a29df6deeff0bf64cc835d242a25b10337",
    "title": "Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers",
    "abstract": "This paper explores the effectiveness of model-generated signals in improving zero-shot generalization of text-to-text Transformers such as T5. We study various designs to pretrain T5 using an auxiliary model to construct more challenging token replacements for the main model to denoise. Key aspects under study include the decoding target, the location of the RTD head, and the masking pattern. Based on these studies, we develop a new model, METRO-T0, which is pretrained using the redesigned ELECTRA-Style pretraining strategies and then prompt-finetuned on a mixture of NLP tasks. METRO-T0 outperforms all similar-sized baselines on prompted NLP benchmarks, such as _T0 Eval_ and MMLU, and rivals the state-of-the-art T0-11B model with only **8%** of its parameters. Our analysis on model\u2019s neural activation and parameter sensitivity reveals that the effectiveness of METRO-T0 stems from more balanced contribution of parameters and better utilization of their capacity. The code and model checkpoints are available at [https://github.com/gonglinyuan/metro_t0](https://github.com/gonglinyuan/metro_t0).",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 32,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.acl-long.724.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "12933-12950"
    },
    "authors": [
      {
        "authorId": "32816503",
        "name": "Linyuan Gong"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "46522098",
        "name": "Xiaodong Liu"
      },
      {
        "authorId": "34765717",
        "name": "Payal Bajaj"
      },
      {
        "authorId": "1892794261",
        "name": "Yiqing Xie"
      },
      {
        "authorId": "2172244319",
        "name": "Alvin Cheung"
      },
      {
        "authorId": "48441311",
        "name": "Jianfeng Gao"
      },
      {
        "authorId": "50706785",
        "name": "Xia Song"
      }
    ]
  },
  {
    "authorId": "2139787803",
    "authorName": "Chenyan Xiong",
    "authorUrl": "https://www.semanticscholar.org/author/2139787803",
    "authorHIndex": 7,
    "authorAffiliations": [],
    "authorPaperCount": 22,
    "authorCitationCount": 222,
    "paperId": "a57b90cfc2eab46b773e65240d4ff910f05f989e",
    "externalIds": {
      "DBLP": "conf/acl/LiLXY00023",
      "ArXiv": "2305.19912",
      "DOI": "10.48550/arXiv.2305.19912",
      "CorpusId": 258987900
    },
    "url": "https://www.semanticscholar.org/paper/a57b90cfc2eab46b773e65240d4ff910f05f989e",
    "title": "Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data",
    "abstract": "This paper presents Structure Aware Dense Retrieval (SANTA) model, which encodes user queries and structured data in one universal embedding space for retrieving structured data. SANTA proposes two pretraining methods to make language models structure-aware and learn effective representations for structured data: 1) Structured Data Alignment, which utilizes the natural alignment relations between structured data and unstructured data for structure-aware pretraining. It contrastively trains language models to represent multi-modal text data and teaches models to distinguish matched structured data for unstructured texts. 2) Masked Entity Prediction, which designs an entity-oriented mask strategy and asks language models to fill in the masked entities. Our experiments show that SANTA achieves state-of-the-art on code search and product search and conducts convincing results in the zero-shot setting. SANTA learns tailored representations for multi-modal text data by aligning structured and unstructured data pairs and capturing structural semantics by masking and predicting entities in the structured data. All codes are available at https://github.com/OpenMatch/OpenMatch.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 46,
    "citationCount": 1,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.19912",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.19912",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2109419838",
        "name": "Xinze Li"
      },
      {
        "authorId": "49047064",
        "name": "Zhenghao Liu"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "12493038",
        "name": "Shi Yu"
      },
      {
        "authorId": "144955064",
        "name": "Yu Gu"
      },
      {
        "authorId": "2109232579",
        "name": "Zhiyuan Liu"
      },
      {
        "authorId": "2110954235",
        "name": "Ge Yu"
      }
    ]
  },
  {
    "authorId": "2139787803",
    "authorName": "Chenyan Xiong",
    "authorUrl": "https://www.semanticscholar.org/author/2139787803",
    "authorHIndex": 7,
    "authorAffiliations": [],
    "authorPaperCount": 22,
    "authorCitationCount": 222,
    "paperId": "b9e8b62bcc019f47a0a015568f70039b3b7c1196",
    "externalIds": {
      "ArXiv": "2310.05155",
      "DBLP": "journals/corr/abs-2310-05155",
      "DOI": "10.48550/arXiv.2310.05155",
      "CorpusId": 263829000
    },
    "url": "https://www.semanticscholar.org/paper/b9e8b62bcc019f47a0a015568f70039b3b7c1196",
    "title": "Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model",
    "abstract": "Large Language Models (LLMs) have demonstrated remarkable progress in utilizing tools, but their closed-source nature and high inference costs pose limitations on their adaptability, necessitating a valid method that leverages smaller, open-sourced models. In this paper, we introduce Toolink, a comprehensive framework that performs task-solving by first creating a toolkit and then integrating the planning and calling of tools through a chain-of-solving (CoS) approach. We first validate the efficacy of Toolink in harnessing the model's creativity and CoS ability on ChatGPT. Subsequently, we curate CoS-GPT, a chain-of-solving dataset designed for tool-using, and finetune the LLaMA-7B model. It results in LLaMA-CoS, a powerful open-source model with advanced tool-planning and tool-calling capabilities. Evaluation on diverse tasks from BIG-bench demonstrates its CoS ability matches that of ChatGPT while its performance surpasses the chain-of-thought approach. Further studies highlight the generalization of LLaMA-CoS to unseen tasks and showcase its capability in using toolkits not explicitly tailored for the target task, affirming its robustness in real-world scenarios. All codes and data are released.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 39,
    "citationCount": 3,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.05155",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2310.05155",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2214580084",
        "name": "Cheng Qian"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "49047064",
        "name": "Zhenghao Liu"
      },
      {
        "authorId": "2109232579",
        "name": "Zhiyuan Liu"
      }
    ]
  },
  {
    "authorId": "2139787803",
    "authorName": "Chenyan Xiong",
    "authorUrl": "https://www.semanticscholar.org/author/2139787803",
    "authorHIndex": 7,
    "authorAffiliations": [],
    "authorPaperCount": 22,
    "authorCitationCount": 222,
    "paperId": "e0401ca2d4fd6d0ed55130a4a24b33ed90111479",
    "externalIds": {
      "DBLP": "conf/emnlp/GeXRO0023",
      "ArXiv": "2302.03754",
      "DOI": "10.48550/arXiv.2302.03754",
      "CorpusId": 256662717
    },
    "url": "https://www.semanticscholar.org/paper/e0401ca2d4fd6d0ed55130a4a24b33ed90111479",
    "title": "Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories",
    "abstract": "In this paper we improve the zero-shot generalization ability of language models via Mixture-Of-Memory Augmentation (MoMA), a mechanism that retrieves augmentation documents from multiple information corpora (\"external memories\"), with the option to\"plug in\"new memory at inference time. We develop a joint learning mechanism that trains the augmentation component with latent labels derived from the end retrieval task, paired with hard negatives from the memory mixture. We instantiate the model in a zero-shot dense retrieval setting by augmenting a strong T5-based retriever with MoMA. Our model, MoMA, obtains strong zero-shot retrieval accuracy on the eighteen tasks included in the standard BEIR benchmark. It outperforms systems that seek generalization from increased model parameters and computation steps. Our analysis further illustrates the necessity of augmenting with mixture-of-memory for robust generalization, the benefits of augmentation learning, and how MoMA utilizes the plug-in memory at inference time without changing its parameters. We plan to open source our code.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 72,
    "citationCount": 4,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.03754",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2302.03754",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "148048326",
        "name": "Suyu Ge"
      },
      {
        "authorId": "2139787803",
        "name": "Chenyan Xiong"
      },
      {
        "authorId": "41016119",
        "name": "Corby Rosset"
      },
      {
        "authorId": "2734525",
        "name": "Arnold Overwijk"
      },
      {
        "authorId": "2111759643",
        "name": "Jiawei Han"
      },
      {
        "authorId": "144609235",
        "name": "Paul N. Bennett"
      }
    ]
  }
]