[
  {
    "profName": "Alexander Hauptmann",
    "authorId": "145788702",
    "authorName": "A. Hauptmann",
    "authorUrl": "https://www.semanticscholar.org/author/145788702",
    "authorHIndex": 27,
    "authorAffiliations": [],
    "authorPaperCount": 59,
    "authorCitationCount": 2295,
    "paperId": "2107b867cb8f8afa30a9a940288d7c8b657f8aa5",
    "externalIds": {
      "ACL": "2023.acl-short.127",
      "DBLP": "conf/acl/WenH23",
      "DOI": "10.18653/v1/2023.acl-short.127",
      "CorpusId": 259370856
    },
    "url": "https://www.semanticscholar.org/paper/2107b867cb8f8afa30a9a940288d7c8b657f8aa5",
    "title": "Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation",
    "abstract": "Zero-shot and few-shot stance detection identify the polarity of text with regard to a certain target when we have only limited or no training resources for the target. Previous work generally formulates the problem into a classification setting, ignoring the potential use of label text. In this paper, we instead utilize a conditional generation framework and formulate the problem as denoising from partially-filled templates, which can better utilize the semantics among input, label, and target texts. We further propose to jointly train an auxiliary task, target prediction, and to incorporate manually constructed incorrect samples with unlikelihood training to improve the representations for both target and label texts. We also verify the effectiveness of target-related Wikipedia knowledge with the generation framework. Experiments show that our proposed method significantly outperforms several strong baselines on VAST, and achieves new state-of-the-art performance.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 31,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.acl-short.127.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "1491-1499"
    },
    "authors": [
      {
        "authorId": "4428136",
        "name": "Haoyang Wen"
      },
      {
        "authorId": "145788702",
        "name": "A. Hauptmann"
      }
    ]
  },
  {
    "profName": "Alexander Hauptmann",
    "authorId": "145788702",
    "authorName": "A. Hauptmann",
    "authorUrl": "https://www.semanticscholar.org/author/145788702",
    "authorHIndex": 27,
    "authorAffiliations": [],
    "authorPaperCount": 59,
    "authorCitationCount": 2295,
    "paperId": "376f494126d1ea4f571ea0263c43ac2b6331800a",
    "externalIds": {
      "ArXiv": "2306.17842",
      "DBLP": "journals/corr/abs-2306-17842",
      "DOI": "10.48550/arXiv.2306.17842",
      "CorpusId": 259308960
    },
    "url": "https://www.semanticscholar.org/paper/376f494126d1ea4f571ea0263c43ac2b6331800a",
    "title": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs",
    "abstract": "In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 52,
    "citationCount": 9,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.17842",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.17842",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "8547960",
        "name": "Lijun Yu"
      },
      {
        "authorId": "2109716647",
        "name": "Yong Cheng"
      },
      {
        "authorId": "1390877035",
        "name": "Zhiruo Wang"
      },
      {
        "authorId": "2107989922",
        "name": "Vivek Kumar"
      },
      {
        "authorId": "3153147",
        "name": "Wolfgang Macherey"
      },
      {
        "authorId": "2145438541",
        "name": "Yanping Huang"
      },
      {
        "authorId": "144711958",
        "name": "David A. Ross"
      },
      {
        "authorId": "145955800",
        "name": "Irfan Essa"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "152790163",
        "name": "Ming Yang"
      },
      {
        "authorId": "1702318",
        "name": "K. Murphy"
      },
      {
        "authorId": "145788702",
        "name": "A. Hauptmann"
      },
      {
        "authorId": "39978626",
        "name": "Lu Jiang"
      }
    ]
  },
  {
    "profName": "Alexander Hauptmann",
    "authorId": "145788702",
    "authorName": "A. Hauptmann",
    "authorUrl": "https://www.semanticscholar.org/author/145788702",
    "authorHIndex": 27,
    "authorAffiliations": [],
    "authorPaperCount": 59,
    "authorCitationCount": 2295,
    "paperId": "405e3910e06c9efe7e660b8697bcb4bab4e92f48",
    "externalIds": {
      "DBLP": "journals/corr/abs-2303-18177",
      "ArXiv": "2303.18177",
      "DOI": "10.1109/CVPR52729.2023.00153",
      "CorpusId": 257900902
    },
    "url": "https://www.semanticscholar.org/paper/405e3910e06c9efe7e660b8697bcb4bab4e92f48",
    "title": "STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition",
    "abstract": "We study the problem of human action recognition using motion capture (MoCap) sequences. Unlike existing techniques that take multiple manual steps to derive standard-ized skeleton representations as model input, we propose a novel Spatial-Temporal Mesh Transformer (STMT) to directly model the mesh sequences. The model uses a hierarchical transformer with intra-frame off-set attention and inter-frame self-attention. The attention mechanism allows the model to freely attend between any two vertex patches to learn nonlocal relationships in the spatial-temporal domain. Masked vertex modeling and future frame prediction are used as two self-supervised tasks to fully activate the bi-directional and auto-regressive attention in our hierarchical transformer. The proposed method achieves state-of-the-art performance compared to skeleton-based and point-cloud-based models on common MoCap benchmarks. Code is available at https://github.com/zgzxy001/STMT.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "referenceCount": 69,
    "citationCount": 4,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2303.18177",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "1526-1536",
      "name": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    "authors": [
      {
        "authorId": "2197848856",
        "name": "Xiaoyu Zhu"
      },
      {
        "authorId": "2319973",
        "name": "Po-Yao (Bernie) Huang"
      },
      {
        "authorId": "118150711",
        "name": "Junwei Liang"
      },
      {
        "authorId": "2147315384",
        "name": "Celso M. de Melo"
      },
      {
        "authorId": "145788702",
        "name": "A. Hauptmann"
      }
    ]
  },
  {
    "profName": "Alexander Hauptmann",
    "authorId": "145788702",
    "authorName": "A. Hauptmann",
    "authorUrl": "https://www.semanticscholar.org/author/145788702",
    "authorHIndex": 27,
    "authorAffiliations": [],
    "authorPaperCount": 59,
    "authorCitationCount": 2295,
    "paperId": "8ccda6de0223bcd897d5dc0efc8f33222a899d0d",
    "externalIds": {
      "ArXiv": "2306.08937",
      "DBLP": "conf/emnlp/YuMSCHD023",
      "DOI": "10.18653/v1/2023.emnlp-industry.66",
      "CorpusId": 259165255
    },
    "url": "https://www.semanticscholar.org/paper/8ccda6de0223bcd897d5dc0efc8f33222a899d0d",
    "title": "DocumentNet: Bridging the Data Gap in Document Pre-training",
    "abstract": "Document understanding tasks, in particular, Visually-rich Document Entity Retrieval (VDER), have gained significant attention in recent years thanks to their broad applications in enterprise AI. However, publicly available data have been scarce for these tasks due to strict privacy constraints and high annotation costs. To make things worse, the non-overlapping entity spaces from different datasets hinder the knowledge transfer between document types. In this paper, we propose a method to collect massive-scale and weakly labeled data from the web to benefit the training of VDER models. The collected dataset, named DocumentNet, does not depend on specific document types or entity sets, making it universally applicable to all VDER tasks. The current DocumentNet consists of 30M documents spanning nearly 400 document types organized in a four-level ontology. Experiments on a set of broadly adopted VDER tasks show significant improvements when DocumentNet is incorporated into the pre-training for both classic and few-shot learning settings. With the recent emergence of large language models (LLMs), DocumentNet provides a large data source to extend their multi-modal capabilities for VDER.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 43,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.emnlp-industry.66.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "707-722"
    },
    "authors": [
      {
        "authorId": "8547960",
        "name": "Lijun Yu"
      },
      {
        "authorId": "2220100953",
        "name": "Jin Miao"
      },
      {
        "authorId": "2112561470",
        "name": "Xiaoyu Sun"
      },
      {
        "authorId": "2108327780",
        "name": "Jiayi Chen"
      },
      {
        "authorId": "145788702",
        "name": "A. Hauptmann"
      },
      {
        "authorId": "2791430",
        "name": "H. Dai"
      },
      {
        "authorId": "2149192376",
        "name": "Wei Wei"
      }
    ]
  }
]