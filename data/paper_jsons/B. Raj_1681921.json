[
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "078f86c6a691806cc71bbef1e734f75690db0ffc",
    "externalIds": {
      "DBLP": "journals/corr/abs-2304-02135",
      "ArXiv": "2304.02135",
      "DOI": "10.1109/CVPR52729.2023.01914",
      "CorpusId": 257952618
    },
    "url": "https://www.semanticscholar.org/paper/078f86c6a691806cc71bbef1e734f75690db0ffc",
    "title": "FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding",
    "abstract": "Although Domain Adaptation in Semantic Scene Segmentation has shown impressive improvement in recent years, the fairness concerns in the domain adaptation have yet to be well defined and addressed. In addition, fairness is one of the most critical aspects when deploying the segmentation models into human-related real-world applications, e.g., autonomous driving, as any unfair predictions could influence human safety. In this paper, we propose a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation. In particular, from the proposed formulated fairness objective, a new adaptation framework will be introduced based on the fair treatment of class distributions. Moreover, to generally model the context of structural dependency, a new conditional structural constraint is introduced to impose the consistency of predicted segmentation. Thanks to the proposed Conditional Structure Network, the self-attention mechanism has sufficiently modeled the structural information of segmentation. Through the ablation studies, the proposed method has shown the performance improvement of the segmentation models and promoted fairness in the model predictions. The experimental results on the two standard benchmarks, i.e., SYNTHIA $\\rightarrow$ Cityscapes and GTA5 $\\rightarrow$ Cityscapes, have shown that our method achieved State-of-the-Art (SOTA) performance11The implementation of FREDOM is available at https://github.com/uark-cviu/FREDOM",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "referenceCount": 55,
    "citationCount": 8,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2304.02135",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "19988-19997",
      "name": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    "authors": [
      {
        "authorId": "35659935",
        "name": "Thanh-Dat Truong"
      },
      {
        "authorId": "144556913",
        "name": "Ngan T. H. Le"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      },
      {
        "authorId": "145863239",
        "name": "J. Cothren"
      },
      {
        "authorId": "1769788",
        "name": "Khoa Luu"
      }
    ],
    "tldr": "This paper proposes a novel Fairness Domain Adaptation (FREDOM) approach to semantic scene segmentation, where a new adaptation framework will be introduced based on the fair treatment of class distributions to generally model the context of structural dependency."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "0a8d38686b18f28aae1222529e6b9e8a60cab1c2",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-09613",
      "ArXiv": "2306.09613",
      "DOI": "10.48550/arXiv.2306.09613",
      "CorpusId": 259187821
    },
    "url": "https://www.semanticscholar.org/paper/0a8d38686b18f28aae1222529e6b9e8a60cab1c2",
    "title": "UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation",
    "abstract": "Multiple Object Tracking (MOT) aims to find bounding boxes and identities of targeted objects in consecutive video frames. While fully-supervised MOT methods have achieved high accuracy on existing datasets, they cannot generalize well on a newly obtained dataset or a new unseen domain. In this work, we first address the MOT problem from the cross-domain point of view, imitating the process of new data acquisition in practice. Then, a new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects. It can also learn and update itself from the target data feedback. The intensive experiments are designed on four challenging settings, including MOTSynth to MOT17, MOT17 to MOT20, MOT17 to VisDrone, and MOT17 to DanceTrack. We then prove the adaptability of the proposed self-supervised learning strategy. The experiments also show superior performance on tracking metrics MOTA and IDF1, compared to fully supervised, unsupervised, and self-supervised state-of-the-art methods.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 62,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.09613",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.09613",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2468571",
        "name": "Pha Nguyen"
      },
      {
        "authorId": "2687827",
        "name": "Kha Gia Quach"
      },
      {
        "authorId": "1701970",
        "name": "J. Gauch"
      },
      {
        "authorId": "1740261",
        "name": "S. Khan"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      },
      {
        "authorId": "1769788",
        "name": "Khoa Luu"
      }
    ],
    "tldr": "A new cross-domain MOT adaptation from existing datasets is proposed without any pre-defined human knowledge in understanding and modeling objects, and it can also learn and update itself from the target data feedback."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "100da279ee981960884a12dfc5a0697c24ed315a",
    "externalIds": {
      "DBLP": "journals/corr/abs-2301-10921",
      "ArXiv": "2301.10921",
      "DOI": "10.48550/arXiv.2301.10921",
      "CorpusId": 256274785
    },
    "url": "https://www.semanticscholar.org/paper/100da279ee981960884a12dfc5a0697c24ed315a",
    "title": "SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning",
    "abstract": "The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 54,
    "citationCount": 31,
    "influentialCitationCount": 2,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.10921",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2301.10921",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2051536212",
        "name": "Hao Chen"
      },
      {
        "authorId": "26151496",
        "name": "R. Tao"
      },
      {
        "authorId": "144356331",
        "name": "Yue Fan"
      },
      {
        "authorId": "2108024273",
        "name": "Yidong Wang"
      },
      {
        "authorId": "1519290245",
        "name": "Jindong Wang"
      },
      {
        "authorId": "48920094",
        "name": "B. Schiele"
      },
      {
        "authorId": "1576441343",
        "name": "Xingxu Xie"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      },
      {
        "authorId": "1794486",
        "name": "M. Savvides"
      }
    ],
    "tldr": "This paper revisits the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrates the inherent quantity-quality trade-off problem of pseudo-labels with thresholding, which may prohibit learning."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "11c50900f50036fb3247be7c83849a8774a4ba60",
    "externalIds": {
      "ArXiv": "2308.03956",
      "DBLP": "journals/corr/abs-2308-03956",
      "DOI": "10.48550/arXiv.2308.03956",
      "CorpusId": 260704305
    },
    "url": "https://www.semanticscholar.org/paper/11c50900f50036fb3247be7c83849a8774a4ba60",
    "title": "Fixed Inter-Neuron Covariability Induces Adversarial Robustness",
    "abstract": "The vulnerability to adversarial perturbations is a major flaw of Deep Neural Networks (DNNs) that raises question about their reliability when in real-world scenarios. On the other hand, human perception, which DNNs are supposed to emulate, is highly robust to such perturbations, indicating that there may be certain features of the human perception that make it robust but are not represented in the current class of DNNs. One such feature is that the activity of biological neurons is correlated and the structure of this correlation tends to be rather rigid over long spans of times, even if it hampers performance and learning. We hypothesize that integrating such constraints on the activations of a DNN would improve its adversarial robustness, and, to test this hypothesis, we have developed the Self-Consistent Activation (SCA) layer, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern. When evaluated on image and sound recognition tasks, the models with a SCA layer achieved high accuracy, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks \\textit{without being trained on adversarially perturbed data",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 33,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.03956",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2308.03956",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "145897481",
        "name": "Muhammad A Shah"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "The SCA layer is developed, which comprises of neurons whose activations are consistent with each other, as they conform to a fixed, but learned, covariability pattern, and exhibited significantly greater robustness than multi-layer perceptron models to state-of-the-art Auto-PGD adversarial attacks."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "22c9eb4868c5cabb26d132e0a160b9a093579f08",
    "externalIds": {
      "DBLP": "journals/aim/GodeBRY23",
      "DOI": "10.1002/aaai.12104",
      "CorpusId": 260728762
    },
    "url": "https://www.semanticscholar.org/paper/22c9eb4868c5cabb26d132e0a160b9a093579f08",
    "title": "Understanding political polarization using language models: A dataset and method",
    "abstract": "Our paper aims to analyze political polarization in US political system using language models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates' views on the economy, healthcare, education, and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model\u2010based method that helps analyze how polarized a candidate is. Our data are divided into two parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, and so forth. We further split this data into four phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization, we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer\u2010based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background. The code and data for the project will be available here: \u201chttps://github.com/samirangode/Understanding_Polarization\u201d",
    "venue": "The AI Magazine",
    "year": 2023,
    "referenceCount": 7,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/aaai.12104",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "44",
      "pages": "248-254",
      "name": "AI Mag."
    },
    "authors": [
      {
        "authorId": "2199184565",
        "name": "Samiran Gode"
      },
      {
        "authorId": "2199182939",
        "name": "Supreeth Bare"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      },
      {
        "authorId": "1576015238",
        "name": "H. Yoo"
      }
    ],
    "tldr": "The main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a language model\u2010based method that helps analyze how polarized a candidate is."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "35a8802facb4441787017ac5c630a8fa0f2413bd",
    "externalIds": {
      "PubMedCentral": "10339019",
      "DOI": "10.1016/j.heliyon.2023.e17865",
      "CorpusId": 259764473,
      "PubMed": "37456023"
    },
    "url": "https://www.semanticscholar.org/paper/35a8802facb4441787017ac5c630a8fa0f2413bd",
    "title": "Prolonged school closure during the pandemic time in successive waves of COVID-19- vulnerability of children to sexual abuses \u2013 A case study in Tamil Nadu, India",
    "abstract": null,
    "venue": "Heliyon",
    "year": 2023,
    "referenceCount": 43,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://www.cell.com/article/S2405844023050739/pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Medicine"
    ],
    "journal": {
      "volume": "9",
      "name": "Heliyon"
    },
    "authors": [
      {
        "authorId": "2142828053",
        "name": "Kandaswamy Paramasivan"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      },
      {
        "authorId": "2222824729",
        "name": "Nandan Sudarasanam"
      },
      {
        "authorId": "2102113776",
        "name": "R. Subburaj"
      }
    ],
    "tldr": "Considering that the median delay in filing CSA complaints was above 30 days in the mild and post-intervention periods, the upsurge of cases in the more relaxed phases indicates increased occurrences of CSA during strict lockdowns."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "3bd320ddb25886417ae90011b00f13f5d558097b",
    "externalIds": {
      "ArXiv": "2307.08217",
      "DBLP": "journals/corr/abs-2307-08217",
      "DOI": "10.48550/arXiv.2307.08217",
      "CorpusId": 259936797
    },
    "url": "https://www.semanticscholar.org/paper/3bd320ddb25886417ae90011b00f13f5d558097b",
    "title": "BASS: Block-wise Adaptation for Speech Summarization",
    "abstract": "End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.",
    "venue": "Interspeech",
    "year": 2023,
    "referenceCount": 28,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.08217",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "volume": "abs/2307.08217",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "145521253",
        "name": "Roshan Sharma"
      },
      {
        "authorId": "2163585699",
        "name": "Kenneth Zheng"
      },
      {
        "authorId": "72401599",
        "name": "Siddhant Arora"
      },
      {
        "authorId": "1746678",
        "name": "Shinji Watanabe"
      },
      {
        "authorId": "153915824",
        "name": "Rita Singh"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "4e6a004e4f9a3b489004a2efb5b25b0bcd0f48b5",
    "externalIds": {
      "DBLP": "journals/corr/abs-2301-00891",
      "ArXiv": "2301.00891",
      "DOI": "10.48550/arXiv.2301.00891",
      "CorpusId": 255393829
    },
    "url": "https://www.semanticscholar.org/paper/4e6a004e4f9a3b489004a2efb5b25b0bcd0f48b5",
    "title": "Understanding Political Polarisation using Language Models: A dataset and method",
    "abstract": "Our paper aims to analyze political polarization in US political system using Language Models, and thereby help candidates make an informed decision. The availability of this information will help voters understand their candidates views on the economy, healthcare, education and other social issues. Our main contributions are a dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is. Our data is divided into 2 parts, background information and political information about a candidate, since our hypothesis is that the political views of a candidate should be based on reason and be independent of factors such as birthplace, alma mater, etc. We further split this data into 4 phases chronologically, to help understand if and how the polarization amongst candidates changes. This data has been cleaned to remove biases. To understand the polarization we begin by showing results from some classical language models in Word2Vec and Doc2Vec. And then use more powerful techniques like the Longformer, a transformer based encoder, to assimilate more information and find the nearest neighbors of each candidate based on their political view and their background.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 20,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.00891",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2301.00891",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2199184565",
        "name": "Samiran Gode"
      },
      {
        "authorId": "2199182939",
        "name": "Supreeth Bare"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      },
      {
        "authorId": "1576015238",
        "name": "H. Yoo"
      }
    ],
    "tldr": "A dataset extracted from Wikipedia that spans the past 120 years and a Language model based method that helps analyze how polarized a candidate is are used to understand the polarization."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "593a603354c09d151440ae044de1d80324a2ab01",
    "externalIds": {
      "DBLP": "conf/icassp/ShahTCZGR23",
      "DOI": "10.1109/icassp49357.2023.10097025",
      "CorpusId": 258540002
    },
    "url": "https://www.semanticscholar.org/paper/593a603354c09d151440ae044de1d80324a2ab01",
    "title": "An Approach to Ontological Learning from Weak Labels",
    "abstract": "Ontologies encompass a formal representation of knowledge through the definition of concepts or properties of a domain, and the relationships between those concepts. In this work, we seek to investigate whether using this ontological information will improve learning from weakly labeled data, which are easier to collect since it requires only the presence or absence of an event to be known. We use the AudioSet ontology and dataset, which contains audio clips weakly labeled with the ontology concepts and the ontology providing the \"Is A\" relations between the concepts. We first re-implemented the model proposed by [1] with modifications to fit the multi-label scenario and then expand on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts. We find that the baseline Twin Neural Network (TNN) does not perform better by incorporating ontology information in the weak and multi-label scenario, but that the GCN does capture the ontology knowledge better for weak, multi-labeled data. We also investigate how different modules can tolerate noises introduced from weak labels and better incorporate ontology information. Our best TNN-GCN model achieves mAP=0.45 and AUC=0.87 for lower-level concepts and mAP=0.72 and AUC=0.86 for higher-level concepts, which is an improvement over the baseline TNN but about the same as our models that do not use ontology information.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "referenceCount": 8,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "1-5",
      "name": "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    "authors": [
      {
        "authorId": "31017418",
        "name": "Ankit Shah"
      },
      {
        "authorId": "2157796077",
        "name": "Larry Tang"
      },
      {
        "authorId": "2216451398",
        "name": "Po Hao Chou"
      },
      {
        "authorId": "2158586492",
        "name": "Yilun Zheng"
      },
      {
        "authorId": "2157426412",
        "name": "Ziqian Ge"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "This work re-implements the model proposed by [1] with modifications to fit the multi-label scenario and expands on that idea by using a Graph Convolutional Network (GCN) to model the ontology information to learn the concepts."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "5a3307b2e64bbcaff1202e261b8a83f7d03418a8",
    "externalIds": {
      "ArXiv": "2307.13948",
      "DBLP": "journals/corr/abs-2307-13948",
      "DOI": "10.1145/3581783.3611779",
      "CorpusId": 260165102
    },
    "url": "https://www.semanticscholar.org/paper/5a3307b2e64bbcaff1202e261b8a83f7d03418a8",
    "title": "Rethinking Voice-Face Correlation: A Geometry View",
    "abstract": "Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.",
    "venue": "ACM Multimedia",
    "year": 2023,
    "referenceCount": 56,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.13948",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "name": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    "authors": [
      {
        "authorId": "2108280244",
        "name": "Xiang Li"
      },
      {
        "authorId": "145357606",
        "name": "Yandong Wen"
      },
      {
        "authorId": "72966973",
        "name": "Muqiao Yang"
      },
      {
        "authorId": "2110107884",
        "name": "Jinglu Wang"
      },
      {
        "authorId": "153915824",
        "name": "Rita Singh"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "This work proposes a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction and finds significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "611f9ee6eef0936462cd78f371798d0699951c59",
    "externalIds": {
      "ArXiv": "2302.08095",
      "DBLP": "conf/icassp/YangKBZHKWR23",
      "DOI": "10.1109/ICASSP49357.2023.10096807",
      "CorpusId": 256900649
    },
    "url": "https://www.semanticscholar.org/paper/611f9ee6eef0936462cd78f371798d0699951c59",
    "title": "Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement",
    "abstract": "Despite rapid advancement in recent years, current speech enhancement models often produce speech that differs in perceptual quality from real clean speech. We propose a learning objective that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics. We identify temporal acoustic parameters \u2013 such as spectral tilt, spectral flux, shimmer, etc. \u2013 that are non-differentiable, and we develop a neural network estimator that can accurately predict their time-series values across an utterance. We also model phoneme-specific weights for each feature, as the acoustic parameters are known to show different behavior in different phonemes. We can add this criterion as an auxiliary loss to any model that produces speech, to optimize speech outputs to match the values of clean speech in these features. Experimentally we show that it improves speech enhancement workflows in both time-domain and time-frequency domain, as measured by standard evaluation metrics. We also provide an analysis of phoneme-dependent improvement on acoustic parameters, demonstrating the additional interpretability that our method provides. This analysis can suggest which features are currently the bottleneck for improvement.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "referenceCount": 38,
    "citationCount": 3,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2302.08095",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "pages": "1-5",
      "name": "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    "authors": [
      {
        "authorId": "72966973",
        "name": "Muqiao Yang"
      },
      {
        "authorId": "2174668832",
        "name": "Joseph Konan"
      },
      {
        "authorId": "2174668503",
        "name": "David Bick"
      },
      {
        "authorId": "2111187803",
        "name": "YUNYANG ZENG"
      },
      {
        "authorId": "2206298901",
        "name": "Shuo Han"
      },
      {
        "authorId": "47311290",
        "name": "Anurag Kumar"
      },
      {
        "authorId": "1746678",
        "name": "Shinji Watanabe"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "A learning objective is proposed that formalizes differences in perceptual quality, by using domain knowledge of acoustic-phonetics, and a neural network estimator is developed that can accurately predict their time-series values across an utterance."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "7333be530df311b3148e9857ce9f481975cf0a9b",
    "externalIds": {
      "ArXiv": "2303.09048",
      "DBLP": "journals/corr/abs-2303-09048",
      "DOI": "10.48550/arXiv.2303.09048",
      "CorpusId": 257557465
    },
    "url": "https://www.semanticscholar.org/paper/7333be530df311b3148e9857ce9f481975cf0a9b",
    "title": "Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms",
    "abstract": "In this paper, we present a method for fine-tuning models trained on the Deep Noise Suppression (DNS) 2020 Challenge to improve their performance on Voice over Internet Protocol (VoIP) applications. Our approach involves adapting the DNS 2020 models to the specific acoustic characteristics of VoIP communications, which includes distortion and artifacts caused by compression, transmission, and platform-specific processing. To this end, we propose a multi-task learning framework for VoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement. We evaluate our approach on a diverse VoIP scenarios and show that it outperforms both industry performance and state-of-the-art methods for speech enhancement on VoIP applications. Our results demonstrate the potential of models trained on DNS-2020 to be improved and tailored to different VoIP platforms using VoIP-DNS, whose findings have important applications in areas such as speech recognition, voice assistants, and telecommunication.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 11,
    "citationCount": 2,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.09048",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "volume": "abs/2303.09048",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2174668832",
        "name": "Joseph Konan"
      },
      {
        "authorId": "1649727619",
        "name": "Ojas Bhargave"
      },
      {
        "authorId": "2202224349",
        "name": "Shikhar Agnihotri"
      },
      {
        "authorId": "2110019379",
        "name": "Hojeong Lee"
      },
      {
        "authorId": "31017418",
        "name": "Ankit Shah"
      },
      {
        "authorId": "2206298901",
        "name": "Shuo Han"
      },
      {
        "authorId": "2111187803",
        "name": "YUNYANG ZENG"
      },
      {
        "authorId": "2202226132",
        "name": "Amanda Shu"
      },
      {
        "authorId": "2143857486",
        "name": "Haohui Liu"
      },
      {
        "authorId": "8776560",
        "name": "Xuankai Chang"
      },
      {
        "authorId": "1500657253",
        "name": "Hamza Khalid"
      },
      {
        "authorId": "2031482648",
        "name": "Minseon Gwak"
      },
      {
        "authorId": "2204265592",
        "name": "Kawon Lee"
      },
      {
        "authorId": "2145949834",
        "name": "Minjeong Kim"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "A multi-task learning framework forVoIP-DNS that jointly optimizes noise suppression and VoIP-specific acoustics for speech enhancement and outperforms both industry performance and state-of-the-art methods for speech Enhancement on VoIP applications is proposed."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "740488982dee323d559f2dae70b1f4b3aa5f7171",
    "externalIds": {
      "ArXiv": "2303.03591",
      "DBLP": "journals/corr/abs-2303-03591",
      "DOI": "10.48550/arXiv.2303.03591",
      "CorpusId": 257378346
    },
    "url": "https://www.semanticscholar.org/paper/740488982dee323d559f2dae70b1f4b3aa5f7171",
    "title": "Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms",
    "abstract": "General-purpose embedding is highly desirable for few-shot even zero-shot learning in many application scenarios, including audio tasks. In order to understand representations better, we conducted a thorough error analysis and visualization of HEAR 2021 submission results. Inspired by the analysis, this work experiments with different front-end audio preprocessing methods, including Constant-Q Transform (CQT) and Short-time Fourier transform (STFT), and proposes a Batch Embedding Covariance Regularization (BECR) term to uncover a more holistic simulation of the frequency information received by the human auditory system. We tested the models on the suite of HEAR 2021 tasks, which encompass a broad category of tasks. Preliminary results show (1) the proposed BECR can incur a more dispersed embedding on the test set, (2) BECR improves the PaSST model without extra computation complexity, and (3) STFT preprocessing outperforms CQT in all tasks we tested. Github:https://github.com/ankitshah009/general_audio_embedding_hear_2021",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 12,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.03591",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "volume": "abs/2303.03591",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "31017418",
        "name": "Ankit Shah"
      },
      {
        "authorId": "2108757768",
        "name": "Shuyi Chen"
      },
      {
        "authorId": "2210856438",
        "name": "Kejun Zhou"
      },
      {
        "authorId": "1681412",
        "name": "Yue Chen"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "Experiments with different front-end audio preprocessing methods are experiments, and a Batch Embedding Covariance Regularization (BECR) term is proposed to uncover a more holistic simulation of the frequency information received by the human auditory system."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "74664618ad3b44eb191ba96fdff5b93f27a29ced",
    "externalIds": {
      "ArXiv": "2308.00854",
      "DBLP": "journals/corr/abs-2308-00854",
      "DOI": "10.48550/arXiv.2308.00854",
      "CorpusId": 260379047
    },
    "url": "https://www.semanticscholar.org/paper/74664618ad3b44eb191ba96fdff5b93f27a29ced",
    "title": "Training on Foveated Images Improves Robustness to Adversarial Attacks",
    "abstract": "Deep neural networks (DNNs) have been shown to be vulnerable to adversarial attacks -- subtle, perceptually indistinguishable perturbations of inputs that change the response of the model. In the context of vision, we hypothesize that an important contributor to the robustness of human visual perception is constant exposure to low-fidelity visual stimuli in our peripheral vision. To investigate this hypothesis, we develop \\RBlur, an image transform that simulates the loss in fidelity of peripheral vision by blurring the image and reducing its color saturation based on the distance from a given fixation point. We show that compared to DNNs trained on the original images, DNNs trained on images transformed by \\RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\\% higher accuracy on perturbed data.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 52,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.00854",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2308.00854",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "145897481",
        "name": "Muhammad A Shah"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "DNNs trained on images transformed by \\RBlur are substantially more robust to adversarial attacks, as well as other, non-adversarial, corruptions, achieving up to 25\\% higher accuracy on perturbed data."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "8665c864d71df1e918d2010778fc06712f4e5550",
    "externalIds": {
      "ArXiv": "2305.12715",
      "DBLP": "journals/corr/abs-2305-12715",
      "DOI": "10.48550/arXiv.2305.12715",
      "CorpusId": 258832327
    },
    "url": "https://www.semanticscholar.org/paper/8665c864d71df1e918d2010778fc06712f4e5550",
    "title": "Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations",
    "abstract": "Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \\textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 136,
    "citationCount": 3,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.12715",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.12715",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2051536212",
        "name": "Hao Chen"
      },
      {
        "authorId": "47287745",
        "name": "Ankit Shah"
      },
      {
        "authorId": "1519290245",
        "name": "Jindong Wang"
      },
      {
        "authorId": "26151496",
        "name": "R. Tao"
      },
      {
        "authorId": "2108024273",
        "name": "Yidong Wang"
      },
      {
        "authorId": "1576441343",
        "name": "Xingxu Xie"
      },
      {
        "authorId": "67154907",
        "name": "Masashi Sugiyama"
      },
      {
        "authorId": "153915824",
        "name": "Rita Singh"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "Imprecise label learning (ILL) is introduced, a framework for the unification of learning with various imprecise label configurations, marking the first unified framework with robust and effective performance across various challenging settings."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "a6e3a10a6286967413e3406374bbeea533640030",
    "externalIds": {
      "ArXiv": "2307.13953",
      "DBLP": "journals/corr/abs-2307-13953",
      "DOI": "10.48550/arXiv.2307.13953",
      "CorpusId": 260164956
    },
    "url": "https://www.semanticscholar.org/paper/a6e3a10a6286967413e3406374bbeea533640030",
    "title": "The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features",
    "abstract": "This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.",
    "venue": "Interspeech",
    "year": 2023,
    "referenceCount": 29,
    "citationCount": 2,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.13953",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "volume": "abs/2307.13953",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2125181846",
        "name": "Liao Qu"
      },
      {
        "authorId": "1818224862",
        "name": "X. Zou"
      },
      {
        "authorId": "2108280244",
        "name": "Xiang Li"
      },
      {
        "authorId": "145357606",
        "name": "Yandong Wen"
      },
      {
        "authorId": "153915824",
        "name": "Rita Singh"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "This work proposes an analysis pipeline to help explore the voice-face relationship in a fine-grained manner, i.s. phonemes v. facial anthropometric measurements (AM), and indicates that AMs are more predictable from vowels compared to consonants, particularly with plosives."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "ac856b6b7b3f32fb34320b7170526d3ab15ba5f3",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-15700",
      "ArXiv": "2305.15700",
      "DOI": "10.48550/arXiv.2305.15700",
      "CorpusId": 258887971
    },
    "url": "https://www.semanticscholar.org/paper/ac856b6b7b3f32fb34320b7170526d3ab15ba5f3",
    "title": "Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments",
    "abstract": "Continual semantic segmentation aims to learn new classes while maintaining the information from the previous classes. Although prior studies have shown impressive progress in recent years, the fairness concern in the continual semantic segmentation needs to be better addressed. Meanwhile, fairness is one of the most vital factors in deploying the deep learning model, especially in human-related or safety applications. In this paper, we present a novel Fairness Continual Learning approach to the semantic segmentation problem. In particular, under the fairness objective, a new fairness continual learning framework is proposed based on class distributions. Then, a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning, i.e., catastrophic forgetting and background shift. Our proposed loss has also been proven as a novel, generalized learning paradigm of knowledge distillation commonly used in continual learning. Moreover, the proposed Conditional Structural Consistency loss further regularized the structural constraint of the predicted segmentation. Our proposed approach has achieved State-of-the-Art performance on three standard scene understanding benchmarks, i.e., ADE20K, Cityscapes, and Pascal VOC, and promoted the fairness of the segmentation model.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 52,
    "citationCount": 2,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.15700",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.15700",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "35659935",
        "name": "Thanh-Dat Truong"
      },
      {
        "authorId": null,
        "name": "Hoang-Quan Nguyen"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      },
      {
        "authorId": "1769788",
        "name": "Khoa Luu"
      }
    ],
    "tldr": "A novel Fairness Continual Learning approach to the semantic segmentation problem is presented, in particular, a new fairness continual learning framework is proposed based on class distributions and a novel Prototypical Contrastive Clustering loss is proposed to address the significant challenges in continual learning."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "dc157eba8bdb4cfe6ee65566d8295939ac5b4b37",
    "externalIds": {
      "ArXiv": "2305.19406",
      "DBLP": "journals/corr/abs-2305-19406",
      "DOI": "10.48550/arXiv.2305.19406",
      "CorpusId": 258987438
    },
    "url": "https://www.semanticscholar.org/paper/dc157eba8bdb4cfe6ee65566d8295939ac5b4b37",
    "title": "PaintSeg: Training-free Segmentation via Painting",
    "abstract": "The paper introduces PaintSeg, a new unsupervised method for segmenting objects without any training. We propose an adversarial masked contrastive painting (AMCP) process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models. During the painting process, inpainting and outpainting are alternated, with the former masking the foreground and filling in the background, and the latter masking the background while recovering the missing part of the foreground object. Inpainting and outpainting, also referred to as I-step and O-step, allow our method to gradually advance the target segmentation mask toward the ground truth without supervision or training. PaintSeg can be configured to work with a variety of prompts, e.g. coarse masks, boxes, scribbles, and points. Our experimental results demonstrate that PaintSeg outperforms existing approaches in coarse mask-prompt, box-prompt, and point-prompt segmentation tasks, providing a training-free solution suitable for unsupervised segmentation.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 70,
    "citationCount": 2,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.19406",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.19406",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2108280244",
        "name": "Xiang Li"
      },
      {
        "authorId": "1955556",
        "name": "Chung-Ching Lin"
      },
      {
        "authorId": "2109306087",
        "name": "Yinpeng Chen"
      },
      {
        "authorId": "2145253136",
        "name": "Zicheng Liu"
      },
      {
        "authorId": "2110107884",
        "name": "Jinglu Wang"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "An adversarial masked contrastive painting process, which creates a contrast between the original image and a painted image in which a masked area is painted using off-the-shelf generative models, providing a training-free solution suitable for unsupervised segmentation."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "e146e5221c124d93f69516c5ae7e1b7b1822848e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2302-08088",
      "ArXiv": "2302.08088",
      "DOI": "10.1109/ICASSP49357.2023.10094773",
      "CorpusId": 256900782
    },
    "url": "https://www.semanticscholar.org/paper/e146e5221c124d93f69516c5ae7e1b7b1822848e",
    "title": "TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement",
    "abstract": "Speech enhancement models have greatly progressed in recent years, but still show limits in perceptual quality of their speech outputs. We propose an objective for perceptual quality based on temporal acoustic parameters. These are fundamental speech features that play an essential role in various applications, including speaker recognition and paralinguistic analysis. We provide a differentiable estimator for four categories of low-level acoustic descriptors involving: frequency-related parameters, energy or amplitude-related parameters, spectral balance parameters, and temporal features. Un-like prior work that looks at aggregated acoustic parameters or a few categories of acoustic parameters, our temporal acoustic parameter (TAP) loss enables auxiliary optimization and improvement of many fine-grained speech characteristics in enhancement workflows. We show that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility. We use data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from our method.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "referenceCount": 35,
    "citationCount": 7,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2302.08088",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "pages": "1-5",
      "name": "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    "authors": [
      {
        "authorId": "2111187803",
        "name": "YUNYANG ZENG"
      },
      {
        "authorId": "2174668832",
        "name": "Joseph Konan"
      },
      {
        "authorId": "2206298901",
        "name": "Shuo Han"
      },
      {
        "authorId": "2174668503",
        "name": "David Bick"
      },
      {
        "authorId": "72966973",
        "name": "Muqiao Yang"
      },
      {
        "authorId": "47311290",
        "name": "Anurag Kumar"
      },
      {
        "authorId": "1746678",
        "name": "Shinji Watanabe"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "This work shows that adding TAPLoss as an auxiliary objective in speech enhancement produces speech with improved perceptual quality and intelligibility and uses data from the Deep Noise Suppression 2020 Challenge to demonstrate that both time-domain models and time-frequency domain models can benefit from the method."
  },
  {
    "profName": "Bhiksha Raj",
    "authorId": "1681921",
    "authorName": "B. Raj",
    "authorUrl": "https://www.semanticscholar.org/author/1681921",
    "authorHIndex": 54,
    "authorAffiliations": [],
    "authorPaperCount": 404,
    "authorCitationCount": 14485,
    "paperId": "feecd2cfb7871a818ba514e8b4b3f9da482f17bc",
    "externalIds": {
      "DBLP": "journals/corr/abs-2302-09719",
      "ArXiv": "2302.09719",
      "DOI": "10.48550/arXiv.2302.09719",
      "CorpusId": 257039066
    },
    "url": "https://www.semanticscholar.org/paper/feecd2cfb7871a818ba514e8b4b3f9da482f17bc",
    "title": "Synergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session",
    "abstract": "Machine Listening, as usually formalized, attempts to perform a task that is, from our perspective, fundamentally human-performable, and performed by humans. Current automated models of Machine Listening vary from purely data-driven approaches to approaches imitating human systems. In recent years, the most promising approaches have been hybrid in that they have used data-driven approaches informed by models of the perceptual, cognitive, and semantic processes of the human system. Not only does the guidance provided by models of human perception and domain knowledge enable better, and more generalizable Machine Listening, in the converse, the lessons learned from these models may be used to verify or improve our models of human perception themselves. This paper summarizes advances in the development of such hybrid approaches, ranging from Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds. The research described herein was presented in a special session on\"Synergy between human and machine approaches to sound/scene recognition and processing\"at the 2023 ICASSP meeting.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 15,
    "citationCount": 8,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.09719",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "volume": "abs/2302.09719",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "38655601",
        "name": "L. Heller"
      },
      {
        "authorId": "2532460",
        "name": "Benjamin Elizalde"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      },
      {
        "authorId": "67345939",
        "name": "Soham Deshmukh"
      }
    ],
    "tldr": "Advances in the development of hybrid approaches to Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds are summarized."
  }
]