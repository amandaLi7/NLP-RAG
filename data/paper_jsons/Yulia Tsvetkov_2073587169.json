[
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "17605c43ca3eb982c99642052ddc21a93d116594",
    "externalIds": {
      "DBLP": "conf/emnlp/SongK0FOWACTAN23",
      "ArXiv": "2305.14716",
      "DOI": "10.48550/arXiv.2305.14716",
      "CorpusId": 258866051
    },
    "url": "https://www.semanticscholar.org/paper/17605c43ca3eb982c99642052ddc21a93d116594",
    "title": "GlobalBench: A Benchmark for Global Progress in Natural Language Processing",
    "abstract": "Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14716",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "14157-14171"
    },
    "authors": [
      {
        "authorId": "148310739",
        "name": "Yueqi Song"
      },
      {
        "authorId": "2218206121",
        "name": "Catherine Cui"
      },
      {
        "authorId": "1452678825",
        "name": "Simran Khanuja"
      },
      {
        "authorId": "144118452",
        "name": "Pengfei Liu"
      },
      {
        "authorId": "48556979",
        "name": "FAHIM FAISAL"
      },
      {
        "authorId": "1475670743",
        "name": "Alissa Ostapenko"
      },
      {
        "authorId": "9162688",
        "name": "Genta Indra Winata"
      },
      {
        "authorId": "8129718",
        "name": "Alham Fikri Aji"
      },
      {
        "authorId": "66986482",
        "name": "Samuel Cahyawijaya"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      },
      {
        "authorId": "49513989",
        "name": "Antonios Anastasopoulos"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "17fbffb05fa14e21d1c506fd5f0f568b955fe983",
    "externalIds": {
      "DBLP": "conf/emnlp/Ahia0GKMST23",
      "ArXiv": "2305.13707",
      "DOI": "10.48550/arXiv.2305.13707",
      "CorpusId": 258841465
    },
    "url": "https://www.semanticscholar.org/paper/17fbffb05fa14e21d1c506fd5f0f568b955fe983",
    "title": "Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models",
    "abstract": "Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.",
    "year": 2023,
    "influentialCitationCount": 2,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.13707",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.13707",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "1452686038",
        "name": "Orevaoghene Ahia"
      },
      {
        "authorId": "51467955",
        "name": "Sachin Kumar"
      },
      {
        "authorId": "1821892",
        "name": "Hila Gonen"
      },
      {
        "authorId": "11348687",
        "name": "Jungo Kasai"
      },
      {
        "authorId": "3407646",
        "name": "David R. Mortensen"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "1bc0dc96d745325d89ec5bee1da1541255e6d1eb",
    "externalIds": {
      "DBLP": "journals/corr/abs-2302-00381",
      "DOI": "10.48550/arXiv.2302.00381",
      "CorpusId": 256459902
    },
    "url": "https://www.semanticscholar.org/paper/1bc0dc96d745325d89ec5bee1da1541255e6d1eb",
    "title": "BotPercent: Estimating Twitter Bot Populations from Groups to Crowds",
    "abstract": "Twitter bot detection has become increasingly important in combating misinformation, identifying malicious online cam-paigns, and protecting the integrity of social media discourse. While existing bot detection literature mostly focuses on identifying individual bots, it remains underexplored how to estimate the proportion of bots within speci\ufb01c communities and social networks, which has great implications for both content moderators and day-to-day users. In this work, we propose community-level bot detection , a novel approach to estimating the amount of malicious interference in online communities by estimating the percentage of bot accounts. Speci\ufb01cally, we introduce BotPercent , an amalgamation of Twitter bot-detection datasets and feature, text, and graph-based models that overcome generalization issues in existing individual-level models, resulting in a more accurate community-level bot estimation. Experiments demonstrate that BotPercent achieves state-of-the-art community-level bot detection performance on the TwiBot-22 benchmark while showing great robustness towards the tampering of speci\ufb01c user features. Armed with BotPercent , we analyze bot rates in different Twitter groups and communities, such as all active Twitter users, users that interact with partisan news media, users that participate in Elon Musk\u2019s content moderation votes, and the political communities in different countries and regions. Our experimental results demonstrate that the existence of Twitter bots is not homogeneous, but rather a spatial-temporal distribution whose heterogeneity should be taken into account for content moderation, social media policy making, and more. The BotPercent implementation is available at https://github.com/TamSiuhin/BotPercent",
    "year": 2023,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.00381",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2302.00381",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2093186816",
        "name": "Zhaoxuan Tan"
      },
      {
        "authorId": "2114887261",
        "name": "Shangbin Feng"
      },
      {
        "authorId": "1947172233",
        "name": "Melanie Sclar"
      },
      {
        "authorId": "2114831715",
        "name": "Herun Wan"
      },
      {
        "authorId": "3326677",
        "name": "Minnan Luo"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "346e4f35a5a81ef893792133ec1fec18f23c1768",
    "externalIds": {
      "DBLP": "conf/fat/FieldCGCPST23",
      "ArXiv": "2305.19409",
      "DOI": "10.1145/3593013.3594094",
      "CorpusId": 258987867
    },
    "url": "https://www.semanticscholar.org/paper/346e4f35a5a81ef893792133ec1fec18f23c1768",
    "title": "Examining risks of racial biases in NLP tools for child protective services",
    "abstract": "Although much literature has established the presence of demographic bias in natural language processing (NLP) models, most work relies on curated bias metrics that may not be reflective of real-world applications. At the same time, practitioners are increasingly using algorithmic tools in high-stakes settings, with particular recent interest in NLP. In this work, we focus on one such setting: child protective services (CPS). CPS workers often write copious free-form text notes about families they are working with, and CPS agencies are actively seeking to deploy NLP models to leverage these data. Given well-established racial bias in this setting, we investigate possible ways deployed NLP is liable to increase racial disparities. We specifically examine word statistics within notes and algorithmic fairness in risk prediction, coreference resolution, and named entity recognition (NER). We document consistent algorithmic unfairness in NER models, possible algorithmic unfairness in coreference resolution models, and little evidence of exacerbated racial bias in risk prediction. While there is existing pronounced criticism of risk prediction, our results expose previously undocumented risks of racial bias in realistic information extraction systems, highlighting potential concerns in deploying them, even though they may appear more benign. Our work serves as a rare realistic examination of NLP algorithmic fairness in a potential deployed setting and a timely investigation of a specific risk associated with deploying NLP in CPS settings.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3593013.3594094",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "name": "Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency"
    },
    "authors": [
      {
        "authorId": "49713890",
        "name": "Anjalie Field"
      },
      {
        "authorId": "48577290",
        "name": "Amanda Coston"
      },
      {
        "authorId": "47404598",
        "name": "Nupoor Gandhi"
      },
      {
        "authorId": "2082393",
        "name": "A. Chouldechova"
      },
      {
        "authorId": "1398859796",
        "name": "Emily Putnam-Hornstein"
      },
      {
        "authorId": "2082303978",
        "name": "David Steier"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "36f7bc27c9a37eb337c35df4ae86f148e13d4e9a",
    "externalIds": {
      "ACL": "2023.acl-long.708",
      "DBLP": "conf/acl/HanSMTCW23",
      "ArXiv": "2306.15091",
      "DOI": "10.48550/arXiv.2306.15091",
      "CorpusId": 259262608
    },
    "url": "https://www.semanticscholar.org/paper/36f7bc27c9a37eb337c35df4ae86f148e13d4e9a",
    "title": "Understanding In-Context Learning via Supportive Pretraining Data",
    "abstract": "In-context learning (ICL) improves language models\u2019 performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model\u2019s ICL ability, by up to 18%. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages ICL. Our work takes a first step towards understanding ICL via analyzing instance-level pretraining data. Our insights have a potential to enhance the ICL ability of language models by actively guiding the construction of pretraining data in the future.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.15091",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "12660-12673"
    },
    "authors": [
      {
        "authorId": "40500540",
        "name": "Xiaochuang Han"
      },
      {
        "authorId": "2082239112",
        "name": "Daniel Simig"
      },
      {
        "authorId": "39980906",
        "name": "Todor Mihaylov"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      },
      {
        "authorId": "1709797",
        "name": "Asli Celikyilmaz"
      },
      {
        "authorId": "1785372925",
        "name": "Tianlu Wang"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "49f1fa0d609ff06564b46270cbc022b7d9d195f4",
    "externalIds": {
      "ArXiv": "2303.18190",
      "DBLP": "journals/corr/abs-2303-18190",
      "DOI": "10.48550/arXiv.2303.18190",
      "CorpusId": 257900638
    },
    "url": "https://www.semanticscholar.org/paper/49f1fa0d609ff06564b46270cbc022b7d9d195f4",
    "title": "Assessing Language Model Deployment with Risk Cards",
    "abstract": "This paper introduces RiskCards, a framework for structured assessment and documentation of risks associated with an application of language models. As with all language, text generated by language models can be harmful, or used to bring about harm. Automating language generation adds both an element of scale and also more subtle or emergent undesirable tendencies to the generated text. Prior work establishes a wide variety of language model harms to many different actors: existing taxonomies identify categories of harms posed by language models; benchmarks establish automated tests of these harms; and documentation standards for models, tasks and datasets encourage transparent reporting. However, there is no risk-centric framework for documenting the complexity of a landscape in which some risks are shared across models and contexts, while others are specific, and where certain conditions may be required for risks to manifest as harms. RiskCards address this methodological gap by providing a generic framework for assessing the use of a given language model in a given scenario. Each RiskCard makes clear the routes for the risk to manifest harm, their placement in harm taxonomies, and example prompt-output pairs. While RiskCards are designed to be open-source, dynamic and participatory, we present a\"starter set\"of RiskCards taken from a broad literature survey, each of which details a concrete risk presentation. Language model RiskCards initiate a community knowledge base which permits the mapping of risks and harms to a specific model or its application scenario, ultimately contributing to a better, safer and shared understanding of the risk landscape.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.18190",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2303.18190",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "113320522",
        "name": "Leon Derczynski"
      },
      {
        "authorId": "90729626",
        "name": "Hannah Rose Kirk"
      },
      {
        "authorId": "143820870",
        "name": "Vidhisha Balachandran"
      },
      {
        "authorId": "51467955",
        "name": "Sachin Kumar"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      },
      {
        "authorId": "119004240",
        "name": "M. Leiser"
      },
      {
        "authorId": "2057036852",
        "name": "Saif Mohammad"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "5471114e37448bea2457b74894b1ecb92bbcfdf6",
    "externalIds": {
      "ArXiv": "2305.08283",
      "ACL": "2023.acl-long.656",
      "DBLP": "conf/acl/FengPLT23",
      "DOI": "10.48550/arXiv.2305.08283",
      "CorpusId": 258686693
    },
    "url": "https://www.semanticscholar.org/paper/5471114e37448bea2457b74894b1ecb92bbcfdf6",
    "title": "From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models",
    "abstract": "Language models (LMs) are pretrained on diverse data sources\u2014news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.",
    "year": 2023,
    "influentialCitationCount": 4,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.08283",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.08283",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2114887261",
        "name": "Shangbin Feng"
      },
      {
        "authorId": "50487261",
        "name": "Chan Young Park"
      },
      {
        "authorId": "2169159066",
        "name": "Yuhan Liu"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
    "externalIds": {
      "DBLP": "conf/emnlp/FengBBT23",
      "ArXiv": "2305.08281",
      "DOI": "10.48550/arXiv.2305.08281",
      "CorpusId": 258685429
    },
    "url": "https://www.semanticscholar.org/paper/663d743272e9ab04f54d9105a3c3a3f6e22dd1dd",
    "title": "FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge",
    "abstract": "Evaluating the factual consistency of automatically generated summaries is essential for the progress and adoption of reliable summarization systems. Despite recent advances, existing factuality evaluation models are not robust, being especially prone to entity and relation errors in new domains. We propose FactKB, a simple new approach to factuality evaluation that is generalizable across domains, in particular with respect to entities and relations. FactKB is based on language models pretrained using facts extracted from external knowledge bases. We introduce three types of complementary factuality pretraining objectives based on direct entity facts, facts grounded in auxiliary knowledge about entities, and facts constructed compositionally through knowledge base walks. The resulting factuality evaluation model achieves state-of-the-art performance on two in-domain news summarization benchmarks as well as on three out-of-domain scientific literature datasets. Further analysis of FactKB shows improved ability to detect erroneous entities and relations in summaries and is robust and generalizable across domains.",
    "year": 2023,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.08281",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.08281",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2114887261",
        "name": "Shangbin Feng"
      },
      {
        "authorId": "143820870",
        "name": "Vidhisha Balachandran"
      },
      {
        "authorId": "2170130468",
        "name": "Yuyang Bai"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "926dece297434dc535733814efca28759b94ab82",
    "externalIds": {
      "DBLP": "conf/emnlp/NjooPSTCT23",
      "ArXiv": "2305.14326",
      "DOI": "10.18653/v1/2023.findings-emnlp.625",
      "CorpusId": 258841861
    },
    "url": "https://www.semanticscholar.org/paper/926dece297434dc535733814efca28759b94ab82",
    "title": "TalkUp: Paving the Way for Understanding Empowering Language",
    "abstract": "Empowering language is important in many real-world contexts, from education to workplace dynamics to healthcare. Though language technologies are growing more prevalent in these contexts, empowerment has seldom been studied in NLP, and moreover, it is inherently challenging to operationalize because of its implicit nature. This work builds from linguistic and social psychology literature to explore what characterizes empowering language. We then crowdsource a novel dataset of Reddit posts labeled for empowerment, reasons why these posts are empowering to readers, and the social relationships between posters and readers. Our preliminary analyses show that this dataset, which we call TalkUp, can be used to train language models that capture empowering and disempowering language. More broadly, TalkUp provides an avenue to explore implication, presuppositions, and how social context influences the meaning of language.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-emnlp.625.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "9334-9354"
    },
    "authors": [
      {
        "authorId": "79336318",
        "name": "Lucille Njoo"
      },
      {
        "authorId": "50487261",
        "name": "Chan Young Park"
      },
      {
        "authorId": "2218440660",
        "name": "Octavia Stappart"
      },
      {
        "authorId": "46189691",
        "name": "Marvin Thielk"
      },
      {
        "authorId": "2070015461",
        "name": "Yi Chu"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "984d4a1d41bfc8184fb77b8aa0eb8e96d536d048",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-14739",
      "ArXiv": "2305.14739",
      "DOI": "10.48550/arXiv.2305.14739",
      "CorpusId": 258866080
    },
    "url": "https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048",
    "title": "Trusting Your Evidence: Hallucinate Less with Context-aware Decoding",
    "abstract": "Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.",
    "year": 2023,
    "influentialCitationCount": 6,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14739",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.14739",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "3040379",
        "name": "Weijia Shi"
      },
      {
        "authorId": "40500540",
        "name": "Xiaochuang Han"
      },
      {
        "authorId": "35084211",
        "name": "M. Lewis"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      },
      {
        "authorId": "1982950",
        "name": "Luke Zettlemoyer"
      },
      {
        "authorId": "3156075",
        "name": "S. Yih"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "ad454e24bd32408559512b4bac4cd5237794210f",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-14857",
      "ArXiv": "2305.14857",
      "DOI": "10.48550/arXiv.2305.14857",
      "CorpusId": 258865558
    },
    "url": "https://www.semanticscholar.org/paper/ad454e24bd32408559512b4bac4cd5237794210f",
    "title": "BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer",
    "abstract": "Despite remarkable advancements in few-shot generalization in natural language processing, most models are developed and evaluated primarily in English. To facilitate research on few-shot cross-lingual transfer, we introduce a new benchmark, called BUFFET, which unifies 15 diverse tasks across 54 languages in a sequence-to-sequence format and provides a fixed set of few-shot examples and instructions. BUFFET is designed to establish a rigorous and equitable evaluation framework for few-shot cross-lingual transfer across a broad range of tasks and languages. Using BUFFET, we perform thorough evaluations of state-of-the-art multilingual large language models with different transfer methods, namely in-context learning and fine-tuning. Our findings reveal significant room for improvement in few-shot in-context cross-lingual transfer. In particular, ChatGPT with in-context learning often performs worse than much smaller mT5-base models fine-tuned on English task data and few-shot in-language examples. Our analysis suggests various avenues for future research in few-shot cross-lingual transfer, such as improved pretraining, understanding, and future evaluations.",
    "year": 2023,
    "influentialCitationCount": 3,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14857",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.14857",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "35584853",
        "name": "Akari Asai"
      },
      {
        "authorId": "35871436",
        "name": "Sneha Kudugunta"
      },
      {
        "authorId": "2118211280",
        "name": "Xinyan Velocity Yu"
      },
      {
        "authorId": "3443287",
        "name": "Terra Blevins"
      },
      {
        "authorId": "1821892",
        "name": "Hila Gonen"
      },
      {
        "authorId": "1557386977",
        "name": "Machel Reid"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      },
      {
        "authorId": "2124014463",
        "name": "Sebastian Ruder"
      },
      {
        "authorId": "2548384",
        "name": "Hannaneh Hajishirzi"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "cb0335107f12d331ace2cbf220eb3c7bdcf653c5",
    "externalIds": {
      "DOI": "10.18653/v1/2023.emnlp-tutorial.5",
      "CorpusId": 266181225
    },
    "url": "https://www.semanticscholar.org/paper/cb0335107f12d331ace2cbf220eb3c7bdcf653c5",
    "title": "Mitigating Societal Harms in Large Language Models",
    "abstract": ",",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.emnlp-tutorial.5.pdf",
      "status": null
    },
    "fieldsOfStudy": null,
    "journal": {
      "name": "Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts"
    },
    "authors": [
      {
        "authorId": "51467955",
        "name": "Sachin Kumar"
      },
      {
        "authorId": "143820870",
        "name": "Vidhisha Balachandran"
      },
      {
        "authorId": "79336318",
        "name": "Lucille Njoo"
      },
      {
        "authorId": "2273733474",
        "name": "Antonios Anastasopoulos"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "d7a3f5c612930a3c08f1632b88934252edc66d67",
    "externalIds": {
      "ACL": "2023.acl-long.780",
      "DBLP": "conf/acl/SclarKWS0T23",
      "ArXiv": "2306.00924",
      "DOI": "10.48550/arXiv.2306.00924",
      "CorpusId": 258999153
    },
    "url": "https://www.semanticscholar.org/paper/d7a3f5c612930a3c08f1632b88934252edc66d67",
    "title": "Minding Language Models\u2019 (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker",
    "abstract": "Theory of Mind (ToM)\u2014the ability to reason about the mental states of other people\u2014is a key element of our social intelligence. Yet, despite their ever more impressive performance, large-scale neural language models still lack basic theory of mind capabilities out-of-the-box. We posit that simply scaling up models will not imbue them with theory of mind due to the inherently symbolic and implicit nature of the phenomenon, and instead investigate an alternative: can we design a decoding-time algorithm that enhances theory of mind of off-the-shelf neural language models without explicit supervision? We present SymbolicToM, a plug-and-play approach to reason about the belief states of multiple characters in reading comprehension tasks via explicit symbolic representation. More concretely, our approach tracks each entity\u2019s beliefs, their estimation of other entities\u2019 beliefs, and higher-order levels of reasoning, all through graphical representations, allowing for more precise and interpretable reasoning than previous approaches. Empirical results on the well-known ToMi benchmark (Le et al., 2019) demonstrate that SymbolicToM dramatically enhances off-the-shelf neural networks\u2019 theory of mind in a zero-shot setting while showing robust out-of-distribution performance compared to supervised baselines. Our work also reveals spurious patterns in existing theory of mind benchmarks, emphasizing the importance of out-of-distribution evaluation and methods that do not overfit a particular dataset.",
    "year": 2023,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.00924",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "13960-13980"
    },
    "authors": [
      {
        "authorId": "1947172233",
        "name": "Melanie Sclar"
      },
      {
        "authorId": "51467955",
        "name": "Sachin Kumar"
      },
      {
        "authorId": "119659229",
        "name": "Peter West"
      },
      {
        "authorId": "32849969",
        "name": "Alane Suhr"
      },
      {
        "authorId": "1699545",
        "name": "Yejin Choi"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "df2beaae63e4d68ef8e762bcd4704c9f11f856d9",
    "externalIds": {
      "ArXiv": "2305.10037",
      "DBLP": "journals/corr/abs-2305-10037",
      "DOI": "10.48550/arXiv.2305.10037",
      "CorpusId": 258740923
    },
    "url": "https://www.semanticscholar.org/paper/df2beaae63e4d68ef8e762bcd4704c9f11f856d9",
    "title": "Can Language Models Solve Graph Problems in Natural Language?",
    "abstract": "Large language models (LLMs) are increasingly adopted for a variety of tasks with implicit graphical structures, such as planning in robotics, multi-hop question answering or knowledge probing, structured commonsense reasoning, and more. While LLMs have advanced the state-of-the-art on these tasks with structure implications, whether LLMs could explicitly process textual descriptions of graphs and structures, map them to grounded conceptual spaces, and perform structured operations remains underexplored. To this end, we propose NLGraph (Natural Language Graph), a comprehensive benchmark of graph-based problem solving designed in natural language. NLGraph contains 29,370 problems, covering eight graph reasoning tasks with varying complexity from simple tasks such as connectivity and shortest path up to complex problems such as maximum flow and simulating graph neural networks. We evaluate LLMs (GPT-3/4) with various prompting approaches on the NLGraph benchmark and find that 1) language models do demonstrate preliminary graph reasoning abilities, 2) the benefit of advanced prompting and in-context learning diminishes on more complex graph problems, while 3) LLMs are also (un)surprisingly brittle in the face of spurious correlations in graph and problem settings. We then propose Build-a-Graph Prompting and Algorithmic Prompting, two instruction-based approaches to enhance LLMs in solving natural language graph problems. Build-a-Graph and Algorithmic prompting improve the performance of LLMs on NLGraph by 3.07% to 16.85% across multiple tasks and settings, while how to solve the most complicated graph reasoning tasks in our setup with language models remains an open research question. The NLGraph benchmark and evaluation code are available at https://github.com/Arthur-Heng/NLGraph.",
    "year": 2023,
    "influentialCitationCount": 9,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.10037",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.10037",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2256778370",
        "name": "Heng Wang"
      },
      {
        "authorId": "2114887261",
        "name": "Shangbin Feng"
      },
      {
        "authorId": "3083253",
        "name": "Tianxing He"
      },
      {
        "authorId": "2093186816",
        "name": "Zhaoxuan Tan"
      },
      {
        "authorId": "40500540",
        "name": "Xiaochuang Han"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      }
    ]
  },
  {
    "authorId": "2073587169",
    "authorName": "Yulia Tsvetkov",
    "authorUrl": "https://www.semanticscholar.org/author/2073587169",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 48,
    "authorCitationCount": 1382,
    "paperId": "f9a5af5b21563b9bdd09630a8dec62d515479678",
    "externalIds": {
      "ACL": "2023.starsem-1.19",
      "DBLP": "conf/starsem/AhiaGBTS23",
      "DOI": "10.18653/v1/2023.starsem-1.19",
      "CorpusId": 260063220
    },
    "url": "https://www.semanticscholar.org/paper/f9a5af5b21563b9bdd09630a8dec62d515479678",
    "title": "LEXPLAIN: Improving Model Explanations via Lexicon Supervision",
    "abstract": "Model explanations that shed light on the model\u2019s predictions are becoming a desired additional output of NLP models, alongside their predictions. Challenges in creating these explanations include making them trustworthy and faithful to the model\u2019s predictions. In this work, we propose a novel framework for guiding model explanations by supervising them explicitly. To this end, our method, LEXplain, uses task-related lexicons to directly supervise model explanations. This approach consistently improves the model\u2019s explanations without sacrificing performance on the task, as we demonstrate on sentiment analysis and toxicity detection. Our analyses show that our method also demotes spurious correlations (i.e., with respect to African American English dialect) when performing the task, improving fairness.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.starsem-1.19.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "207-216"
    },
    "authors": [
      {
        "authorId": "1452686038",
        "name": "Orevaoghene Ahia"
      },
      {
        "authorId": "1821892",
        "name": "Hila Gonen"
      },
      {
        "authorId": "143820870",
        "name": "Vidhisha Balachandran"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      },
      {
        "authorId": "144365875",
        "name": "Noah A. Smith"
      }
    ]
  }
]