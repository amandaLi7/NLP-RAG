[
  {
    "profName": "Matt Gormley",
    "authorId": "1762110",
    "authorName": "Matthew R. Gormley",
    "authorUrl": "https://www.semanticscholar.org/author/1762110",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 42,
    "authorCitationCount": 1117,
    "paperId": "d6ae4c0679bdceb029f652efd2a854ac5ade772f",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-01387",
      "ACL": "2023.bigpicture-1.9",
      "ArXiv": "2310.01387",
      "DOI": "10.48550/arXiv.2310.01387",
      "CorpusId": 263605610
    },
    "url": "https://www.semanticscholar.org/paper/d6ae4c0679bdceb029f652efd2a854ac5ade772f",
    "title": "It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "abstract": "Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area.",
    "venue": "BIGPICTURE",
    "year": 2023,
    "referenceCount": 94,
    "citationCount": 6,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01387",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2310.01387",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2138301112",
        "name": "Amanda Bertsch"
      },
      {
        "authorId": "2253395527",
        "name": "Alex Xie"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "1762110",
        "name": "Matthew R. Gormley"
      }
    ]
  },
  {
    "profName": "Matt Gormley",
    "authorId": "1762110",
    "authorName": "Matthew R. Gormley",
    "authorUrl": "https://www.semanticscholar.org/author/1762110",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 42,
    "authorCitationCount": 1117,
    "paperId": "dbc368bc8b49347dd27679894524fa62f88492c9",
    "externalIds": {
      "ArXiv": "2305.01625",
      "DBLP": "journals/corr/abs-2305-01625",
      "DOI": "10.48550/arXiv.2305.01625",
      "CorpusId": 258436892
    },
    "url": "https://www.semanticscholar.org/paper/dbc368bc8b49347dd27679894524fa62f88492c9",
    "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input",
    "abstract": "Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 56,
    "citationCount": 38,
    "influentialCitationCount": 3,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.01625",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.01625",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2138301112",
        "name": "Amanda Bertsch"
      },
      {
        "authorId": "47051926",
        "name": "Uri Alon"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "1762110",
        "name": "Matthew R. Gormley"
      }
    ]
  },
  {
    "profName": "Matt Gormley",
    "authorId": "1762110",
    "authorName": "Matthew R. Gormley",
    "authorUrl": "https://www.semanticscholar.org/author/1762110",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 42,
    "authorCitationCount": 1117,
    "paperId": "e80f1b4c254a5c135f5f3416ba3a863f8ec4e06c",
    "externalIds": {
      "DBLP": "conf/acl/ChengJRKLSG23",
      "ArXiv": "2307.03859",
      "ACL": "2023.acl-long.416",
      "DOI": "10.48550/arXiv.2307.03859",
      "CorpusId": 259370675
    },
    "url": "https://www.semanticscholar.org/paper/e80f1b4c254a5c135f5f3416ba3a863f8ec4e06c",
    "title": "MDACE: MIMIC Documents Annotated with Code Evidence",
    "abstract": "We introduce a dataset for evidence/rationale extraction on an extreme multi-label classification task over long medical documents. One such task is Computer-Assisted Coding (CAC) which has improved significantly in recent years, thanks to advances in machine learning technologies. Yet simply predicting a set of final codes for a patient encounter is insufficient as CAC systems are required to provide supporting textual evidence to justify the billing codes. A model able to produce accurate and reliable supporting evidence for each code would be a tremendous benefit. However, a human annotated code evidence corpus is extremely difficult to create because it requires specialized knowledge. In this paper, we introduce MDACE, the first publicly available code evidence dataset, which is built on a subset of the MIMIC-III clinical records. The dataset \u2013 annotated by professional medical coders \u2013 consists of 302 Inpatient charts with 3,934 evidence spans and 52 Profee charts with 5,563 evidence spans. We implemented several evidence extraction methods based on the EffectiveCAN model (Liu et al., 2021) to establish baseline performance on this dataset. MDACE can be used to evaluate code evidence extraction methods for CAC systems, as well as the accuracy and interpretability of deep learning models for multi-label classification. We believe that the release of MDACE will greatly improve the understanding and application of deep learning technologies for medical coding and document classification.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 44,
    "citationCount": 1,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.03859",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2307.03859",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2117908368",
        "name": "Hua Cheng"
      },
      {
        "authorId": "2161140029",
        "name": "Rana Jafari"
      },
      {
        "authorId": "2221287414",
        "name": "April Russell"
      },
      {
        "authorId": "2139750121",
        "name": "Russell Klopfer"
      },
      {
        "authorId": "2221287131",
        "name": "Edmond Lu"
      },
      {
        "authorId": "48227397",
        "name": "Benjamin Striner"
      },
      {
        "authorId": "1762110",
        "name": "Matthew R. Gormley"
      }
    ]
  },
  {
    "profName": "Matt Gormley",
    "authorId": "1762110",
    "authorName": "Matthew R. Gormley",
    "authorUrl": "https://www.semanticscholar.org/author/1762110",
    "authorHIndex": 17,
    "authorAffiliations": [],
    "authorPaperCount": 42,
    "authorCitationCount": 1117,
    "paperId": "ebb3d299213bae89b5d302cc3dfc36573ec83956",
    "externalIds": {
      "ACL": "2023.clinicalnlp-1.51",
      "DBLP": "conf/acl-clinicalnlp/MathurRKPBG23",
      "ArXiv": "2306.17384",
      "DOI": "10.48550/arXiv.2306.17384",
      "CorpusId": 259309155
    },
    "url": "https://www.semanticscholar.org/paper/ebb3d299213bae89b5d302cc3dfc36573ec83956",
    "title": "SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization",
    "abstract": "Medical dialogue summarization is challenging due to the unstructured nature of medical conversations, the use of medical terminologyin gold summaries, and the need to identify key information across multiple symptom sets. We present a novel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA 2023 Shared Task. Our approach for sectionwise summarization (Task A) is a two-stage process of selecting semantically similar dialogues and using the top-k similar dialogues as in-context examples for GPT-4. For full-note summarization (Task B), we use a similar solution with k=1. We achieved 3rd place in Task A (2nd among all teams), 4th place in Task B Division Wise Summarization (2nd among all teams), 15th place in Task A Section Header Classification (9th among all teams), and 8th place among all teams in Task B. Our results highlight the effectiveness of few-shot prompting for this task, though we also identify several weaknesses of prompting-based approaches. We compare GPT-4 performance with several finetuned baselines. We find that GPT-4 summaries are more abstractive and shorter. We make our code publicly available.",
    "venue": "Clinical Natural Language Processing Workshop",
    "year": 2023,
    "referenceCount": 48,
    "citationCount": 4,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.17384",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "490-502"
    },
    "authors": [
      {
        "authorId": "2003826361",
        "name": "Yash Mathur"
      },
      {
        "authorId": "2031481495",
        "name": "Sanketh Rangreji"
      },
      {
        "authorId": "32536265",
        "name": "Raghav Kapoor"
      },
      {
        "authorId": "2220962946",
        "name": "Medha Palavalli"
      },
      {
        "authorId": "2138301112",
        "name": "Amanda Bertsch"
      },
      {
        "authorId": "1762110",
        "name": "Matthew R. Gormley"
      }
    ]
  }
]