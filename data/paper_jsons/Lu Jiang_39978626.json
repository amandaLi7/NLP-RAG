[
  {
    "profName": "Lu Jiang",
    "authorId": "39978626",
    "authorName": "Lu Jiang",
    "authorUrl": "https://www.semanticscholar.org/author/39978626",
    "authorHIndex": 41,
    "authorAffiliations": [
      "Google Research"
    ],
    "authorPaperCount": 79,
    "authorCitationCount": 7875,
    "paperId": "2a3213cb3c755f036d5dfec7261d726a819c78c1",
    "externalIds": {
      "DBLP": "conf/icml/ChangZBML00MFRL23",
      "ArXiv": "2301.00704",
      "DOI": "10.48550/arXiv.2301.00704",
      "CorpusId": 255372955
    },
    "url": "https://www.semanticscholar.org/paper/2a3213cb3c755f036d5dfec7261d726a819c78c1",
    "title": "Muse: Text-To-Image Generation via Masked Generative Transformers",
    "abstract": "We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io",
    "venue": "International Conference on Machine Learning",
    "year": 2023,
    "referenceCount": 87,
    "citationCount": 235,
    "influentialCitationCount": 17,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.00704",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2301.00704",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2914394",
        "name": "Huiwen Chang"
      },
      {
        "authorId": "2146204239",
        "name": "Han Zhang"
      },
      {
        "authorId": "152630175",
        "name": "Jarred Barber"
      },
      {
        "authorId": "2199119286",
        "name": "AJ Maschinot"
      },
      {
        "authorId": "143923528",
        "name": "Jos\u00e9 Lezama"
      },
      {
        "authorId": "39978626",
        "name": "Lu Jiang"
      },
      {
        "authorId": "152790163",
        "name": "Ming Yang"
      },
      {
        "authorId": "1702318",
        "name": "K. Murphy"
      },
      {
        "authorId": "1768236",
        "name": "W. Freeman"
      },
      {
        "authorId": "144544291",
        "name": "Michael Rubinstein"
      },
      {
        "authorId": "2167749913",
        "name": "Yuanzhen Li"
      },
      {
        "authorId": "1707347",
        "name": "Dilip Krishnan"
      }
    ]
  },
  {
    "profName": "Lu Jiang",
    "authorId": "39978626",
    "authorName": "Lu Jiang",
    "authorUrl": "https://www.semanticscholar.org/author/39978626",
    "authorHIndex": 41,
    "authorAffiliations": [
      "Google Research"
    ],
    "authorPaperCount": 79,
    "authorCitationCount": 7875,
    "paperId": "376f494126d1ea4f571ea0263c43ac2b6331800a",
    "externalIds": {
      "ArXiv": "2306.17842",
      "DBLP": "journals/corr/abs-2306-17842",
      "DOI": "10.48550/arXiv.2306.17842",
      "CorpusId": 259308960
    },
    "url": "https://www.semanticscholar.org/paper/376f494126d1ea4f571ea0263c43ac2b6331800a",
    "title": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs",
    "abstract": "In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 52,
    "citationCount": 9,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.17842",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.17842",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "8547960",
        "name": "Lijun Yu"
      },
      {
        "authorId": "2109716647",
        "name": "Yong Cheng"
      },
      {
        "authorId": "1390877035",
        "name": "Zhiruo Wang"
      },
      {
        "authorId": "2107989922",
        "name": "Vivek Kumar"
      },
      {
        "authorId": "3153147",
        "name": "Wolfgang Macherey"
      },
      {
        "authorId": "2145438541",
        "name": "Yanping Huang"
      },
      {
        "authorId": "144711958",
        "name": "David A. Ross"
      },
      {
        "authorId": "145955800",
        "name": "Irfan Essa"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "152790163",
        "name": "Ming Yang"
      },
      {
        "authorId": "1702318",
        "name": "K. Murphy"
      },
      {
        "authorId": "145788702",
        "name": "A. Hauptmann"
      },
      {
        "authorId": "39978626",
        "name": "Lu Jiang"
      }
    ]
  },
  {
    "profName": "Lu Jiang",
    "authorId": "39978626",
    "authorName": "Lu Jiang",
    "authorUrl": "https://www.semanticscholar.org/author/39978626",
    "authorHIndex": 41,
    "authorAffiliations": [
      "Google Research"
    ],
    "authorPaperCount": 79,
    "authorCitationCount": 7875,
    "paperId": "861370f7c2d18bed09905fde334a19cc96e83e14",
    "externalIds": {
      "ArXiv": "2306.00983",
      "DBLP": "journals/corr/abs-2306-00983",
      "DOI": "10.48550/arXiv.2306.00983",
      "CorpusId": 258999204
    },
    "url": "https://www.semanticscholar.org/paper/861370f7c2d18bed09905fde334a19cc96e83e14",
    "title": "StyleDrop: Text-to-Image Generation in Any Style",
    "abstract": "Pre-trained large text-to-image models synthesize impressive images with an appropriate use of text prompts. However, ambiguities inherent in natural language and out-of-distribution effects make it hard to synthesize image styles, that leverage a specific design pattern, texture or material. In this paper, we introduce StyleDrop, a method that enables the synthesis of images that faithfully follow a specific style using a text-to-image model. The proposed method is extremely versatile and captures nuances and details of a user-provided style, such as color schemes, shading, design patterns, and local and global effects. It efficiently learns a new style by fine-tuning very few trainable parameters (less than $1\\%$ of total model parameters) and improving the quality via iterative training with either human or automated feedback. Better yet, StyleDrop is able to deliver impressive results even when the user supplies only a single image that specifies the desired style. An extensive study shows that, for the task of style tuning text-to-image models, StyleDrop implemented on Muse convincingly outperforms other methods, including DreamBooth and textual inversion on Imagen or Stable Diffusion. More results are available at our project website: https://styledrop.github.io",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 43,
    "citationCount": 35,
    "influentialCitationCount": 4,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.00983",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.00983",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "1729571",
        "name": "Kihyuk Sohn"
      },
      {
        "authorId": "31601235",
        "name": "Nataniel Ruiz"
      },
      {
        "authorId": "3436470",
        "name": "Kimin Lee"
      },
      {
        "authorId": "2218790269",
        "name": "Daniel Castro Chin"
      },
      {
        "authorId": "2197077579",
        "name": "Irina Blok"
      },
      {
        "authorId": "2914394",
        "name": "Huiwen Chang"
      },
      {
        "authorId": "152630175",
        "name": "Jarred Barber"
      },
      {
        "authorId": "39978626",
        "name": "Lu Jiang"
      },
      {
        "authorId": "2905100",
        "name": "Glenn Entis"
      },
      {
        "authorId": "2167749913",
        "name": "Yuanzhen Li"
      },
      {
        "authorId": "2153968179",
        "name": "Yuan Hao"
      },
      {
        "authorId": "145955800",
        "name": "Irfan Essa"
      },
      {
        "authorId": "144544291",
        "name": "Michael Rubinstein"
      },
      {
        "authorId": "1707347",
        "name": "Dilip Krishnan"
      }
    ]
  },
  {
    "profName": "Lu Jiang",
    "authorId": "39978626",
    "authorName": "Lu Jiang",
    "authorUrl": "https://www.semanticscholar.org/author/39978626",
    "authorHIndex": 41,
    "authorAffiliations": [
      "Google Research"
    ],
    "authorPaperCount": 79,
    "authorCitationCount": 7875,
    "paperId": "8670133f0839a83c47c5bd2a506d0254d73f1a2c",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-00763",
      "ArXiv": "2306.00763",
      "DOI": "10.48550/arXiv.2306.00763",
      "CorpusId": 258999626
    },
    "url": "https://www.semanticscholar.org/paper/8670133f0839a83c47c5bd2a506d0254d73f1a2c",
    "title": "Learning Disentangled Prompts for Compositional Image Synthesis",
    "abstract": "We study domain-adaptive image synthesis, the problem of teaching pretrained image generative models a new style or concept from as few as one image to synthesize novel images, to better understand the compositional image synthesis. We present a framework that leverages a pretrained class-conditional generation model and visual prompt tuning. Specifically, we propose a novel source class distilled visual prompt that learns disentangled prompts of semantic (e.g., class) and domain (e.g., style) from a few images. Learned domain prompt is then used to synthesize images of any classes in the style of target domain. We conduct studies on various target domains with the number of images ranging from one to a few to many, and show qualitative results which show the compositional generalization of our method. Moreover, we show that our method can help improve zero-shot domain adaptation classification accuracy.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 61,
    "citationCount": 4,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.00763",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.00763",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "1729571",
        "name": "Kihyuk Sohn"
      },
      {
        "authorId": "47291134",
        "name": "Albert Eaton Shaw"
      },
      {
        "authorId": "2153968179",
        "name": "Yuan Hao"
      },
      {
        "authorId": "2146204239",
        "name": "Han Zhang"
      },
      {
        "authorId": "2454625",
        "name": "Luisa F. Polan\u00eda"
      },
      {
        "authorId": "2914394",
        "name": "Huiwen Chang"
      },
      {
        "authorId": "39978626",
        "name": "Lu Jiang"
      },
      {
        "authorId": "145955800",
        "name": "Irfan Essa"
      }
    ]
  },
  {
    "profName": "Lu Jiang",
    "authorId": "39978626",
    "authorName": "Lu Jiang",
    "authorUrl": "https://www.semanticscholar.org/author/39978626",
    "authorHIndex": 41,
    "authorAffiliations": [
      "Google Research"
    ],
    "authorPaperCount": 79,
    "authorCitationCount": 7875,
    "paperId": "985f0c89c5a607742ec43c1fdc2cbfe54541cbad",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-05737",
      "ArXiv": "2310.05737",
      "DOI": "10.48550/arXiv.2310.05737",
      "CorpusId": 263830733
    },
    "url": "https://www.semanticscholar.org/paper/985f0c89c5a607742ec43c1fdc2cbfe54541cbad",
    "title": "Language Model Beats Diffusion - Tokenizer is Key to Visual Generation",
    "abstract": "While Large Language Models (LLMs) are the dominant models for generative tasks in language, they do not perform as well as diffusion models on image and video generation. To effectively use LLMs for visual generation, one crucial component is the visual tokenizer that maps pixel-space inputs to discrete tokens appropriate for LLM learning. In this paper, we introduce MAGVIT-v2, a video tokenizer designed to generate concise and expressive tokens for both videos and images using a common token vocabulary. Equipped with this new tokenizer, we show that LLMs outperform diffusion models on standard image and video generation benchmarks including ImageNet and Kinetics. In addition, we demonstrate that our tokenizer surpasses the previously top-performing video tokenizer on two more tasks: (1) video compression comparable to the next-generation video codec (VCC) according to human evaluations, and (2) learning effective representations for action recognition tasks.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 79,
    "citationCount": 12,
    "influentialCitationCount": 2,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.05737",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2310.05737",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "8547960",
        "name": "Lijun Yu"
      },
      {
        "authorId": "2256999290",
        "name": "Jos'e Lezama"
      },
      {
        "authorId": "1387987945",
        "name": "Nitesh B. Gundavarapu"
      },
      {
        "authorId": "2256995349",
        "name": "Luca Versari"
      },
      {
        "authorId": "2256996545",
        "name": "Kihyuk Sohn"
      },
      {
        "authorId": "3144223",
        "name": "David C. Minnen"
      },
      {
        "authorId": "2198464317",
        "name": "Yong Cheng"
      },
      {
        "authorId": "2265716291",
        "name": "Agrim Gupta"
      },
      {
        "authorId": "2257336985",
        "name": "Xiuye Gu"
      },
      {
        "authorId": "2257000091",
        "name": "Alexander G. Hauptmann"
      },
      {
        "authorId": "2257000670",
        "name": "Boqing Gong"
      },
      {
        "authorId": "2257132345",
        "name": "Ming-Hsuan Yang"
      },
      {
        "authorId": "145955800",
        "name": "Irfan Essa"
      },
      {
        "authorId": "2257003564",
        "name": "David A. Ross"
      },
      {
        "authorId": "39978626",
        "name": "Lu Jiang"
      }
    ]
  },
  {
    "profName": "Lu Jiang",
    "authorId": "39978626",
    "authorName": "Lu Jiang",
    "authorUrl": "https://www.semanticscholar.org/author/39978626",
    "authorHIndex": 41,
    "authorAffiliations": [
      "Google Research"
    ],
    "authorPaperCount": 79,
    "authorCitationCount": 7875,
    "paperId": "c5202ab27294d5c1eb4d2f0ca7e82afef91888f0",
    "externalIds": {
      "DBLP": "journals/corr/abs-2307-03166",
      "ArXiv": "2307.03166",
      "DOI": "10.48550/arXiv.2307.03166",
      "CorpusId": 259360947
    },
    "url": "https://www.semanticscholar.org/paper/c5202ab27294d5c1eb4d2f0ca7e82afef91888f0",
    "title": "VideoGLUE: Video General Understanding Evaluation of Foundation Models",
    "abstract": "We evaluate existing foundation models video understanding capabilities using a carefully designed experiment protocol consisting of three hallmark tasks (action recognition, temporal localization, and spatiotemporal localization), eight datasets well received by the community, and four adaptation methods tailoring a foundation model (FM) for a downstream task. Moreover, we propose a scalar VideoGLUE score (VGS) to measure an FMs efficacy and efficiency when adapting to general video understanding tasks. Our main findings are as follows. First, task-specialized models significantly outperform the six FMs studied in this work, in sharp contrast to what FMs have achieved in natural language and image understanding. Second,video-native FMs, whose pretraining data contains the video modality, are generally better than image-native FMs in classifying motion-rich videos, localizing actions in time, and understanding a video of more than one action. Third, the video-native FMs can perform well on video tasks under light adaptations to downstream tasks(e.g., freezing the FM backbones), while image-native FMs win in full end-to-end finetuning. The first two observations reveal the need and tremendous opportunities to conduct research on video-focused FMs, and the last confirms that both tasks and adaptation methods matter when it comes to the evaluation of FMs. Our code is released under: https://github.com/tensorflow/models/tree/master/official/projects/videoglue.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 64,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.03166",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2307.03166",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "36001694",
        "name": "Liangzhe Yuan"
      },
      {
        "authorId": "1387987945",
        "name": "Nitesh B. Gundavarapu"
      },
      {
        "authorId": "48096253",
        "name": "Long Zhao"
      },
      {
        "authorId": null,
        "name": "Hao Zhou"
      },
      {
        "authorId": "2115350367",
        "name": "Yin Cui"
      },
      {
        "authorId": "39978626",
        "name": "Lu Jiang"
      },
      {
        "authorId": "2184563494",
        "name": "Xu Yang"
      },
      {
        "authorId": "51502783",
        "name": "Menglin Jia"
      },
      {
        "authorId": "47447630",
        "name": "Tobias Weyand"
      },
      {
        "authorId": "2217253911",
        "name": "Luke Friedman"
      },
      {
        "authorId": "89903811",
        "name": "Mikhail Sirotenko"
      },
      {
        "authorId": "3154495",
        "name": "H. Wang"
      },
      {
        "authorId": "3302320",
        "name": "Florian Schroff"
      },
      {
        "authorId": "2595180",
        "name": "Hartwig Adam"
      },
      {
        "authorId": "152790163",
        "name": "Ming Yang"
      },
      {
        "authorId": "2115431213",
        "name": "Ting Liu"
      },
      {
        "authorId": "40206014",
        "name": "Boqing Gong"
      }
    ]
  }
]