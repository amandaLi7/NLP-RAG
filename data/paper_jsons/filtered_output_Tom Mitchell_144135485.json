[
  {
    "authorId": "144135485",
    "authorName": "Tom M. Mitchell",
    "authorUrl": "https://www.semanticscholar.org/author/144135485",
    "authorHIndex": 12,
    "authorAffiliations": [],
    "authorPaperCount": 40,
    "authorCitationCount": 1072,
    "paperId": "5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-02412",
      "ArXiv": "2305.02412",
      "DOI": "10.48550/arXiv.2305.02412",
      "CorpusId": 258480064
    },
    "url": "https://www.semanticscholar.org/paper/5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f",
    "title": "Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents",
    "abstract": "Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.02412",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.02412",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "46220633",
        "name": "Yue Wu"
      },
      {
        "authorId": "2008204295",
        "name": "So Yeon Min"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      },
      {
        "authorId": "1746466",
        "name": "A. Azaria"
      },
      {
        "authorId": "152244300",
        "name": "Yuan-Fang Li"
      },
      {
        "authorId": "144135485",
        "name": "Tom M. Mitchell"
      },
      {
        "authorId": "9358910",
        "name": "Shrimai Prabhumoye"
      }
    ]
  },
  {
    "authorId": "144135485",
    "authorName": "Tom M. Mitchell",
    "authorUrl": "https://www.semanticscholar.org/author/144135485",
    "authorHIndex": 12,
    "authorAffiliations": [],
    "authorPaperCount": 40,
    "authorCitationCount": 1072,
    "paperId": "a71809bc20c51fc047eff57d3d1abce2aec978d3",
    "externalIds": {
      "DBLP": "series/faia/SilverM23",
      "DOI": "10.48550/arXiv.2304.13626",
      "CorpusId": 260444794
    },
    "url": "https://www.semanticscholar.org/paper/a71809bc20c51fc047eff57d3d1abce2aec978d3",
    "title": "The Roles of Symbols in Neural-based AI: They are Not What You Think!",
    "abstract": null,
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2304.13626",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "1-28"
    },
    "authors": [
      {
        "authorId": "49601276",
        "name": "D. Silver"
      },
      {
        "authorId": "144135485",
        "name": "Tom M. Mitchell"
      }
    ]
  },
  {
    "authorId": "144135485",
    "authorName": "Tom M. Mitchell",
    "authorUrl": "https://www.semanticscholar.org/author/144135485",
    "authorHIndex": 12,
    "authorAffiliations": [],
    "authorPaperCount": 40,
    "authorCitationCount": 1072,
    "paperId": "f406aceba4f29cc7cfbe7edb2f52f01374486589",
    "externalIds": {
      "DBLP": "conf/emnlp/AzariaM23",
      "ArXiv": "2304.13734",
      "DOI": "10.48550/arXiv.2304.13734",
      "CorpusId": 258352729
    },
    "url": "https://www.semanticscholar.org/paper/f406aceba4f29cc7cfbe7edb2f52f01374486589",
    "title": "The Internal State of an LLM Knows When its Lying",
    "abstract": "While Large Language Models (LLMs) have shown exceptional performance in various tasks, one of their most prominent drawbacks is generating inaccurate or false information with a confident tone. In this paper, we provide evidence that the LLM's internal state can be used to reveal the truthfulness of statements. This includes both statements provided to the LLM, and statements that the LLM itself generates. Our approach is to train a classifier that outputs the probability that a statement is truthful, based on the hidden layer activations of the LLM as it reads or generates the statement. Experiments demonstrate that given a set of test sentences, of which half are true and half false, our trained classifier achieves an average of 71\\% to 83\\% accuracy labeling which sentences are true versus false, depending on the LLM base model. Furthermore, we explore the relationship between our classifier's performance and approaches based on the probability assigned to the sentence by the LLM. We show that while LLM-assigned sentence probability is related to sentence truthfulness, this probability is also dependent on sentence length and the frequencies of words in the sentence, resulting in our trained classifier providing a more reliable approach to detecting truthfulness, highlighting its potential to enhance the reliability of LLM-generated content and its practical applicability in real-world scenarios.",
    "year": 2023,
    "influentialCitationCount": 5,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2304.13734",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "967-976"
    },
    "authors": [
      {
        "authorId": "1746466",
        "name": "A. Azaria"
      },
      {
        "authorId": "144135485",
        "name": "Tom M. Mitchell"
      }
    ]
  }
]