[
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "0a425c0d87c674b142104a07e17c5084b3ad28ca",
    "externalIds": {
      "DBLP": "journals/corr/abs-2302-12247",
      "DOI": "10.48550/arXiv.2302.12247",
      "CorpusId": 257102902
    },
    "url": "https://www.semanticscholar.org/paper/0a425c0d87c674b142104a07e17c5084b3ad28ca",
    "title": "Quantifying & Modeling Feature Interactions: An Information Decomposition Framework",
    "abstract": "The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and inte-grating information from different signals. Despite these empirical advances, there remain fundamental research questions: how can we quantify the nature of interactions that exist among input features? Subsequently, how can we capture these interactions using suitable data-driven methods? To answer this question, we propose an information-theoretic approach to quantify the degree of redundancy , uniqueness , and synergy across input features, which we term the PID statistics of a multimodal distribution. Using 2 newly proposed estimators that scale to high-dimensional distributions, we demonstrate their usefulness in quantifying the interactions within multimodal datasets, the nature of interactions captured by multimodal models, and principled approaches for model selection. We conduct extensive experiments on both synthetic datasets where the PID statistics are known and on large-scale multimodal benchmarks where PID estimation was previously impossible. Finally, to demonstrate the real-world applicability of our approach, we present three case studies in pathology, mood prediction, and robotic perception where our framework accurately recommends strong multimodal models for each application.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 93,
    "citationCount": 9,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2302.12247",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2302.12247",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "2153511720",
        "name": "Yun Cheng"
      },
      {
        "authorId": "2152774190",
        "name": "Xiang Fan"
      },
      {
        "authorId": "37087787",
        "name": "Chun Kai Ling"
      },
      {
        "authorId": "2199946235",
        "name": "Suzanne Nie"
      },
      {
        "authorId": "2108279369",
        "name": "Richard J. Chen"
      },
      {
        "authorId": "4692365",
        "name": "Zihao Deng"
      },
      {
        "authorId": "37122655",
        "name": "Faisal Mahmood"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "114eafdb14145f002d503f259d768d67dae87479",
    "externalIds": {
      "DOI": "10.1167/jov.23.9.5487",
      "CorpusId": 261372998
    },
    "url": "https://www.semanticscholar.org/paper/114eafdb14145f002d503f259d768d67dae87479",
    "title": "Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings",
    "abstract": null,
    "venue": "Journal of Vision",
    "year": 2023,
    "referenceCount": 0,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": null,
    "fieldsOfStudy": null,
    "journal": {
      "name": "Journal of Vision"
    },
    "authors": [
      {
        "authorId": "49772780",
        "name": "Arish Alreja"
      },
      {
        "authorId": "152813999",
        "name": "Michael J. Ward"
      },
      {
        "authorId": "2236474925",
        "name": "J. A. Colan"
      },
      {
        "authorId": "2237087099",
        "name": "Qianli Ma"
      },
      {
        "authorId": "2053790192",
        "name": "R. M. Richardson"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      },
      {
        "authorId": "10294075",
        "name": "A. Ghuman"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "40fb36ee67fdde99b196b4d1772de114aa821698",
    "externalIds": {
      "ArXiv": "2306.16413",
      "DBLP": "journals/corr/abs-2306-16413",
      "DOI": "10.48550/arXiv.2306.16413",
      "CorpusId": 259274645
    },
    "url": "https://www.semanticscholar.org/paper/40fb36ee67fdde99b196b4d1772de114aa821698",
    "title": "MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning",
    "abstract": "Learning multimodal representations involves integrating information from multiple heterogeneous sources of data. In order to accelerate progress towards understudied modalities and tasks while ensuring real-world robustness, we release MultiZoo, a public toolkit consisting of standardized implementations of>20 core multimodal algorithms and MultiBench, a large-scale benchmark spanning 15 datasets, 10 modalities, 20 prediction tasks, and 6 research areas. Together, these provide an automated end-to-end machine learning pipeline that simplifies and standardizes data loading, experimental setup, and model evaluation. To enable holistic evaluation, we offer a comprehensive methodology to assess (1) generalization, (2) time and space complexity, and (3) modality robustness. MultiBench paves the way towards a better understanding of the capabilities and limitations of multimodal models, while ensuring ease of use, accessibility, and reproducibility. Our toolkits are publicly available, will be regularly updated, and welcome inputs from the community.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 48,
    "citationCount": 2,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.16413",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.16413",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "2066413750",
        "name": "Yiwei Lyu"
      },
      {
        "authorId": "2152774190",
        "name": "Xiang Fan"
      },
      {
        "authorId": "2152116534",
        "name": "Arav Agarwal"
      },
      {
        "authorId": "2153511720",
        "name": "Yun Cheng"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "47a4ac301820c3ea7da4efb8e2466cc6468ad631",
    "externalIds": {
      "ArXiv": "2305.14728",
      "DBLP": "conf/acl/0001M23",
      "DOI": "10.48550/arXiv.2305.14728",
      "CorpusId": 258866171
    },
    "url": "https://www.semanticscholar.org/paper/47a4ac301820c3ea7da4efb8e2466cc6468ad631",
    "title": "SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations",
    "abstract": "Although deep language representations have become the dominant form of language featurization in recent years, in many settings it is important to understand a model's decision-making process. This necessitates not only an interpretable model but also interpretable features. In particular, language must be featurized in a way that is interpretable while still characterizing the original text well. We present SenteCon, a method for introducing human interpretability in deep language representations. Given a passage of text, SenteCon encodes the text as a layer of interpretable categories in which each dimension corresponds to the relevance of a specific category. Our empirical evaluations indicate that encoding language with SenteCon provides high-level interpretability at little to no cost to predictive performance on downstream tasks. Moreover, we find that SenteCon outperforms existing interpretable language representations with respect to both its downstream performance and its agreement with human characterizations of the text.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 50,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14728",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "4312-4331"
    },
    "authors": [
      {
        "authorId": "2060138164",
        "name": "Victoria Lin"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "64703e760f662b1c0f647931bb63fe57e5ba91e4",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-08149",
      "ArXiv": "2306.08149",
      "DOI": "10.1145/3577190.3614115",
      "CorpusId": 259164865
    },
    "url": "https://www.semanticscholar.org/paper/64703e760f662b1c0f647931bb63fe57e5ba91e4",
    "title": "Neural Mixed Effects for Nonlinear Personalized Predictions",
    "abstract": "Personalized prediction is a machine learning approach that predicts a person\u2019s future observations based on their past labeled observations and is typically used for sequential tasks, e.g., to predict daily mood ratings. When making personalized predictions, a model can combine two types of trends: (a) trends shared across people, i.e., person-generic trends, such as being happier on weekends, and (b) unique trends for each person, i.e., person-specific trends, such as a stressful weekly meeting. Mixed effect models are popular statistical models to study both trends by combining person-generic and person-specific parameters. Though linear mixed effect models are gaining popularity in machine learning by integrating them with neural networks, these integrations are currently limited to linear person-specific parameters: ruling out nonlinear person-specific trends. In this paper, we propose Neural Mixed Effect (NME) models to optimize nonlinear person-specific parameters anywhere in a neural network in a scalable manner1. NME combines the efficiency of neural network optimization with nonlinear mixed effects modeling. Empirically, we observe that NME improves performance across six unimodal and multimodal datasets, including a smartphone dataset to predict daily mood and a mother-adolescent dataset to predict affective state sequences where half the mothers experience symptoms of depression. Furthermore, we evaluate NME for two model architectures, including for neural conditional random fields (CRF) to predict affective state sequences where the CRF learns nonlinear person-specific temporal transitions between affective states. Analysis of these person-specific transitions on the mother-adolescent dataset shows interpretable trends related to the mother\u2019s depression symptoms.",
    "venue": "International Conference on Multimodal Interaction",
    "year": 2023,
    "referenceCount": 61,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3577190.3614115",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "name": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    "authors": [
      {
        "authorId": "2159324",
        "name": "T. W\u00f6rtwein"
      },
      {
        "authorId": "2060747421",
        "name": "Nicholas Allen"
      },
      {
        "authorId": "2165789644",
        "name": "Lisa B. Sheeber"
      },
      {
        "authorId": "5512630",
        "name": "R. Auerbach"
      },
      {
        "authorId": "1737918",
        "name": "J. Cohn"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "6838c43e702a3f995967ba2e3edd5f65ff5f5511",
    "externalIds": {
      "DBLP": "conf/icmi/BilalpurHCSAMC23",
      "DOI": "10.1145/3577190.3614136",
      "CorpusId": 263629967
    },
    "url": "https://www.semanticscholar.org/paper/6838c43e702a3f995967ba2e3edd5f65ff5f5511",
    "title": "SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior",
    "abstract": "Depression strongly impacts parents\u2019 behavior. Does parents\u2019 depression strongly affect the behavior of their children as well? To investigate this question, we compared dyadic interactions between 73 depressed and 75 non-depressed mothers and their adolescent child. Families were of low income and 84% were white. Child behavior was measured from audio-video recordings using manual annotation of verbal and nonverbal behavior by expert coders and by multimodal computational measures of facial expression, face and head dynamics, prosody, speech behavior, and linguistics. For both sets of measures, we used Support Vector Machines. For computational measures, we investigated the relative contribution of single versus multiple modalities using a novel approach to SHapley Additive exPlanations (SHAP). Computational measures outperformed manual ratings by human experts. Among individual computational measures, prosody was the most informative. SHAP reduction resulted in a four-fold decrease in the number of features and highest performance (77% accuracy; positive and negative agreements at 75% and 76%, respectively). These findings suggest that maternal depression strongly impacts the behavior of adolescent children; differences are most revealed in prosody; multimodal features together with SHAP reduction are most powerful.",
    "venue": "International Conference on Multimodal Interaction",
    "year": 2023,
    "referenceCount": 36,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3577190.3614136",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "name": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    "authors": [
      {
        "authorId": "23973985",
        "name": "Maneesh Bilalpur"
      },
      {
        "authorId": "71088012",
        "name": "Saurabh Hinduja"
      },
      {
        "authorId": "2253782441",
        "name": "Laura Cariola"
      },
      {
        "authorId": "2165789644",
        "name": "Lisa B. Sheeber"
      },
      {
        "authorId": "2253741044",
        "name": "Nicholas B Allen"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      },
      {
        "authorId": "2206204148",
        "name": "Jeffrey F. Cohn"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "7dab13685363176edc5cc7882d0890811d2cb584",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-14083",
      "ArXiv": "2305.14083",
      "DOI": "10.48550/arXiv.2305.14083",
      "CorpusId": 258841779
    },
    "url": "https://www.semanticscholar.org/paper/7dab13685363176edc5cc7882d0890811d2cb584",
    "title": "Counterfactual Augmentation for Multimodal Learning Under Presentation Bias",
    "abstract": "In real-world machine learning systems, labels are often derived from user behaviors that the system wishes to encourage. Over time, new models must be trained as new training examples and features become available. However, feedback loops between users and models can bias future user behavior, inducing a presentation bias in the labels that compromises the ability to train new models. In this paper, we propose counterfactual augmentation, a novel causal method for correcting presentation bias using generated counterfactual labels. Our empirical evaluations demonstrate that counterfactual augmentation yields better downstream performance compared to both uncorrected models and existing bias-correction methods. Model analyses further indicate that the generated counterfactuals align closely with true counterfactuals in an oracle setting.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 31,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14083",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "592-606"
    },
    "authors": [
      {
        "authorId": "2060138293",
        "name": "Victoria Lin"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      },
      {
        "authorId": "1780137",
        "name": "D. Dimitriadis"
      },
      {
        "authorId": "2109668081",
        "name": "Srinagesh Sharma"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "8d0c37eee7162f33178979b4183f0211e2dcae0d",
    "externalIds": {
      "ArXiv": "2305.14577",
      "DBLP": "journals/corr/abs-2305-14577",
      "DOI": "10.48550/arXiv.2305.14577",
      "CorpusId": 258865571
    },
    "url": "https://www.semanticscholar.org/paper/8d0c37eee7162f33178979b4183f0211e2dcae0d",
    "title": "Difference-Masking: Choosing What to Mask in Continued Pretraining",
    "abstract": "The self-supervised objective of masking-and-predicting has led to promising performance gains on a variety of downstream tasks. However, while most approaches randomly mask tokens, there is strong intuition that deciding what to mask can substantially improve learning outcomes. We investigate this in continued pretraining setting in which pretrained models continue to pretrain on domain-specific data before performing some downstream task. We introduce Difference-Masking, a masking strategy that automatically chooses what to mask during continued pretraining by considering what makes a task domain different from the pretraining domain. Empirically, we find that Difference-Masking outperforms baselines on continued pretraining settings across four diverse language-only and multimodal video tasks.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 56,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14577",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.14577",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2000786644",
        "name": "Alex Wilf"
      },
      {
        "authorId": "1900302322",
        "name": "Syeda Nahida Akter"
      },
      {
        "authorId": "1413897871",
        "name": "Leena Mathur"
      },
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "31322912",
        "name": "Sheryl Mathew"
      },
      {
        "authorId": "2218143553",
        "name": "Mengrou Shou"
      },
      {
        "authorId": "46841006",
        "name": "Eric Nyberg"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "8d53c510928ad1164aebea4d9477812ed1893be2",
    "externalIds": {
      "DBLP": "conf/icmi/MathurMM23",
      "ArXiv": "2305.10827",
      "DOI": "10.1145/3577190.3614171",
      "CorpusId": 258762712
    },
    "url": "https://www.semanticscholar.org/paper/8d53c510928ad1164aebea4d9477812ed1893be2",
    "title": "Expanding the Role of Affective Phenomena in Multimodal Interaction Research",
    "abstract": "In recent decades, the field of affective computing has made substantial progress in advancing the ability of AI systems to recognize and express affective phenomena, such as affect and emotions, during human-human and human-machine interactions. This paper describes our examination of research at the intersection of multimodal interaction and affective computing, with the objective of observing trends and identifying understudied areas. We examined over 16,000 papers from selected conferences in multimodal interaction, affective computing, and natural language processing: ACM International Conference on Multimodal Interaction, AAAC International Conference on Affective Computing and Intelligent Interaction, Annual Meeting of the Association for Computational Linguistics, and Conference on Empirical Methods in Natural Language Processing. We identified 910 affect-related papers and present our analysis of the role of affective phenomena in these papers. We find that this body of research has primarily focused on enabling machines to recognize or express affect and emotion; there has been limited research on how affect and emotion predictions might, in turn, be used by AI systems to enhance machine understanding of human social behaviors and cognitive states. Based on our analysis, we discuss directions to expand the role of affective phenomena in multimodal interaction research.",
    "venue": "International Conference on Multimodal Interaction",
    "year": 2023,
    "referenceCount": 87,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3577190.3614171",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "name": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    "authors": [
      {
        "authorId": "1413897871",
        "name": "Leena Mathur"
      },
      {
        "authorId": "2217758079",
        "name": "Maja J Matari'c"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "90b09bdb1bd78875ee8d8d324a568a36955e4765",
    "externalIds": {
      "DBLP": "conf/icmi/LiangCSM23",
      "ArXiv": "2306.04125",
      "DOI": "10.1145/3577190.3614151",
      "CorpusId": 259095686
    },
    "url": "https://www.semanticscholar.org/paper/90b09bdb1bd78875ee8d8d324a568a36955e4765",
    "title": "Multimodal Fusion Interactions: A Study of Human and Automatic Quantification",
    "abstract": "In order to perform multimodal fusion of heterogeneous signals, we need to understand their interactions: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how humans annotate two categorizations of multimodal interactions: (1) partial labels, where different annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator annotates the label given the first modality before asking them to explicitly reason about how their answer changes when given the second. We further propose an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions, uniqueness: the extent to which one modality enables a prediction that the other does not, and synergy: the extent to which both modalities enable one to make a prediction that one would not otherwise make using individual modalities. Through experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate and efficient method for quantifying multimodal interactions.",
    "venue": "International Conference on Multimodal Interaction",
    "year": 2023,
    "referenceCount": 79,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3577190.3614151",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "name": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    "authors": [
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "2153511720",
        "name": "Yun Cheng"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "a988c09b7e76e86a93edcbf3f284dd028b0fb406",
    "externalIds": {
      "ArXiv": "2306.04539",
      "DBLP": "journals/corr/abs-2306-04539",
      "DOI": "10.48550/arXiv.2306.04539",
      "CorpusId": 259096096
    },
    "url": "https://www.semanticscholar.org/paper/a988c09b7e76e86a93edcbf3f284dd028b0fb406",
    "title": "Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications",
    "abstract": "In many machine learning systems that jointly learn from multiple modalities, a core research question is to understand the nature of multimodal interactions: the emergence of new task-relevant information during learning from both modalities that was not present in either alone. We study this challenge of interaction quantification in a semi-supervised setting with only labeled unimodal data and naturally co-occurring multimodal data (e.g., unlabeled images and captions, video and corresponding audio) but when labeling them is time-consuming. Using a precise information-theoretic definition of interactions, our key contributions are the derivations of lower and upper bounds to quantify the amount of multimodal interactions in this semi-supervised setting. We propose two lower bounds based on the amount of shared information between modalities and the disagreement between separately trained unimodal classifiers, and derive an upper bound through connections to approximate algorithms for min-entropy couplings. We validate these estimated bounds and show how they accurately track true interactions. Finally, two semi-supervised multimodal applications are explored based on these theoretical results: (1) analyzing the relationship between multimodal performance and estimated interactions, and (2) self-supervised learning that embraces disagreement between modalities beyond agreement as is typically done.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 107,
    "citationCount": 3,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.04539",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "journal": {
      "volume": "abs/2306.04539",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "37087787",
        "name": "Chun Kai Ling"
      },
      {
        "authorId": "2153511720",
        "name": "Yun Cheng"
      },
      {
        "authorId": "71931352",
        "name": "A. Obolenskiy"
      },
      {
        "authorId": "2144409312",
        "name": "Yudong Liu"
      },
      {
        "authorId": "1471734043",
        "name": "Rohan Pandey"
      },
      {
        "authorId": "2000786644",
        "name": "Alex Wilf"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "bd94ea913fcf8698f2257f87a17755b46a420458",
    "externalIds": {
      "DBLP": "conf/icmi/VailGBFSCM23",
      "DOI": "10.1145/3577190.3614118",
      "CorpusId": 263742861
    },
    "url": "https://www.semanticscholar.org/paper/bd94ea913fcf8698f2257f87a17755b46a420458",
    "title": "Representation Learning for Interpersonal and Multimodal Behavior Dynamics: A Multiview Extension of Latent Change Score Models",
    "abstract": "Characterizing the dynamics of behavior across multiple modalities and individuals is a vital component of computational behavior analysis. This is especially important in certain applications, such as psychotherapy, where individualized tracking of behavior patterns can provide valuable information about the patient\u2019s mental state. Conventional methods that rely on aggregate statistics and correlational metrics may not always suffice, as they are often unable to capture causal relationships or evaluate the true probability of identified patterns. To address these challenges, we present a novel approach to learning multimodal and interpersonal representations of behavior dynamics during one-on-one interaction. Our approach is enabled by the introduction of a multiview extension of latent change score models, which facilitates the concurrent capture of both inter-modal and interpersonal behavior dynamics and the identification of directional relationships between them. A core advantage of our approach is its high level of interpretability while simultaneously achieving strong predictive performance. We evaluate our approach within the domain of therapist-client interactions, with the objective of gaining a deeper understanding about the collaborative relationship between the two, a crucial element of the therapeutic process. Our results demonstrate improved performance over conventional approaches that rely upon summary statistics or correlational metrics. Furthermore, since our multiview approach includes the explicit modeling of uncertainty, it naturally lends itself to integration with probabilistic classifiers, such as Gaussian process models. We demonstrate that this integration leads to even further improved performance, all the while maintaining highly interpretable qualities. Our analysis provides compelling motivation for further exploration of stochastic systems within computational models of behavior.",
    "venue": "International Conference on Multimodal Interaction",
    "year": 2023,
    "referenceCount": 57,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3577190.3614118",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "name": "Proceedings of the 25th International Conference on Multimodal Interaction"
    },
    "authors": [
      {
        "authorId": "2404385",
        "name": "A. Vail"
      },
      {
        "authorId": "36185909",
        "name": "J. Girard"
      },
      {
        "authorId": "3822686",
        "name": "Lauren M. Bylsma"
      },
      {
        "authorId": "2053154214",
        "name": "Jay Fournier"
      },
      {
        "authorId": "2256151835",
        "name": "Holly A. Swartz"
      },
      {
        "authorId": "2206204148",
        "name": "Jeffrey F. Cohn"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "d01cc51c0d06583b809833a5f7ce71101d278528",
    "externalIds": {
      "DBLP": "conf/chi/LiangLCJDWMS23",
      "DOI": "10.1145/3544549.3585604",
      "CorpusId": 258217161
    },
    "url": "https://www.semanticscholar.org/paper/d01cc51c0d06583b809833a5f7ce71101d278528",
    "title": "MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models",
    "abstract": "The nature of human and computer interactions are inherently multimodal, which has led to substantial interest in building interpretable, interactive, and reliable multimodal interfaces. However, modern multimodal models and interfaces are typically black-box neural networks, which makes it challenging to understand their internal mechanics. How can we visualize their internal workings in order to empower stakeholders to visualize model behavior, perform model debugging, and promote trust in these models? Our paper proposes MultiViz, a method for analyzing the behavior of multimodal models via 4 stages: (1) unimodal importance, (2) cross-modal interactions, (3) multimodal representations and (4) multimodal prediction. MultiViz includes modular visualization tools for each stage before combining outputs from all stages through an interactive and human-in-the-loop API. Through user studies with 21 participants on 8 trained models across 6 real-world tasks, we show that the complementary stages in MultiViz together enable users to (1) simulate model predictions, (2) assign interpretable concepts to features, (3) perform error analysis on model misclassifications, and (4) use insights from error analysis to debug models. MultiViz is publicly available at https://github.com/pliang279/MultiViz, will be regularly updated with new visualization tools and metrics, and welcomes input from the community1.",
    "venue": "CHI Extended Abstracts",
    "year": 2023,
    "referenceCount": 98,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "name": "Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems"
    },
    "authors": [
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "2066413750",
        "name": "Yiwei Lyu"
      },
      {
        "authorId": "1509809381",
        "name": "Gunjan Chhablani"
      },
      {
        "authorId": "2146677401",
        "name": "Nihal Jain"
      },
      {
        "authorId": "4692365",
        "name": "Zihao Deng"
      },
      {
        "authorId": "50141732",
        "name": "Xingbo Wang"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "dcb4f2b9b0e6da0d629878d1ad0469aee3df2020",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-04898",
      "ArXiv": "2306.04898",
      "DOI": "10.1109/CVPR52729.2023.00765",
      "CorpusId": 259108864
    },
    "url": "https://www.semanticscholar.org/paper/dcb4f2b9b0e6da0d629878d1ad0469aee3df2020",
    "title": "Understanding Masked Autoencoders via Hierarchical Latent Variable Models",
    "abstract": "Masked autoencoder (MAE), a simple and effective self-supervised learning framework based on the reconstruction of masked image regions, has recently achieved prominent success in a variety of vision tasks. Despite the emergence of intriguing empirical observations on MAE, a theoretically principled understanding is still lacking. In this work, we formally characterize and justify existing empirical in-sights and provide theoretical guarantees of MAE. We formulate the underlying data-generating process as a hierarchical latent variable model, and show that under reasonable assumptions, MAE provably identifies a set of latent variables in the hierarchical model, explaining why MAE can extract high-level information from pixels. Further, we show how key hyperparameters in MAE (the masking ratio and the patch size) determine which true latent variables to be recovered, therefore influencing the level of semantic information in the representation. Specifically, extremely large or small masking ratios inevitably lead to low-level representations. Our theory offers coherent explanations of existing empirical observations and provides insights for potential empirical improvements and fundamental limitations of the masked-reconstruction paradigm. We conduct extensive experiments to validate our theoretical insights.",
    "venue": "Computer Vision and Pattern Recognition",
    "year": 2023,
    "referenceCount": 74,
    "citationCount": 8,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2306.04898",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "7918-7928",
      "name": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
    },
    "authors": [
      {
        "authorId": "2069275317",
        "name": "Lingjing Kong"
      },
      {
        "authorId": "1384374825",
        "name": "Martin Q. Ma"
      },
      {
        "authorId": "2155315836",
        "name": "Guan-Hong Chen"
      },
      {
        "authorId": "143977260",
        "name": "E. Xing"
      },
      {
        "authorId": "1784472",
        "name": "Yuejie Chi"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      },
      {
        "authorId": "2175349484",
        "name": "Kun Zhang"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "e1b2a35a000ca296c32284b323c7e36a28fe0693",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-05268",
      "ArXiv": "2306.05268",
      "DOI": "10.48550/arXiv.2306.05268",
      "CorpusId": 259108395
    },
    "url": "https://www.semanticscholar.org/paper/e1b2a35a000ca296c32284b323c7e36a28fe0693",
    "title": "Factorized Contrastive Learning: Going Beyond Multi-view Redundancy",
    "abstract": "In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 94,
    "citationCount": 6,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.05268",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.05268",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "4692365",
        "name": "Zihao Deng"
      },
      {
        "authorId": "1384374825",
        "name": "Martin Q. Ma"
      },
      {
        "authorId": "145085305",
        "name": "James Y. Zou"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      }
    ]
  },
  {
    "authorId": "49933077",
    "authorName": "Louis-Philippe Morency",
    "authorUrl": "https://www.semanticscholar.org/author/49933077",
    "authorHIndex": 79,
    "authorAffiliations": [],
    "authorPaperCount": 443,
    "authorCitationCount": 28711,
    "paperId": "f891e9eeedbf20cdc54429ffcc0402a10f48494e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-04597",
      "ArXiv": "2306.04597",
      "ACL": "2023.acl-short.30",
      "DOI": "10.48550/arXiv.2306.04597",
      "CorpusId": 259095603
    },
    "url": "https://www.semanticscholar.org/paper/f891e9eeedbf20cdc54429ffcc0402a10f48494e",
    "title": "Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions",
    "abstract": "Societal biases present in pre-trained large language models are a critical issue as these models have been shown to propagate biases in countless downstream applications, rendering them unfair towards specific groups of people. Since large-scale retraining of these models from scratch is both time and compute-expensive, a variety of approaches have been previously proposed that de-bias a pre-trained model. While the majority of current state-of-the-art debiasing methods focus on changes to the training regime, in this paper, we propose data intervention strategies as a powerful yet simple technique to reduce gender bias in pre-trained models. Specifically, we empirically show that by fine-tuning a pre-trained model on only 10 debiased (intervened) training examples, the tendency to favor any gender is significantly reduced. Since our proposed method only needs a few training examples, we argue that our few-shot de-biasing approach is highly feasible and practical. Through extensive experimentation, we show that our de-biasing technique performs better than competitive state-of-the-art baselines with minimal loss in language modeling ability.",
    "venue": "Annual Meeting of the Association for Computational Linguistics",
    "year": 2023,
    "referenceCount": 31,
    "citationCount": 7,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.04597",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "340-351"
    },
    "authors": [
      {
        "authorId": "2221493995",
        "name": "Himanshu Thakur"
      },
      {
        "authorId": "1819271266",
        "name": "Atishay Jain"
      },
      {
        "authorId": "2127734657",
        "name": "Praneetha Vaddamanu"
      },
      {
        "authorId": "28130078",
        "name": "P. Liang"
      },
      {
        "authorId": "49933077",
        "name": "Louis-Philippe Morency"
      }
    ]
  }
]