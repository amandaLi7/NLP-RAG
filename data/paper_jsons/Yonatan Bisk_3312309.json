[
  {
    "authorId": "3312309",
    "authorName": "Yonatan Bisk",
    "authorUrl": "https://www.semanticscholar.org/author/3312309",
    "authorHIndex": 33,
    "authorAffiliations": [
      "Carnegie Mellon University"
    ],
    "authorPaperCount": 98,
    "authorCitationCount": 7048,
    "paperId": "376f494126d1ea4f571ea0263c43ac2b6331800a",
    "externalIds": {
      "ArXiv": "2306.17842",
      "DBLP": "journals/corr/abs-2306-17842",
      "DOI": "10.48550/arXiv.2306.17842",
      "CorpusId": 259308960
    },
    "url": "https://www.semanticscholar.org/paper/376f494126d1ea4f571ea0263c43ac2b6331800a",
    "title": "SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs",
    "abstract": "In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 52,
    "citationCount": 9,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.17842",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.17842",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "8547960",
        "name": "Lijun Yu"
      },
      {
        "authorId": "2109716647",
        "name": "Yong Cheng"
      },
      {
        "authorId": "1390877035",
        "name": "Zhiruo Wang"
      },
      {
        "authorId": "2107989922",
        "name": "Vivek Kumar"
      },
      {
        "authorId": "3153147",
        "name": "Wolfgang Macherey"
      },
      {
        "authorId": "2145438541",
        "name": "Yanping Huang"
      },
      {
        "authorId": "144711958",
        "name": "David A. Ross"
      },
      {
        "authorId": "145955800",
        "name": "Irfan Essa"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "152790163",
        "name": "Ming Yang"
      },
      {
        "authorId": "1702318",
        "name": "K. Murphy"
      },
      {
        "authorId": "145788702",
        "name": "A. Hauptmann"
      },
      {
        "authorId": "39978626",
        "name": "Lu Jiang"
      }
    ]
  },
  {
    "authorId": "3312309",
    "authorName": "Yonatan Bisk",
    "authorUrl": "https://www.semanticscholar.org/author/3312309",
    "authorHIndex": 33,
    "authorAffiliations": [
      "Carnegie Mellon University"
    ],
    "authorPaperCount": 98,
    "authorCitationCount": 7048,
    "paperId": "3b0c02955e88f5862e61b560c7f70ba8cf235b1d",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-11565",
      "ArXiv": "2306.11565",
      "DOI": "10.48550/arXiv.2306.11565",
      "CorpusId": 259203746
    },
    "url": "https://www.semanticscholar.org/paper/3b0c02955e88f5862e61b560c7f70ba8cf235b1d",
    "title": "HomeRobot: Open-Vocabulary Mobile Manipulation",
    "abstract": "HomeRobot (noun): An affordable compliant robot that navigates homes and manipulates a wide range of objects in order to complete everyday tasks. Open-Vocabulary Mobile Manipulation (OVMM) is the problem of picking any object in any unseen environment, and placing it in a commanded location. This is a foundational challenge for robots to be useful assistants in human environments, because it involves tackling sub-problems from across robotics: perception, language understanding, navigation, and manipulation are all essential to OVMM. In addition, integration of the solutions to these sub-problems poses its own substantial challenges. To drive research in this area, we introduce the HomeRobot OVMM benchmark, where an agent navigates household environments to grasp novel objects and place them on target receptacles. HomeRobot has two components: a simulation component, which uses a large and diverse curated object set in new, high-quality multi-room home environments; and a real-world component, providing a software stack for the low-cost Hello Robot Stretch to encourage replication of real-world experiments across labs. We implement both reinforcement learning and heuristic (model-based) baselines and show evidence of sim-to-real transfer. Our baselines achieve a 20% success rate in the real world; our experiments identify ways future research work improve performance. See videos on our website: https://ovmm.github.io/.",
    "venue": "Conference on Robot Learning",
    "year": 2023,
    "referenceCount": 115,
    "citationCount": 16,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.11565",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.11565",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2088846095",
        "name": "Sriram Yenamandra"
      },
      {
        "authorId": "1410121720",
        "name": "A. Ramachandran"
      },
      {
        "authorId": "1838683872",
        "name": "Karmesh Yadav"
      },
      {
        "authorId": "32451493",
        "name": "Austin S. Wang"
      },
      {
        "authorId": "2066532783",
        "name": "Mukul Khanna"
      },
      {
        "authorId": "81588783",
        "name": "Th\u00e9ophile Gervet"
      },
      {
        "authorId": "2220329333",
        "name": "Tsung-Yen Yang"
      },
      {
        "authorId": "2061364783",
        "name": "Vidhi Jain"
      },
      {
        "authorId": "30933599",
        "name": "Alexander Clegg"
      },
      {
        "authorId": "2115151351",
        "name": "John Turner"
      },
      {
        "authorId": "145276578",
        "name": "Z. Kira"
      },
      {
        "authorId": "2295141",
        "name": "M. Savva"
      },
      {
        "authorId": "145830541",
        "name": "Angel X. Chang"
      },
      {
        "authorId": "2142753065",
        "name": "Devendra Singh Chaplot"
      },
      {
        "authorId": "1746610",
        "name": "Dhruv Batra"
      },
      {
        "authorId": "3012475",
        "name": "Roozbeh Mottaghi"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "1977464",
        "name": "Chris Paxton"
      }
    ]
  },
  {
    "authorId": "3312309",
    "authorName": "Yonatan Bisk",
    "authorUrl": "https://www.semanticscholar.org/author/3312309",
    "authorHIndex": 33,
    "authorAffiliations": [
      "Carnegie Mellon University"
    ],
    "authorPaperCount": 98,
    "authorCitationCount": 7048,
    "paperId": "5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-02412",
      "ArXiv": "2305.02412",
      "DOI": "10.48550/arXiv.2305.02412",
      "CorpusId": 258480064
    },
    "url": "https://www.semanticscholar.org/paper/5ce2f1dff23a5620f77f9b11f1e534422ab8ff3f",
    "title": "Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents",
    "abstract": "Pre-trained large language models (LLMs) capture procedural knowledge about the world. Recent work has leveraged LLM's ability to generate abstract plans to simplify challenging control tasks, either by action scoring, or action modeling (fine-tuning). However, the transformer architecture inherits several constraints that make it difficult for the LLM to directly serve as the agent: e.g. limited input lengths, fine-tuning inefficiency, bias from pre-training, and incompatibility with non-text environments. To maintain compatibility with a low-level trainable actor, we propose to instead use the knowledge in LLMs to simplify the control problem, rather than solving it. We propose the Plan, Eliminate, and Track (PET) framework. The Plan module translates a task description into a list of high-level sub-tasks. The Eliminate module masks out irrelevant objects and receptacles from the observation for the current sub-task. Finally, the Track module determines whether the agent has accomplished each sub-task. On the AlfWorld instruction following benchmark, the PET framework leads to a significant 15% improvement over SOTA for generalization to human goal specifications.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 41,
    "citationCount": 16,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.02412",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.02412",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "46220633",
        "name": "Yue Wu"
      },
      {
        "authorId": "2008204295",
        "name": "So Yeon Min"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      },
      {
        "authorId": "1746466",
        "name": "A. Azaria"
      },
      {
        "authorId": "152244300",
        "name": "Yuan-Fang Li"
      },
      {
        "authorId": "144135485",
        "name": "Tom M. Mitchell"
      },
      {
        "authorId": "9358910",
        "name": "Shrimai Prabhumoye"
      }
    ]
  },
  {
    "authorId": "3312309",
    "authorName": "Yonatan Bisk",
    "authorUrl": "https://www.semanticscholar.org/author/3312309",
    "authorHIndex": 33,
    "authorAffiliations": [
      "Carnegie Mellon University"
    ],
    "authorPaperCount": 98,
    "authorCitationCount": 7048,
    "paperId": "69b8cd15966c4c9c3e44e71769e557f1c87fb3f9",
    "externalIds": {
      "DBLP": "journals/corr/abs-2309-08508",
      "ArXiv": "2309.08508",
      "DOI": "10.48550/arXiv.2309.08508",
      "CorpusId": 262012665
    },
    "url": "https://www.semanticscholar.org/paper/69b8cd15966c4c9c3e44e71769e557f1c87fb3f9",
    "title": "MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception",
    "abstract": "A holistic understanding of object properties across diverse sensory modalities (e.g., visual, audio, and haptic) is essential for tasks ranging from object categorization to complex manipulation. Drawing inspiration from cognitive science studies that emphasize the significance of multi-sensory integration in human perception, we introduce MOSAIC (Multi-modal Object property learning with Self-Attention and Integrated Comprehension), a novel framework designed to facilitate the learning of unified multi-sensory object property representations. While it is undeniable that visual information plays a prominent role, we acknowledge that many fundamental object properties extend beyond the visual domain to encompass attributes like texture, mass distribution, or sounds, which significantly influence how we interact with objects. In MOSAIC, we leverage this profound insight by distilling knowledge from the extensive pre-trained Contrastive Language-Image Pre-training (CLIP) model, aligning these representations not only across vision but also haptic and auditory sensory modalities. Through extensive experiments on a dataset where a humanoid robot interacts with 100 objects across 10 exploratory behaviors, we demonstrate the versatility of MOSAIC in two task families: object categorization and object-fetching tasks. Our results underscore the efficacy of MOSAIC's unified representations, showing competitive performance in category recognition through a simple linear probe setup and excelling in the fetch object task under zero-shot transfer conditions. This work pioneers the application of CLIP-based sensory grounding in robotics, promising a significant leap in multi-sensory perception capabilities for autonomous systems. We have released the code, datasets, and additional results: https://github.com/gtatiya/MOSAIC.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 35,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.08508",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2309.08508",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "1394711029",
        "name": "Gyan Tatiya"
      },
      {
        "authorId": "26253744",
        "name": "Jonathan M Francis"
      },
      {
        "authorId": "2243295123",
        "name": "Ho-Hsiang Wu"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "1715858",
        "name": "J. Sinapov"
      }
    ]
  },
  {
    "authorId": "3312309",
    "authorName": "Yonatan Bisk",
    "authorUrl": "https://www.semanticscholar.org/author/3312309",
    "authorHIndex": 33,
    "authorAffiliations": [
      "Carnegie Mellon University"
    ],
    "authorPaperCount": 98,
    "authorCitationCount": 7048,
    "paperId": "8035a247980cb18abf2bb7b9d96e7d4c63622ef2",
    "externalIds": {
      "ArXiv": "2309.10103",
      "DBLP": "journals/corr/abs-2309-10103",
      "DOI": "10.48550/arXiv.2309.10103",
      "CorpusId": 261945162
    },
    "url": "https://www.semanticscholar.org/paper/8035a247980cb18abf2bb7b9d96e7d4c63622ef2",
    "title": "Reasoning about the Unseen for Efficient Outdoor Object Navigation",
    "abstract": "Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 36,
    "citationCount": 1,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.10103",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2309.10103",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2242010169",
        "name": "Quanting Xie"
      },
      {
        "authorId": "2238206121",
        "name": "Tianyi Zhang"
      },
      {
        "authorId": "2243950262",
        "name": "Kedi Xu"
      },
      {
        "authorId": "1389944402",
        "name": "M. Johnson-Roberson"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      }
    ]
  },
  {
    "authorId": "3312309",
    "authorName": "Yonatan Bisk",
    "authorUrl": "https://www.semanticscholar.org/author/3312309",
    "authorHIndex": 33,
    "authorAffiliations": [
      "Carnegie Mellon University"
    ],
    "authorPaperCount": 98,
    "authorCitationCount": 7048,
    "paperId": "b777aa86b5a1d49ce8eababc5c2ee56d3562801e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2302-06117",
      "ArXiv": "2302.06117",
      "DOI": "10.48550/arXiv.2302.06117",
      "CorpusId": 256826923
    },
    "url": "https://www.semanticscholar.org/paper/b777aa86b5a1d49ce8eababc5c2ee56d3562801e",
    "title": "The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment",
    "abstract": "Increased focus on the computational efficiency of NLP systems has motivated the design of efficient model architectures and improvements to underlying hardware accelerators. However, the resulting increases in computational throughput and reductions in floating point operations have not directly translated to improvements in wall-clock inference latency. We demonstrate that these discrepancies can be largely attributed to bottlenecks introduced by deep learning frameworks. We denote this phenomenon as the \\textit{framework tax}, and observe that the disparity is growing as hardware speed increases over time. In this work, we examine this phenomenon through a series of case studies analyzing the effects of model design decisions, framework paradigms, and hardware platforms on total model latency. Code is available at https://github.com/JaredFern/Framework-Tax.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 64,
    "citationCount": 2,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.06117",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2302.06117",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "152793333",
        "name": "Jared Fernandez"
      },
      {
        "authorId": "39960571",
        "name": "Jacob Kahn"
      },
      {
        "authorId": "2166313248",
        "name": "Clara Na"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "2268272",
        "name": "Emma Strubell"
      }
    ]
  },
  {
    "authorId": "3312309",
    "authorName": "Yonatan Bisk",
    "authorUrl": "https://www.semanticscholar.org/author/3312309",
    "authorHIndex": 33,
    "authorAffiliations": [
      "Carnegie Mellon University"
    ],
    "authorPaperCount": 98,
    "authorCitationCount": 7048,
    "paperId": "e41482f4ee984f17382f6cdd900df094d928be06",
    "externalIds": {
      "ArXiv": "2307.13854",
      "DBLP": "journals/corr/abs-2307-13854",
      "DOI": "10.48550/arXiv.2307.13854",
      "CorpusId": 260164780
    },
    "url": "https://www.semanticscholar.org/paper/e41482f4ee984f17382f6cdd900df094d928be06",
    "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
    "abstract": "With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 63,
    "citationCount": 60,
    "influentialCitationCount": 7,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.13854",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2307.13854",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2149163534",
        "name": "Shuyan Zhou"
      },
      {
        "authorId": "40027632",
        "name": "Frank F. Xu"
      },
      {
        "authorId": "2115313911",
        "name": "Hao Zhu"
      },
      {
        "authorId": "144101734",
        "name": "Xuhui Zhou"
      },
      {
        "authorId": "145250604",
        "name": "Robert Lo"
      },
      {
        "authorId": "66820957",
        "name": "Abishek Sridhar"
      },
      {
        "authorId": "144691454",
        "name": "Xianyi Cheng"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "47070750",
        "name": "Daniel Fried"
      },
      {
        "authorId": "47051926",
        "name": "Uri Alon"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "3312309",
    "authorName": "Yonatan Bisk",
    "authorUrl": "https://www.semanticscholar.org/author/3312309",
    "authorHIndex": 33,
    "authorAffiliations": [
      "Carnegie Mellon University"
    ],
    "authorPaperCount": 98,
    "authorCitationCount": 7048,
    "paperId": "e7b3b692b0816821aafc0d354749bc3802cbf6ac",
    "externalIds": {
      "DBLP": "journals/corr/abs-2303-01502",
      "ArXiv": "2303.01502",
      "DOI": "10.48550/arXiv.2303.01502",
      "CorpusId": 257280165
    },
    "url": "https://www.semanticscholar.org/paper/e7b3b692b0816821aafc0d354749bc3802cbf6ac",
    "title": "Computational Language Acquisition with Theory of Mind",
    "abstract": "Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.",
    "venue": "International Conference on Learning Representations",
    "year": 2023,
    "referenceCount": 35,
    "citationCount": 6,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.01502",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2303.01502",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "46263614",
        "name": "Andy T. Liu"
      },
      {
        "authorId": "2115314674",
        "name": "Hao Zhu"
      },
      {
        "authorId": "1381444447",
        "name": "Emmy Liu"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  }
]