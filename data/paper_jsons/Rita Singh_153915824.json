[
  {
    "profName": "Rita Singh",
    "authorId": "153915824",
    "authorName": "Rita Singh",
    "authorUrl": "https://www.semanticscholar.org/author/153915824",
    "authorHIndex": 25,
    "authorAffiliations": [],
    "authorPaperCount": 146,
    "authorCitationCount": 2781,
    "paperId": "3bd320ddb25886417ae90011b00f13f5d558097b",
    "externalIds": {
      "ArXiv": "2307.08217",
      "DBLP": "journals/corr/abs-2307-08217",
      "DOI": "10.48550/arXiv.2307.08217",
      "CorpusId": 259936797
    },
    "url": "https://www.semanticscholar.org/paper/3bd320ddb25886417ae90011b00f13f5d558097b",
    "title": "BASS: Block-wise Adaptation for Speech Summarization",
    "abstract": "End-to-end speech summarization has been shown to improve performance over cascade baselines. However, such models are difficult to train on very large inputs (dozens of minutes or hours) owing to compute restrictions and are hence trained with truncated model inputs. Truncation leads to poorer models, and a solution to this problem rests in block-wise modeling, i.e., processing a portion of the input frames at a time. In this paper, we develop a method that allows one to train summarization models on very long sequences in an incremental manner. Speech summarization is realized as a streaming process, where hypothesis summaries are updated every block based on new acoustic information. We devise and test strategies to pass semantic context across the blocks. Experiments on the How2 dataset demonstrate that the proposed block-wise training method improves by 3 points absolute on ROUGE-L over a truncated input baseline.",
    "venue": "Interspeech",
    "year": 2023,
    "referenceCount": 28,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.08217",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "volume": "abs/2307.08217",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "145521253",
        "name": "Roshan Sharma"
      },
      {
        "authorId": "2163585699",
        "name": "Kenneth Zheng"
      },
      {
        "authorId": "72401599",
        "name": "Siddhant Arora"
      },
      {
        "authorId": "1746678",
        "name": "Shinji Watanabe"
      },
      {
        "authorId": "153915824",
        "name": "Rita Singh"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "This paper develops a method that allows one to train summarization models on very long sequences in an incremental manner and devise and test strategies to pass semantic context across the blocks."
  },
  {
    "profName": "Rita Singh",
    "authorId": "153915824",
    "authorName": "Rita Singh",
    "authorUrl": "https://www.semanticscholar.org/author/153915824",
    "authorHIndex": 25,
    "authorAffiliations": [],
    "authorPaperCount": 146,
    "authorCitationCount": 2781,
    "paperId": "5a3307b2e64bbcaff1202e261b8a83f7d03418a8",
    "externalIds": {
      "ArXiv": "2307.13948",
      "DBLP": "journals/corr/abs-2307-13948",
      "DOI": "10.1145/3581783.3611779",
      "CorpusId": 260165102
    },
    "url": "https://www.semanticscholar.org/paper/5a3307b2e64bbcaff1202e261b8a83f7d03418a8",
    "title": "Rethinking Voice-Face Correlation: A Geometry View",
    "abstract": "Previous works on voice-face matching and voice-guided face synthesis demonstrate strong correlations between voice and face, but mainly rely on coarse semantic cues such as gender, age, and emotion. In this paper, we aim to investigate the capability of reconstructing the 3D facial shape from voice from a geometry perspective without any semantic information. We propose a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction. By leveraging AMs as a proxy to link the voice and face geometry, we can eliminate the influence of unpredictable AMs and make the face geometry tractable. Our approach is evaluated on our proposed dataset with ground-truth 3D face scans and corresponding voice recordings, and we find significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium. Our work offers a new perspective on voice-face correlation and can serve as a good empirical study for anthropometry science.",
    "venue": "ACM Multimedia",
    "year": 2023,
    "referenceCount": 56,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.13948",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "name": "Proceedings of the 31st ACM International Conference on Multimedia"
    },
    "authors": [
      {
        "authorId": "2108280244",
        "name": "Xiang Li"
      },
      {
        "authorId": "145357606",
        "name": "Yandong Wen"
      },
      {
        "authorId": "72966973",
        "name": "Muqiao Yang"
      },
      {
        "authorId": "2110107884",
        "name": "Jinglu Wang"
      },
      {
        "authorId": "153915824",
        "name": "Rita Singh"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "This work proposes a voice-anthropometric measurement (AM)-face paradigm, which identifies predictable facial AMs from the voice and uses them to guide 3D face reconstruction and finds significant correlations between voice and specific parts of the face geometry, such as the nasal cavity and cranium."
  },
  {
    "profName": "Rita Singh",
    "authorId": "153915824",
    "authorName": "Rita Singh",
    "authorUrl": "https://www.semanticscholar.org/author/153915824",
    "authorHIndex": 25,
    "authorAffiliations": [],
    "authorPaperCount": 146,
    "authorCitationCount": 2781,
    "paperId": "63a4150c9ad87c003de43b32828c8ceec6bb4468",
    "externalIds": {
      "DBLP": "journals/entropy/Singh23",
      "PubMedCentral": "10297681",
      "DOI": "10.3390/e25060897",
      "CorpusId": 259269897,
      "PubMed": "37372241"
    },
    "url": "https://www.semanticscholar.org/paper/63a4150c9ad87c003de43b32828c8ceec6bb4468",
    "title": "A Gene-Based Algorithm for Identifying Factors That May Affect a Speaker\u2019s Voice",
    "abstract": "Over the past decades, many machine-learning- and artificial-intelligence-based technologies have been created to deduce biometric or bio-relevant parameters of speakers from their voice. These voice profiling technologies have targeted a wide range of parameters, from diseases to environmental factors, based largely on the fact that they are known to influence voice. Recently, some have also explored the prediction of parameters whose influence on voice is not easily observable through data-opportunistic biomarker discovery techniques. However, given the enormous range of factors that can possibly influence voice, more informed methods for selecting those that may be potentially deducible from voice are needed. To this end, this paper proposes a simple path-finding algorithm that attempts to find links between vocal characteristics and perturbing factors using cytogenetic and genomic data. The links represent reasonable selection criteria for use by computational by profiling technologies only, and are not intended to establish any unknown biological facts. The proposed algorithm is validated using a simple example from medical literature\u2014that of the clinically observed effects of specific chromosomal microdeletion syndromes on the vocal characteristics of affected people. In this example, the algorithm attempts to link the genes involved in these syndromes to a single example gene (FOXP2) that is known to play a broad role in voice production. We show that in cases where strong links are exposed, vocal characteristics of the patients are indeed reported to be correspondingly affected. Validation experiments and subsequent analyses confirm that the methodology could be potentially useful in predicting the existence of vocal signatures in na\u00efve cases where their existence has not been otherwise observed.",
    "venue": "Entropy",
    "year": 2023,
    "referenceCount": 82,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/1099-4300/25/6/897/pdf?version=1685718244",
      "status": null
    },
    "fieldsOfStudy": [
      "Medicine",
      "Computer Science"
    ],
    "journal": {
      "volume": "25",
      "name": "Entropy"
    },
    "authors": [
      {
        "authorId": "153915824",
        "name": "Rita Singh"
      }
    ],
    "tldr": "A simple path-finding algorithm that attempts to find links between vocal characteristics and perturbing factors using cytogenetic and genomic data and shows that in cases where strong links are exposed, vocal characteristics of the patients are indeed reported to be correspondingly affected."
  },
  {
    "profName": "Rita Singh",
    "authorId": "153915824",
    "authorName": "Rita Singh",
    "authorUrl": "https://www.semanticscholar.org/author/153915824",
    "authorHIndex": 25,
    "authorAffiliations": [],
    "authorPaperCount": 146,
    "authorCitationCount": 2781,
    "paperId": "721b39472c801124b5e3102edffe9d6f0754e1c2",
    "externalIds": {
      "PubMedCentral": "10378572",
      "DBLP": "journals/entropy/ZhaoS23",
      "DOI": "10.3390/e25071039",
      "CorpusId": 259893099,
      "PubMed": "37509986"
    },
    "url": "https://www.semanticscholar.org/paper/721b39472c801124b5e3102edffe9d6f0754e1c2",
    "title": "Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation",
    "abstract": "During phonation, the vocal folds exhibit a self-sustained oscillatory motion, which is influenced by the physical properties of the speaker\u2019s vocal folds and driven by the balance of bio-mechanical and aerodynamic forces across the glottis. Subtle changes in the speaker\u2019s physical state can affect voice production and alter these oscillatory patterns. Measuring these can be valuable in developing computational tools that analyze voice to infer the speaker\u2019s state. Traditionally, vocal fold oscillations (VFOs) are measured directly using physical devices in clinical settings. In this paper, we propose a novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker-by-speaker basis. The approach, called the ADLES-VFT algorithm, is proposed in the context of a joint model that combines a phonation model (with a glottal flow waveform as the output) and a vocal tract acoustic wave propagation model such that the output of the joint model is an estimated waveform. The ADLES-VFT algorithm is a forward-backward algorithm which minimizes the error between the recorded waveform and the output of this joint model to estimate its parameters. Once estimated, these parameter values are used in conjunction with a phonation model to obtain its solutions. Since the parameters correlate with the physical properties of the vocal folds of the speaker, model solutions obtained using them represent the individualized VFOs for each speaker. The approach is flexible and can be applied to various phonation models. In addition to presenting the methodology, we show how the VFOs can be quantified from a dynamical systems perspective for classification purposes. Mathematical derivations are provided in an appendix for better readability.",
    "venue": "Entropy",
    "year": 2023,
    "referenceCount": 55,
    "citationCount": 1,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://www.mdpi.com/1099-4300/25/7/1039/pdf?version=1689132568",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Medicine"
    ],
    "journal": {
      "volume": "25",
      "name": "Entropy"
    },
    "authors": [
      {
        "authorId": "2223487053",
        "name": "Wayne Zhao"
      },
      {
        "authorId": "153915824",
        "name": "Rita Singh"
      }
    ],
    "tldr": "A novel analysis-by-synthesis approach that allows us to infer the VFOs directly from recorded speech signals on an individualized, speaker- by-speaker basis is proposed and it is shown how the V FOs can be quantified from a dynamical systems perspective for classification purposes."
  },
  {
    "profName": "Rita Singh",
    "authorId": "153915824",
    "authorName": "Rita Singh",
    "authorUrl": "https://www.semanticscholar.org/author/153915824",
    "authorHIndex": 25,
    "authorAffiliations": [],
    "authorPaperCount": 146,
    "authorCitationCount": 2781,
    "paperId": "8665c864d71df1e918d2010778fc06712f4e5550",
    "externalIds": {
      "ArXiv": "2305.12715",
      "DBLP": "journals/corr/abs-2305-12715",
      "DOI": "10.48550/arXiv.2305.12715",
      "CorpusId": 258832327
    },
    "url": "https://www.semanticscholar.org/paper/8665c864d71df1e918d2010778fc06712f4e5550",
    "title": "Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations",
    "abstract": "Learning with reduced labeling standards, such as noisy label, partial label, and multiple label candidates, which we generically refer to as \\textit{imprecise} labels, is a commonplace challenge in machine learning tasks. Previous methods tend to propose specific designs for every emerging imprecise label configuration, which is usually unsustainable when multiple configurations of imprecision coexist. In this paper, we introduce imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations. ILL leverages expectation-maximization (EM) for modeling the imprecise label information, treating the precise labels as latent variables.Instead of approximating the correct labels for training, it considers the entire distribution of all possible labeling entailed by the imprecise information. We demonstrate that ILL can seamlessly adapt to partial label learning, semi-supervised learning, noisy label learning, and, more importantly, a mixture of these settings. Notably, ILL surpasses the existing specified techniques for handling imprecise labels, marking the first unified framework with robust and effective performance across various challenging settings. We hope our work will inspire further research on this topic, unleashing the full potential of ILL in wider scenarios where precise labels are expensive and complicated to obtain.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 136,
    "citationCount": 3,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.12715",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.12715",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2051536212",
        "name": "Hao Chen"
      },
      {
        "authorId": "47287745",
        "name": "Ankit Shah"
      },
      {
        "authorId": "1519290245",
        "name": "Jindong Wang"
      },
      {
        "authorId": "26151496",
        "name": "R. Tao"
      },
      {
        "authorId": "2108024273",
        "name": "Yidong Wang"
      },
      {
        "authorId": "1576441343",
        "name": "Xingxu Xie"
      },
      {
        "authorId": "67154907",
        "name": "Masashi Sugiyama"
      },
      {
        "authorId": "153915824",
        "name": "Rita Singh"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "Imprecise label learning (ILL) is introduced, a framework for the unification of learning with various imprecise label configurations, marking the first unified framework with robust and effective performance across various challenging settings."
  },
  {
    "profName": "Rita Singh",
    "authorId": "153915824",
    "authorName": "Rita Singh",
    "authorUrl": "https://www.semanticscholar.org/author/153915824",
    "authorHIndex": 25,
    "authorAffiliations": [],
    "authorPaperCount": 146,
    "authorCitationCount": 2781,
    "paperId": "a6e3a10a6286967413e3406374bbeea533640030",
    "externalIds": {
      "ArXiv": "2307.13953",
      "DBLP": "journals/corr/abs-2307-13953",
      "DOI": "10.48550/arXiv.2307.13953",
      "CorpusId": 260164956
    },
    "url": "https://www.semanticscholar.org/paper/a6e3a10a6286967413e3406374bbeea533640030",
    "title": "The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features",
    "abstract": "This work unveils the enigmatic link between phonemes and facial features. Traditional studies on voice-face correlations typically involve using a long period of voice input, including generating face images from voices and reconstructing 3D face meshes from voices. However, in situations like voice-based crimes, the available voice evidence may be short and limited. Additionally, from a physiological perspective, each segment of speech -- phoneme -- corresponds to different types of airflow and movements in the face. Therefore, it is advantageous to discover the hidden link between phonemes and face attributes. In this paper, we propose an analysis pipeline to help us explore the voice-face relationship in a fine-grained manner, i.e., phonemes v.s. facial anthropometric measurements (AM). We build an estimator for each phoneme-AM pair and evaluate the correlation through hypothesis testing. Our results indicate that AMs are more predictable from vowels compared to consonants, particularly with plosives. Additionally, we observe that if a specific AM exhibits more movement during phoneme pronunciation, it is more predictable. Our findings support those in physiology regarding correlation and lay the groundwork for future research on speech-face multimodal learning.",
    "venue": "Interspeech",
    "year": 2023,
    "referenceCount": 29,
    "citationCount": 2,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.13953",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "volume": "abs/2307.13953",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2125181846",
        "name": "Liao Qu"
      },
      {
        "authorId": "1818224862",
        "name": "X. Zou"
      },
      {
        "authorId": "2108280244",
        "name": "Xiang Li"
      },
      {
        "authorId": "145357606",
        "name": "Yandong Wen"
      },
      {
        "authorId": "153915824",
        "name": "Rita Singh"
      },
      {
        "authorId": "1681921",
        "name": "B. Raj"
      }
    ],
    "tldr": "This work proposes an analysis pipeline to help explore the voice-face relationship in a fine-grained manner, i.s. phonemes v. facial anthropometric measurements (AM), and indicates that AMs are more predictable from vowels compared to consonants, particularly with plosives."
  },
  {
    "profName": "Rita Singh",
    "authorId": "153915824",
    "authorName": "Rita Singh",
    "authorUrl": "https://www.semanticscholar.org/author/153915824",
    "authorHIndex": 25,
    "authorAffiliations": [],
    "authorPaperCount": 146,
    "authorCitationCount": 2781,
    "paperId": "ad22af138fa1d1490cda0301abf8159a7c30c5a2",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-11834",
      "ArXiv": "2305.11834",
      "DOI": "10.48550/arXiv.2305.11834",
      "CorpusId": 258823141
    },
    "url": "https://www.semanticscholar.org/paper/ad22af138fa1d1490cda0301abf8159a7c30c5a2",
    "title": "Pengi: An Audio Language Model for Audio Tasks",
    "abstract": "In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question&Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 68,
    "citationCount": 33,
    "influentialCitationCount": 7,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.11834",
      "status": null
    },
    "fieldsOfStudy": [
      "Engineering",
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.11834",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "67345939",
        "name": "Soham Deshmukh"
      },
      {
        "authorId": "2532460",
        "name": "Benjamin Elizalde"
      },
      {
        "authorId": "153915824",
        "name": "Rita Singh"
      },
      {
        "authorId": "2145337420",
        "name": "Huaming Wang"
      }
    ],
    "tldr": "Pengi is introduced, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks, and shows that connecting language models with audio models is a major step towards general-purpose audio understanding."
  }
]