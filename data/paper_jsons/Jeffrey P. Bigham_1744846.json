[
  {
    "authorId": "1744846",
    "authorName": "Jeffrey P. Bigham",
    "authorUrl": "https://www.semanticscholar.org/author/1744846",
    "authorHIndex": 56,
    "authorAffiliations": [],
    "authorPaperCount": 288,
    "authorCitationCount": 10897,
    "paperId": "0e84679cf0945a2868245ba2be68c90453e48f2e",
    "externalIds": {
      "DBLP": "journals/corr/abs-2301-08372",
      "ArXiv": "2301.08372",
      "DOI": "10.48550/arXiv.2301.08372",
      "CorpusId": 256080602
    },
    "url": "https://www.semanticscholar.org/paper/0e84679cf0945a2868245ba2be68c90453e48f2e",
    "title": "Screen Correspondence: Mapping Interchangeable Elements between UIs",
    "abstract": "Understanding user interface (UI) functionality is a useful yet challenging task for both machines and people. In this paper, we investigate a machine learning approach for screen correspondence, which allows reasoning about UIs by mapping their elements onto previously encountered examples with known functionality and properties. We describe and implement a model that incorporates element semantics, appearance, and text to support correspondence computation without requiring any labeled examples. Through a comprehensive performance evaluation, we show that our approach improves upon baselines by incorporating multi-modal properties of UIs. Finally, we show three example applications where screen correspondence facilitates better UI understanding for humans and machines: (i) instructional overlay generation, (ii) semantic UI element search, and (iii) automated interface testing.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 70,
    "citationCount": 4,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.08372",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2301.08372",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2109186166",
        "name": "Jason Wu"
      },
      {
        "authorId": "3246971",
        "name": "Amanda Swearngin"
      },
      {
        "authorId": "2141934656",
        "name": "Xiaoyi Zhang"
      },
      {
        "authorId": "2057155807",
        "name": "Jeffrey Nichols"
      },
      {
        "authorId": "1744846",
        "name": "Jeffrey P. Bigham"
      }
    ]
  },
  {
    "authorId": "1744846",
    "authorName": "Jeffrey P. Bigham",
    "authorUrl": "https://www.semanticscholar.org/author/1744846",
    "authorHIndex": 56,
    "authorAffiliations": [],
    "authorPaperCount": 288,
    "authorCitationCount": 10897,
    "paperId": "3f261fb980858e39129af5bc8a6c8565d7bb8329",
    "externalIds": {
      "DBLP": "journals/access/PaulinoCYBLVGBP23",
      "DOI": "10.1109/ACCESS.2023.3319597",
      "CorpusId": 263180410
    },
    "url": "https://www.semanticscholar.org/paper/3f261fb980858e39129af5bc8a6c8565d7bb8329",
    "title": "Exploring Stigmergic Collaboration and Task Modularity Through an Expert Crowdsourcing Annotation System: The Case of Storm Phenomena in the Euro-Atlantic Region",
    "abstract": "Extreme weather events, such as windstorms, hurricanes, and heat waves, exert a significant impact on global natural catastrophes and pose substantial challenges for weather forecasting systems. To enhance the accuracy and preparedness for extreme weather events, this study explores the potential of using expert crowdsourcing in storm forecasting research through the application of stigmergic collaboration. We present the development and implementation of an expert Crowdsourcing for Semantic Annotation of Atmospheric Phenomena (eCSAAP) system, designed to leverage the collective knowledge and experience of meteorological experts. Through a participatory co-creation process, we iteratively developed a web-based annotation tool capable of capturing multi-faceted insights from weather data and generating visualizations for expert crowdsourcing campaigns. In this context, this article investigates the intrinsic coordination among experts engaged in crowdsourcing tasks focused on the semantic annotation of extreme weather events. The study brings insights about the behavior of expert crowds by considering the cognitive biases and highlighting the impact of existing annotations on the quality of data gathered from the crowd and the collective knowledge generated. The insights regarding the crowdsourcing dynamics, particularly stigmergy, offer a promising starting point for utilizing stigmergic collaboration as an effective coordination mechanism for weather experts in crowdsourcing platforms but also in other domains requiring expertise-driven collective intelligence.",
    "venue": "IEEE Access",
    "year": 2023,
    "referenceCount": 111,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://ieeexplore.ieee.org/ielx7/6287639/6514899/10265012.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "11",
      "pages": "106485-106502",
      "name": "IEEE Access"
    },
    "authors": [
      {
        "authorId": "9574315",
        "name": "Dennis Paulino"
      },
      {
        "authorId": "121795834",
        "name": "Ant\u00f3nio Correia"
      },
      {
        "authorId": "40936903",
        "name": "M. Yagui"
      },
      {
        "authorId": "2244407761",
        "name": "Jo\u00e3o Barroso"
      },
      {
        "authorId": "2248556495",
        "name": "Margarida L. R. Liberato"
      },
      {
        "authorId": "1784835",
        "name": "A. Vivacqua"
      },
      {
        "authorId": "2248197228",
        "name": "Andrea Grover"
      },
      {
        "authorId": "1744846",
        "name": "Jeffrey P. Bigham"
      },
      {
        "authorId": "145288702",
        "name": "Hugo Paredes"
      }
    ]
  },
  {
    "authorId": "1744846",
    "authorName": "Jeffrey P. Bigham",
    "authorUrl": "https://www.semanticscholar.org/author/1744846",
    "authorHIndex": 56,
    "authorAffiliations": [],
    "authorPaperCount": 288,
    "authorCitationCount": 10897,
    "paperId": "7ee7d32c3e7f01f49c40e5ba7e0b4d5e9ea040c9",
    "externalIds": {
      "DBLP": "journals/cacm/ValenciaSRBBA24",
      "DOI": "10.1145/3610939",
      "CorpusId": 266436632
    },
    "url": "https://www.semanticscholar.org/paper/7ee7d32c3e7f01f49c40e5ba7e0b4d5e9ea040c9",
    "title": "Nonverbal Communication through Expressive Objects",
    "abstract": "Augmentative and alternative communication (AAC) devices enable speech-based communication, but generating speech is not the only resource needed to have a successful conversation. Being able to signal one wishes to take a turn by raising a hand or providing some other cue is critical in securing a turn to speak. Experienced conversation partners know how to recognize the nonverbal communication an augmented communicator (AC) displays, but these same nonverbal gestures can be hard to interpret by people who meet an AC for the first time. Prior work has identified motion through robots and expressive objects as a modality that can support communication. In this work, we work closely with an AAC user to understand how motion through a physical expressive object can support their communication. We present our process and resulting lessons on the designed object and the co-design process.",
    "venue": "Communications of the ACM",
    "year": 2023,
    "referenceCount": 18,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3610939",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "67",
      "pages": "123 - 131",
      "name": "Communications of the ACM"
    },
    "authors": [
      {
        "authorId": "48114196",
        "name": "Stephanie Valencia"
      },
      {
        "authorId": "2275634215",
        "name": "Mark Steidl"
      },
      {
        "authorId": "2275635568",
        "name": "Michael L. Rivera"
      },
      {
        "authorId": "2275629380",
        "name": "Cynthia L. Bennett"
      },
      {
        "authorId": "1744846",
        "name": "Jeffrey P. Bigham"
      },
      {
        "authorId": "1882027",
        "name": "H. Admoni"
      }
    ]
  },
  {
    "authorId": "1744846",
    "authorName": "Jeffrey P. Bigham",
    "authorUrl": "https://www.semanticscholar.org/author/1744846",
    "authorHIndex": 56,
    "authorAffiliations": [],
    "authorPaperCount": 288,
    "authorCitationCount": 10897,
    "paperId": "8ab27849799286459465d2262f926354093b20a9",
    "externalIds": {
      "ArXiv": "2305.14296",
      "DBLP": "journals/corr/abs-2305-14296",
      "DOI": "10.48550/arXiv.2305.14296",
      "CorpusId": 258840901
    },
    "url": "https://www.semanticscholar.org/paper/8ab27849799286459465d2262f926354093b20a9",
    "title": "USB: A Unified Summarization Benchmark Across Tasks and Domains",
    "abstract": "While the NLP community has produced numerous summarization benchmarks, none provide the rich annotations required to simultaneously address many important problems related to control and reliability. We introduce a Wikipedia-derived benchmark, complemented by a rich set of crowd-sourced annotations, that supports $8$ interrelated tasks: (i) extractive summarization; (ii) abstractive summarization; (iii) topic-based summarization; (iv) compressing selected sentences into a one-line summary; (v) surfacing evidence for a summary sentence; (vi) predicting the factual accuracy of a summary sentence; (vii) identifying unsubstantiated spans in a summary sentence; (viii) correcting factual errors in summaries. We compare various methods on this benchmark and discover that on multiple tasks, moderately-sized fine-tuned models consistently outperform much larger few-shot prompted language models. For factuality-related tasks, we also evaluate existing heuristics to create training data and find that training on them results in worse performance than training on $20\\times$ less human-labeled data. Our articles draw from $6$ domains, facilitating cross-domain analysis. On some tasks, the amount of training data matters more than the domain where it comes from, while for other tasks training specifically on data from the target domain, even if limited, is more beneficial.",
    "venue": "Conference on Empirical Methods in Natural Language Processing",
    "year": 2023,
    "referenceCount": 50,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14296",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "8826-8845"
    },
    "authors": [
      {
        "authorId": "38716503",
        "name": "Kundan Krishna"
      },
      {
        "authorId": "1491232062",
        "name": "Prakhar Gupta"
      },
      {
        "authorId": "98806251",
        "name": "S. Ramprasad"
      },
      {
        "authorId": "1912476",
        "name": "Byron C. Wallace"
      },
      {
        "authorId": "1744846",
        "name": "Jeffrey P. Bigham"
      },
      {
        "authorId": "32219137",
        "name": "Zachary Chase Lipton"
      }
    ]
  },
  {
    "authorId": "1744846",
    "authorName": "Jeffrey P. Bigham",
    "authorUrl": "https://www.semanticscholar.org/author/1744846",
    "authorHIndex": 56,
    "authorAffiliations": [],
    "authorPaperCount": 288,
    "authorCitationCount": 10897,
    "paperId": "98abc6de98a24d599cf009a9670eaa5c97cba9bb",
    "externalIds": {
      "DBLP": "conf/chi/WuWSPNB23",
      "ArXiv": "2301.13280",
      "DOI": "10.1145/3544548.3581158",
      "CorpusId": 256416083
    },
    "url": "https://www.semanticscholar.org/paper/98abc6de98a24d599cf009a9670eaa5c97cba9bb",
    "title": "WebUI: A Dataset for Enhancing Visual UI Understanding with Web Semantics",
    "abstract": "Modeling user interfaces (UIs) from visual information allows systems to make inferences about the functionality and semantics needed to support use cases in accessibility, app automation, and testing. Current datasets for training machine learning models are limited in size due to the costly and time-consuming process of manually collecting and annotating UIs. We crawled the web to construct WebUI, a large dataset of 400,000 rendered web pages associated with automatically extracted metadata. We analyze the composition of WebUI and show that while automatically extracted data is noisy, most examples meet basic criteria for visual UI modeling. We applied several strategies for incorporating semantics found in web pages to increase the performance of visual UI understanding models in the mobile domain, where less labeled data is available: (i) element detection, (ii) screen classification and (iii) screen similarity.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2023,
    "referenceCount": 60,
    "citationCount": 15,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3544548.3581158",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "name": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems"
    },
    "authors": [
      {
        "authorId": "2109186166",
        "name": "Jason Wu"
      },
      {
        "authorId": "2191079032",
        "name": "Siyan Wang"
      },
      {
        "authorId": "2203460332",
        "name": "Siman Shen"
      },
      {
        "authorId": "152609527",
        "name": "Yi-Hao Peng"
      },
      {
        "authorId": "2057156585",
        "name": "Jeffrey Nichols"
      },
      {
        "authorId": "1744846",
        "name": "Jeffrey P. Bigham"
      }
    ]
  },
  {
    "authorId": "1744846",
    "authorName": "Jeffrey P. Bigham",
    "authorUrl": "https://www.semanticscholar.org/author/1744846",
    "authorHIndex": 56,
    "authorAffiliations": [],
    "authorPaperCount": 288,
    "authorCitationCount": 10897,
    "paperId": "c84f0a204827065338c412ace037952900a1a279",
    "externalIds": {
      "ArXiv": "2308.08726",
      "DBLP": "conf/uist/WuKSSBN23",
      "DOI": "10.1145/3586183.3606824",
      "CorpusId": 261030456
    },
    "url": "https://www.semanticscholar.org/paper/c84f0a204827065338c412ace037952900a1a279",
    "title": "Never-ending Learning of User Interfaces",
    "abstract": "Machine learning models have been trained to predict semantic information about user interfaces (UIs) to make apps more accessible, easier to test, and to automate. Currently, most models rely on datasets of static screenshots that are labeled by human annotators, a process that is costly and surprisingly error-prone for certain tasks. For example, workers labeling whether a UI element is \u201ctappable\u201d from a screenshot must guess using visual signifiers, and do not have the benefit of tapping on the UI element in the running app and observing the effects. In this paper, we present the Never-ending UI Learner, an app crawler that automatically installs real apps from a mobile app store and crawls them to infer semantic properties of UIs by interacting with UI elements, discovering new and challenging training examples to learn from, and continually updating machine learning models designed to predict these semantics. The Never-ending UI Learner so far has crawled for more than 5,000 device-hours, performing over half a million actions on 6,000 apps to train three computer vision models for i) tappability prediction, ii) draggability prediction, and iii) screen similarity.",
    "venue": "ACM Symposium on User Interface Software and Technology",
    "year": 2023,
    "referenceCount": 58,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3586183.3606824",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "name": "Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology"
    },
    "authors": [
      {
        "authorId": "2109186166",
        "name": "Jason Wu"
      },
      {
        "authorId": "2251531",
        "name": "Rebecca Krosnick"
      },
      {
        "authorId": "3396325",
        "name": "E. Schoop"
      },
      {
        "authorId": "3246971",
        "name": "Amanda Swearngin"
      },
      {
        "authorId": "1744846",
        "name": "Jeffrey P. Bigham"
      },
      {
        "authorId": "2057156585",
        "name": "Jeffrey Nichols"
      }
    ]
  },
  {
    "authorId": "1744846",
    "authorName": "Jeffrey P. Bigham",
    "authorUrl": "https://www.semanticscholar.org/author/1744846",
    "authorHIndex": 56,
    "authorAffiliations": [],
    "authorPaperCount": 288,
    "authorCitationCount": 10897,
    "paperId": "d757a58200254625c3326a32a1da6fa8eaa2eff3",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-05446",
      "ArXiv": "2306.05446",
      "DOI": "10.48550/arXiv.2306.05446",
      "CorpusId": 259129537
    },
    "url": "https://www.semanticscholar.org/paper/d757a58200254625c3326a32a1da6fa8eaa2eff3",
    "title": "Latent Phrase Matching for Dysarthric Speech",
    "abstract": "Many consumer speech recognition systems are not tuned for people with speech disabilities, resulting in poor recognition and user experience, especially for severe speech differences. Recent studies have emphasized interest in personalized speech models from people with atypical speech patterns. We propose a query-by-example-based personalized phrase recognition system that is trained using small amounts of speech, is language agnostic, does not assume a traditional pronunciation lexicon, and generalizes well across speech difference severities. On an internal dataset collected from 32 people with dysarthria, this approach works regardless of severity and shows a 60% improvement in recall relative to a commercial speech recognition system. On the public EasyCall dataset of dysarthric speech, our approach improves accuracy by 30.5%. Performance degrades as the number of phrases increases, but consistently outperforms ASR systems when trained with 50 unique phrases.",
    "venue": "Interspeech",
    "year": 2023,
    "referenceCount": 32,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.05446",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Engineering"
    ],
    "journal": {
      "volume": "abs/2306.05446",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "98658818",
        "name": "Colin S. Lea"
      },
      {
        "authorId": "27371588",
        "name": "Dianna Yee"
      },
      {
        "authorId": "30888277",
        "name": "Jaya Narain"
      },
      {
        "authorId": "2109594780",
        "name": "Zifang Huang"
      },
      {
        "authorId": "2114426903",
        "name": "Lauren Tooley"
      },
      {
        "authorId": "1744846",
        "name": "Jeffrey P. Bigham"
      },
      {
        "authorId": "1689620",
        "name": "Leah Findlater"
      }
    ]
  },
  {
    "authorId": "1744846",
    "authorName": "Jeffrey P. Bigham",
    "authorUrl": "https://www.semanticscholar.org/author/1744846",
    "authorHIndex": 56,
    "authorAffiliations": [],
    "authorPaperCount": 288,
    "authorCitationCount": 10897,
    "paperId": "f29922cbfaf825d5e1d4986dc01bda74b4d88e04",
    "externalIds": {
      "DBLP": "conf/chi/LeaHNTYTGBF23",
      "ArXiv": "2302.09044",
      "DOI": "10.1145/3544548.3581224",
      "CorpusId": 257019977
    },
    "url": "https://www.semanticscholar.org/paper/f29922cbfaf825d5e1d4986dc01bda74b4d88e04",
    "title": "From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition",
    "abstract": "Consumer speech recognition systems do not work as well for many people with speech differences, such as stuttering, relative to the rest of the general population. However, what is not clear is the degree to which these systems do not work, how they can be improved, or how much people want to use them. In this paper, we first address these questions using results from a 61-person survey from people who stutter and find participants want to use speech recognition but are frequently cut off, misunderstood, or speech predictions do not represent intent. In a second study, where 91 people who stutter recorded voice assistant commands and dictation, we quantify how dysfluencies impede performance in a consumer-grade speech recognition system. Through three technical investigations, we demonstrate how many common errors can be prevented, resulting in a system that cuts utterances off 79.1% less often and improves word error rate from 25.4% to 9.9%.",
    "venue": "International Conference on Human Factors in Computing Systems",
    "year": 2023,
    "referenceCount": 91,
    "citationCount": 5,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://dl.acm.org/doi/pdf/10.1145/3544548.3581224",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "name": "Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems"
    },
    "authors": [
      {
        "authorId": "98658818",
        "name": "Colin S. Lea"
      },
      {
        "authorId": "2109594780",
        "name": "Zifang Huang"
      },
      {
        "authorId": "30888277",
        "name": "Jaya Narain"
      },
      {
        "authorId": "2114426903",
        "name": "Lauren Tooley"
      },
      {
        "authorId": "27371588",
        "name": "Dianna Yee"
      },
      {
        "authorId": "2214744586",
        "name": "Dung Tien Tran"
      },
      {
        "authorId": "1765829",
        "name": "P. Georgiou"
      },
      {
        "authorId": "1744846",
        "name": "Jeffrey P. Bigham"
      },
      {
        "authorId": "1689620",
        "name": "Leah Findlater"
      }
    ]
  }
]