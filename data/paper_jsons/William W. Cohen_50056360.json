[
  {
    "profName": "William Cohen",
    "authorId": "50056360",
    "authorName": "William W. Cohen",
    "authorUrl": "https://www.semanticscholar.org/author/50056360",
    "authorHIndex": 88,
    "authorAffiliations": [
      "Google"
    ],
    "authorPaperCount": 440,
    "authorCitationCount": 41245,
    "paperId": "2ca8e50ffd6e2e67f3fe2fbf1af57dbedb4cf493",
    "externalIds": {
      "ArXiv": "2308.08661",
      "DBLP": "journals/corr/abs-2308-08661",
      "DOI": "10.48550/arXiv.2308.08661",
      "CorpusId": 261031074
    },
    "url": "https://www.semanticscholar.org/paper/2ca8e50ffd6e2e67f3fe2fbf1af57dbedb4cf493",
    "title": "Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions",
    "abstract": "Many open-domain questions are under-specified and thus have multiple possible answers, each of which is correct under a different interpretation of the question. Answering such ambiguous questions is challenging, as it requires retrieving and then reasoning about diverse information from multiple passages. We present a new state-of-the-art for answering ambiguous questions that exploits a database of unambiguous questions generated from Wikipedia. On the challenging ASQA benchmark, which requires generating long-form answers that summarize the multiple answers to an ambiguous question, our method improves performance by 15% (relative improvement) on recall measures and 10% on measures which evaluate disambiguating questions from predicted outputs. Retrieving from the database of generated questions also gives large improvements in diverse passage retrieval (by matching user questions q to passages p indirectly, via questions q' generated from p).",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 26,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.08661",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2308.08661",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "3456820",
        "name": "Haitian Sun"
      },
      {
        "authorId": "50056360",
        "name": "William W. Cohen"
      },
      {
        "authorId": "145124475",
        "name": "R. Salakhutdinov"
      }
    ],
    "tldr": "A new state-of-the-art for answering ambiguous questions that exploits a database of unambiguous questions generated from Wikipedia, which improves performance by 15% (relative improvement) on recall measures and 10% on measures which evaluate disambiguating questions from predicted outputs."
  },
  {
    "profName": "William Cohen",
    "authorId": "50056360",
    "authorName": "William W. Cohen",
    "authorUrl": "https://www.semanticscholar.org/author/50056360",
    "authorHIndex": 88,
    "authorAffiliations": [
      "Google"
    ],
    "authorPaperCount": 440,
    "authorCitationCount": 41245,
    "paperId": "646cca9de110726000a6e44560743b241a4d7f91",
    "externalIds": {
      "DBLP": "journals/corr/abs-2308-14903",
      "ArXiv": "2308.14903",
      "DOI": "10.48550/arXiv.2308.14903",
      "CorpusId": 261276895
    },
    "url": "https://www.semanticscholar.org/paper/646cca9de110726000a6e44560743b241a4d7f91",
    "title": "MEMORY-VQ: Compression for Tractable Internet-Scale Memory",
    "abstract": "Retrieval augmentation is a powerful but expensive method to make language models more knowledgeable about the world. Memory-based methods like LUMEN pre-compute token representations for retrieved passages to drastically speed up inference. However, memory also leads to much greater storage requirements from storing pre-computed representations. We propose MEMORY-VQ, a new method to reduce storage requirements of memory-augmented models without sacrificing performance. Our method uses a vector quantization variational autoencoder (VQ-VAE) to compress token representations. We apply MEMORY-VQ to the LUMEN model to obtain LUMEN-VQ, a memory model that achieves a 16x compression rate with comparable performance on the KILT benchmark. LUMEN-VQ enables practical retrieval augmentation even for extremely large retrieval corpora.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 44,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.14903",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2308.14903",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2220287479",
        "name": "Yury Zemlyanskiy"
      },
      {
        "authorId": "21379393",
        "name": "Michiel de Jong"
      },
      {
        "authorId": "2546951",
        "name": "L. Vilnis"
      },
      {
        "authorId": "2217756237",
        "name": "Santiago Ontan'on"
      },
      {
        "authorId": "50056360",
        "name": "William W. Cohen"
      },
      {
        "authorId": "144074891",
        "name": "Sumit K. Sanghai"
      },
      {
        "authorId": "1643737606",
        "name": "J. Ainslie"
      }
    ],
    "tldr": "This work proposes MEMORY-VQ, a new method to reduce storage requirements of memory-augmented models without sacrificing performance, which uses a vector quantization variational autoencoder (VQ-VAE) to compress token representations."
  },
  {
    "profName": "William Cohen",
    "authorId": "50056360",
    "authorName": "William W. Cohen",
    "authorUrl": "https://www.semanticscholar.org/author/50056360",
    "authorHIndex": 88,
    "authorAffiliations": [
      "Google"
    ],
    "authorPaperCount": 440,
    "authorCitationCount": 41245,
    "paperId": "83b8e18488d8f31dd017ec0b26531cef4b635b36",
    "externalIds": {
      "DBLP": "journals/corr/abs-2304-00186",
      "ArXiv": "2304.00186",
      "DOI": "10.48550/arXiv.2304.00186",
      "CorpusId": 257913352
    },
    "url": "https://www.semanticscholar.org/paper/83b8e18488d8f31dd017ec0b26531cef4b635b36",
    "title": "Subject-driven Text-to-Image Generation via Apprenticeship Learning",
    "abstract": "Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with in-context learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by apprenticeship learning, where a single apprentice model is learned from data generated by a massive number of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train a massive number of expert models, each specializing in a different subject. The apprentice model SuTI then learns to imitate the behavior of these fine-tuned experts. SuTI can generate high-quality and customized subject-specific images 20x faster than optimization-based SoTA methods. On the challenging DreamBench and DreamBench-v2, our human evaluation shows that SuTI significantly outperforms existing models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen and DreamBooth, especially on the subject and text alignment aspects.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 44,
    "citationCount": 55,
    "influentialCitationCount": 6,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2304.00186",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2304.00186",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2928777",
        "name": "Wenhu Chen"
      },
      {
        "authorId": "2804000",
        "name": "Hexiang Hu"
      },
      {
        "authorId": "1527095795",
        "name": "Yandong Li"
      },
      {
        "authorId": "2213321774",
        "name": "Nataniel Rui"
      },
      {
        "authorId": "34760532",
        "name": "Xuhui Jia"
      },
      {
        "authorId": "2142348146",
        "name": "Ming-Wei Chang"
      },
      {
        "authorId": "50056360",
        "name": "William W. Cohen"
      }
    ],
    "tldr": "Human evaluation shows that SuTI significantly outperforms existing models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen and DreamBooth, especially on the subject and text alignment aspects."
  },
  {
    "profName": "William Cohen",
    "authorId": "50056360",
    "authorName": "William W. Cohen",
    "authorUrl": "https://www.semanticscholar.org/author/50056360",
    "authorHIndex": 88,
    "authorAffiliations": [
      "Google"
    ],
    "authorPaperCount": 440,
    "authorCitationCount": 41245,
    "paperId": "c67099476f2b505dfd5a22c817707fad83de9994",
    "externalIds": {
      "DBLP": "journals/corr/abs-2306-10231",
      "ArXiv": "2306.10231",
      "DOI": "10.48550/arXiv.2306.10231",
      "CorpusId": 259203489
    },
    "url": "https://www.semanticscholar.org/paper/c67099476f2b505dfd5a22c817707fad83de9994",
    "title": "GLIMMER: generalized late-interaction memory reranker",
    "abstract": "Memory-augmentation is a powerful approach for efficiently incorporating external information into language models, but leads to reduced performance relative to retrieving text. Recent work introduced LUMEN, a memory-retrieval hybrid that partially pre-computes memory and updates memory representations on the fly with a smaller live encoder. We propose GLIMMER, which improves on this approach through 1) exploiting free access to the powerful memory representations by applying a shallow reranker on top of memory to drastically improve retrieval quality at low cost, and 2) incorporating multi-task training to learn a general and higher quality memory and live encoder. GLIMMER achieves strong gains in performance at faster speeds compared to LUMEN and FiD on the KILT benchmark of knowledge-intensive tasks.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 53,
    "citationCount": 3,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.10231",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.10231",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "21379393",
        "name": "Michiel de Jong"
      },
      {
        "authorId": "2220287479",
        "name": "Yury Zemlyanskiy"
      },
      {
        "authorId": "143883142",
        "name": "Nicholas FitzGerald"
      },
      {
        "authorId": "144074891",
        "name": "Sumit K. Sanghai"
      },
      {
        "authorId": "50056360",
        "name": "William W. Cohen"
      },
      {
        "authorId": "1643737606",
        "name": "J. Ainslie"
      }
    ],
    "tldr": "GLIMMER is proposed, which improves on LUMEN through exploiting free access to the powerful memory representations by applying a shallow reranker on top of memory to drastically improve retrieval quality at low cost and incorporating multi-task training to learn a general and higher quality memory and live encoder."
  }
]