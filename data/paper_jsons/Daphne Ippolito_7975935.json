[
  {
    "profName": "Daphne Ippolito",
    "authorId": "7975935",
    "authorName": "Daphne Ippolito",
    "authorUrl": "https://www.semanticscholar.org/author/7975935",
    "authorHIndex": 25,
    "authorAffiliations": [],
    "authorPaperCount": 44,
    "authorCitationCount": 6460,
    "paperId": "03fb535de5cfcf435705a079334ac60f501226ab",
    "externalIds": {
      "DBLP": "journals/corr/abs-2309-04858",
      "ACL": "2023.inlg-main.28",
      "ArXiv": "2309.04858",
      "DOI": "10.48550/arXiv.2309.04858",
      "CorpusId": 261681722
    },
    "url": "https://www.semanticscholar.org/paper/03fb535de5cfcf435705a079334ac60f501226ab",
    "title": "Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System",
    "abstract": "Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model\u2019s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).",
    "venue": "International Conference on Natural Language Generation",
    "year": 2023,
    "referenceCount": 15,
    "citationCount": 3,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.04858",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "396-406"
    },
    "authors": [
      {
        "authorId": "7975935",
        "name": "Daphne Ippolito"
      },
      {
        "authorId": "2483738",
        "name": "Nicholas Carlini"
      },
      {
        "authorId": "3844009",
        "name": "Katherine Lee"
      },
      {
        "authorId": "3490923",
        "name": "Milad Nasr"
      },
      {
        "authorId": "2239157286",
        "name": "Yun William Yu"
      }
    ],
    "tldr": "Methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling) are presented, which has implications for detecting generated text."
  },
  {
    "profName": "Daphne Ippolito",
    "authorId": "7975935",
    "authorName": "Daphne Ippolito",
    "authorUrl": "https://www.semanticscholar.org/author/7975935",
    "authorHIndex": 25,
    "authorAffiliations": [],
    "authorPaperCount": 44,
    "authorCitationCount": 6460,
    "paperId": "1567bcac0ab09269c9d0ff33c9a406132417fab9",
    "externalIds": {
      "ArXiv": "2305.13169",
      "DBLP": "journals/corr/abs-2305-13169",
      "DOI": "10.48550/arXiv.2305.13169",
      "CorpusId": 258832491
    },
    "url": "https://www.semanticscholar.org/paper/1567bcac0ab09269c9d0ff33c9a406132417fab9",
    "title": "A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity",
    "abstract": "Pretraining is the preliminary and fundamental step in developing capable language models (LM). Despite this, pretraining data design is critically under-documented and often guided by empirically unsupported intuitions. To address this, we pretrain 28 1.5B parameter decoder-only models, training on data curated (1) at different times, (2) with varying toxicity and quality filters, and (3) with different domain compositions. First, we quantify the effect of pretraining data age. A temporal shift between evaluation data and pretraining data leads to performance degradation, which is not overcome by finetuning. Second, we explore the effect of quality and toxicity filters, showing a trade-off between performance on standard benchmarks and risk of toxic generations. Our findings indicate there does not exist a one-size-fits-all solution to filtering training data. We also find that the effects of different types of filtering are not predictable from text domain characteristics. Lastly, we empirically validate that the inclusion of heterogeneous data sources, like books and web, is broadly beneficial and warrants greater prioritization. These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which we hope will help support more informed data-centric decisions in LM development.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 118,
    "citationCount": 32,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.13169",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.13169",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "29909347",
        "name": "S. Longpre"
      },
      {
        "authorId": "32918271",
        "name": "Gregory Yauney"
      },
      {
        "authorId": "49849144",
        "name": "Emily Reif"
      },
      {
        "authorId": "3844009",
        "name": "Katherine Lee"
      },
      {
        "authorId": "145625142",
        "name": "Adam Roberts"
      },
      {
        "authorId": "2368067",
        "name": "Barret Zoph"
      },
      {
        "authorId": "65855107",
        "name": "Denny Zhou"
      },
      {
        "authorId": "119640649",
        "name": "Jason Wei"
      },
      {
        "authorId": "2148473059",
        "name": "Kevin Robinson"
      },
      {
        "authorId": "38917723",
        "name": "David M. Mimno"
      },
      {
        "authorId": "7975935",
        "name": "Daphne Ippolito"
      }
    ],
    "tldr": "These findings constitute the largest set of experiments to validate, quantify, and expose many undocumented intuitions about text pretraining, which are hoped to help support more informed data-centric decisions in LM development."
  },
  {
    "profName": "Daphne Ippolito",
    "authorId": "7975935",
    "authorName": "Daphne Ippolito",
    "authorUrl": "https://www.semanticscholar.org/author/7975935",
    "authorHIndex": 25,
    "authorAffiliations": [],
    "authorPaperCount": 44,
    "authorCitationCount": 6460,
    "paperId": "2e965b5d97c2d6fb4af284307735be39283792ba",
    "externalIds": {
      "DBLP": "conf/uss/CarliniHNJSTBIW23",
      "ArXiv": "2301.13188",
      "DOI": "10.48550/arXiv.2301.13188",
      "CorpusId": 256389993
    },
    "url": "https://www.semanticscholar.org/paper/2e965b5d97c2d6fb4af284307735be39283792ba",
    "title": "Extracting Training Data from Diffusion Models",
    "abstract": "Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.",
    "venue": "USENIX Security Symposium",
    "year": 2023,
    "referenceCount": 78,
    "citationCount": 222,
    "influentialCitationCount": 24,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.13188",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2301.13188",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2483738",
        "name": "Nicholas Carlini"
      },
      {
        "authorId": "9200194",
        "name": "Jamie Hayes"
      },
      {
        "authorId": "3490923",
        "name": "Milad Nasr"
      },
      {
        "authorId": "40844378",
        "name": "Matthew Jagielski"
      },
      {
        "authorId": "3482535",
        "name": "Vikash Sehwag"
      },
      {
        "authorId": "2444919",
        "name": "Florian Tram\u00e8r"
      },
      {
        "authorId": "1718064",
        "name": "B. Balle"
      },
      {
        "authorId": "7975935",
        "name": "Daphne Ippolito"
      },
      {
        "authorId": "145217343",
        "name": "Eric Wallace"
      }
    ],
    "tldr": "The results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training."
  },
  {
    "profName": "Daphne Ippolito",
    "authorId": "7975935",
    "authorName": "Daphne Ippolito",
    "authorUrl": "https://www.semanticscholar.org/author/7975935",
    "authorHIndex": 25,
    "authorAffiliations": [],
    "authorPaperCount": 44,
    "authorCitationCount": 6460,
    "paperId": "8724579d3f126e753a0451d98ff57b165f722e72",
    "externalIds": {
      "ArXiv": "2306.15447",
      "DBLP": "journals/corr/abs-2306-15447",
      "DOI": "10.48550/arXiv.2306.15447",
      "CorpusId": 259262181
    },
    "url": "https://www.semanticscholar.org/paper/8724579d3f126e753a0451d98ff57b165f722e72",
    "title": "Are aligned neural networks adversarially aligned?",
    "abstract": "Large language models are now tuned to align with the goals of their creators, namely to be\"helpful and harmless.\"These models should respond helpfully to user questions, but refuse to answer requests that could cause harm. However, adversarial users can construct inputs which circumvent attempts at alignment. In this work, we study to what extent these models remain aligned, even when interacting with an adversarial user who constructs worst-case inputs (adversarial examples). These inputs are designed to cause the model to emit harmful content that would otherwise be prohibited. We show that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models: even when current NLP-based attacks fail, we can find adversarial inputs with brute force. As a result, the failure of current attacks should not be seen as proof that aligned text models remain aligned under adversarial inputs. However the recent trend in large-scale ML models is multimodal models that allow users to provide images that influence the text that is generated. We show these models can be easily attacked, i.e., induced to perform arbitrary un-aligned behavior through adversarial perturbation of the input image. We conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models.",
    "venue": "arXiv.org",
    "year": 2023,
    "referenceCount": 53,
    "citationCount": 67,
    "influentialCitationCount": 8,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.15447",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.15447",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2483738",
        "name": "Nicholas Carlini"
      },
      {
        "authorId": "3490923",
        "name": "Milad Nasr"
      },
      {
        "authorId": "1415982317",
        "name": "Christopher A. Choquette-Choo"
      },
      {
        "authorId": "40844378",
        "name": "Matthew Jagielski"
      },
      {
        "authorId": "8687620",
        "name": "Irena Gao"
      },
      {
        "authorId": "2135149490",
        "name": "Anas Awadalla"
      },
      {
        "authorId": "2572525",
        "name": "Pang Wei Koh"
      },
      {
        "authorId": "7975935",
        "name": "Daphne Ippolito"
      },
      {
        "authorId": "3844009",
        "name": "Katherine Lee"
      },
      {
        "authorId": "2444919",
        "name": "Florian Tram\u00e8r"
      },
      {
        "authorId": "152772922",
        "name": "Ludwig Schmidt"
      }
    ],
    "tldr": "It is shown that existing NLP-based optimization attacks are insufficiently powerful to reliably attack aligned text models, and conjecture that improved NLP attacks may demonstrate this same level of adversarial control over text-only models."
  }
]