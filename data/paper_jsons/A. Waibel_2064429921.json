[
  {
    "profName": "Alexander Waibel",
    "authorId": "2064429921",
    "authorName": "A. Waibel",
    "authorUrl": "https://www.semanticscholar.org/author/2064429921",
    "authorHIndex": 6,
    "authorAffiliations": [],
    "authorPaperCount": 18,
    "authorCitationCount": 69,
    "paperId": "aab2ed83bc3739a20e90ae1d97dcf45f3bc8e508",
    "externalIds": {
      "DBLP": "conf/icassp/NguyenNNDLW23",
      "DOI": "10.1109/ICASSP49357.2023.10094599",
      "CorpusId": 258531035
    },
    "url": "https://www.semanticscholar.org/paper/aab2ed83bc3739a20e90ae1d97dcf45f3bc8e508",
    "title": "AdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization",
    "abstract": "Inverse text normalization (ITN) is the task that transforms text in spoken-form into written-form. While automatic speech recognition (ASR) produces text in spoken-form, human and natural language understanding systems prefer to consume text in written-form. ITN generally deals with semiotic phrases (e.g., numbers, date, time). However, lack of studies to deal with phonetization phrases, which is ASR\u2019s output when it handles unseen data (e.g., foreign-named entities, domain names), although these exist in the same form in the spoken-form text. The reason is that phonetization phrases are infinite patterns and language-dependent. In this study, we introduce a novel end2end model that can handle both semiotic phrases (SEP) and phonetization phrases (PHP), named AdapITN. We call it \"Adap\" because it allows for handling unseen PHP. The model performs only when necessary by providing a mechanism to narrow normalized regions and external query knowledge, reducing the runtime significantly.",
    "venue": "IEEE International Conference on Acoustics, Speech, and Signal Processing",
    "year": 2023,
    "referenceCount": 22,
    "citationCount": 1,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": null,
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "1-5",
      "name": "ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
    },
    "authors": [
      {
        "authorId": "2165093057",
        "name": "T. Nguyen"
      },
      {
        "authorId": "2216415969",
        "name": "Le Duc Minh Nhat"
      },
      {
        "authorId": "2057071045",
        "name": "Quang Minh Nguyen"
      },
      {
        "authorId": "2527751",
        "name": "Quoc Truong Do"
      },
      {
        "authorId": "1983382",
        "name": "C. Luong"
      },
      {
        "authorId": "2064429921",
        "name": "A. Waibel"
      }
    ],
    "tldr": "A novel end2end model that can handle both semiotic phrases (SEP) and phonetization phrases (PHP), named AdapITN is introduced, named \"Adap\" because it allows for handling unseen PHP."
  },
  {
    "profName": "Alexander Waibel",
    "authorId": "2064429921",
    "authorName": "A. Waibel",
    "authorUrl": "https://www.semanticscholar.org/author/2064429921",
    "authorHIndex": 6,
    "authorAffiliations": [],
    "authorPaperCount": 18,
    "authorCitationCount": 69,
    "paperId": "f3e237e794bc4cd8df7f3e31d0caa2f7ee8cd06b",
    "externalIds": {
      "ArXiv": "2305.03873",
      "ACL": "2023.loresmt-1.1",
      "DBLP": "journals/corr/abs-2305-03873",
      "DOI": "10.48550/arXiv.2305.03873",
      "CorpusId": 258486942
    },
    "url": "https://www.semanticscholar.org/paper/f3e237e794bc4cd8df7f3e31d0caa2f7ee8cd06b",
    "title": "Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages",
    "abstract": "In many humanitarian scenarios, translation into severely low resource languages often does not require a universal translation engine, but a dedicated text-specific translation engine. For example, healthcare records, hygienic procedures, government communication, emergency procedures and religious texts are all limited texts. While generic translation engines for all languages do not exist, translation of multilingually known limited texts into new, endangered languages may be possible and reduce human translation effort. We attempt to leverage translation resources from rich resource languages to efficiently produce best possible translation quality for well known texts, which is available in multiple languages, in a new, severely low resource language. We examine two approaches: 1.) best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and 2.) we adapt large general multilingual translation engines from many other languages to focus on a specific text in a new, unknown language. We find that adapting large pretrained multilingual models to the domain/text first and then to the severely low resource language works best. If we also select a best set of seed sentences, we can improve average chrF performance on new test languages from a baseline of 21.9 to 50.7, while reducing the number of seed sentences to only \u223c1,000 in the new, unknown language.",
    "venue": "LORESMT",
    "year": 2023,
    "referenceCount": 82,
    "citationCount": 0,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.03873",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.03873",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "144812501",
        "name": "Zhong Zhou"
      },
      {
        "authorId": "2920247",
        "name": "J. Niehues"
      },
      {
        "authorId": "2064429921",
        "name": "A. Waibel"
      }
    ],
    "tldr": "This work examines two approaches to best selection of seed sentences to jump start translations in a new language in view of best generalization to the remainder of a larger targeted text(s), and it finds that adapting large pretrained multilingual models to the domain/text first and then to the severely low resource language works best."
  },
  {
    "profName": "Alexander Waibel",
    "authorId": "2064429921",
    "authorName": "A. Waibel",
    "authorUrl": "https://www.semanticscholar.org/author/2064429921",
    "authorHIndex": 6,
    "authorAffiliations": [],
    "authorPaperCount": 18,
    "authorCitationCount": 69,
    "paperId": "f547c7ec86cbc0989e87f0e23f7e0b2cfc5259c3",
    "externalIds": {
      "ACL": "2023.iwslt-1.37",
      "DBLP": "conf/iwslt/PolakLPNWB23",
      "DOI": "10.18653/v1/2023.iwslt-1.37",
      "CorpusId": 259376517
    },
    "url": "https://www.semanticscholar.org/paper/f547c7ec86cbc0989e87f0e23f7e0b2cfc5259c3",
    "title": "Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023",
    "abstract": "In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional encoder-decoder models. The policy prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer. We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45% relative RTF).",
    "venue": "International Workshop on Spoken Language Translation",
    "year": 2023,
    "referenceCount": 32,
    "citationCount": 5,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.iwslt-1.37.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "389-396"
    },
    "authors": [
      {
        "authorId": "2066246895",
        "name": "Peter Pol\u00e1k"
      },
      {
        "authorId": "1609468282",
        "name": "Danni Liu"
      },
      {
        "authorId": "50214018",
        "name": "Ngoc-Quan Pham"
      },
      {
        "authorId": "2920247",
        "name": "J. Niehues"
      },
      {
        "authorId": "2064429921",
        "name": "A. Waibel"
      },
      {
        "authorId": "143832874",
        "name": "Ondrej Bojar"
      }
    ],
    "tldr": "This year's submission to the Simultaneous Track at IWSLT 2023 is described, and a novel online policy for attentional encoder-decoder models is proposed that prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer."
  }
]