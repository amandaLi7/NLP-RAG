[
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3",
    "externalIds": {
      "DBLP": "journals/corr/abs-2302-05738",
      "ArXiv": "2302.05738",
      "DOI": "10.48550/arXiv.2302.05738",
      "CorpusId": 256827706
    },
    "url": "https://www.semanticscholar.org/paper/03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3",
    "title": "Cross-Modal Fine-Tuning: Align then Refine",
    "abstract": "Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.05738",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "31030-31056"
    },
    "authors": [
      {
        "authorId": "2143669058",
        "name": "Junhong Shen"
      },
      {
        "authorId": "51517360",
        "name": "Liam Li"
      },
      {
        "authorId": "32273391",
        "name": "L. Dery"
      },
      {
        "authorId": "2081410501",
        "name": "Corey Staten"
      },
      {
        "authorId": "10398264",
        "name": "M. Khodak"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "145532827",
        "name": "Ameet Talwalkar"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "11a571eaab42a6ffb1d938635a093315e392756d",
    "externalIds": {
      "DBLP": "journals/corr/abs-2309-07423",
      "ArXiv": "2309.07423",
      "ACL": "2023.wmt-1.40",
      "DOI": "10.48550/arXiv.2309.07423",
      "CorpusId": 261824661
    },
    "url": "https://www.semanticscholar.org/paper/11a571eaab42a6ffb1d938635a093315e392756d",
    "title": "ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages",
    "abstract": "Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2309.07423",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2309.07423",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2240569372",
        "name": "Nathaniel R. Robinson"
      },
      {
        "authorId": "1988654955",
        "name": "Perez Ogayo"
      },
      {
        "authorId": "3407646",
        "name": "David R. Mortensen"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "17605c43ca3eb982c99642052ddc21a93d116594",
    "externalIds": {
      "DBLP": "conf/emnlp/SongK0FOWACTAN23",
      "ArXiv": "2305.14716",
      "DOI": "10.48550/arXiv.2305.14716",
      "CorpusId": 258866051
    },
    "url": "https://www.semanticscholar.org/paper/17605c43ca3eb982c99642052ddc21a93d116594",
    "title": "GlobalBench: A Benchmark for Global Progress in Natural Language Processing",
    "abstract": "Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.14716",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "14157-14171"
    },
    "authors": [
      {
        "authorId": "148310739",
        "name": "Yueqi Song"
      },
      {
        "authorId": "2218206121",
        "name": "Catherine Cui"
      },
      {
        "authorId": "1452678825",
        "name": "Simran Khanuja"
      },
      {
        "authorId": "144118452",
        "name": "Pengfei Liu"
      },
      {
        "authorId": "48556979",
        "name": "FAHIM FAISAL"
      },
      {
        "authorId": "1475670743",
        "name": "Alissa Ostapenko"
      },
      {
        "authorId": "9162688",
        "name": "Genta Indra Winata"
      },
      {
        "authorId": "8129718",
        "name": "Alham Fikri Aji"
      },
      {
        "authorId": "66986482",
        "name": "Samuel Cahyawijaya"
      },
      {
        "authorId": "2073587169",
        "name": "Yulia Tsvetkov"
      },
      {
        "authorId": "49513989",
        "name": "Antonios Anastasopoulos"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "1786a2f9140ed7211b21302977de64e948b92308",
    "externalIds": {
      "ArXiv": "2302.07867",
      "DBLP": "journals/corr/abs-2302-07867",
      "DOI": "10.48550/arXiv.2302.07867",
      "CorpusId": 256868633
    },
    "url": "https://www.semanticscholar.org/paper/1786a2f9140ed7211b21302977de64e948b92308",
    "title": "Learning Performance-Improving Code Edits",
    "abstract": "The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.",
    "year": 2023,
    "influentialCitationCount": 3,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.07867",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2302.07867",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "21626987",
        "name": "Aman Madaan"
      },
      {
        "authorId": "2129995371",
        "name": "Alex Shypula"
      },
      {
        "authorId": "47051926",
        "name": "Uri Alon"
      },
      {
        "authorId": "33798741",
        "name": "Milad Hashemi"
      },
      {
        "authorId": "1770926",
        "name": "Parthasarathy Ranganathan"
      },
      {
        "authorId": "46286308",
        "name": "Yiming Yang"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "2112229",
        "name": "A. Yazdanbakhsh"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "31366ff634fc905affd78dbd8ddc9a872c006a87",
    "externalIds": {
      "DBLP": "journals/corr/abs-2302-05527",
      "ArXiv": "2302.05527",
      "DOI": "10.48550/arXiv.2302.05527",
      "CorpusId": 256827797
    },
    "url": "https://www.semanticscholar.org/paper/31366ff634fc905affd78dbd8ddc9a872c006a87",
    "title": "CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code",
    "abstract": "Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score",
    "year": 2023,
    "influentialCitationCount": 5,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.05527",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "13921-13937"
    },
    "authors": [
      {
        "authorId": "2149163534",
        "name": "Shuyan Zhou"
      },
      {
        "authorId": "47051926",
        "name": "Uri Alon"
      },
      {
        "authorId": "2114357424",
        "name": "Sumit Agarwal"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "405f1a5602867c66e015491c26d2be5504eed458",
    "externalIds": {
      "ArXiv": "2306.06804",
      "DBLP": "journals/corr/abs-2306-06804",
      "ACL": "2023.americasnlp-1.13",
      "DOI": "10.48550/arXiv.2306.06804",
      "CorpusId": 259138596
    },
    "url": "https://www.semanticscholar.org/paper/405f1a5602867c66e015491c26d2be5504eed458",
    "title": "Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction",
    "abstract": "Neural models have drastically advanced state of the art for machine translation (MT) between high-resource languages. Traditionally, these models rely on large amounts of training data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of parallel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open questions, product of an increased interest of the NLP community in these languages.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2306.06804",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science",
      "Mathematics"
    ],
    "journal": {
      "volume": "abs/2306.06804",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "153151470",
        "name": "Manuel Mager"
      },
      {
        "authorId": "2061975276",
        "name": "R. Bhatnagar"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "4160376",
        "name": "Ngoc Thang Vu"
      },
      {
        "authorId": "3422953",
        "name": "Katharina Kann"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "43c0f77f116f986b53eb04f5c9b33f10132ded55",
    "externalIds": {
      "ACL": "2023.computel-1.4",
      "DBLP": "journals/corr/abs-2302-13410",
      "ArXiv": "2302.13410",
      "DOI": "10.48550/arXiv.2302.13410",
      "CorpusId": 257219604
    },
    "url": "https://www.semanticscholar.org/paper/43c0f77f116f986b53eb04f5c9b33f10132ded55",
    "title": "User-Centric Evaluation of OCR Systems for Kwak\u2019wala",
    "abstract": "There has been recent interest in improving optical character recognition (OCR) for endangered languages, particularly because a large number of documents and books in these languages are not in machine-readable formats. The performance of OCR systems is typically evaluated using automatic metrics such as character and word error rates. While error rates are useful for the comparison of different models and systems, they do not measure whether and how the transcriptions produced from OCR tools are useful to downstream users. In this paper, we present a human-centric evaluation of OCR systems, focusing on the Kwak'wala language as a case study. With a user study, we show that utilizing OCR reduces the time spent in the manual transcription of culturally valuable documents -- a task that is often undertaken by endangered language community members and researchers -- by over 50%. Our results demonstrate the potential benefits that OCR tools can have on downstream language documentation and revitalization efforts.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2302.13410",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2302.13410",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "7391530",
        "name": "Shruti Rijhwani"
      },
      {
        "authorId": "102758386",
        "name": "Daisy Rosenblum"
      },
      {
        "authorId": "2209989543",
        "name": "Michayla King"
      },
      {
        "authorId": "49513989",
        "name": "Antonios Anastasopoulos"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "4d74a5048b884e8bb3842240abf98915c619c8f8",
    "externalIds": {
      "ArXiv": "2306.01200",
      "DBLP": "conf/acl/JainKSF0NZ23",
      "DOI": "10.18653/v1/2023.findings-acl.537",
      "CorpusId": 259064002
    },
    "url": "https://www.semanticscholar.org/paper/4d74a5048b884e8bb3842240abf98915c619c8f8",
    "title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning",
    "abstract": "Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.",
    "year": 2023,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.findings-acl.537.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2306.01200",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2116998524",
        "name": "Sameer Jain"
      },
      {
        "authorId": "17320214",
        "name": "Vaishakh Keshava"
      },
      {
        "authorId": "1644192946",
        "name": "Swarnashree Mysore Sathyendra"
      },
      {
        "authorId": "2058640028",
        "name": "Patrick Fernandes"
      },
      {
        "authorId": "144118452",
        "name": "Pengfei Liu"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "2384711",
        "name": "Chunting Zhou"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "5ea8eedcb31859c5730dd1da3804e1be529ffabb",
    "externalIds": {
      "ArXiv": "2303.16750",
      "DBLP": "journals/corr/abs-2303-16750",
      "DOI": "10.48550/arXiv.2303.16750",
      "CorpusId": 257805004
    },
    "url": "https://www.semanticscholar.org/paper/5ea8eedcb31859c5730dd1da3804e1be529ffabb",
    "title": "A Gold Standard Dataset for the Reviewer Assignment Problem",
    "abstract": "Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information.",
    "year": 2023,
    "influentialCitationCount": 2,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.16750",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2303.16750",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "50825200",
        "name": "Ivan Stelmakh"
      },
      {
        "authorId": "1771118",
        "name": "J. Wieting"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "1737249",
        "name": "Nihar B. Shah"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "659be1ff350634f50cc066d258ee6a45e697e552",
    "externalIds": {
      "DBLP": "conf/sigmorphon/HeTR0MNL23",
      "ACL": "2023.sigmorphon-1.22",
      "DOI": "10.18653/v1/2023.sigmorphon-1.22",
      "CorpusId": 259833803
    },
    "url": "https://www.semanticscholar.org/paper/659be1ff350634f50cc066d258ee6a45e697e552",
    "title": "SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing",
    "abstract": "In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://aclanthology.org/2023.sigmorphon-1.22.pdf",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "209-216"
    },
    "authors": [
      {
        "authorId": "2107034039",
        "name": "Taiqi He"
      },
      {
        "authorId": "2219036626",
        "name": "Lindia Tjuatja"
      },
      {
        "authorId": "2067645226",
        "name": "Nathaniel R. Robinson"
      },
      {
        "authorId": "1746678",
        "name": "Shinji Watanabe"
      },
      {
        "authorId": "3407646",
        "name": "David R. Mortensen"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "145585627",
        "name": "L. Levin"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "74b05bba46db21e589a2cc0f916f81069b0368ef",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-00955",
      "ArXiv": "2305.00955",
      "DOI": "10.48550/arXiv.2305.00955",
      "CorpusId": 258426970
    },
    "url": "https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef",
    "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation",
    "abstract": "Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.",
    "year": 2023,
    "influentialCitationCount": 7,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.00955",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.00955",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2058640028",
        "name": "Patrick Fernandes"
      },
      {
        "authorId": "21626987",
        "name": "Aman Madaan"
      },
      {
        "authorId": "1381444447",
        "name": "Emmy Liu"
      },
      {
        "authorId": "1748971692",
        "name": "Ant\u00f3nio Farinhas"
      },
      {
        "authorId": "144869806",
        "name": "Pedro Henrique Martins"
      },
      {
        "authorId": "2138301112",
        "name": "Amanda Bertsch"
      },
      {
        "authorId": "34876539",
        "name": "Jos\u00e9 G. C. de Souza"
      },
      {
        "authorId": "2149163534",
        "name": "Shuyan Zhou"
      },
      {
        "authorId": "35232494",
        "name": "Tongshuang Sherry Wu"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "1400227478",
        "name": "Andr\u00e9 F. T. Martins"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "7a5b44ea10a51708e18786595c8d70b18950da11",
    "externalIds": {
      "DBLP": "journals/corr/abs-2307-13528",
      "ArXiv": "2307.13528",
      "DOI": "10.48550/arXiv.2307.13528",
      "CorpusId": 260154834
    },
    "url": "https://www.semanticscholar.org/paper/7a5b44ea10a51708e18786595c8d70b18950da11",
    "title": "FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
    "abstract": "The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",
    "year": 2023,
    "influentialCitationCount": 5,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.13528",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2307.13528",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2047713083",
        "name": "Ethan Chern"
      },
      {
        "authorId": "2224851117",
        "name": "Steffi Chern"
      },
      {
        "authorId": "2108956946",
        "name": "Shiqi Chen"
      },
      {
        "authorId": "30300197",
        "name": "Weizhe Yuan"
      },
      {
        "authorId": "2224772135",
        "name": "Kehua Feng"
      },
      {
        "authorId": "2110714400",
        "name": "Chunting Zhou"
      },
      {
        "authorId": "6215698",
        "name": "Junxian He"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "144118452",
        "name": "Pengfei Liu"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "88884b8806262a4095036041e3567d450dba39f7",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-06983",
      "ArXiv": "2305.06983",
      "DOI": "10.48550/arXiv.2305.06983",
      "CorpusId": 258615731
    },
    "url": "https://www.semanticscholar.org/paper/88884b8806262a4095036041e3567d450dba39f7",
    "title": "Active Retrieval Augmented Generation",
    "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
    "year": 2023,
    "influentialCitationCount": 6,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.06983",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.06983",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2669515",
        "name": "Zhengbao Jiang"
      },
      {
        "authorId": "40027632",
        "name": "Frank F. Xu"
      },
      {
        "authorId": "49715441",
        "name": "Luyu Gao"
      },
      {
        "authorId": "48064856",
        "name": "Zhiqing Sun"
      },
      {
        "authorId": "1409707585",
        "name": "Qian Liu"
      },
      {
        "authorId": "2173509991",
        "name": "Jane Dwivedi-Yu"
      },
      {
        "authorId": "46286308",
        "name": "Yiming Yang"
      },
      {
        "authorId": "144987107",
        "name": "Jamie Callan"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "8e8a1489bf4d782d2435cdeb93f7d1f165747c63",
    "externalIds": {
      "ArXiv": "2307.00524",
      "DBLP": "journals/corr/abs-2307-00524",
      "DOI": "10.48550/arXiv.2307.00524",
      "CorpusId": 259317075
    },
    "url": "https://www.semanticscholar.org/paper/8e8a1489bf4d782d2435cdeb93f7d1f165747c63",
    "title": "Large Language Models Enable Few-Shot Clustering",
    "abstract": "Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2307.00524",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2307.00524",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2061499362",
        "name": "Vijay Viswanathan"
      },
      {
        "authorId": "24868638",
        "name": "Kiril Gashteovski"
      },
      {
        "authorId": "19752252",
        "name": "Carolin (Haas) Lawrence"
      },
      {
        "authorId": "35232494",
        "name": "Tongshuang Sherry Wu"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "9bce3661f01825ad56dc9d2b3d254fd9e3792360",
    "externalIds": {
      "DBLP": "journals/corr/abs-2305-11789",
      "ArXiv": "2305.11789",
      "DOI": "10.48550/arXiv.2305.11789",
      "CorpusId": 258823453
    },
    "url": "https://www.semanticscholar.org/paper/9bce3661f01825ad56dc9d2b3d254fd9e3792360",
    "title": "Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach",
    "abstract": "Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.11789",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.11789",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "143655216",
        "name": "Masahiro Kaneko"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "1764004",
        "name": "Naoaki Okazaki"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "b4987da792dd45a84232cfb06d71b1c2ec488f38",
    "externalIds": {
      "DBLP": "conf/emnlp/LiuCN23",
      "ArXiv": "2310.07081",
      "DOI": "10.48550/arXiv.2310.07081",
      "CorpusId": 265907218
    },
    "url": "https://www.semanticscholar.org/paper/b4987da792dd45a84232cfb06d71b1c2ec488f38",
    "title": "Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting",
    "abstract": "Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute accuracy, but also holds potential benefits for non-idiomatic sentences.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.07081",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "15095-15111"
    },
    "authors": [
      {
        "authorId": "2266945896",
        "name": "Emmy Liu"
      },
      {
        "authorId": "2266465306",
        "name": "Aditi Chaudhary"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "c432aff446d55e72a28394a1508e760cc9a25c08",
    "externalIds": {
      "DBLP": "conf/icml/Xu0N23",
      "ArXiv": "2301.02828",
      "DOI": "10.48550/arXiv.2301.02828",
      "CorpusId": 255546631
    },
    "url": "https://www.semanticscholar.org/paper/c432aff446d55e72a28394a1508e760cc9a25c08",
    "title": "Why do Nearest Neighbor Language Models Work?",
    "abstract": "Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.",
    "year": 2023,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2301.02828",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2301.02828",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "40027632",
        "name": "Frank F. Xu"
      },
      {
        "authorId": "47051926",
        "name": "Uri Alon"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "c5207241406586f4263b235667e004b71ea68953",
    "externalIds": {
      "ArXiv": "2305.18185",
      "ACL": "2023.starsem-1.14",
      "DBLP": "conf/starsem/TjuatjaLLN23",
      "DOI": "10.48550/arXiv.2305.18185",
      "CorpusId": 258959069
    },
    "url": "https://www.semanticscholar.org/paper/c5207241406586f4263b235667e004b71ea68953",
    "title": "Syntax and Semantics Meet in the \u201cMiddle\u201d: Probing the Syntax-Semantics Interface of LMs Through Agentivity",
    "abstract": "Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms\u2014i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2305.18185",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "149-164"
    },
    "authors": [
      {
        "authorId": "2219036626",
        "name": "Lindia Tjuatja"
      },
      {
        "authorId": "1381444447",
        "name": "Emmy Liu"
      },
      {
        "authorId": "145585627",
        "name": "L. Levin"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "cc1705fe421c70d85254b557634bd4669fdd49b0",
    "externalIds": {
      "ArXiv": "2305.16636",
      "ACL": "2023.acl-long.573",
      "DBLP": "journals/corr/abs-2305-16636",
      "DOI": "10.48550/arXiv.2305.16636",
      "CorpusId": 258947254
    },
    "url": "https://www.semanticscholar.org/paper/cc1705fe421c70d85254b557634bd4669fdd49b0",
    "title": "DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions",
    "abstract": "Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We operationalize the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval algorithms on our test set and present a superior bi-encoder retriever for text-based dataset recommendation. This system, trained on the DataFinder Dataset, finds more relevant search results than existing third-party dataset search engines. To encourage progress on dataset recommendation, we release our dataset and models to the public.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.16636",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.16636",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2061499362",
        "name": "Vijay Viswanathan"
      },
      {
        "authorId": "49715441",
        "name": "Luyu Gao"
      },
      {
        "authorId": "35232494",
        "name": "Tongshuang Sherry Wu"
      },
      {
        "authorId": "144118452",
        "name": "Pengfei Liu"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "d5dd7230cccace7e77095d3b5fd8394850f59170",
    "externalIds": {
      "DBLP": "conf/acl/KabraLKAWCAON23",
      "ArXiv": "2305.16171",
      "DOI": "10.48550/arXiv.2305.16171",
      "CorpusId": 258887835
    },
    "url": "https://www.semanticscholar.org/paper/d5dd7230cccace7e77095d3b5fd8394850f59170",
    "title": "Multi-lingual and Multi-cultural Figurative Language Understanding",
    "abstract": "Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, \\datasetname, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.",
    "year": 2023,
    "influentialCitationCount": 1,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.16171",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "8269-8284"
    },
    "authors": [
      {
        "authorId": "1735001746",
        "name": "Anubha Kabra"
      },
      {
        "authorId": "1381444447",
        "name": "Emmy Liu"
      },
      {
        "authorId": "1452678825",
        "name": "Simran Khanuja"
      },
      {
        "authorId": "8129718",
        "name": "Alham Fikri Aji"
      },
      {
        "authorId": "9162688",
        "name": "Genta Indra Winata"
      },
      {
        "authorId": "2220548276",
        "name": "Samuel Cahyawijaya"
      },
      {
        "authorId": "2056773747",
        "name": "Anuoluwapo Aremu"
      },
      {
        "authorId": "1988654955",
        "name": "Perez Ogayo"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "d6ae4c0679bdceb029f652efd2a854ac5ade772f",
    "externalIds": {
      "DBLP": "journals/corr/abs-2310-01387",
      "ACL": "2023.bigpicture-1.9",
      "ArXiv": "2310.01387",
      "DOI": "10.48550/arXiv.2310.01387",
      "CorpusId": 263605610
    },
    "url": "https://www.semanticscholar.org/paper/d6ae4c0679bdceb029f652efd2a854ac5ade772f",
    "title": "It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
    "abstract": "Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2310.01387",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2310.01387",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2138301112",
        "name": "Amanda Bertsch"
      },
      {
        "authorId": "2253395527",
        "name": "Alex Xie"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "1762110",
        "name": "Matthew R. Gormley"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "dbc368bc8b49347dd27679894524fa62f88492c9",
    "externalIds": {
      "ArXiv": "2305.01625",
      "DBLP": "journals/corr/abs-2305-01625",
      "DOI": "10.48550/arXiv.2305.01625",
      "CorpusId": 258436892
    },
    "url": "https://www.semanticscholar.org/paper/dbc368bc8b49347dd27679894524fa62f88492c9",
    "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input",
    "abstract": "Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .",
    "year": 2023,
    "influentialCitationCount": 3,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2305.01625",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2305.01625",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2138301112",
        "name": "Amanda Bertsch"
      },
      {
        "authorId": "47051926",
        "name": "Uri Alon"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "1762110",
        "name": "Matthew R. Gormley"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "e41482f4ee984f17382f6cdd900df094d928be06",
    "externalIds": {
      "ArXiv": "2307.13854",
      "DBLP": "journals/corr/abs-2307-13854",
      "DOI": "10.48550/arXiv.2307.13854",
      "CorpusId": 260164780
    },
    "url": "https://www.semanticscholar.org/paper/e41482f4ee984f17382f6cdd900df094d928be06",
    "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
    "abstract": "With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.",
    "year": 2023,
    "influentialCitationCount": 7,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.13854",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2307.13854",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2149163534",
        "name": "Shuyan Zhou"
      },
      {
        "authorId": "40027632",
        "name": "Frank F. Xu"
      },
      {
        "authorId": "2115313911",
        "name": "Hao Zhu"
      },
      {
        "authorId": "144101734",
        "name": "Xuhui Zhou"
      },
      {
        "authorId": "145250604",
        "name": "Robert Lo"
      },
      {
        "authorId": "66820957",
        "name": "Abishek Sridhar"
      },
      {
        "authorId": "144691454",
        "name": "Xianyi Cheng"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "47070750",
        "name": "Daniel Fried"
      },
      {
        "authorId": "47051926",
        "name": "Uri Alon"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43",
    "externalIds": {
      "DBLP": "conf/emnlp/ViswanathanZBWN23",
      "ArXiv": "2308.12261",
      "DOI": "10.48550/arXiv.2308.12261",
      "CorpusId": 261075905
    },
    "url": "https://www.semanticscholar.org/paper/e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43",
    "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions",
    "abstract": "Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.12261",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2308.12261",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2061499362",
        "name": "Vijay Viswanathan"
      },
      {
        "authorId": "2023526",
        "name": "Chenyang Zhao"
      },
      {
        "authorId": "2138301112",
        "name": "Amanda Bertsch"
      },
      {
        "authorId": "35232494",
        "name": "Tongshuang Sherry Wu"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "e7b3b692b0816821aafc0d354749bc3802cbf6ac",
    "externalIds": {
      "DBLP": "journals/corr/abs-2303-01502",
      "ArXiv": "2303.01502",
      "DOI": "10.48550/arXiv.2303.01502",
      "CorpusId": 257280165
    },
    "url": "https://www.semanticscholar.org/paper/e7b3b692b0816821aafc0d354749bc3802cbf6ac",
    "title": "Computational Language Acquisition with Theory of Mind",
    "abstract": "Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "http://arxiv.org/pdf/2303.01502",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2303.01502",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "46263614",
        "name": "Andy T. Liu"
      },
      {
        "authorId": "2115314674",
        "name": "Hao Zhu"
      },
      {
        "authorId": "1381444447",
        "name": "Emmy Liu"
      },
      {
        "authorId": "3312309",
        "name": "Yonatan Bisk"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "f640e89fcede075b4bde3b2fa0dc78f591589ba3",
    "externalIds": {
      "ACL": "2023.trustnlp-1.6",
      "ArXiv": "2307.04507",
      "DBLP": "journals/corr/abs-2307-04507",
      "DOI": "10.48550/arXiv.2307.04507",
      "CorpusId": 259501636
    },
    "url": "https://www.semanticscholar.org/paper/f640e89fcede075b4bde3b2fa0dc78f591589ba3",
    "title": "Improving Factuality of Abstractive Summarization via Contrastive Reward Learning",
    "abstract": "Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries. Code and human evaluation results will be publicly available at \\url{https://github.com/EthanC111/factuality_summarization}.",
    "year": 2023,
    "influentialCitationCount": 0,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2307.04507",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "volume": "abs/2307.04507",
      "name": "ArXiv"
    },
    "authors": [
      {
        "authorId": "2047713083",
        "name": "Ethan Chern"
      },
      {
        "authorId": "1390877035",
        "name": "Zhiruo Wang"
      },
      {
        "authorId": "2221736765",
        "name": "Sanjan Das"
      },
      {
        "authorId": "2118766697",
        "name": "Bhavuk Sharma"
      },
      {
        "authorId": "144118452",
        "name": "Pengfei Liu"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      }
    ]
  },
  {
    "authorId": "1700325",
    "authorName": "Graham Neubig",
    "authorUrl": "https://www.semanticscholar.org/author/1700325",
    "authorHIndex": 75,
    "authorAffiliations": [],
    "authorPaperCount": 600,
    "authorCitationCount": 24256,
    "paperId": "fd80f7f3673fc6ca02f192d5d73426f11a4be659",
    "externalIds": {
      "DBLP": "journals/corr/abs-2308-07286",
      "ArXiv": "2308.07286",
      "ACL": "2023.wmt-1.100",
      "DOI": "10.48550/arXiv.2308.07286",
      "CorpusId": 260886800
    },
    "url": "https://www.semanticscholar.org/paper/fd80f7f3673fc6ca02f192d5d73426f11a4be659",
    "title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
    "abstract": "Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.",
    "year": 2023,
    "influentialCitationCount": 2,
    "isOpenAccess": true,
    "openAccessPdf": {
      "url": "https://arxiv.org/pdf/2308.07286",
      "status": null
    },
    "fieldsOfStudy": [
      "Computer Science"
    ],
    "journal": {
      "pages": "1066-1083"
    },
    "authors": [
      {
        "authorId": "2058640028",
        "name": "Patrick Fernandes"
      },
      {
        "authorId": "145346875",
        "name": "Daniel Deutsch"
      },
      {
        "authorId": "2056981575",
        "name": "M. Finkelstein"
      },
      {
        "authorId": "47718053",
        "name": "Parker Riley"
      },
      {
        "authorId": "145644643",
        "name": "Andr\u00e9 F. T. Martins"
      },
      {
        "authorId": "1700325",
        "name": "Graham Neubig"
      },
      {
        "authorId": "2499663",
        "name": "Ankush Garg"
      },
      {
        "authorId": "144797264",
        "name": "J. Clark"
      },
      {
        "authorId": "35307070",
        "name": "Markus Freitag"
      },
      {
        "authorId": "2345617",
        "name": "Orhan Firat"
      }
    ]
  }
]