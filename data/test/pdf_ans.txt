December 11-15; December 11, 12, 13, 14, 15
April 11-13; April 11, April 12, April 13
Semester drop deadline
No
13
“My Heart is in the Work.”; “My heart is in the work.”
Stephanie Kwolek
11-928 and 11-929
Yes
Daphne Ippolito
Fisher-CallHome Spanish English, IWSLT22 Tunisian Arabic-English, and BOLT Chinese-English
11711 Advanced Natural Language Processing
Yiming Yang
09:30AM-10:50AM
PH 100
Fall 2023, Spring 2024, Summer 2024
Fall 2023, Spring 2024
Tom Mitchell
Eric Nyberg
Yulia Tsvetkov
Yes, Taylor Berg-Kirkpatrick
Shinji Watanabe
A Masters student who uses courses taken as part of another degree program (at Carnegie Mellon or elsewhere) toward their program requirements cannot use those same courses toward any other M.S. degree offered by the School of Computer Science without prior approval. (SCS policy)
11611, 11685, 11700, 11741, 11785, 11910, 11920, 11925, 11928, 11930
Five years, although six years is also common
During the summer between the first and second years of the program
One
One
Five years from the time that the student matriculates into the program
11777 Multimodal Machine Learning
Lei Li
Robert Frederking, Daniel Fried
Eric Nyberg, Teruko Mitamura
#1
#4
146
School of Computer Science
Randy Pausch, Jeffrey Zaslow
More than 400 startups
More than 152 spinup companies
20
47
14491
213
ArXiv
No journal data available.
Computer Science
6
Annual Meeting of the Association for Computational Linguistics, arXiv.org, Conference on Machine Translation, SIGDIAL Conferences, DSTC
The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics
http://arxiv.org/pdf/2305.11806
Ricardo Rei, Nuno M. Guerreiro, Marcos Vinicius Treviso, Luisa Coheur, A. Lavie, Andre Martins
This study reveals that neural explainability metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token- level neural saliency maps with Multidimensional Quality Metrics annotations and with synthetically-generated critical translation errors.
Neural metrics for machine translation evaluation, such as COMET, exhibit significant improvements in their correlation with human judgments, as compared to traditional metrics based on lexical overlap, such as BLEU. Yet, neural metrics are, to a great extent, ‚Äúblack boxes‚Äù returning a single sentence-level score without transparency about the decision-making process. In this work, we develop and compare several neural explainability methods and demonstrate their effectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our study reveals that these metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and with synthetically-generated critical translation errors. To ease future research, we release our code at: https://github.com/Unbabel/COMET/tree/explainable-metrics
ArXiv, volume: abs/2308.16797; ArXiv
DSTC
4
J. Mendoncca, Patricia Pereira, Joao Paulo Carvalho, A. Lavie, I. Trancoso
J. Mendoncca
bcefc74b20649fd41ea05d87a3fa512d2559fc8d
Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation
A novel framework that takes advantage of the strengths of current evaluation models with the newly-established paradigm of prompting Large Language Models (LLMs) to achieve the desired properties of robustness and multilinguality for dialogue evaluation metrics.
Conference on Machine Translation, pages: 578-628; Conference on Machine Translation
Conference on Machine Translation
8
Markus Freitag, Nitika Mathur, Chi-kiu Lo, Eleftherios Avramidis, Ricardo Rei, Brian Thompson, Tom Kocmi, Frederic Blain, Daniel Deutsch, Craig Stewart, Chrysoula Zerva, Sheila Castilho, A. Lavie, George F. Foster
Markus Freitag