
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 Trusting Your Evidence 
Hallucinate Less with Context-aware Decoding
Weijia Shi1 Xiaochuang Han1 
Mike Lewis2Yulia Tsvetkov1Luke Zettlemoyer1Scott Yih2
1University of Washington, Seattle, WA,2Meta AI
{swj0419, xhan77}@cs.washington.edu
Language models (LMs) often struggle to pay
enough attention to the input context, and generate
 texts that are unfaithful or contain hallucinations.
 To mitigate this issue, we present
context-aware decoding (CAD), which follows
a contrastive output distribution that amplifies
the difference between the output probabilities
when a model is used with and without context.
 Our experiments show that CAD, without
additional training, significantly improves the
faithfulness of different LM families, including
OPT, GPT, LLaMA and FLAN-T5 for summarization
 tasks (e.g., 14.3% gain for LLaMA in
factuality metrics). Furthermore, CAD is particularly
 effective in overriding a model s prior
knowledge when it contradicts the provided
context, leading to substantial improvements in
tasks where resolving the knowledge conflict is
Language models (LMs) are remarkably effective
in generating coherent and fluent continuations of
a prompt or document prefix. During generation,
they mostly rely on two sources of knowledge  (1)
prior knowledge , which is learned during pretraining
 and stored implicitly within the model parameters 
 (2) context knowledge , which is passed as inputs
 in the prefix context (Chan et al., 2022). However,
 it remains an open question how a pretrained
LM, particularly a vanilla LM without task-specific
finetuning, balances these two knowledge sources
Previous research shows that LMs can fail to pay
enough attention to new information introduced
in the context knowledge. This can lead to hallucination
 in summarization (Maynez et al., 2020 
Pagnoni et al., 2021), where the generated summaries
 include facts not present in the input document.
 Insufficient attention to context is especially
 problematic when the context knowled
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 ge
 Equal contribution. Order randomly determined.
Figure 1  An illustration of context-aware decoding.
contradicts with the prior knowledge (Longpre
et al., 2021  Zhou et al., 2023). For instance, when
LLaMA (Touvron et al., 2023) is presented with a
latest document  Argentina won the FIFA World
Cups in 1978,1986 and 2022 ...  in its context (Figure
 1), it still predicts  Two  in response to the
question  How many World Cups have Argentina
won  , due in part to the outdated training data.
In this work, we present a simple context- aware
decoding (CAD) method to encourage the LM to
attend to its context during generation. As shown
in Figure 1, CAD samples from a new output distribution,
 which amplifies the difference between
output probabilities with and without the context
document. This provides a new form of contrastive
decoding (Li et al., 2022), which effectively downweights
 the prior knowledge when more relevant
contextual information is provided. CAD can be
used with off-the-shelf pretrained language models
without any additional training.
Experimental results from summarization tasks
show that context-aware decoding significantly
enhances the generation faithfulness of various
vanilla LMs including OPT (Zhang et al., 2022),
GPT-Neo (Black et al., 2021), LLaMA (Touvron
et al., 2023) and instruction-finetuned LMs such as
FLAN (Chung et al., 2022). For instance, when applied
 to LLaMA-30B in CNN-DM, CAD leads to
substantial improvement in both ROUGE-L (21%)arXiv 2305.14739v1  [cs.CL]  24 May 2023and summary factuality evaluation metrics (14.3%).
More notably, CAD is especially beneficial for
knowledge conflicting tasks, where the context
contains information contradictory to the model s
prior knowledge. CAD brings a 2.9x improvement
 to LLaMA-30B on a knowledge conflicts QA
dataset (Longpre et al., 2021). Furthermore, we
observe that this gain brought by CAD increases
as the model size grows in knowledge conflicts
tasks. These results demonstrate the potenti
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 al of
CAD in mitigating hallucinations in text generation
and overriding prior knowledge with reliable and
Given a language model θ, an input query x, and
a context cthat contains some external knowledge
unfamiliar orin conflict to the model s prior knowledge,
 we ask our model θto generate a response
ygiven the the query and context. The response
can be directly sampled (autoregressively) from the
probability distribution conditioned on query xand
 exp logitθ(yt c,x,y t)
However, in cases where the context ccontains
knowledge that is out-of-distribution with respect
toθ, we hypothesize that the model can struggle
to effectively attend to cand overly rely on the
prior knowledge encoded in θ. For instance, as
illustrated in Figure 1, when the context cstates
 Argentina won the FIFA World Cups in 1978, 1986
and 2022 ... , it contradicts the LM s outdated prior
knowledge that Argentina has won the World Cup
twice. The language model may still incorrectly
predict  Two  even when presented with the contextcand
2.2 Context-aware Decoding
To mitigate such issues, we factor out the prior
knowledge from the model s original output distribution
 contrastively. Here, we model the prior
knowledge as pθ(yt x,y t)and adjust the
model s original output probability distribution using
 the pointwise mutual information (PMI) between
 the context cand the generation yt, condi-tioned on x,y t. Formally, we have 
 pθ(yt c,x,y t)(pθ(yt c,x,y t)
where the output probability is a product-of-experts
of the original output probability and PMI weighted
byα. Essentially, outputs that become much more
likely when the context is included are preferred
This expression is not a valid probability distribution
 and needs to be normalized across all possible
values of yt. By rearranging the terms, we obtain
yt softmax [(1 α)logitθ(yt c,x,y t)
Larger αmeans more weight on our adjustment
(α 0reduces to regular decoding).1We refer
to this simple method as context-aware decoding.
From the adjusted output
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
  distribution  p, we can
apply various sampling strategies, such as nucleus
sampling (Holtzman et al., 2019).
Essentially, context-aware decoding is just a contrastive
 ensemble between the logits of pθ(yt 
c,x,y t)andpθ(yt x,y t). A similar contrastive
 objective is universal in image generation,
 where classifier-free diffusion models (Ho
and Salimans, 2022) predict diffusion noise with
(1 α)ϵθ(x,c) αϵθ(x), withcbeing a control to
the image. In text generation, Malkin et al. (2021)
propose coherence boosting with the same intuition,
 with a focus on contrasting the full input and
a short premise-free input, promoting coherence
w.r.t. the long context. Instead of using a single
model θin this work, different models can also
be used in the distribution adjustments to demote
unwanted model behaviors or distill expert model s
capability (Liu et al., 2021  Li et al., 2022).
We perform evaluation on tasks that require LMs to
read and reason over contexts and produce outputs
that are faithful to the contexts. Following prior
work (Zhang et al., 2023  Zhou et al., 2023), we
evaluate the models using prompting.
1If we identify an external knowledge cconditionally independent
 to the generation, pθ(yt c,x,y t) pθ(yt 
x,y t), even a non-zero αwould not have an impact to the
original output distribution.3.1 Datasets and Metrics
Summarization We conduct summarization experiments
 on two news datasets  CNN-DM (See
et al., 2017) and XSUM (Narayan et al., 2018). We
use ROUGE-L (Lin, 2004) to evaluate summarization
 quality. To measure the factual consistency
of summaries, we adopt BERT-Precision (Pagnoni
et al., 2021) as well as FactKB (Feng et al., 2023),
which has been demonstrated to achieve high correlations
 with human judgment on the two summarization
Knowledge Conflicts We evaluate performance
on two knowledge conflict datasets  MemoTrap
(Liu and Liu, 2023) and NQ-Swap (Longpre et al.,
2021). MemoTrap is created to investigate whether
language models could fall into memori
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 zation traps.
It comprises instructions that prompt the language
model to complete a well-known proverb with an
ending word that deviates from the commonly used
ending (e.g., Write a quote that ends in the word
 early   Better late than ). NQ-Swap is based on
a QA dataset, natural questions (NQ) (Kwiatkowski
et al., 2019), where the objective is to answer questions
 based on a reliable gold document. To generate
 NQ-Swap, Longpre et al. (2021) first identify
questions in NQ with named entity answers, find
the supportive document for each question and then
replace the gold answer entity in the document with
a random entity. A faithful LM should generate the
replaced entity as the answer when given the question
 and modified document. We also include the
original NQ dataset with the question and original
 document for evaluation. We use Exact Match
(EM) as the evaluation metric for NQ-Swap, NQ
In Table 1, we show illustrative examples of the
contexts we aim to upweight for the model and the
queries across different datasets. We hope LMs pay
more attention to the source document in XSUM
and NQ-Swap. On the other hand, we hope LMs
focus more on the instruction in MemoTrap.
3.2 Models and Baselines
We apply CAD to pretrained language models including
 OPT (13B and 30B) (Zhang et al., 2022),
GPT-Neo (2.7B and 20B) (Black et al., 2021),
LLaMA (13B and 30B) (Touvron et al., 2023)
and instruction-finetuned language models such
as FLAN-T5 (XL 3B and XXL 11B) (Chung et al.,
c Article  Prison Link Cymru had 1,099 referrals in
2015-16 and said some ex-offenders were living
rough for up to a year before finding suitable accommodation
x Summarize the article in one sentence. Summary 
c Tesla CEO Elon Musk is now in charge of Twitter ,
x Who is Twitter CEO now 
c Write a quote that ends in the word "early" 
Table 1  An illustation of the inputs to CAD applied to
each dataset. CAD upweights the context c(in red) by
sampling each token from softmax [(1 α)logitθ(yt 
c,x,y t) αlogitθ(y
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 t x,y t)].
CAD introduces a hyperparameter αto control
the adjustment level. We set α 0.5for all models
 evaluated on the summarization datasets and
α 1for all models evaluated on the knowledge
conflict datasets. We observed that α 0.5generally
 yielded good results across all settings and
all datasets, but a slightly higher αis more effective
 in the knowledge conflict setting, where the
prior knowledge needs to be factored out more. We
investigate the effect of αin Section 4.2.
For the baselines, we use regular decoding
following prior work (Longpre et al., 2021 
Kwiatkowski et al., 2019) to use greedy decoding
 for knowledge conflict tasks and top- psampling
 with p 0.9 for summarization tasks (Holtzman
 et al., 2019). For CAD, we use the same
sampling strategies on top of the adjusted output
probability distribution.
Summarization Table 2 reports the results on
CNN-DM and XSUM. We observe that CAD
outperforms the standard decoding algorithm by
a large margin in all eight models across both
datasets. Specifically, when applied to LLAMA30B
 in CNN-DM, CAD leads to 21% increase in
ROUGE-L, 14.3% increase in factKB and 7.8%
increase in BERT-P. This result demonstrates that
CAD could effectively improve the quality and factuality
 of the generated summaries from a diverse
set of language models.CNN-DM XSUM
Model Decoding ROUGE-L factKB BERT-P ROUGE-L factKB BERT-P
OPT13BRegular 22.0 77.8 86.5 16.4 47.2 85.2
CAD 27.4 84.1 90.8 18.2 64.9 87.5
30BRegular 22.2 81.7 87.0 17.4 38.2 86.1
CAD 28.4 87.0 90.2 19.5 45.6 89.3
GPT-Neo3BRegular 24.3 80.5 87.5 17.6 54.0 86.6
CAD 27.7 87.5 90.6 18.1 65.1 89.1
20BRegular 18.7 68.3 85.2 14.9 42.2 85.7
CAD 24.5 77.5 89.4 19.0 63.3 90.6
LLaMA13BRegular 27.1 80.2 89.5 19.0 53.5 87.8
CAD 32.6 90.8 93.0 21.1 73.4 91.7
30BRegular 25.8 76.8 88.5 18.7 47.7 87.1
CAD 31.8 87.8 92.2 22.0 66.4 90.3
FLAN3BRegular 25.5 90.2 91.6 18.8 31.9 88.2
CAD 26.1 93.9 92.1 19.5 35.9 88.8
11BRegular 25.4 90.4 91.4 19.4 29.8 88.3
CAD 27.1 93.1 92.2 20.0 35.0 88
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 .8
Table 2  CAD consistently outperform the regular decoding method in terms of both summary quality metric
(ROUGE-L) and summary factuality (factKB and BERT-P). The best scores for each setting are boldfaced.
FLAN 3B and 11B refer to FLAN-T5 XL and FLAN-T5 XXL respectively.
Model Decoding Memo. NQ NQ-SWAP
OPT13BReg. 32.5 29.2 18.8
30BReg. 28.4 29.4 14.7
GPT.3BReg. 22.5 31.9 19.1
20BReg. 37.1 22.8 16.1
LLAMA13BReg. 23.8 22.3 11.7
30BReg. 25.8 23.8 9.6
FLAN3BReg. 69.2 81.8 71.4
11BReg. 82.0 85.5 73.0
Table 3  CAD outperforms the regular decoding method
(Reg.) in all settings except for FLAN-T5 on NQ.
Note that FLAN-T5 is trained on NQ dataset during
instruction-finetuning.
Knowledge Conflicts Our results for the knowledge
 conflict datasets, NQ-SWAP and MemoTrap,
as well as the original NQ are detailed in Table 3.
CAD is significantly better than the regular decoding
 in all settings, with the exception of a minor decrease
 observed for FLAN-T5 on the non-conflict
NQ dataset.2Despite this, CAD achieves substan2This
 slight decline can be attributed to the fact that this
particular NQ dataset is included in the instruction-finetuning
sets used by FLAN-T5, and hence, the model has been previously
 trained on it.tially better performance on the knowledge conflict
 datasets, e.g., CAD improve GPT-Neo 20B by
54.4% on Memotrap and by 128% on NQ-SWAP.
This substantial improvement suggests that contextaware
 decoding is particularly beneficial for LMs
to adhere to the given context, in scenarios where
the model s prior knowledge contradicts with the
Qualitative analyais We provide qualitative examples
 for XSUM and Memotrap in Table 4. In
XSUM, the regular decoding generates texts that
is not mentioned in the article, whereas CAD produces
 output exclusively based on the information
in the input article. For MemoTrap, the standard
decoding disregards the instruction and generates
the memorized ending, while CAD adheres to the
instruction within the given context and produces
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 
CAD brings consistent improvement to LMs
with different sizes. In Tables 2 and 3, we show
that CAD could be used to enhance a diverse set of
LM families, including OPT, GPT-Neo, LLaMA,
and FLAN-T5. Here we further investigate whether
CAD is effective in improving language models of
different sizes. Specifically, we focus on OPT models
 across a range of sizes  125M, 350M, 1.3B,
2.7B, 6.7B, 13B, 30B. As depicted in Figure 2,
we observe that the performance gain brought byFigure 2  OPT models of varying sizes consistently benefit from CAD. The x-axis indicates the size of language
models and the y-axis is the performance.
Article He passed away peacefully in hospital on Tuesday
 after a short illness. Born in Tourmakeady,
County Mayo, he worked as a teacher before securing
 a part in the premiere of the Brian Friel
play Translations in 1980. Lally became a household
 name in Ireland for his role as Miley Byrne
in the RTE soap opera Glenroe and later starred in
the BBC series Ballykissangel. He also appeared
in the Hollywood movie Alexander and provided
the voice for the Oscar-nominated, animated Irish
film, The Secret of Kells. As a fluent Irish speaker
and advocate of the language, Lally had roles in
several Irish language films ...
Regular Westminister actor Pat Lally died in hospital on
Tuesday night aged 82
CAD Actor Lally, best known for Glenroe and Ballykissangel,
 has died in hospital on Tuesday
Input Write a quote that ends in the word  early . Better
Table 4  Qualitative examples of contrast-aware decoding.
 The nonfactual or inconsistent texts are highlighted
CAD stays consistent with different model sizes in
CNN-DM. In Memotrap and NQSWAP, this gain
increases as the model size grows, indicating that
larger LMs can have a greater tendency to rely on
their prior knowledge instead of reading the contexts,
 thereby benefiting more from CAD.
Effect of adjustment level αContext-aware
decoding introduces a hyperparameter α, which
serves to control the adjustment 
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 level of CAD (a
small αmakes the distribution closer to the original
next token distribution). We conduct experimentswith various values of αand present the results in
Figure 3. Across all three datasets, we find λ 0.5
consistently provide robust improvements over regular
 decoding. Further increasing the value of α
yields additional improvement in tasks involving
Summarization Factuality Summarization models
 have shown a tendency to generate hallucinated
texts (Maynez et al., 2020  Pagnoni et al., 2021).
This has led to growing efforts to improve the factual
 consistency, including applying attentions to
fact triples extracted from source documents (Cao
et al., 2018  Zhu et al., 2021), optimizing summarization
 models towards a factual consistency
metrics (Nan et al., 2021  Cao and Wang, 2021),
learning a post-editing error corrector (Dong et al.,
2020) and removing noisy training samples (Kang
and Hashimoto, 2020  Goyal and Durrett, 2021).
However, all these methods require additional finetuning
 and are not directly suitable for zero-shot
and few-shot prompting scenarios.
Knowledge Conflicts When presented with an
updated document with conflicting knowledge,
we expect language models to generate responses
based on the provided contexts rather than relying
solely on outdated parametric knowledge. This setting
 is especially valuable to retrieval-augmented
language models (Khandelwal et al., 2020  Shi
et al., 2023  Min et al., 2022  Yasunaga et al., 2023),
where documents retrieved from external databases
are used as additional input to provide LMs additional
 knowledge. However, simply adding documents
 does not always change the model predictions,
 as current LMs often overlook the contextsFigure 3  Effect of the adjustment level α. The y-axis is the performance and the x-axis is α.
and rely heavily on their prior parametric knowledge
 (Longpre et al., 2021  Chen et al., 2022). Existing
 approaches for improving model s faithfulness
 to the context, such as the 
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 prompting-based
method (Zhou et al., 2023), are limited in that
they could only apply to large-scale instructionfinetuned
 LMs like OpenAI s text-davinci-003. In
contrast, our work investigates a decoding strategy
to tackle this problem, which is applicable to any
Contrastive Decoding Methods Contrastive decoding
 methods have been extensively explored for
text generation. Coherence boosting (Malkin et al.,
2021) demotes a short context from a full context,
focusing on the longer-range context for coherence
and overall better generation quality. MMI-based
decoding (Li et al., 2015) uses a contrastive formulation
 to improve output diversity in dialog generation.
 In this work, we adopt a same intuition and
focus on analyzing the knowledge conflict scenarios
 where the faithfulness to the context is particularly
 important but difficult for the regular decoding
methods. DExperts (Liu et al., 2021) demotes the
output distribution of an anti-expert (e.g., exposed
to toxic language) to help lead the generations free
from the unwanted attributes. Contrastive decoding
(Li et al., 2022) demotes an amateur model (e.g.,
models with a very small number of parameters)
to help distill the expert knowledge learned in the
larger, more competitive models. In general, contrastive
 decoding has shown to be a general way to
control model outputs, which we reinforce by considering
 the new case of factual consistency with
the textual context.6 Conclusion
Off-the-shelf language models may suffer from an
insufficient attention to the supplied context compared
 to its learned prior knowledge, leading to
an unfaithful generation to the input context. We
present context-aware decoding, a simple inferencetime
 method that downweights an output probability
 associated with the model s prior knowledge to
promote models  attention to the contextual information.
 We experiment on two families of tasks
that require a strong attention to the context, summarization
 and knowledge conflicts tasks. 
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 We show
that CAD provides more reliable and factual outputs
 across different language models of various
Sid Black, Gao Leo, Phil Wang, Connor Leahy,
and Stella Biderman. 2021. GPT-Neo  Large
Scale Autoregressive Language Modeling with MeshTensorflow.
Shuyang Cao and Lu Wang. 2021. Cliff  Contrastive
 learning for improving faithfulness and factuality
 in abstractive summarization. arXiv preprint
Ziqiang Cao, Furu Wei, Wenjie Li, and Sujian Li. 2018.
Faithful to the original  Fact aware neural abstractive
summarization. In Proceedings of the AAAI Conference
 on Artificial Intelligence , volume 32.
Stephanie C. Y . Chan, Adam Santoro, Andrew Kyle
Lampinen, Jane X. Wang, Aaditya K Singh,
Pierre H. Richemond, Jay Mcclelland, and Felix
Hill. 2022. Data distributional properties drive emergent
 in-context learning in transformers. ArXiv ,
Hung-Ting Chen, Michael Zhang, and Eunsol Choi.
2022. Rich knowledge sources bring complex knowl-edge conflicts  Recalibrating models to reflect conflicting
 evidence. In Proceedings of the 2022 Conference
 on Empirical Methods in Natural Language
Processing , pages 2292 2307, Abu Dhabi, United
Arab Emirates. Association for Computational Linguistics.
Hyung Won Chung, Le Hou, Shayne Longpre, Barret
 Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, et al.
2022. Scaling instruction-finetuned language models.
arXiv preprint arXiv 2210.11416 .
Yue Dong, Shuohang Wang, Zhe Gan, Yu Cheng, Jackie
Chi Kit Cheung, and Jingjing Liu. 2020. Multifact
 correction in abstractive text summarization. In
Proceedings of the 2020 Conference on Empirical
Methods in Natural Language Processing (EMNLP) ,
pages 9320 9331, Online. Association for Computational
Shangbin Feng, Vidhisha Balachandran, Yuyang Bai,
and Yulia Tsvetkov. 2023. Factkb  Generalizable factuality
 evaluation using language models enhanced
with factual knowledge.
Tanya Goyal and Greg Durrett. 2021. Annotating and
modeling fine-grained factuality in sum
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 marization.
InProceedings of the 2021 Conference of the North
American Chapter of the Association for Computational
 Linguistics  Human Language Technologies ,
pages 1449 1462, Online. Association for Computational
Jonathan Ho and Tim Salimans. 2022. Classifier-free
diffusion guidance. ArXiv , abs/2207.12598.
Ari Holtzman, Jan Buys, Li Du, Maxwell Forbes, and
Yejin Choi. 2019. The curious case of neural text degeneration.
 In International Conference on Learning
Daniel Kang and Tatsunori B. Hashimoto. 2020. Improved
 natural language generation via loss truncation.
 In Proceedings of the 58th Annual Meeting of
the Association for Computational Linguistics , pages
718 731, Online. Association for Computational Linguistics.
Urvashi Khandelwal, Omer Levy, Dan Jurafsky, Luke
Zettlemoyer, and Mike Lewis. 2020. Generalization
through memorization  Nearest neighbor language
models. In International Conference on Learning
Tom Kwiatkowski, Jennimaria Palomaki, Olivia Redfield,
 Michael Collins, Ankur Parikh, Chris Alberti,
Danielle Epstein, Illia Polosukhin, Jacob Devlin, Kenton
 Lee, et al. 2019. Natural questions  a benchmark
for question answering research. Transactions of the
Association for Computational Linguistics , 7 452 
Jiwei Li, Michel Galley, Chris Brockett, Jianfeng Gao,
and William B. Dolan. 2015. A diversity-promoting
objective function for neural conversation models.
ArXiv , abs/1510.03055.Xiang Lisa Li, Ari Holtzman, Daniel Fried, Percy Liang,
Jason Eisner, Tatsunori Hashimoto, Luke Zettlemoyer,
 and Mike Lewis. 2022. Contrastive decoding 
Open-ended text generation as optimization. ArXiv ,
Chin-Yew Lin. 2004. ROUGE  A package for automatic
 evaluation of summaries. In Text Summarization
 Branches Out , pages 74 81, Barcelona, Spain.
Association for Computational Linguistics.
Alisa Liu and Jiacheng Liu. 2023. The
memotrap dataset. https //github.
com/inverse-scaling/prize/blob/main/
data-release/README.md .
Alisa Liu, Maarten Sap, Ximing Lu, Swabha
Swayamdip
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 ta, Chandra Bhagavatula, Noah A. Smith,
and Yejin Choi. 2021. Dexperts  Decoding-time controlled
 text generation with experts and anti-experts.
InAnnual Meeting of the Association for Computational
Shayne Longpre, Kartik Perisetla, Anthony Chen,
Nikhil Ramesh, Chris DuBois, and Sameer Singh.
2021. Entity-based knowledge conflicts in question
answering. In Proceedings of the 2021 Conference
on Empirical Methods in Natural Language Processing,
 pages 7052 7063, Online and Punta Cana, Dominican
 Republic. Association for Computational
Nikolay Malkin, Zhen Wang, and Nebojsa Jojic. 2021.
Coherence boosting  When your pretrained language
model is not paying enough attention. In Annual
Meeting of the Association for Computational Linguistics
Joshua Maynez, Shashi Narayan, Bernd Bohnet, and
Ryan McDonald. 2020. On faithfulness and factuality
 in abstractive summarization. In Proceedings
of the 58th Annual Meeting of the Association for
Computational Linguistics , pages 1906 1919, Online.
 Association for Computational Linguistics.
Sewon Min, Weijia Shi, Mike Lewis, Xilun Chen, Wen
tau Yih, Hannaneh Hajishirzi, and Luke Zettlemoyer.
2022. Nonparametric masked language modeling.
Feng Nan, Cicero Nogueira dos Santos, Henghui Zhu,
Patrick Ng, Kathleen McKeown, Ramesh Nallapati,
Dejiao Zhang, Zhiguo Wang, Andrew O. Arnold, and
Bing Xiang. 2021. Improving factual consistency
of abstractive summarization via question answering.
InProceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the
11th International Joint Conference on Natural Language
 Processing (Volume 1  Long Papers) , pages
6881 6894, Online. Association for Computational
Shashi Narayan, Shay B. Cohen, and Mirella Lapata.
2018. Don t give me the details, just the summary 
topic-aware convolutional neural networks for extreme
 summarization. In Proceedings of the 2018Conference on Empirical Methods in Natural Language
 Processing , pages 1797 1807, Brussels, Belgium.
 Association fo
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 r Computational Linguistics.
Artidoro Pagnoni, Vidhisha Balachandran, and Yulia
Tsvetkov. 2021. Understanding factuality in abstractive
 summarization with FRANK  A benchmark for
factuality metrics. In Proceedings of the 2021 Conference
 of the North American Chapter of the Association
 for Computational Linguistics  Human Language
 Technologies , pages 4812 4829, Online. Association
 for Computational Linguistics.
Abigail See, Peter J Liu, and Christopher D Manning.
 2017. Get to the point  Summarization
with pointer-generator networks. arXiv preprint
Weijia Shi, Sewon Min, Michihiro Yasunaga, Minjoon
Seo, Rich James, Mike Lewis, Luke Zettlemoyer, and
Wen tau Yih. 2023. Replug  Retrieval-augmented
black-box language models.
Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier
Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro,
Faisal Azhar, et al. 2023. LLaMA  Open and efficient
 foundation language models. arXiv preprint
Michihiro Yasunaga, Armen Aghajanyan, Weijia Shi,
Rich James, Jure Leskovec, Percy Liang, Mike Lewis,
Luke Zettlemoyer, and Wen-tau Yih. 2023. Retrievalaugmented
 multimodal language modeling. In International
 Conference on Machine Learning (ICML) .
Susan Zhang, Stephen Roller, Naman Goyal, Mikel
Artetxe, Moya Chen, Shuohui Chen, Christopher Dewan,
 Mona Diab, Xian Li, Xi Victoria Lin, et al. 2022.
Opt  Open pre-trained transformer language models.
arXiv preprint arXiv 2205.01068 .
Tianyi Zhang, Faisal Ladhak, Esin Durmus, Percy Liang,
Kathleen McKeown, and Tatsunori B. Hashimoto.
2023. Benchmarking large language models for news
Wenxuan Zhou, Sheng Zhang, Hoifung Poon, and
Muhao Chen. 2023. Context-faithful prompting for
large language models. ArXiv , abs/2303.11315.
Chenguang Zhu, William Hinthorn, Ruochen Xu,
Qingkai Zeng, Michael Zeng, Xuedong Huang, and
Meng Jiang. 2021. Enhancing factual consistency
of abstractive summarization. In Proceedings of the
2021 Conference of the North American Chapter of
t
 ## PAPERID
 984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## TITLE
 Trusting Your Evidence: Hallucinate Less with Context-aware Decoding
 ## ABSTRACT
 Language models (LMs) often struggle to pay enough attention to the input context, and generate texts that are unfaithful or contain hallucinations. To mitigate this issue, we present context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context. Our experiments show that CAD, without additional training, significantly improves the faithfulness of different LM families, including OPT, GPT, LLaMA and FLAN-T5 for summarization tasks (e.g., 14.3% gain for LLaMA in factuality metrics). Furthermore, CAD is particularly effective in overriding a model's prior knowledge when it contradicts the provided context, leading to substantial improvements in tasks where resolving the knowledge conflict is essential.
 ## AUTHORNAME
 Yulia Tsvetkov
 ## JOURNAL
 {'volume': 'abs/2305.14739', 'name': 'ArXiv'}
 ## FIELDSOFSTUDY
 ['Computer Science']
 ## URL
 https://www.semanticscholar.org/paper/984d4a1d41bfc8184fb77b8aa0eb8e96d536d048
 ## YEAR
 2023
 ## TLDR
 Context-aware decoding (CAD), which follows a contrastive output distribution that amplifies the difference between the output probabilities when a model is used with and without context, improves the faithfulness of different LM families.
 ## VENUE
 arXiv.org
 he Association for Computational Linguistics  Human
 Language Technologies , pages 718 733, Online.
Association for Computational Linguistics.