
 ## PROFNAME
 Bhiksha Raj
 ## AUTHORID
 1681921
 ## AUTHORNAME
 B. Raj
 ## AUTHORURL
 https://www.semanticscholar.org/author/1681921
 ## AUTHORHINDEX
 54
 ## AUTHORAFFILIATIONS
 []
 ## AUTHORPAPERCOUNT
 404
 ## AUTHORCITATIONCOUNT
 14485
 ## PAPERID
 feecd2cfb7871a818ba514e8b4b3f9da482f17bc
 ## EXTERNALIDS
 {'DBLP': 'journals/corr/abs-2302-09719', 'ArXiv': '2302.09719', 'DOI': '10.48550/arXiv.2302.09719', 'CorpusId': 257039066}
 ## URL
 https://www.semanticscholar.org/paper/feecd2cfb7871a818ba514e8b4b3f9da482f17bc
 ## TITLE
 Synergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session
 ## ABSTRACT
 Machine Listening, as usually formalized, attempts to perform a task that is, from our perspective, fundamentally human-performable, and performed by humans. Current automated models of Machine Listening vary from purely data-driven approaches to approaches imitating human systems. In recent years, the most promising approaches have been hybrid in that they have used data-driven approaches informed by models of the perceptual, cognitive, and semantic processes of the human system. Not only does the guidance provided by models of human perception and domain knowledge enable better, and more generalizable Machine Listening, in the converse, the lessons learned from these models may be used to verify or improve our models of human perception themselves. This paper summarizes advances in the development of such hybrid approaches, ranging from Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds. The research described herein was presented in a special session on"Synergy between human and machine approaches to sound/scene recognition and processing"at the 2023 ICASSP meeting.
 ## VENUE
 arXiv.org
 ## YEAR
 2023
 ## REFERENCECOUNT
 15
 ## CITATIONCOUNT
 8
 ## INFLUENTIALCITATIONCOUNT
 0
 ## ISOPENACCESS
 True
 ## OPENACCESSPDF
 {'url': 'http://arxiv.org/pdf/2302.09719', 'status': None}
 ## FIELDSOFSTUDY
 ['Computer Science', 'Engineering']
 ## JOURNAL
 {'volume': 'abs/2302.09719', 'name': 'ArXiv'}
 ## AUTHORS
 [{'authorId': '38655601', 'name': 'L. Heller'}, {'authorId': '2532460', 'name': 'Benjamin Elizalde'}, {'authorId': '1681921', 'name': 'B. Raj'}, {'authorId': '67345939', 'name': 'Soham Deshmukh'}]
 ## TLDR
 Advances in the development of hybrid approaches to Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds are summarized.
 arXiv 2302.09719v2  [eess.AS]  24 Feb 2023SYNERGY BETWEEN HUMAN AND MACHINE APPROACHES TO SOUND/SCEN E
RECOGNITION AND PROCESSING  AN OVERVIEW OF ICASSP SPECIAL S ESSION
Laurie M. Heller1 , Benjamin Elizalde2 , Bhiksha Raj3,4 , Soham Deshmukh2
 Special session co-organizers
1Department of Psychology, Carnegie Mellon University
3Languaged Technologies Institute, Carnegie Mellon Univer sity
4Mohammed bin Zayed University of AI
laurieheller@cmu.edu, bhikshar@andrew.cmu.edu, {benjaminm,sdeshmukh }@microsoft.com
Machine Listening, as usually formalized, attempts to perform
 a task that is, from our perspective, fundamentally
human-performable, and performed by humans. Current automated
 models of Machine Listening vary from purely datadriven
 approaches to approaches imitating human systems. I n
recent years, the most promising approaches have been hybri d
in that they have used data-driven approaches informed by
models of the perceptual, cognitive, and semantic processe s
of the human system. Not only does the guidance provided
by models of human perception and domain knowledge enable
 better, and more generalizable Machine Listening, in
the converse, the lessons learned from these models may be
used to verify or improve our models of human perception
themselves. This paper summarizes advances in the development
 of such hybrid approaches, ranging from Machine
Listening models that are informed by models of peripheral
(human) auditory processes, to those that employ or derive
semantic information encoded in relations between sounds.
The research described herein was presented in a special ses sion
 on  Synergy between human and machine approaches to
sound/scene recognition and processing  (ICASSP 2023).
Human auditory knowledge has been used to improve many
areas of machine listening and signal processing (Virtanen
et al [1], Lyon [2], Blauert [3], Bowen [4]), such as  sound
event classiﬁcation, sound synthesis, speech recognition , and
binaural and spatial sound processing. These synergies hav e
impacts in human-oriented applications such as alerting pe ople
 to important sounds, perceptual and cognitive assessme nt,
hearing aids, and spatializing sounds in virtual reality. W hile
the objective of machine listening system is to eventually
achieve superhuman performance, the benchmark remains
human performance. Human performance is the emergent
outcome of many biological and cognitive processes, from th e
early-stage processing of the auditory signal, to the late- stagesemantic processes that drive our interpretations. Automa ted
models of machine listening system may take one of three
  A purely data-driven approach that has no explicit reference
 to the intermediate perceptual and cognitive processes
 employed by the human 
  An imitation approach that attempts to model and
mimic the human process in detail  or
  A hybrid approach that uses models of the perceptual,
cognitive, and semantic processes of the human system
to inform a data-driven approach.
Of these, the hybrid approach remains the most promising,
 particularly when combined with the power of recent
deep-learning models (cf. Turian et al [5], Elizalde et al [6 ].,
Pranay et al [7], Anderson et al [8], Tashev et al [9], Ananthabhotla
 [10], Wang et al [11], Zeghidour et al [12]). We
deﬁne hybrid approaches as approaches that learn from datadriven
 methods and use knowledge about human perception
and cognition to constrain the optimization problem and mak e
end-to-end learning easier. These methods are usually fast er
to converge and provide better metrics on the tasks they are
solving. This special session on  Synergy between human
and machine approaches to sound/scene recognition and processing 
 brought together researchers who have worked on a
variety of aspects of this hybrid approach, ranging from Machine
 Listening models that are informed by, or inform models
 of peripheral (human) auditory processes, to those that employ
 or derive semantic information encoded in relations be tween,
 and response to sounds. By bringing this group of researchers
 together, we identiﬁed synergies between the dat adriven
 and perception/cognition-based approaches that co ntribute
 signiﬁcantly to advances in both Machine Listening
and our knowledge of human auditory perception and cognition.2.
 HYBRID DATA-DRIVEN APPROACHES FOR
The hybrid data-driven approaches explored in the special
session can be grouped into two types  (1) models that enhance
 our capabilities through the use of semantics and perceptual
 analysis, (2) studies that enhance our understandi ng
of the potential and pitfalls of using data-driven models to assess
2.1. Usage of semantics and perceptual analysis for sound
understanding, discrimination, and synthesis
Cognitive neuroscience research has shown that humans exploit
 higher-level semantic information about sound sourc es
to understand sound events and infer their context. The approaches
 learn the semantic information from modality complementary
 to audio, for example, textual descriptions [13 ,
14] or from available ontology [15]. In this section, we summarize
 the ﬁndings of research that probes the semantics of
Esposito et al., in their work entitled  Semanticallyinformed
 deep neural networks for sound recognition  propose
 SemDNN, a neural network architecture that learns
semantic relations from text embeddings (word2vec) while
learning sound recognition. They show that SemDNN embeddings
 approximate human dissimilarity ratings of natur al
sounds better than those of a traditional (one-hot encoding)
 sound categorization network (CatDNN). First, they us e
two evaluation metrics  ranking score and average maximum
 cosine similarity score (AMCSS). This evaluation was
performed for 1 internal dataset and 4 public sound event
classiﬁcation datasets. Second, they compare all the netwo rk
architectures with human behavioral data using representational
 similarity analysis (RSA). Overall, they conclud e
that training with continuous semantic embeddings provide s
more accurate semantic labelling of sounds and they suggest
extending this approach to use different aspects of sound
Ontologies deﬁne concepts and the relation between concepts
 in a structured form which has semantic meaning. In
 An approach to ontological learning from weak labels, 
Shah et al. explore using ontological information for learn ing
sound event classiﬁers. The authors use a graph convolutional
 neural network (GCN) with an ontological layer for
learning the hierarchical structure between sound events. The
authors conclude that although GCN as part of the ontology
layer captures the ontology knowledge, the model does not
perform better by incorporating this ontology information in
the weak and multi-labeled sound event classiﬁcation task.
Thoidis et al., in  Perceptual analysis of speaker embeddings
 for voice discrimination between machine and human
listening,  investigate the relationship between machine listening
 models an
 ## PROFNAME
 Bhiksha Raj
 ## AUTHORID
 1681921
 ## AUTHORNAME
 B. Raj
 ## AUTHORURL
 https://www.semanticscholar.org/author/1681921
 ## AUTHORHINDEX
 54
 ## AUTHORAFFILIATIONS
 []
 ## AUTHORPAPERCOUNT
 404
 ## AUTHORCITATIONCOUNT
 14485
 ## PAPERID
 feecd2cfb7871a818ba514e8b4b3f9da482f17bc
 ## EXTERNALIDS
 {'DBLP': 'journals/corr/abs-2302-09719', 'ArXiv': '2302.09719', 'DOI': '10.48550/arXiv.2302.09719', 'CorpusId': 257039066}
 ## URL
 https://www.semanticscholar.org/paper/feecd2cfb7871a818ba514e8b4b3f9da482f17bc
 ## TITLE
 Synergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session
 ## ABSTRACT
 Machine Listening, as usually formalized, attempts to perform a task that is, from our perspective, fundamentally human-performable, and performed by humans. Current automated models of Machine Listening vary from purely data-driven approaches to approaches imitating human systems. In recent years, the most promising approaches have been hybrid in that they have used data-driven approaches informed by models of the perceptual, cognitive, and semantic processes of the human system. Not only does the guidance provided by models of human perception and domain knowledge enable better, and more generalizable Machine Listening, in the converse, the lessons learned from these models may be used to verify or improve our models of human perception themselves. This paper summarizes advances in the development of such hybrid approaches, ranging from Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds. The research described herein was presented in a special session on"Synergy between human and machine approaches to sound/scene recognition and processing"at the 2023 ICASSP meeting.
 ## VENUE
 arXiv.org
 ## YEAR
 2023
 ## REFERENCECOUNT
 15
 ## CITATIONCOUNT
 8
 ## INFLUENTIALCITATIONCOUNT
 0
 ## ISOPENACCESS
 True
 ## OPENACCESSPDF
 {'url': 'http://arxiv.org/pdf/2302.09719', 'status': None}
 ## FIELDSOFSTUDY
 ['Computer Science', 'Engineering']
 ## JOURNAL
 {'volume': 'abs/2302.09719', 'name': 'ArXiv'}
 ## AUTHORS
 [{'authorId': '38655601', 'name': 'L. Heller'}, {'authorId': '2532460', 'name': 'Benjamin Elizalde'}, {'authorId': '1681921', 'name': 'B. Raj'}, {'authorId': '67345939', 'name': 'Soham Deshmukh'}]
 ## TLDR
 Advances in the development of hybrid approaches to Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds are summarized.
 d human listeners in a speaker veriﬁcationtask. A Convolutional Neural Network (CNN) is trained to
conduct one-shot speaker veriﬁcation. The CNN is trained
using a joint loss function which incorporates the cosine
distance between the latent features and their correspondi ng
class centers in the penalty of the loss function. The proposed
 loss function improves one-shot speaker veriﬁcation
performance and makes the network more robust to noise
over state-of-the-art approaches. The authors also conduc t
tests which conclude that a substantial overlap exists betw een
machine and human listening in a voice discrimination task.
The aim of sound-matching algorithms is to ﬁnd a set
of sound synthesis parameters that minimize the perceptual
distance between a synthesized sound and its target audio.
The current literature uses multiple loss functions rangin g
from mean square error (known as P-loss) to mean square error
 in the spectro-temporal domain (known as spectral loss) .
Han et al., in  Perceptual neural physical sound matching, 
devise a Perceptual-Neural-Physical loss (PNP), which is a
perceptually-motivated metric that is an approximation of
spectral loss which maintains the same training time as spec tral
 loss. Their perceptual similarity metric is an idealiz ed
model of spectro-temporal responses in the primary auditor y
cortex and reﬂects human judgements. Their PNP loss guides
the synthesis of drum sounds using wavelets.
2.2. Potential and pitfalls of using data-driven models to
assess human listening
Khalil et al., in  Using machine learning to understand the
relationships between audiometric data, speech perceptio n,
temporal processing, and cognition,  develop a data-drive n
analysis of perceptual and physiological factors affectin g human
 speech comprehension. The approach uses an ensemble
of machine learning models to ﬁnd the best-performing models
 that predict the outcomes of three different speech perception
 tests. The models take 147 features derived from audiometric
 measurements as inputs. The features also contai n
new composite variables that represent properties of the en tire
 hearing range. The researchers reported the explanato ry
power of the different features on the listeners  performan ce
in the three tasks. The prediction models suggest that the mi dfrequency
 range from 1 to 4 kHz is crucial for speech perception
 since the corresponding features explain most of th e
variance in the data. In contrast, cognition-related featu res
contribute little to the predictions. Hence the machine lea rning
 results serve to remind researchers to sufﬁciently acco unt
for mid-frequency hearing loss when investigating extende d
high-frequency threshold and cognitive effects on speech p erception.
In  Classifying non-individual head-related transfer fun ctions
 with a computational auditory model  calibration and
metrics,  Daugintis et al. use a multi-feature Bayesian sph erical
 auditory sound localization model to assess the goodne ss
of non-individual head-related transfer functions (HRTFs ) fora human listener. Their template comparison-based model re turns
 a directional probability distribution that is combi ned
with a prior belief. The model is calibrated to individuals,
based on their sound localization performance. This paper
provides a theoretical framework for a model-based metric
that accounts both for acoustic and psychoacoustic similar ities
 in HRTFs. Once perceptually validated, this method
could be used as a metric in combination with other methods
to enable consistent selection of a well-matched high-qual ity
HRTF. The ultimate goal is to improve the experience of binaural
 spatial audio technologies.
In this paper, we presented two classes of hybrid approaches .
The ﬁrst approach consisted of four models that enhance our
understanding through the use of semantics and perceptual
analysis. Two different studies trained deep neural networ ks
with semantic information in the hopes of predicting human
 data  while the SemDNN showed beneﬁts of using text
embeddings for predicting dissimilarity ratings, the onto logical
 learning GCN did not beneﬁt from incorporating sound
ontologies for weak multi-labeled sound events. Furthermore,
 two studies used perceptual analysis to help compare
sounds  while speaker embeddings in a CNN improved oneshot
 speaker veriﬁcation and agreed with human judgements
of voice similarity, a sound-matching algorithm improved a
synthesizer of percussive sounds by comparing the target an d
synthesized sound in terms of a perceptual similarity metric.
 These four studies demonstrated the potential beneﬁts of
using domain knowledge generated by humans to improve
The second hybrid approach consisted of two studies that
enhance our understanding of the potential and pitfalls of u sing
 data-driven models to assess human listening. First, ma chine
 learning models were moderately predictive of the out comes
 of speech perception tests and clearly identiﬁed the
most important variables among those analyzed. Second, a
computational model of human sound localization was applied
 to HRTF evaluation to provide a perceptual foundation
for automated HRTF personalization techniques.
In conclusion, we demonstrated beneﬁts of a hybrid datadriven
 approach to machine listening that is informed by mod els
 of the perceptual, cognitive, and semantic processes of
the human system. Challenges encountered by some of these
studies, for example in automatic assessments of human listening,
 may lead to future improvements. Taken as a whole,
these studies demonstrated the potential beneﬁts of using a uditory
 models and domain knowledge generated by humans
to improve Machine Listening.5. REFERENCES
[1] Tuomas Virtanen, Mark D Plumbley, and Dan Ellis,
Computational analysis of sound scenes and events ,
[2] Richard F Lyon, Human and machine hearing  extracting
 meaning from sound , Cambridge University Press,
[3] Jens Blauert, Spatial hearing  the psychophysics of human
 sound localization , MIT press, 1997.
[4] Bowen Zhi, Dmitry N. Zotkin, and Ramani Duraiswami,
 Towards fast and convenient end-to-end hrtf personalization, 
 in ICASSP 2022 - 2022 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) , 2022, pp. 441 445.
[5] Joseph Turian, Jordie Shier, Humair Raj Khan, Bhiksha
 Raj, Bj  orn W. Schuller, Christian J. Steinmetz,
Colin Malloy, George Tzanetakis, Gissel Velarde, Kirk
McNally, Max Henry, Nicolas Pinto, Camille Nouﬁ,
Christian Clough, Dorien Herremans, Eduardo Fonseca,
Jesse Engel, Justin Salamon, Philippe Esling, Pranay
Manocha, Shinji Watanabe, Zeyu Jin, and Yonatan Bisk,
 HEAR  Holistic Evaluation of Audio Representations, 
inProceedings of the NeurIPS 2021 Competitions and
Demonstrations Track , Douwe Kiela, Marco Ciccone,
and Barbara Caputo, Eds. 06 14 Dec 2022, vol. 176 of
Proceedings of Machine Learning Research , pp. 125 
[6] Benjamin Elizalde, Radu Revutchi, Samarjit Das, Bhiksha
 Raj, Ian Lane
 ## PROFNAME
 Bhiksha Raj
 ## AUTHORID
 1681921
 ## AUTHORNAME
 B. Raj
 ## AUTHORURL
 https://www.semanticscholar.org/author/1681921
 ## AUTHORHINDEX
 54
 ## AUTHORAFFILIATIONS
 []
 ## AUTHORPAPERCOUNT
 404
 ## AUTHORCITATIONCOUNT
 14485
 ## PAPERID
 feecd2cfb7871a818ba514e8b4b3f9da482f17bc
 ## EXTERNALIDS
 {'DBLP': 'journals/corr/abs-2302-09719', 'ArXiv': '2302.09719', 'DOI': '10.48550/arXiv.2302.09719', 'CorpusId': 257039066}
 ## URL
 https://www.semanticscholar.org/paper/feecd2cfb7871a818ba514e8b4b3f9da482f17bc
 ## TITLE
 Synergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session
 ## ABSTRACT
 Machine Listening, as usually formalized, attempts to perform a task that is, from our perspective, fundamentally human-performable, and performed by humans. Current automated models of Machine Listening vary from purely data-driven approaches to approaches imitating human systems. In recent years, the most promising approaches have been hybrid in that they have used data-driven approaches informed by models of the perceptual, cognitive, and semantic processes of the human system. Not only does the guidance provided by models of human perception and domain knowledge enable better, and more generalizable Machine Listening, in the converse, the lessons learned from these models may be used to verify or improve our models of human perception themselves. This paper summarizes advances in the development of such hybrid approaches, ranging from Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds. The research described herein was presented in a special session on"Synergy between human and machine approaches to sound/scene recognition and processing"at the 2023 ICASSP meeting.
 ## VENUE
 arXiv.org
 ## YEAR
 2023
 ## REFERENCECOUNT
 15
 ## CITATIONCOUNT
 8
 ## INFLUENTIALCITATIONCOUNT
 0
 ## ISOPENACCESS
 True
 ## OPENACCESSPDF
 {'url': 'http://arxiv.org/pdf/2302.09719', 'status': None}
 ## FIELDSOFSTUDY
 ['Computer Science', 'Engineering']
 ## JOURNAL
 {'volume': 'abs/2302.09719', 'name': 'ArXiv'}
 ## AUTHORS
 [{'authorId': '38655601', 'name': 'L. Heller'}, {'authorId': '2532460', 'name': 'Benjamin Elizalde'}, {'authorId': '1681921', 'name': 'B. Raj'}, {'authorId': '67345939', 'name': 'Soham Deshmukh'}]
 ## TLDR
 Advances in the development of hybrid approaches to Machine Listening models that are informed by models of peripheral (human) auditory processes, to those that employ or derive semantic information encoded in relations between sounds are summarized.
 , and Laurie M. Heller,  Identifying
actions for sound event classiﬁcation,  in 2021 IEEE
Workshop on Applications of Signal Processing to Audio
 and Acoustics (WASPAA) , 2021, pp. 26 30.
[7] Pranay Manocha, Adam Finkelstein, Richard Zhang,
Nicholas J. Bryan, Gautham J. Mysore, and Zeyu Jin,
 A Differentiable Perceptual Audio Metric Learned
from Just Noticeable Differences,  in Proc. Interspeech
2020 , 2020, pp. 2852 2856.
[8] Anderson R. Avila, Hannes Gamper, Chandan Reddy,
Ross Cutler, Ivan Tashev, and Johannes Gehrke,  Nonintrusive
 speech quality assessment using neural networks, 
 in ICASSP 2019 - 2019 IEEE International
Conference on Acoustics, Speech and Signal Processing
(ICASSP) , 2019, pp. 631 635.
[9] Jinkyu Lee and Ivan Tashev,  High-level feature representation
 using recurrent neural network for speech
emotion recognition,  in Interspeech 2015 , 2015.[10] Ishwarya Ananthabhotla, Cognitive Audio  Enabling
Auditory Interfaces with an Understanding of How We
Hear , Ph.D. thesis, Massachusetts Institute of Technology,
[11] Shuai Wang, Yanmin Qian, and Kai Yu,  What does the
speaker embedding encode ,  in Interspeech , 2017, pp.
[12] Neil Zeghidour, Olivier Teboul, F  elix de Chaumont
 Quitry, and Marco Tagliasacchi,  Leaf  A learnable
 frontend for audio classiﬁcation,  ICLR , 2021.
[13] Benjamin Elizalde, Soham Deshmukh, Mahmoud Al Ismail,
 and Huaming Wang,  Clap  Learning audio concepts
 from natural language supervision,  arXiv preprint
arXiv 2206.04769 , 2022.
[14] Soham Deshmukh, Benjamin Elizalde, and Huaming
Wang,  Audio retrieval with wavtext5k and clap training, 
 arXiv preprint arXiv 2209.14275 , 2022.
[15] Abelino Jimenez, Benjamin Elizalde, and Bhiksha Raj,
 Sound event classiﬁcation using ontology-based neural
networks,  in Proceedings of the Annual Conference on
Neural Information Processing Systems , 2018, vol. 9.