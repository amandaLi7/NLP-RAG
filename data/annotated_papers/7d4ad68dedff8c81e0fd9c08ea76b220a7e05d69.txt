
 ## PROFNAME
 Shinji Watanabe
 ## AUTHORID
 1746678
 ## AUTHORNAME
 Shinji Watanabe
 ## AUTHORURL
 https://www.semanticscholar.org/author/1746678
 ## AUTHORHINDEX
 67
 ## AUTHORAFFILIATIONS
 []
 ## AUTHORPAPERCOUNT
 597
 ## AUTHORCITATIONCOUNT
 22222
 ## PAPERID
 7d4ad68dedff8c81e0fd9c08ea76b220a7e05d69
 ## EXTERNALIDS
 {'ArXiv': '2302.06774', 'DBLP': 'conf/icassp/WuCCWGBA23', 'DOI': '10.1109/ICASSP49357.2023.10096796', 'CorpusId': 256846751}
 ## URL
 https://www.semanticscholar.org/paper/7d4ad68dedff8c81e0fd9c08ea76b220a7e05d69
 ## TITLE
 Speaker-Independent Acoustic-to-Articulatory Speech Inversion
 ## ABSTRACT
 To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.
 ## VENUE
 IEEE International Conference on Acoustics, Speech, and Signal Processing
 ## YEAR
 2023
 ## REFERENCECOUNT
 40
 ## CITATIONCOUNT
 10
 ## INFLUENTIALCITATIONCOUNT
 1
 ## ISOPENACCESS
 True
 ## OPENACCESSPDF
 {'url': 'https://arxiv.org/pdf/2302.06774', 'status': None}
 ## FIELDSOFSTUDY
 ['Computer Science', 'Engineering']
 ## JOURNAL
 {'pages': '1-5', 'name': 'ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)'}
 ## AUTHORS
 [{'authorId': '2111194238', 'name': 'Peter Wu'}, {'authorId': '2119257114', 'name': 'Li-Wei Chen'}, {'authorId': '2064541737', 'name': 'Cheol Jun Cho'}, {'authorId': '1746678', 'name': 'Shinji Watanabe'}, {'authorId': '145965043', 'name': 'L. Goldstein'}, {'authorId': '1690706', 'name': 'A. Black'}, {'authorId': '1692246', 'name': 'G. Anumanchipalli'}]
 ## TLDR
 This work builds an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers, and proposes a resynthesis-based AAI evaluation metric that does not rely on articulatory labels.
 SPEAKER-INDEPENDENT ACOUSTIC-TO-ARTICULATORY SPEECH INVERSION
Peter Wu1,Li-Wei Chen2,Cheol Jun Cho1,
Shinji Watanabe2,Louis Goldstein3,Alan W Black2,Gopala K. Anumanchipalli1,
1University of California, Berkeley,2Carnegie Mellon University,3University of Southern California
To build speech processing methods that can handle
speech as naturally as humans, researchers have explored
multiple ways of building an invertible mapping from speech
to an interpretable space. The articulatory space is a promising
 inversion target, since this space captures the mechanics
of speech production. To this end, we build an acousticto-articulatory
 inversion (AAI) model that leverages selfsupervision
 to generalize to unseen speakers. Our approach
obtains 0.784 correlation on an electromagnetic articulography
 (EMA) dataset, improving the state-of-the-art by 12.5%.
Additionally, we show the interpretability of these representations
 through directly comparing the behavior of estimated
representations with speech production behavior. Finally, we
propose a resynthesis-based AAI evaluation metric that does
not rely on articulatory labels, demonstrating its efficacy with
an 18-speaker dataset.
Index Terms  articulatory inversion, articulatory speech
Interpretable representations of speech waveforms are valuable
 for tasks like diagnosing voice disorders [1] and building
generalizable, controllable speech synthesizers [2, 3, 4]. Articulatory
 speech processing is a promising direction for making
 speech representations interpretable, since methods in this
direction aim to directly model information about vocal tract
physiology [5, 6, 7]. Specifically, articulatory representations
directly describe the movement of articulators, e.g., the jaw,
Currently, building speaker-independent speech processing
 models with articulatory data remains challenging, since
such data is limited. Most articulatory datasets [8, 9, 10, 11]
contain only a few hours of high-quality speech, much less
than popular non-articulatory ones [12]. This is mainly since
the equipment and processes used to acquire articulatory labels
 are expensive [13]. A promising, cost-effective alternative
 to manually collecting articulatory data is acoustic-toarticulatory
 inversion (AAI), which aims to automatically estimate
 articulatory features directly from speech signals [14].
In recent years, deep learning algorithms have become
popular state-of-the-art methods for AAI [15, 16, 17, 18, 19,
Fig. 1 . EMA features (points) and tract variables (segments).
Diagram extends [23]. Details in Sections 2.1 and 3.1.
20, 21, 22]. Since these methods are data-driven and depend
on the limited amount of articulatory data, they are unable to
sufficiently generalize to unseen speakers to our knowledge.
In this work, we aim to help bridge this gap through improving
 the AAI deep learning framework. Specifically, we examine
 different architecture configurations, features and training
objectives to showcase a suitable framework that improves
the state-of-the-art by 12.5% using autoregression, adversarial
 training, and self supervision. We also evaluate predicted
features through the articulatory phonology lens, comparing
phoneme-level articulatory trajectories with expected human
speech production behavior. Analyzing trajectories this way
allows us to perform evaluation on any speaker, not just those
who have recorded articulatory labels. Our analysis of estimated
 articulator movements also highlights the interpretability
 of our speech representations. Finally, we also evaluate
inversion on speakers without articulatory labels using resynthesis
 and show that our inversion approach is also effective
here. Code and additional related information are available at
https //github.com/articulatory/articulatory.
2.1. MOCHA-TIMIT EMA Dataset
We train our AAI models on an 8-speaker dataset using
MOCHA-TIMIT [8, 9] and MNGU0 [10], containing 5.2
hours of 16 kHz speech. Data is collected with the speakerarXiv 2302.06774v2  [eess.AS]  24 Jul 2023Fig. 2 . Estimating the palate location with a convex hull on
tongue data for HPRC speaker F04. Details in Section 3.1.
instrumented in an electromagnetic articulography (EMA)
machine providing 200 Hz samples of EMA features. These
12-dimensional features contain the midsagittal x and y coordinates
 of jaw, lip, and tongue positions. Specifically, the 6
x-y positions correspond to the lower incisor, upper lip, lower
lip, tongue tip, tongue body, and tongue dorsum, as visualized
in Figure 1. To study how well our AAI approach generalizes
 to unseen speakers, we arbitrarily choose MSAK as the
unseen speaker, evaluating on all MSAK data and training on
the other 7 speakers. Within the 7-speaker training set, we
randomly set off 10% of the utterances for the validation set.
2.2. HPRC EMA Dataset
We also follow previous works [13, 24, 25] to evaluate on the
Haskins Production Rate Comparison (HPRC) [11] database.
HPRC is an 8-speaker dataset containing 7.9 hours of 44.1
kHz speech and 100 Hz EMA. We downsampled the audio
to 16kHz before further processing. As done in previous
work [13], we only consider information along the midsagittal
 plane and do not use the provided mouth left and jaw
left data, matching our MOCHA-TIMIT EMA feature set
described in Section 2.1.
For our AAI train-val-test split, we hold out 1 male and
1 female speaker for our test set and train on the remaining
6 speakers, as done in [15]. While [15] also used these two
speakers in their validation set, we put all of the data from
both speakers in the test set in order to fully experiment within
the unseen speaker setting. We note that our formulation increases
 the difficulty of the task compared to [15] since hyperparameter
 tuning would not lead to overfitting on the test
speakers. However, our AAI method still obtains state-of-theart
 Pearson correlation scores with this dataset, as detailed in
Section 5. For our train-val split, we use the same 90%-10%
approach as the MOCHA-TIMIT split in Section 2.1 above.
2.3. Combined EMA Dataset
To train more generalizable models using a larger articulatory
 dataset, we combine MOCHA-TIMIT [8, 9, 10] and
HPRC [11], forming a 16-speaker EMA dataset. Here, we
create a train-val-test split with 1000 val and 1000 test utterances,
 randomly assigning each utterance to one of the three
sets. We note that all speakers appear in all three sets. Ourresynthesis experiments use models trained on this combined
dataset, discussed in Section 5.3.
2.4. Unseen Speaker Test Data without EMA Labels
We further evaluate our inversion approach using the CMU
ARCTIC database [26, 27], an 18-speaker dataset composed
of 14.3 hours of 16 kHz speech. In addition to the aforementioned
 held-out-speaker experiments, evaluations with
this dataset also test our approach on unseen speakers. Moreover,
 ARCTIC contains various accents that are not present
in our training set, testing the generalizability of our model
to unseen accents. Sin
 ## PROFNAME
 Shinji Watanabe
 ## AUTHORID
 1746678
 ## AUTHORNAME
 Shinji Watanabe
 ## AUTHORURL
 https://www.semanticscholar.org/author/1746678
 ## AUTHORHINDEX
 67
 ## AUTHORAFFILIATIONS
 []
 ## AUTHORPAPERCOUNT
 597
 ## AUTHORCITATIONCOUNT
 22222
 ## PAPERID
 7d4ad68dedff8c81e0fd9c08ea76b220a7e05d69
 ## EXTERNALIDS
 {'ArXiv': '2302.06774', 'DBLP': 'conf/icassp/WuCCWGBA23', 'DOI': '10.1109/ICASSP49357.2023.10096796', 'CorpusId': 256846751}
 ## URL
 https://www.semanticscholar.org/paper/7d4ad68dedff8c81e0fd9c08ea76b220a7e05d69
 ## TITLE
 Speaker-Independent Acoustic-to-Articulatory Speech Inversion
 ## ABSTRACT
 To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.
 ## VENUE
 IEEE International Conference on Acoustics, Speech, and Signal Processing
 ## YEAR
 2023
 ## REFERENCECOUNT
 40
 ## CITATIONCOUNT
 10
 ## INFLUENTIALCITATIONCOUNT
 1
 ## ISOPENACCESS
 True
 ## OPENACCESSPDF
 {'url': 'https://arxiv.org/pdf/2302.06774', 'status': None}
 ## FIELDSOFSTUDY
 ['Computer Science', 'Engineering']
 ## JOURNAL
 {'pages': '1-5', 'name': 'ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)'}
 ## AUTHORS
 [{'authorId': '2111194238', 'name': 'Peter Wu'}, {'authorId': '2119257114', 'name': 'Li-Wei Chen'}, {'authorId': '2064541737', 'name': 'Cheol Jun Cho'}, {'authorId': '1746678', 'name': 'Shinji Watanabe'}, {'authorId': '145965043', 'name': 'L. Goldstein'}, {'authorId': '1690706', 'name': 'A. Black'}, {'authorId': '1692246', 'name': 'G. Anumanchipalli'}]
 ## TLDR
 This work builds an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers, and proposes a resynthesis-based AAI evaluation metric that does not rely on articulatory labels.
 ce ARCTIC does not contain ground
truth EMA labels, we evaluate our AAI model qualitatively
by visualizing the predicted normalized lip aperture (LA)
for individual vowels, detailed in Section 5.2. We conduct
further AAI evaluations with this dataset quantitatively using
a resynthesis approach discussed in Section 5.3.
V ocal tract positions in raw EMA are fairly speaker-dependent,
which can be unsuitable for objective multi-speaker AAI
evaluation [13]. Thus, we follow [13] to derive tract variables
(TVs) that are more speaker-independent. As visualized in
Figure 1, we adopted 9 TVs  lip aperture (LA), lip protrusion
(LP), tongue body constriction location (TBCL), tongue body
constriction degree (TBCD), tongue tip constriction location
(TTCL), and tongue tip constriction degree (TTCD).
Since one of our articulatory datasets does not contain
tongue palate data, unlike [13], we estimate the palate location.
 Specifically, we estimate the palate with a convex hull
fitted on the tongue position data [28]. E.g., Figure 2 plots the
estimated palate locations for the MOCHA-TIMIT speaker
MAPS. Then, we calculate the distance between the 3 EMA
tongue positions (tip, body, dorsum) and the estimated palate
as in [13]. All 9 TVs are calculated element-wise across time
and thus have the same sampling rate as raw EMA features.
Like [13], we scale each speaker s TVs to between -1 and 1.
3.2. Phonemic Features
Since EMA data is limited, performing multi-task learning
(MTL) with additional features can reduce overfitting [29,
29]. Phonemic features implicitly contain articulatory information
 and properties not in EMA, e.g., voicing, and thus
could improve AAI performance through MTL. [13] showed
that adding a task of classifying aligned IPA-based phoneme
labels provided in the dataset improves inversion quality. We
extend this MTL approach to the fully unseen speaker setting
with our baseline in this work.
Since AAI is typically formulated as a regression task,
we also explore phonemic features that can be estimated inthis manner. Specifically, we map each phoneme to an 18dimensional
 vector, where each dimension corresponds to a
different (place, manner) pair and is a binary value denoting
the presence of the respective phoneme type, as in [4]. For
all phonemic features, we use the same sampling rate as the
EMA data in the respective datasets.
3.3. Self-supervised Learning Features
Recent works [30] explored using pretrained self-supervised
learning (SSL) speech features in speech processing tasks.
These features, trained on large-scale unsupervised speech
corpora, have been shown to capture linguistic information
 [31]. The use of SSL features in articulatory tasks is
still relatively unexplored. To help bridge this, we use the
final layer of HuBERT [31, 30], a recent SSL model that has
achieved state-of-the-art speech recognition performance.1
3.4. Speaker Embeddings
We note that the aforementioned features lack lots of speaker
information, since the EMA-based features are normalized
and the phoneme-based ones are effectively non-acoustic.
Moreover, EMA- and phoneme-based features inherently
lack speaker attributes like pitch [28]. Thus, to help our synthesis
 model generate speech more like the target speaker, we
add an ECAPA-TDNN speaker embedding [32] to the input.
4.1. Articulatory Inversion Model
For our baseline AAI model, we use the bidirectional-GRUbased
 architecture in [15]. Specifically, this model is comprised
 of a two-layer bidirectional GRU [33] with size-256
hidden states followed by a two-layer multi-layer perceptron
(MLP) [34] with 128 hidden units. A probability-0.3 dropout
layer succeeds each layer, and a batch norm layer precedes the
MLP output layer. The model outputs 50 dimensions, 9 for
TVs and 41 for classifying phonemes, with Tanh applied to
the TV dimensions. We optimize our baseline using the mean
absolute error (MAE) loss on the TVs added to 0.5times the
cross-entropy loss on the phonemes, as in [15]. We use the
same normalized MFCCs as [15] as inputs to our baseline.
For our proposed inversion model, we build on the baseline
 by making it use chunked autoregression [35, 28] and adversarial
 training [36, 28]. Specifically, we encode outputs using
 an MLP and concatenate the encodings to subsequent input
 [35, 28], which could help with modelling articulation dependencies
 across time, e.g., coarticulation [37]. With adversarial
 training, we use a convolutional neural network (CNN)
discriminator to encourage the model to output more realistic
 estimates [36, 28]. We observed that using MAE with our
1We used the hubert large ll60k model in https //github.com/s3prl/s3prl.Model MOCHA HPRC
MFCC, MTL (Baseline) [15] 0.502 0 .697
HuBERT, MTL 0.678 0.782
HuBERT, MTL, AR, GAN 0.672 0.784
Table 1 . Pearson correlation coefficients (PCCs) for heldout-speaker
 AAI. Models use multi-task learning (MTL), autoregression
 (AR) and adversarial training (GAN).3
18-dimensional phonemic features, as introduced in Section
3.2, outperformed cross-entropy with one-hot phonemes here,
potentially since the former let the discriminator more easily
compare estimates with ground truths. Finally, we use the HuBERT
 features in Section 3.3 as inputs, linearly interpolated
to match the TV sampling frequencies.
4.2. Articulatory Synthesis Model
For our resynthesis experiments in Section 5.3, we use HiFiCAR
 [28], a generative model composed of residual CNN
layers, to synthesize waveforms directly from our TV articulatory
 features. In order to provide the synthesizer with
speaker information, we also concatenate the speaker embedding
 in Section 3.4 to each time step in the input.
5.1. Articulatory Inversion
Table 1 summarizes our AAI results, measured using Pearson
correlation as done previously [15]. Our approaches in the
last two rows improve the SOTA performance on the HPRC
dataset by 12.5%and outperform the SOTA model by 0.176
correlation on MOCHA-TIMIT. Switching from MFCC inputs
 to HuBERT ones yields the largest improvement, suggesting
 that self-supervised features are better than spectral
ones for multi-speaker AAI tasks. We attribute the lower
performance on MOCHA-TIMIT compared to HPRC for all
models due to HPRC having more hours of speech and potentially
 higher-quality EMA labels. This performance difference
 is probably not due to phoneme label quality, as the
phoneme classification accuracy on MOCHA, 0.682, is comparable
 to that on HPRC, 0.673.
To further study which types of inputs our model contributes
 improved inversions, we compare the baseline with
our proposed method at the phoneme level. Specifically, we
calculate the average L1 distance between the predicted and
ground truth tract variables, defined in Section 3.1, in our
3MOCHA results are updated from ICASSP 2023 version after correcting
a preprocessing error.AA AO AW ER F K L M N NG OY P R TH V
[15] .206 .214 .229 .211 .208 .222 .222 .216 .195 .244 .226 .206 .206 .207 .198
Ours .175 .182 .187 .168 .163 
 ## PROFNAME
 Shinji Watanabe
 ## AUTHORID
 1746678
 ## AUTHORNAME
 Shinji Watanabe
 ## AUTHORURL
 https://www.semanticscholar.org/author/1746678
 ## AUTHORHINDEX
 67
 ## AUTHORAFFILIATIONS
 []
 ## AUTHORPAPERCOUNT
 597
 ## AUTHORCITATIONCOUNT
 22222
 ## PAPERID
 7d4ad68dedff8c81e0fd9c08ea76b220a7e05d69
 ## EXTERNALIDS
 {'ArXiv': '2302.06774', 'DBLP': 'conf/icassp/WuCCWGBA23', 'DOI': '10.1109/ICASSP49357.2023.10096796', 'CorpusId': 256846751}
 ## URL
 https://www.semanticscholar.org/paper/7d4ad68dedff8c81e0fd9c08ea76b220a7e05d69
 ## TITLE
 Speaker-Independent Acoustic-to-Articulatory Speech Inversion
 ## ABSTRACT
 To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.
 ## VENUE
 IEEE International Conference on Acoustics, Speech, and Signal Processing
 ## YEAR
 2023
 ## REFERENCECOUNT
 40
 ## CITATIONCOUNT
 10
 ## INFLUENTIALCITATIONCOUNT
 1
 ## ISOPENACCESS
 True
 ## OPENACCESSPDF
 {'url': 'https://arxiv.org/pdf/2302.06774', 'status': None}
 ## FIELDSOFSTUDY
 ['Computer Science', 'Engineering']
 ## JOURNAL
 {'pages': '1-5', 'name': 'ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)'}
 ## AUTHORS
 [{'authorId': '2111194238', 'name': 'Peter Wu'}, {'authorId': '2119257114', 'name': 'Li-Wei Chen'}, {'authorId': '2064541737', 'name': 'Cheol Jun Cho'}, {'authorId': '1746678', 'name': 'Shinji Watanabe'}, {'authorId': '145965043', 'name': 'L. Goldstein'}, {'authorId': '1690706', 'name': 'A. Black'}, {'authorId': '1692246', 'name': 'G. Anumanchipalli'}]
 ## TLDR
 This work builds an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers, and proposes a resynthesis-based AAI evaluation metric that does not rely on articulatory labels.
 .193 .177 .176 .163 .213 .191 .178 .173 .179 .165
Diff. .031 .032 .042 .042 .045 .029 .045 .041 .032 .031 .035 .028 .033 .028 .033
Table 2 . Average L1 distances (  ) between estimated and ground truth TVs per phoneme. Phonemes listed here have the
largest difference between the L1 of our method and that of the baseline [15]. Details in 5.1.
Fig. 3 . Predicted normalized lip aperture (LA) for our approach
 (blue) and the baseline (green) across 18 ARCTIC
speakers for each vowel.
HPRC test set for each phoneme. We obtain phoneme alignments
 with the Montreal Forced Aligner [38] and use the L1
distance metric since it can be computed at the frame level,
e.g., as opposed to a correlation-based metric. Table 2 summarizes
 these results. We observe that our model outperforms
the baseline noticeably on nasals (M, N, NG) and liquids (L,
R). One potential reason for this is that modelling improvements
 may help compensate for the lack of voicing and velar
information in EMA. Generally, we observed that our model
performs better than the baseline across all phonemes, and
we encourage readers to see the supplementary material for
5.2. Inversion Analysis without EMA Labels
Figure 3 contains the predicted normalized lip aperture (LA)
for our approach and the baseline across 18 ARCTIC speakers
 for each vowel. As mentioned in Section 2.4, all of these
speakers are unseen during training. Both approaches are
able to generally predict the biologically plausible LA values.
 For example, the lip aperture, visualized in Figure 1, is
predicted to be wide for the  AE  sound and narrow for the
 UW  sound. However, our model is much more consistent
across speakers, as evinced by the lower variance. We note
that this lower variance is consistent with the fact that the
TVs are normalized and much less speaker dependent than
EMA features [13]. Since the baseline incorrectly estimates
lip aperture for a larger number of speakers, this suggests that
our method is better at generalizing to unseen speakers.
5.3. Resynthesis Analysis without EMA Labels
We can also compare inversion performance through resynthesizing
 speech from estimated features (Section 4.2) andModel AWB SLT Average
Baseline [15] 9.23 10 .24 10 .94 0.76
Ours 8.03 9 .69 9 .52 0.59
Table 3 . MCDs (  ) in resynthesis analysis experiments on
ARCTIC speakers AWB and SLT, as well as the average and
standard deviation across all speakers.
evaluating synthesis quality. To study performance on unseen
 speakers this way, we resynthesize ARCTIC utterances
using our synthesis model and each inversion method. Then,
we compute the DTW-MCD [39] between the predicted and
ground truth waveforms. Table 3 summarizes these results.
Our approach outperforms the baseline for all speakers, consistent
 with our inversion results in Table 1 and earlier singlespeaker
 experiments [40]. We note that the MCD values are
high since we did not optimize for synthesis quality in this
work. Given the nascent nature of the multi-speaker articulatory
 synthesis direction, we plan to improve such models
and continue validating inversion performance in this manner
 using MCD and more synthesis metrics [39, 41] moving
In this work, we devise an acoustic-to-articulatory inversion
(AAI) approach in the context of generalizing to unseen
speakers, improving state-of-the-art by 12.5% on the HPRC
task [11]. We also show the interpretability of the estimated
representations through directly comparing them with speech
production behavior, evincing how this analysis can be done
without labeled articulatory data. Finally, we propose a
resynthesis-based AAI evaluation metric that does not rely on
articulatory labels. We demonstrate the efficacy of this evaluation
 method using ARCTIC and observe results consistent
with our inversion correlation metrics. In the future, we plan
to extend our methodology to high-fidelity resynthesis and
AAI for articulatory features beyond EMA.
This research is supported by the following grants to PI
Anumanchipalli   NSF award 2106928, Google Research
Scholar Award, Rose Hills Foundation and Noyce Foundation.8.
[1] Y . Lu, C. E. Wiltshire, K. E. Watkins, et al.,  Characteristics
of articulatory gestures in stuttered speech  A case study using
real-time magnetic resonance imaging,  Journal of Communication
 Disorders , vol. 97, pp. 106213, 2022.
[2] H.-S. Choi, J. Lee, W. Kim, et al.,  Neural analysis and synthesis 
 Reconstructing speech from self-supervised representations, 
[3] A. Polyak et al.,  Speech Resynthesis from Discrete Disentangled
 Self-Supervised Representations,  in Interspeech , 2021.
[4] G. K. Anumanchipalli, J. Chartier, and E. F. Chang,  Speech
synthesis from neural decoding of spoken sentences,  Nature ,
vol. 568, no. 7753, pp. 493 498, 2019.
[5] G. Fant,  What can basic research contribute to speech synthesis , 
 Journal of Phonetics , vol. 19, no. 1, pp. 75 90, 1991.
[6] P. Rubin, T. Baer, and P. Mermelstein,  An articulatory synthesizer
 for perceptual research,  The Journal of the Acoustical
Society of America , vol. 70, no. 2, pp. 321 328, 1981.
[7] C. Scully,  Articulatory synthesis,  in Speech production and
speech modelling , pp. 151 186. Springer, 1990.
[8] A. Wrench,  The mocha-timit articulatory database,  1999.
[9] A. A. Wrench,  A multi-channel/multi-speaker articulatory
 database for continuous speech recognition research., 
[10] K. Richmond, P. Hoole, and S. King,  Announcing the electromagnetic
 articulography (day 1) subset of the mngu0 articulatory
 corpus,  in Interspeech , 08 2011, pp. 1505 1508.
[11] M. K. Tiede et al.,  Quantifying kinematic aspects of reduction
in a contrasting rate production task,  Journal of the Acoustical
Society of America , 2017.
[12] H. Zen et al.,  LibriTTS  A corpus derived from librispeech
for text-to-speech,  in Interspeech , 2019.
[13] N. Seneviratne et al.,  Multi-corpus acoustic-to-articulatory
speech inversion.,  in Interspeech , 2019.
[14] T. Toda, A. Black, and K. Tokuda,  Acoustic-to-articulatory
inversion mapping with gaussian mixture model,  in Eighth International
 Conference on Spoken Language Processing , 2004.
[15] Y . M. Siriwardena et al.,  Acoustic-to-articulatory speech inversion
 with multi-task learning,  Interspeech , 2022.
[16] J. Wang et al.,  Acoustic-to-articulatory inversion based on
speech decomposition and auxiliary feature,  in ICASSP , 2022.
[17] G. Sun, Z. Huang, L. Wang, and P. Zhang,  Temporal
convolution network based joint optimization of acoustic-toarticulatory
 inversion,  Applied Sciences , 2021.
[18] B. Uria, I. Murray, S. Renals, and K. Richmond,  Deep architectures
 for articulatory inversion,  in Interspeech , 2012.
[19] A. S. Shahrebabaki, S. M. Siniscalchi, and T. Svendsen,  Raw
speech-to-articulatory inversion by temporal filtering and decimation, 
[20] N. Bozorg, M. T. Johnson, and M. Soleymanpour,  Autoregressive
 articulatory wavenet flow for speaker-independent
acoustic-to-articulatory inversion,  in SpeD . IEEE, 2021.[21] S. K. 
 ## PROFNAME
 Shinji Watanabe
 ## AUTHORID
 1746678
 ## AUTHORNAME
 Shinji Watanabe
 ## AUTHORURL
 https://www.semanticscholar.org/author/1746678
 ## AUTHORHINDEX
 67
 ## AUTHORAFFILIATIONS
 []
 ## AUTHORPAPERCOUNT
 597
 ## AUTHORCITATIONCOUNT
 22222
 ## PAPERID
 7d4ad68dedff8c81e0fd9c08ea76b220a7e05d69
 ## EXTERNALIDS
 {'ArXiv': '2302.06774', 'DBLP': 'conf/icassp/WuCCWGBA23', 'DOI': '10.1109/ICASSP49357.2023.10096796', 'CorpusId': 256846751}
 ## URL
 https://www.semanticscholar.org/paper/7d4ad68dedff8c81e0fd9c08ea76b220a7e05d69
 ## TITLE
 Speaker-Independent Acoustic-to-Articulatory Speech Inversion
 ## ABSTRACT
 To build speech processing methods that can handle speech as naturally as humans, researchers have explored multiple ways of building an invertible mapping from speech to an interpretable space. The articulatory space is a promising inversion target, since this space captures the mechanics of speech production. To this end, we build an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers. Our approach obtains 0.784 correlation on an electromagnetic articulography (EMA) dataset, improving the state-of-the-art by 12.5%. Additionally, we show the interpretability of these representations through directly com-paring the behavior of estimated representations with speech production behavior. Finally, we propose a resynthesis-based AAI evaluation metric that does not rely on articulatory labels, demonstrating its efficacy with an 18-speaker dataset.
 ## VENUE
 IEEE International Conference on Acoustics, Speech, and Signal Processing
 ## YEAR
 2023
 ## REFERENCECOUNT
 40
 ## CITATIONCOUNT
 10
 ## INFLUENTIALCITATIONCOUNT
 1
 ## ISOPENACCESS
 True
 ## OPENACCESSPDF
 {'url': 'https://arxiv.org/pdf/2302.06774', 'status': None}
 ## FIELDSOFSTUDY
 ['Computer Science', 'Engineering']
 ## JOURNAL
 {'pages': '1-5', 'name': 'ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)'}
 ## AUTHORS
 [{'authorId': '2111194238', 'name': 'Peter Wu'}, {'authorId': '2119257114', 'name': 'Li-Wei Chen'}, {'authorId': '2064541737', 'name': 'Cheol Jun Cho'}, {'authorId': '1746678', 'name': 'Shinji Watanabe'}, {'authorId': '145965043', 'name': 'L. Goldstein'}, {'authorId': '1690706', 'name': 'A. Black'}, {'authorId': '1692246', 'name': 'G. Anumanchipalli'}]
 ## TLDR
 This work builds an acoustic-to-articulatory inversion (AAI) model that leverages autoregression, adversarial training, and self supervision to generalize to unseen speakers, and proposes a resynthesis-based AAI evaluation metric that does not rely on articulatory labels.
 Maharana, A. Illa, R. Mannem, et al.,  Acoustic-toarticulatory
 inversion for dysarthric speech by using crosscorpus
 acoustic-articulatory data,  in ICASSP . IEEE, 2021.
[22] S. Udupa, A. Roy, A. Singh, et al.,  Estimating articulatory
movements in speech production with transformer networks, 
arXiv preprint arXiv 2104.05017 , 2021.
[23] J. Chartier, G. K. Anumanchipalli, K. Johnson, and E. F.
Chang,  Encoding of articulatory kinematic trajectories in human
 speech sensorimotor cortex,  Neuron , 2018.
[24] A. S. Shahrebabaki, G. Salvi, T. Svendsen, and S. M. Siniscalchi,
  Acoustic-to-articulatory mapping with joint optimization
 of deep speech enhancement and articulatory inversion
models,  TASLP , vol. 30, pp. 135 147, 2022.
[25] A. S. Shahrebabaki, N. Olfati, A. S. Imran, et al.,  A two-stage
deep modeling approach to articulatory inversion,  in ICASSP ,
[26] J. Kominek and A. W. Black,  The CMU Arctic speech
databases,  in Fifth ISCA workshop on speech synthesis , 2004.
[27] A. Wilkinson, A. Parlikar, S. Sitaram, et al.,  Open-source
consumer-grade indic text to speech.,  in SSW, 2016.
[28] P. Wu, S. Watanabe, L. Goldstein, et al.,  Deep speech synthesis
 from articulatory representations,  in Interspeech , 2022.
[29] Y . Zhang and Q. Yang,  An overview of multi-task learning, 
National Science Review , 2018.
[30] S. wen Yang, P.-H. Chi, Y .-S. Chuang, et al.,  SUPERB 
Speech Processing Universal PERformance Benchmark,  in
Interspeech , 2021, pp. 1194 1198.
[31] W.-N. Hsu, B. Bolte, Y .-H. H. Tsai, et al.,  Hubert  Selfsupervised
 speech representation learning by masked prediction
 of hidden units,  TASLP , 2021.
[32] M. Ravanelli, T. Parcollet, P. Plantinga, et al.,  SpeechBrain 
A general-purpose speech toolkit,  2021, arXiv 2106.04624.
[33] J. Chung, C. Gulcehre, K. Cho, and Y . Bengio,  Empirical
 evaluation of gated recurrent neural networks on sequence
modeling,  NeurIPS , 2014.
[34] D. E. Rumelhart, G. E. Hinton, and R. J. Williams, Learning
Internal Representations by Error Propagation , MIT Press,
[35] M. Morrison, R. Kumar, K. Kumar, et al.,  Chunked autoregressive
 gan for conditional waveform synthesis,  in ICLR ,
[36] J. Kong, J. Kim, and J. Bae,  HiFi-GAN  Generative adversarial
 networks for efficient and high fidelity speech synthesis,  in
[37] C. P. Browman and L. Goldstein,  Articulatory phonology  An
overview,  Phonetica , 1992.
[38] M. McAuliffe et al.,  Montreal forced aligner  Trainable textspeech
 alignment using kaldi,  in Interspeech , 2017.
[39] S. Watanabe, T. Hori, S. Karita, et al.,  ESPnet  End-to-end
speech processing toolkit,  in Interspeech , 2018.
[40] K. Richmond, Z. Ling, J. Yamagishi, et al.,  On the evaluation
of inversion mapping performance in the acoustic domain,  in
[41] T. Hayashi et al.,  Espnet2-tts  Extending the edge of tts research, 
 arXiv preprint arXiv 2110.07840 , 2021.