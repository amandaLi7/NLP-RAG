Question,Answer,Notes
What is the author ID of Yiming Yang?,46286308,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What is the H-index of Yiming Yang?,17,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What is the semantic scholar author name of Yiming Yang?,Yiming Yang,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What is the semantic scholar url of Yiming Yang?,https://www.semanticscholar.org/author/46286308,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What is the affiliation of Yiming Yang?,"LTI (CMU), No other affiliations",##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What is the paper ID of the paper Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers?,4ce987d4f8ae0f4680808c318980d42a82b9aa89,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What are the external IDs of the paper Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers?,"{'DBLP': 'journals/corr/abs-2302-01925', 'ArXiv': '2302.01925', 'DOI': '10.48550/arXiv.2302.01925', 'CorpusId': 256598356}",##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What is the URL of the paper Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers?,https://www.semanticscholar.org/paper/4ce987d4f8ae0f4680808c318980d42a82b9aa89,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What is the abstract of the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers'?,"We propose a new class of linear Transformers called FourierLearner-Transformers (FLTs), which incorporate a wide range of relative positional encoding mechanisms (RPEs). These include regular RPE techniques applied for nongeometric data, as well as novel RPEs operating on the sequences of tokens embedded in higher-dimensional Euclidean spaces (e.g. point clouds). FLTs construct the optimal RPE mechanism implicitly by learning its spectral representation. As opposed to other architectures combining efficient low-rank linear attention with RPEs, FLTs remain practical in terms of their memory usage and do not require additional assumptions about the structure of the RPE-mask. FLTs allow also for applying certain structural inductive bias techniques to specify masking strategies, e.g. they provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling. We also thoroughly tested FLTs on other data modalities and tasks, such as: image classification and 3D molecular modeling. For 3D-data FLTs are, to the best of our knowledge, the first Transformers architectures providing RPE-enhanced linear attention.",##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
In which venue was the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers' published?,arXiv.org,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
In what year was the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers' published?,2023,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
How many references are in the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers'?,60,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
How many citations does the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers' have?,5,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What is the citation count of 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers' have?,5,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
How many influential citations does the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers' have?,1,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
Is the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers' open access?,Yes,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What is the open access PDF URL of the paper titled 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers'?,http://arxiv.org/pdf/2302.01925,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What are the fields of study for the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers'?,Computer Science,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What is the journal name for the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers'?,"ArXiv, volume: abs/2302.01925; ArXiv",##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
Who are the authors of the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers'?,"K. Choromanski, Shanda Li, Valerii Likhosherstov, Kumar Avinava Dubey, Shengjie Luo, Di He, Yiming Yang, Tamas Sarlos, Thomas Weingarten, Adrian Weller",##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
Who is the first author of the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers'?,K. Choromanski,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
What is the TLDR summary of the paper 'Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers'?,FLTs are the first Transformers architectures providing RPE-enhanced linear attention and provide a way to learn the so-called local RPEs introduced in this paper and providing accuracy gains as compared with several other linear Transformers for language modeling.,##Title: Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers
