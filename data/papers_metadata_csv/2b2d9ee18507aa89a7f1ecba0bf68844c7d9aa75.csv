Question,Answer,Notes
What is the author ID of Daniel Fried?,47070750,##Title: Grounding Language Models to Images for Multimodal Generation
What is the H-index of Daniel Fried?,26,##Title: Grounding Language Models to Images for Multimodal Generation
What is the semantic scholar author name of Daniel Fried?,Daniel Fried,##Title: Grounding Language Models to Images for Multimodal Generation
What is the semantic scholar url of Daniel Fried?,https://www.semanticscholar.org/author/47070750,##Title: Grounding Language Models to Images for Multimodal Generation
What are affiliation of Daniel Fried?,Carnegie Mellon University,##Title: Grounding Language Models to Images for Multimodal Generation
What is the paper ID of the paper Grounding Language Models to Images for Multimodal Generation?,2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75,##Title: Grounding Language Models to Images for Multimodal Generation
What are the external IDs of the paper Grounding Language Models to Images for Multimodal Generation?,"{'DBLP': 'journals/corr/abs-2301-13823', 'DOI': '10.48550/arXiv.2301.13823', 'CorpusId': 256416164}",##Title: Grounding Language Models to Images for Multimodal Generation
What is the URL of the paper Grounding Language Models to Images for Multimodal Generation?,https://www.semanticscholar.org/paper/2b2d9ee18507aa89a7f1ecba0bf68844c7d9aa75,##Title: Grounding Language Models to Images for Multimodal Generation
What is the abstract of the paper 'Grounding Language Models to Images for Multimodal Generation'?,"We propose an efﬁcient method to ground pre-trained text-only language models to the visual domain, enabling them to process and generate arbitrarily interleaved image-and-text data. Our method leverages the abilities of language models learnt from large scale text-only pretraining, such as in-context learning and free-form text generation. We keep the language model frozen, and ﬁnetune input and output linear layers to enable cross-modality interactions. This allows our model to process arbitrarily interleaved image-and-text inputs, and generate free-form text interleaved with retrieved images. We achieve strong zero-shot performance on grounded tasks such as contextual image retrieval and multimodal dialogue, and showcase compelling interactive abilities. Our approach works with any off-the-shelf language model and paves the way towards an ef-fective, general solution for leveraging pretrained language models in visually grounded settings.",##Title: Grounding Language Models to Images for Multimodal Generation
In which venue was the paper 'Grounding Language Models to Images for Multimodal Generation' published?,arXiv.org,##Title: Grounding Language Models to Images for Multimodal Generation
In what year was the paper 'Grounding Language Models to Images for Multimodal Generation' published?,2023,##Title: Grounding Language Models to Images for Multimodal Generation
How many references are in the paper 'Grounding Language Models to Images for Multimodal Generation'?,55,##Title: Grounding Language Models to Images for Multimodal Generation
How many citations does the paper 'Grounding Language Models to Images for Multimodal Generation' have?,60,##Title: Grounding Language Models to Images for Multimodal Generation
What is the citation count of 'Grounding Language Models to Images for Multimodal Generation' have?,60,##Title: Grounding Language Models to Images for Multimodal Generation
How many influential citations does the paper 'Grounding Language Models to Images for Multimodal Generation' have?,5,##Title: Grounding Language Models to Images for Multimodal Generation
Is the paper 'Grounding Language Models to Images for Multimodal Generation' open access?,Yes,##Title: Grounding Language Models to Images for Multimodal Generation
What is the open access PDF URL of the paper titled 'Grounding Language Models to Images for Multimodal Generation'?,http://arxiv.org/pdf/2301.13823,##Title: Grounding Language Models to Images for Multimodal Generation
What are the fields of study for the paper 'Grounding Language Models to Images for Multimodal Generation'?,Computer Science,##Title: Grounding Language Models to Images for Multimodal Generation
What is the journal name for the paper 'Grounding Language Models to Images for Multimodal Generation'?,"ArXiv, volume: abs/2301.13823; ArXiv",##Title: Grounding Language Models to Images for Multimodal Generation
Who are the authors of the paper 'Grounding Language Models to Images for Multimodal Generation'?,"Jing Yu Koh, R. Salakhutdinov, Daniel Fried",##Title: Grounding Language Models to Images for Multimodal Generation
Who is the first author of the paper 'Grounding Language Models to Images for Multimodal Generation'?,Jing Yu Koh,##Title: Grounding Language Models to Images for Multimodal Generation
What is the TLDR summary of the paper 'Grounding Language Models to Images for Multimodal Generation'?,"An ef-fective, general solution for leveraging pretrained language models in visually grounded settings, enabling them to process and generate arbitrarily interleaved image-and-text data.",##Title: Grounding Language Models to Images for Multimodal Generation
