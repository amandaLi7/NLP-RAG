Question,Answer,Notes
What is the author ID of Louis-Philippe Morency?,49933077,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What is the H-index of Louis-Philippe Morency?,79,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What is the semantic scholar author name of Louis-Philippe Morency?,Louis-Philippe Morency,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What is the semantic scholar author name of Louis-Philippe Morency?,https://www.semanticscholar.org/author/49933077,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What is the affiliation of Louis-Philippe Morency?,"LTI (CMU), No other affiliations",##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What is the paper ID of the paper Factorized Contrastive Learning: Going Beyond Multi-view Redundancy?,e1b2a35a000ca296c32284b323c7e36a28fe0693,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What are the external IDs of the paper Factorized Contrastive Learning: Going Beyond Multi-view Redundancy?,"{'DBLP': 'journals/corr/abs-2306-05268', 'ArXiv': '2306.05268', 'DOI': '10.48550/arXiv.2306.05268', 'CorpusId': 259108395}",##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What is the URL of the paper Factorized Contrastive Learning: Going Beyond Multi-view Redundancy?,https://www.semanticscholar.org/paper/e1b2a35a000ca296c32284b323c7e36a28fe0693,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What is the abstract of the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy'?,"In a wide range of multimodal tasks, contrastive learning has become a particularly appealing approach since it can successfully learn representations from abundant unlabeled data with only pairing information (e.g., image-caption or video-audio pairs). Underpinning these approaches is the assumption of multi-view redundancy - that shared information between modalities is necessary and sufficient for downstream tasks. However, in many real-world settings, task-relevant information is also contained in modality-unique regions: information that is only present in one modality but still relevant to the task. How can we learn self-supervised multimodal representations to capture both shared and unique information relevant to downstream tasks? This paper proposes FactorCL, a new multimodal representation learning method to go beyond multi-view redundancy. FactorCL is built from three new contributions: (1) factorizing task-relevant information into shared and unique representations, (2) capturing task-relevant information via maximizing MI lower bounds and removing task-irrelevant information via minimizing MI upper bounds, and (3) multimodal data augmentations to approximate task relevance without labels. On large-scale real-world datasets, FactorCL captures both shared and unique information and achieves state-of-the-art results on six benchmarks",##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
In which venue was the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy' published?,arXiv.org,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
In what year was the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy' published?,2023,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
How many references are in the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy'?,94,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
How many citations does the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy' have?,6,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What is the citation count of 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy' have?,6,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
How many influential citations does the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy' have?,0,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
Is the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy' open access?,Yes,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What is the open access PDF URL of the paper titled 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy'?,http://arxiv.org/pdf/2306.05268,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What are the fields of study for the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy'?,Computer Science,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What is the journal name for the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy'?,"ArXiv, volume: abs/2306.05268; ArXiv",##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
Who are the authors of the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy'?,"P. Liang, Zihao Deng, Martin Q. Ma, James Y. Zou, Louis-Philippe Morency, R. Salakhutdinov",##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
Who is the first author of the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy'?,P. Liang,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
What is the TLDR summary of the paper 'Factorized Contrastive Learning: Going Beyond Multi-view Redundancy'?,FactorCL is a new multimodal representation learning method to go beyond multi-view redundancy and captures both shared and unique information and achieves state-of-the-art results on six benchmarks.,##Title: Factorized Contrastive Learning: Going Beyond Multi-view Redundancy
