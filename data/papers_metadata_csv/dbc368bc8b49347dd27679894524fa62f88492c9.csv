Question,Answer,Notes
What is the author ID of Matt Gormley?,1762110,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What is the H-index of Matt Gormley?,17,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What is the semantic scholar author name of Matt Gormley?,Matthew R. Gormley,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What is the semantic scholar author name of Matt Gormley?,https://www.semanticscholar.org/author/1762110,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What is the affiliation of Matt Gormley?,"LTI (CMU), No other affiliations",##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What is the paper ID of the paper Unlimiformer: Long-Range Transformers with Unlimited Length Input?,dbc368bc8b49347dd27679894524fa62f88492c9,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What are the external IDs of the paper Unlimiformer: Long-Range Transformers with Unlimited Length Input?,"{'ArXiv': '2305.01625', 'DBLP': 'journals/corr/abs-2305-01625', 'DOI': '10.48550/arXiv.2305.01625', 'CorpusId': 258436892}",##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What is the URL of the paper Unlimiformer: Long-Range Transformers with Unlimited Length Input?,https://www.semanticscholar.org/paper/dbc368bc8b49347dd27679894524fa62f88492c9,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What is the abstract of the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?,"Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .",##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
In which venue was the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' published?,arXiv.org,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
In what year was the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' published?,2023,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
How many references are in the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?,56,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
How many citations does the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' have?,38,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What is the citation count of 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' have?,38,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
How many influential citations does the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' have?,3,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
Is the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' open access?,Yes,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What is the open access PDF URL of the paper titled 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?,http://arxiv.org/pdf/2305.01625,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What are the fields of study for the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?,Computer Science,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What is the journal name for the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?,"ArXiv, volume: abs/2305.01625; ArXiv",##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
Who are the authors of the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?,"Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley",##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
Who is the first author of the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?,Amanda Bertsch,##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
What is the TLDR summary of the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?,"This work proposes Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores.",##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input
