Question,Answer,Notes
What is the author ID of Yulia Tsvetkov?,2073587169,##Title: Understanding In-Context Learning via Supportive Pretraining Data
What is the H-index of Yulia Tsvetkov?,17,##Title: Understanding In-Context Learning via Supportive Pretraining Data
What is the semantic scholar author name of Yulia Tsvetkov?,Yulia Tsvetkov,##Title: Understanding In-Context Learning via Supportive Pretraining Data
What is the semantic scholar url of Yulia Tsvetkov?,https://www.semanticscholar.org/author/2073587169,##Title: Understanding In-Context Learning via Supportive Pretraining Data
What is the affiliation of Yulia Tsvetkov?,"LTI (CMU), No other affiliations",##Title: Understanding In-Context Learning via Supportive Pretraining Data
What is the paper ID of the paper Understanding In-Context Learning via Supportive Pretraining Data?,36f7bc27c9a37eb337c35df4ae86f148e13d4e9a,##Title: Understanding In-Context Learning via Supportive Pretraining Data
What are the external IDs of the paper Understanding In-Context Learning via Supportive Pretraining Data?,"{'ACL': '2023.acl-long.708', 'DBLP': 'conf/acl/HanSMTCW23', 'ArXiv': '2306.15091', 'DOI': '10.48550/arXiv.2306.15091', 'CorpusId': 259262608}",##Title: Understanding In-Context Learning via Supportive Pretraining Data
What is the URL of the paper Understanding In-Context Learning via Supportive Pretraining Data?,https://www.semanticscholar.org/paper/36f7bc27c9a37eb337c35df4ae86f148e13d4e9a,##Title: Understanding In-Context Learning via Supportive Pretraining Data
What is the abstract of the paper 'Understanding In-Context Learning via Supportive Pretraining Data'?,"In-context learning (ICL) improves language models’ performance on a variety of NLP tasks by simply demonstrating a handful of examples at inference time. It is not well understood why ICL ability emerges, as the model has never been specifically trained on such demonstrations. Unlike prior work that explores implicit mechanisms behind ICL, we study ICL via investigating the pretraining data. Specifically, we first adapt an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL. We observe that a continued pretraining on this small subset significantly improves the model’s ICL ability, by up to 18%. We then compare the supportive subset constrastively with random subsets of pretraining data and discover: (1) The supportive pretraining data to ICL do not have a higher domain relevance to downstream tasks. (2) The supportive pretraining data have a higher mass of rarely occurring, long-tail tokens. (3) The supportive pretraining data are challenging examples where the information gain from long-range context is below average, indicating learning to incorporate difficult long-range context encourages ICL. Our work takes a first step towards understanding ICL via analyzing instance-level pretraining data. Our insights have a potential to enhance the ICL ability of language models by actively guiding the construction of pretraining data in the future.",##Title: Understanding In-Context Learning via Supportive Pretraining Data
In which venue was the paper 'Understanding In-Context Learning via Supportive Pretraining Data' published?,Annual Meeting of the Association for Computational Linguistics,##Title: Understanding In-Context Learning via Supportive Pretraining Data
In what year was the paper 'Understanding In-Context Learning via Supportive Pretraining Data' published?,2023,##Title: Understanding In-Context Learning via Supportive Pretraining Data
How many references are in the paper 'Understanding In-Context Learning via Supportive Pretraining Data'?,44,##Title: Understanding In-Context Learning via Supportive Pretraining Data
How many citations does the paper 'Understanding In-Context Learning via Supportive Pretraining Data' have?,8,##Title: Understanding In-Context Learning via Supportive Pretraining Data
What is the citation count of 'Understanding In-Context Learning via Supportive Pretraining Data' have?,8,##Title: Understanding In-Context Learning via Supportive Pretraining Data
How many influential citations does the paper 'Understanding In-Context Learning via Supportive Pretraining Data' have?,0,##Title: Understanding In-Context Learning via Supportive Pretraining Data
Is the paper 'Understanding In-Context Learning via Supportive Pretraining Data' open access?,Yes,##Title: Understanding In-Context Learning via Supportive Pretraining Data
What is the open access PDF URL of the paper titled 'Understanding In-Context Learning via Supportive Pretraining Data'?,http://arxiv.org/pdf/2306.15091,##Title: Understanding In-Context Learning via Supportive Pretraining Data
What are the fields of study for the paper 'Understanding In-Context Learning via Supportive Pretraining Data'?,Computer Science,##Title: Understanding In-Context Learning via Supportive Pretraining Data
What is the journal name for the paper 'Understanding In-Context Learning via Supportive Pretraining Data'?,"Annual Meeting of the Association for Computational Linguistics, pages: 12660-12673; Annual Meeting of the Association for Computational Linguistics",##Title: Understanding In-Context Learning via Supportive Pretraining Data
Who are the authors of the paper 'Understanding In-Context Learning via Supportive Pretraining Data'?,"Xiaochuang Han, Daniel Simig, Todor Mihaylov, Yulia Tsvetkov, Asli Celikyilmaz, Tianlu Wang",##Title: Understanding In-Context Learning via Supportive Pretraining Data
Who is the first author of the paper 'Understanding In-Context Learning via Supportive Pretraining Data'?,Xiaochuang Han,##Title: Understanding In-Context Learning via Supportive Pretraining Data
What is the TLDR summary of the paper 'Understanding In-Context Learning via Supportive Pretraining Data'?,"This work first adapts an iterative, gradient-based approach to find a small subset of pretraining data that supports ICL and observes that a continued pretraining on this small subset significantly improves the model’s ICL ability, by up to 18%.",##Title: Understanding In-Context Learning via Supportive Pretraining Data
