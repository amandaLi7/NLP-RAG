Question,Answer,Notes
What is the name of this paper?,Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the author ID of Alexander Rudnicky?,3156164,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the H-index of Alexander Rudnicky?,8,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the semantic scholar author name of Alexander Rudnicky?,A. Rudnicky,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the semantic scholar author name of Alexander Rudnicky?,https://www.semanticscholar.org/author/3156164,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the affiliation of Alexander Rudnicky?,"LTI (CMU), No other affiliations on Semantic Scholar","##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the paper ID of the paper Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation?,465ec2212d865e875e64638b3dd1ecaac21c5ddd,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What are the external IDs of the paper Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation?,"{'DBLP': 'conf/emnlp/ChiFRR23', 'ArXiv': '2305.03796', 'DOI': '10.48550/arXiv.2305.03796', 'CorpusId': 258557586}","##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the URL of the paper Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation?,https://www.semanticscholar.org/paper/465ec2212d865e875e64638b3dd1ecaac21c5ddd,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the abstract of the paper 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation'?,"Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.","##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
In which venue was the paper 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation' published?,Conference on Empirical Methods in Natural Language Processing,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
In what year was the paper 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation' published?,2023,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
How many references are in the paper 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation'?,47,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
How many citations does the paper 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation' have?,4,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the citation count of 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation' have?,4,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
How many influential citations does the paper 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation' have?,0,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
Is the paper 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation' open access?,Yes,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the open access PDF URL of the paper titled 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation'?,http://arxiv.org/pdf/2305.03796,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What are the fields of study for the paper titled 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation'?,Computer Science,"##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the journal name for the paper titled 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation'?,"Conference on Empirical Methods in Natural Language Processing, pages: 5972-5984; Conference on Empirical Methods in Natural Language Processing","##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
Who are the authors of the paper 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation'?,"Ta-Chung Chi, Ting-Han Fan, A. Rudnicky, P. Ramadge","##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
What is the TLDR summary of the paper 'Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation'?,"Inspired by the notion of working memory, a new Transformer variant named RegularGPT is proposed, which constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY.","##Author: Alexander Rudnicky, ##Title: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation"
