Question,Answer,Notes
What is the author ID of Graham Neubig?,1700325,##Title: Why do Nearest Neighbor Language Models Work?
What is the H-index of Graham Neubig?,75,##Title: Why do Nearest Neighbor Language Models Work?
What is the semantic scholar author name of Graham Neubig?,Graham Neubig,##Title: Why do Nearest Neighbor Language Models Work?
What is the semantic scholar author name of Graham Neubig?,https://www.semanticscholar.org/author/1700325,##Title: Why do Nearest Neighbor Language Models Work?
What is the affiliation of Graham Neubig?,"LTI (CMU), No other affiliations",##Title: Why do Nearest Neighbor Language Models Work?
What is the paper ID of the paper Why do Nearest Neighbor Language Models Work??,c432aff446d55e72a28394a1508e760cc9a25c08,##Title: Why do Nearest Neighbor Language Models Work?
What are the external IDs of the paper Why do Nearest Neighbor Language Models Work??,"{'DBLP': 'conf/icml/Xu0N23', 'ArXiv': '2301.02828', 'DOI': '10.48550/arXiv.2301.02828', 'CorpusId': 255546631}",##Title: Why do Nearest Neighbor Language Models Work?
What is the URL of the paper Why do Nearest Neighbor Language Models Work??,https://www.semanticscholar.org/paper/c432aff446d55e72a28394a1508e760cc9a25c08,##Title: Why do Nearest Neighbor Language Models Work?
What is the abstract of the paper 'Why do Nearest Neighbor Language Models Work?'?,"Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.",##Title: Why do Nearest Neighbor Language Models Work?
In which venue was the paper 'Why do Nearest Neighbor Language Models Work?' published?,International Conference on Machine Learning,##Title: Why do Nearest Neighbor Language Models Work?
In what year was the paper 'Why do Nearest Neighbor Language Models Work?' published?,2023,##Title: Why do Nearest Neighbor Language Models Work?
How many references are in the paper 'Why do Nearest Neighbor Language Models Work?'?,44,##Title: Why do Nearest Neighbor Language Models Work?
How many citations does the paper 'Why do Nearest Neighbor Language Models Work?' have?,10,##Title: Why do Nearest Neighbor Language Models Work?
What is the citation count of 'Why do Nearest Neighbor Language Models Work?' have?,10,##Title: Why do Nearest Neighbor Language Models Work?
How many influential citations does the paper 'Why do Nearest Neighbor Language Models Work?' have?,1,##Title: Why do Nearest Neighbor Language Models Work?
Is the paper 'Why do Nearest Neighbor Language Models Work?' open access?,Yes,##Title: Why do Nearest Neighbor Language Models Work?
What is the open access PDF URL of the paper titled 'Why do Nearest Neighbor Language Models Work?'?,http://arxiv.org/pdf/2301.02828,##Title: Why do Nearest Neighbor Language Models Work?
What are the fields of study for the paper 'Why do Nearest Neighbor Language Models Work?'?,Computer Science,##Title: Why do Nearest Neighbor Language Models Work?
What is the journal name for the paper 'Why do Nearest Neighbor Language Models Work?'?,"ArXiv, volume: abs/2301.02828; ArXiv",##Title: Why do Nearest Neighbor Language Models Work?
Who are the authors of the paper 'Why do Nearest Neighbor Language Models Work?'?,"Frank F. Xu, Uri Alon, Graham Neubig",##Title: Why do Nearest Neighbor Language Models Work?
Who is the first author of the paper 'Why do Nearest Neighbor Language Models Work?'?,Frank F. Xu,##Title: Why do Nearest Neighbor Language Models Work?
What is the TLDR summary of the paper 'Why do Nearest Neighbor Language Models Work?'?,"This paper identifies three main reasons why k-nearest neighbor language models (kNN-LM) perform better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution.",##Title: Why do Nearest Neighbor Language Models Work?
