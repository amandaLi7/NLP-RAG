Question,Answer,Notes
What is the author ID of Maarten Sap?,2729164,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What is the H-index of Maarten Sap?,36,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What is the semantic scholar author name of Maarten Sap?,Maarten Sap,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What is the semantic scholar url of Maarten Sap?,https://www.semanticscholar.org/author/2729164,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What are affiliation of Maarten Sap?,"Carnegie Mellon University, Allen Institute for AI",##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What is the paper ID of the paper Improving Language Models with Advantage-based Offline Policy Gradients?,9d2dc57903e99f33b9cf727c3903718751d82663,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What are the external IDs of the paper Improving Language Models with Advantage-based Offline Policy Gradients?,"{'ArXiv': '2305.14718', 'DBLP': 'journals/corr/abs-2305-14718', 'DOI': '10.48550/arXiv.2305.14718', 'CorpusId': 258865581}",##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What is the URL of the paper Improving Language Models with Advantage-based Offline Policy Gradients?,https://www.semanticscholar.org/paper/9d2dc57903e99f33b9cf727c3903718751d82663,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What is the abstract of the paper 'Improving Language Models with Advantage-based Offline Policy Gradients'?,"Language Models (LMs) achieve substantial language capabilities when finetuned using Reinforcement Learning with Human Feedback (RLHF). However, RLHF is an unstable and data-hungry process that continually requires new high-quality LM-generated data for finetuning. We introduce Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data. By assuming the entire LM output sequence as a single action, A-LoL allows incorporating sequence-level classifiers or human-designed scoring functions as rewards. Subsequently, by using LM's internal sequence-level value estimate, A-LoL filters negative advantage (low-quality) data points during training, making it resilient to noise. Overall, A-LoL is an easy-to-implement LM training recipe that is sample-efficient and stable. We demonstrate the effectiveness of A-LoL and its variants with a set of four different language generation tasks. We compare against both online RL (PPO) and recent preference-based (DPO, PRO) and reward-based (GOLD) offline RL baselines. On the commonly-used RLHF benchmark, Helpful and Harmless Assistant (HHA), LMs trained with A-LoL methods achieve the highest diversity while also being rated more safe and helpful than baselines according to humans. Additionally, in the remaining three tasks, A-LoL could optimize multiple distinct reward functions even when using noisy or suboptimal training data. We also release our experimental code. https://github.com/abaheti95/LoL-RL",##Title: Improving Language Models with Advantage-based Offline Policy Gradients
In which venue was the paper 'Improving Language Models with Advantage-based Offline Policy Gradients' published?,arXiv.org,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
In what year was the paper 'Improving Language Models with Advantage-based Offline Policy Gradients' published?,2023,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
How many references are in the paper 'Improving Language Models with Advantage-based Offline Policy Gradients'?,91,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
How many citations does the paper 'Improving Language Models with Advantage-based Offline Policy Gradients' have?,4,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What is the citation count of 'Improving Language Models with Advantage-based Offline Policy Gradients' have?,4,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
How many influential citations does the paper 'Improving Language Models with Advantage-based Offline Policy Gradients' have?,0,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
Is the paper 'Improving Language Models with Advantage-based Offline Policy Gradients' open access?,Yes,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What is the open access PDF URL of the paper titled 'Improving Language Models with Advantage-based Offline Policy Gradients'?,https://arxiv.org/pdf/2305.14718,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What are the fields of study for the paper 'Improving Language Models with Advantage-based Offline Policy Gradients'?,Computer Science,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What is the journal name for the paper 'Improving Language Models with Advantage-based Offline Policy Gradients'?,"ArXiv, volume: abs/2305.14718; ArXiv",##Title: Improving Language Models with Advantage-based Offline Policy Gradients
Who are the authors of the paper 'Improving Language Models with Advantage-based Offline Policy Gradients'?,"Ashutosh Baheti, Ximing Lu, Faeze Brahman, Ronan Le Bras, Maarten Sap, Mark O. Riedl",##Title: Improving Language Models with Advantage-based Offline Policy Gradients
Who is the first author of the paper 'Improving Language Models with Advantage-based Offline Policy Gradients'?,Ashutosh Baheti,##Title: Improving Language Models with Advantage-based Offline Policy Gradients
What is the TLDR summary of the paper 'Improving Language Models with Advantage-based Offline Policy Gradients'?,"Advantage-Leftover Lunch RL (A-LoL), a new class of offline policy gradient algorithms that enable RL training on any pre-existing data that assumes the entire LM output sequence as a single action, and allows incorporating sequence-level classifiers or human-designed scoring functions as rewards.",##Title: Improving Language Models with Advantage-based Offline Policy Gradients
