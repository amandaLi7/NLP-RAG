Question,Answer,Notes
What is the author ID of Shinji Watanabe?,1746678,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What is the H-index of Shinji Watanabe?,67,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What is the semantic scholar author name of Shinji Watanabe?,Shinji Watanabe,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What is the semantic scholar url of Shinji Watanabe?,https://www.semanticscholar.org/author/1746678,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What is the affiliation of Shinji Watanabe?,"LTI (CMU), No other affiliations",##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What is the paper ID of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models?,e4f2d75856ce149b994f079ae50fd33ca47245d3,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What are the external IDs of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models?,"{'ArXiv': '2305.17651', 'DBLP': 'journals/corr/abs-2305-17651', 'DOI': '10.48550/arXiv.2305.17651', 'CorpusId': 258959211}",##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What is the URL of the paper DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models?,https://www.semanticscholar.org/paper/e4f2d75856ce149b994f079ae50fd33ca47245d3,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What is the abstract of the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models'?,"Self-supervised learning (SSL) has achieved notable success in many speech processing tasks, but the large model size and heavy computational cost hinder the deployment. Knowledge distillation trains a small student model to mimic the behavior of a large teacher model. However, the student architecture usually needs to be manually designed and will remain fixed during training, which requires prior knowledge and can lead to suboptimal performance. Inspired by recent success of task-specific structured pruning, we propose DPHuBERT, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning. Experiments on SUPERB show that DPHuBERT outperforms pure distillation methods in almost all tasks. Moreover, DPHuBERT requires little training time and performs well with limited training data, making it suitable for resource-constrained applications. Our method can also be applied to various speech SSL models. Our code and models will be publicly available.",##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
In which venue was the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models' published?,Interspeech,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
In what year was the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models' published?,2023,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
How many references are in the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models'?,42,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
How many citations does the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models' have?,11,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What is the citation count of 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models' have?,11,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
How many influential citations does the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models' have?,1,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
Is the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models' open access?,Yes,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What is the open access PDF URL of the paper titled 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models'?,http://arxiv.org/pdf/2305.17651,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What are the fields of study for the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models'?,"Computer Science, Engineering",##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What is the journal name for the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models'?,"ArXiv, volume: abs/2305.17651; ArXiv",##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
Who are the authors of the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models'?,"Yifan Peng, Yui Sudo, Muhammad Shakeel, Shinji Watanabe",##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
Who is the first author of the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models'?,Yifan Peng,##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
What is the TLDR summary of the paper 'DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models'?,"DPHuBERT is proposed, a novel task-agnostic compression method for speech SSL based on joint distillation and pruning that requires little training time and performs well with limited training data, making it suitable for resource-constrained applications.",##Title: DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models
