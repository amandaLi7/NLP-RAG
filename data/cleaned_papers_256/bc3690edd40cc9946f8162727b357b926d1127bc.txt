JOINT MODELLING OF SPOKEN LANGUAGE UNDERSTANDING TASKS WITH INTEGRATED DIALOG HISTORY Siddhant Arora1, Hayato Futami2, Emiru Tsunoo2, Brian Yan1, Shinji Watanabe1 1Carnegie Mellon University,2Sony Group Corporation, Japan Most human interactions occur in t 
e form of spoken conversations where the semantic meaning of a given utterance depends on the context. 
Each utterance in spoken conversation can be represented by many semantic and speaker attributes, and there has been an interest in building Spoken Language Understanding (SLU) systems for automatically predicting these attributes. 
Recent work has shown that incorporating dialogue history can help advance SLU performance. However, separate models are used for each SLU task, leading to an increase in inference time and computation cost. 
Motivated by this, we aim to ask: can we jointly model all the SLU tasks while incorporating context to facilitate low-latency and lightweight inference? To answer this, we propose a novel model architecture that learns dialog context to jointly predict t 
e intent, dialog act, speaker role, and emotion for the spoken utterance. Note that our joint prediction is based on an autoregressive model and we need to decide the prediction order of dialog attributes, which is not trivial. 
To mitigate the issue, we also propose an order agnostic training method. Our experiments show that our joint model achieves similar results to task-speci c classi ers and can effectively integrate dialog context to further improve the SLU performance. 
1 Index Terms  spoken language understanding, spoken dialog system, end-to-end systems, joint modelling, speaker attributes Spoken dialogue systems aim to enable dialogue agents to engage in a more natural conversation with humans. 
They have commonly represented a possible dialogue by a series of frames [1, 2]. Each frame represents the type of task the user seeks and has attributes representing the information that can help the system to complete the task. 
These spoken dialog systems aim to automatically identify the topic of conversations as well as other dialog frame attributes (e.g., dialogue act, emotion) to engage in conversation with the user. 
Conventional Spoken Language Understanding (SLU) [3, 4] systems independently process each utterance in a conversation. However, the meaning of an utterance in a spoken dialog depends on the context. 
Prior work has shown dialogue context to be particularly useful in resolving ambiguities and co-references [5, 6]. As a result, there has been extensive work on incorporating context to improve Natural Language Understanding (NLU) performance [7 9]. 
Prior work [10 12] on conversational speech has also shown strong improvements in ASR performance by incorporating dialog context. Thus, several approaches [13 16] have looked into incorporating dialog history in pipeline based SLU systems. 
Recently, there has been some work [17 19] in incorporating context to improve endto-end (E2E) SLU performance. One such approach [17] integrates 1Our code & models are publicly available as part of ESPnet-SLU toolkit. 
dialog history into an E2E SLU system by using ASR transcripts of previous spoken utterances. There has also been an effort [18] to incorporate context directly from the audio of previous utterances. 
However, these works mostly focus on building a single model for each SLU task like intent classi cation, dialog act classi cation, and emotion recognition. To jointly predict all the SLU tasks, we have to execute all models separately. 
Consequently, these models not only have a large memory footprint but also have high latency, which can affect the naturalness of spoken conversations [20] when these systems are deployed in commercial applications like voice assistants. 
Prior work has shown that jointly modeling different speech processing tasks together in a united framework can perform comparable to task-speci c models [21, 22] while reducing latency [23]. 
Motivated by this work, we ask the following questions: (i) Can we jointly model all SLU tasks while incorporating context in a single uni ed implementation without much loss in performance? (ii) Does incorporating the SLU tags predicted at previous spoke 
utterances by the joint model help in better modeling spoken dialogue context? We seek to answer these questions by proposing a novel E2E SLU architecture that jointly models all the SLU tasks while effectively learning context from previous spoken uttera 
ces. This joint model uses an autoregressive decoder to predict dialog attributes one by one and the order in which the model predicts the SLU attributes may impact model performance. 
Hence, we investigate (iii) if we can use order agnostic training to make the model automatically predict in the optimal ordering during inference. 
Inspired by prior work on ASR [24], we also investigate (iv) if an intermediate CTC loss advances SLU performance further. 
We conduct extensive experiments on the recently released HarperValley [25] Bank dataset, which consists of dialogs between users and consumer bank agents. 
Our results show that our joint model performs at par with the individual classi ers for each SLU task, and incorporating dialog context and order agnostic training can further lead to signi cant improvements in performance. 
Our code and models are made publicly available as part of the ESPnet-SLU [26] toolkit. The key contributions of our work are summarised below. 
We propose a novel joint model that uses dialog context to jointly predict the intent, dialog act, speaker role and emotion. We propose order agnostic training of the joint model and show an improvement in performance across all SLU tasks. 
We investigate the ef cacy of using SLU tags predicted from previous utterances to model dialog context. We show that incorporating the usage of intermediate CTC loss can advance SLU performance. 2. 
BACKGROUND: DIALOG CONTEXT IN SLU The formulation for SLU with integrated dialog context extends the well-studied framework of NLU systems [7 9]. For NLU systems,arXiv:2305.00926v1  [cs. 
CL]  1 May 2023dialog context sequence is represented as sequence of Cutterances, i.e.,S={sc|c= 1,...,C}. Each utterance in the dialog context sequence is represented as sc={wcn V|n= 1,...,Nc}, with lengthNcand vocabularyV. 
Each utterance has a tag for each of the NLU tasks. In this work, each utterance has a tag from label setsLda,Lic,LsrandLerindicating dialogue act classi cation, intent classi cation, speaker role prediction, and emotion recognition, respectively. 
This produces a label sequence of the same length Cfor each task, for instance, Yda={yda Using the maximum a posteriori theory, NLU models seek to output Yda, Yic, Ysr, and  Yerthat maximise the posterior distribution P(Yda|S),P(Yic|S),P(Ysr|S)andP(Yer|S) 
ivenS, respectively. SLU introduces an additional complexity of modeling dialog context from the spoken utterance. Dialog context sequence is formed byCspoken utterances, i.e., X={xc|c= 1,...,C}. Each spoken utterance xc={xct Rd|t= 1,... 
,Tc}is a sequence ofddimensional speech feature of length Tcframes. 
Similar to the NLU formulation, SLU systems seek to estimate the label sequence Yda, Yic, Ysrand Yerthat maximise the posterior distribution P(Yda|X),P(Yic|X),P(Ysr|X)andP(Yer|X)givenX, respectively. 
We can model these posterior distributions as described in the 2.1. Seperate E2E model with dialog context Prior work [17] models each posterior, e.g. 
, P(Yda|X), using a sequence of transcripts Sby applying the Viterbi approximation: SP(Yda|S,X)P(S|X) (1) SP(Yda|S,X)P(S|X) (2) Their approach then assumes the conditional independence of c|s1:c 1fromx1:c 1,yda 1:c 1to simplify the Eq 2: c|s1:c 1,xc)P(S|X) 
(3) Transcripts are computed using a separate ASR module that seeks to estimate  Sthat maximises P(S|X). Using  S, we can modify Eq 3: Prior work [17] models P(yda c| s1:c 1,xc)in Eq. 
4 by passing ASR transcripts  s1:c 1to a pretrained language model (LM) like BERT [27] and then concatenating these context embeddings to the acoustic embedding obtained from xc. 
They focus on building a separate model for each of the SLU tasks, which in the above description is dialogue act classi cation. 
Thus, to predict all the SLU tasks, all the separate models that estimate P(Yda|X),P(Yic|X),P(Ysr|X), andP(Yer|X)independently need to be executed which can increase latency and computational cost and also does not consider the dependency between SLU task 
. 3. PROPOSED JOINT E2E MODEL W/ DIALOG CONTEXT In this work, we extend the prior work [17] on dialog integration discussed in section 2.1 and propose to jointly model all SLU tasks. 
We denote a single target containing all the SLU tags as EncoderxSContext  || YContext Semantic EncoderDecodercjoinYCTCFig. 
1 : Diagram of our joint E2E model incorporating dialog history for jointly predicting all SLU tasks R= (Yda,Yic,Ysr,Yer)and modify Eq. 4 with rc= (yda c=1P(rc|r1:c 1, s1:c 1,xc) (5) The SLU tags predicted for the previous spoken utterances i.e. 
1:c 1)may be incorporated (Eq. 5) to better model the dialog context unlike in prior work [17] which assumes conditional independence of yda In  5.1, we con rm experimentally whether this previous SLU tag condition is helpful. 
Further, by jointly modeling all the SLU tasks, we expect signi cantly lower latency and lightweight inference. To realize this formulation, we propose a joint model architecture shown in Fig. 1. The input speech signal for each utterance i.e., xcin Eq. 
5, is passed through an acoustic encoder (Encoder aco) to generate acoustic embeddings caco. 
caco= Encoder aco(xc) (6) We concatenate ASR transcripts  s1:c 1and SLU tags r1:c 1for all previous spoken utterances and pass them through semantic encoder (Encoder sem) like a pretrained LM to encode the dialog history: ccont= Encoder sem(concat( s1:c 1 
r1:c 1)) (7) The output of the semantic encoder is also passed to a linear layer to ensure that context embeddings cconthave the same hidden dimension as acoustic embeddings caco. 
The acoustic and context embeddings are concatenated together ( concat( caco,ccont)) and attended by a joint encoder Encoder jointo produce the joint embedding cjoin: cjoin= Encoder join(concat( caco,ccont)) (8) The model is trained using joint CTC-attent 
on training [28], where the CTC objective function is used to train the attention model encoder as an auxiliary task. Because we use an autoregressive decoder to predict tags one by one (Eq. 
11), the likelihood P(rc|xc, s1:c 1,r1:c 1)is dependent on the order of SLU tags in the target sequence rc. This has been referred to as label ambiguity (or permutation) problem in prior work [29, 30]. 
Inspired by prior work on permutation invariant training [30 32], we use CTC objective function to perform permutation-free training as shown in Eq. 9, which is referred to as order agnostic training in this work. 
Let z be the output sequence variable computed from the joint embedding cjoin, then the optimal permutation order  is computed as: wherePis the set of 4! possible permutations of SLU tags (da,ic,sr,er),  is one such permutation and r target with the order 
of SLU tags indicated by  . Later, the optimalModel DA ( ) IC ( ) SR ( ) ER ( ) RTF ( ) Latency (msec.) (  ) Parameters ( ) Train time (sec.) (  ) Slowest Total Slowest Total Separate E2E model without context* ([18]) 54. 
0 Separate E2E model with context* ([18]) 58.3 Separate E2E models without context  2.1 55.5 47.1 91.9 91.4 1.58 3.28 1153 4515 457.6M 4296 Joint E2E model without context  3 55.8 47.7 91.5 90.6 1.15 1.58 293 1153 114. 
4M 1381 Joint E2E model with context  3 58.1 86.1 95.0 90.9 1.14 1.57 286 1133 152.7M 1960 + order agnostic training (Eq. 9) ( proposed ) 58.8 86.5 95.1 91.1 1.15 1.58 289 1140 152.7M 2040 + previous SLU tag condition (Eq. 5) ( ablation ) 58.3 86.5 94. 
6 90.8 1.23 1.54 310 1080 165. 
4M 2613 Table 1 : Results presenting the performance of our joint model both with and without using dialog context on dialog act (DA) recognition, intent classi cation (IC), speaker role (SR) prediction and emotion recognition (ER). 
* We show DA results of prior work although they have different data preparation setup (sec. 4.1). Best joint model results are bolded and further underlined if they surpass separate E2E models. 
Model DA ( ) IC ( ) SR ( ) ER ( ) Joint E2E Model without context 54.8 47.6 91.1 90.9 w/ inter ctc 55.8 47.7 91.5 90.6 Table 2 : Results showing the impact of intermediate CTC to advance permutation  is used for computing the attention decoder loss2. 
For each pair of optimal decoding order and reference target ( ,rc), the attention likelihood is calculated as shown below. 
hl= Decoder( cJOIN,(r c)l|xc, s1:c 1,r1:c 1,(r c)1:l 1) = SoftmaxOut( hl), where SoftmaxOut denotes a linear layer followed by the softmax c)lis thelthterm in target sequence. 
The joint likelihoodPAttn(r c|xc, s1:c 1,r1:c 1)can then be computed as a product of the individual likelihood for each of the SLU tags (i.e., from lin [1,4]). 
It is important to note that this  order agnostic training does not add any new model parameters. During inference, the model automatically picks the tag order, i.e., unlike training, we do not enforce the predicted tag order to have the minimum CTC loss. 
Further, our joint models can also be trained with an auxiliary ASR objective [26, 33] by making the model generate both the SLU tags cand ASR transcript sc. 
To show the effectiveness of our joint model, we conducted experiments on publicly available HarperValleyBank [25] spoken dialog corpus, where dialogs are simulated conversations between bank employees and customers. 
The corpus consists of 1,446 conversations with 23 hours of audio and 25,730 utterances with human annotated transcripts. 
The utterances are spoken by 59 unique speakers and have annotations for the dialog act, intent of the conversation, speaker role, and emotional valence. 
In this work, we trained a joint model that performs intent classi cation, dialog act recognition, emotion recognition and speaker attribute prediction. 
Dialogue act recognition is a multi-label multiclass classi cation whereas all other tasks are single-label multiclass classi cation. We followed the setup in prior work [18] and split the conversations into train, valid and test set3. 
Similar to [17], 2We use the CTC loss to compute optimal order instead of decoder loss for lightweight modeling. 
3However, the prior work [17, 18] also uses an off the shelf ASR model to realign the audio with transcripts making their results not directly comparable to results obtained by our setup.Model DA ( ) IC ( ) SR ( ) ER ( ) Joint E2E Model with context 58. 
1 86.1 95.0 90.9 Joint E2E Model with oracle context 58.5 87.0 95.2 91.1 Table 3 : Results showing the robustness of using the ASR transcripts instead of the oracle transcripts. 
we also removed non-lexical tokens such as [noise],[laughter] from the transcript. Our training set contains 1,174 conversations (9.2 hours of audio, 15,424 utterances), valid set contains 73 conversations (0. 
6 hours of audio, 964 utterances) and our test set contains 199 conversations (1.6 hours of audio, 2903 utterances). 
We report macro F1 for dialog act prediction and accuracy for intent classi cation, speaker role prediction and emotion recognition as recommended by prior work [17, 25]. 
The latency of our system is reported using two metrics: (1) Real time factor (RTF), which is the average time taken to process an input audio  le as a ratio of the duration of input and (2) Endpoint latency which is the elapsed time from the utterance en 
to getting predictions for SLU tasks. We also show the training time per epoch for all our models. 4.2. 
Architecture details and training Our approach is compared to state-of-the-art SLU task-speci c baselines referred to as  Separate E2E models without context c| s1:c 1,xc)in Eq. 4). We also compared with our proposed model without context, i.e. 
,  Joint E2E model without context  which optimizes P(rc|xc)instead of P(rc|r1:c 1,xc, s1:c 1)in Eq. 5. This baseline joint E2E model does not incorporate order agnostic training de ned in Eq. 
9 and instead is trained using a  xed order (yda,yic,ysr,yer)of SLU tags. 
To understand the ef cacy of our proposed modi cations to [17], we  rst trained a joint model (referred to as  Joint E2E model with context ) that does not use SLU tags of previous spoken utterances (i.e. 
optimize P(rc|xc, s1:c 1)instead ofP(rc|r1:c 1,xc, s1:c 1) in Eq. 5) and also does not utilize order agnostic training. In our ablation study, we further trained proposed joint model that incorporates order agnostic training  as de ned in Eq. 
9 and another joint model that incorporates  previous SLU tag condition  i.e. tags predicted for previous spoken utterances ( r1:c 1in Eq. 5). 
Our models were implemented in pytorch [34] and experiments were conducted through the ESPNet-SLU [26, 35] toolkit. All the models were trained using an auxiliary ASR objective. 
Our taskspeci c baselines consist of 12 layer conformer [36, 37] encoder that inputs features extracted by a strong self-supervised speech model WavLM [38] and 6 layer transformer [39, 40] decoder. 
Our  Joint E2E model without context  has a similar architecture as task speci c baselines. Prior work [24] used an intermediate CTC loss attached to an intermediate layer in the encoder network to regularize CTC training and improve performance. 
Inspired by this work, we experimented with adding intermediate CTC loss at layers 4, 6 and8 of our conformer encoder. The  Joint E2E model without context is used to generate dialog context4 s1:i 1. 
We incorporated the usage of Transformers library [41] to get BERT-base-uncased [27] as semantic encoder ( encoder SEMin Eq. 7). The conformer architecture with intermediate CTC loss is leveraged as the joint encoder (encoder JOINin Eq. 
8) in our proposed models. Teacher forcing is used for all our models with integrated dialog context i.e., the context was created using ground truth transcripts during training. 
The training and inference of our models were performed using a single NVIDIA Tesla V100-32GB GPU. The inference was computed using 4 parallel jobs and we use the time when the utterance has been processed by all the jobs (i.e. 
time taken by slowest job) as well as the sum of time taken by all the jobs to compute latency metrics. All hyperparameters were selected based on validation performance. 
Table 1 shows the results of our joint model both with and without using dialog context. The performance of our  Separate E2E models without context  (section 2. 
1) is similar to the baseline results reported in prior work [18], though these results are not directly comparable because of different data preparation setups. 
Our Joint E2E model without context  performs at par with these taskspeci c models while signi cantly reducing latency5and the number of trainable model parameters, showcasing the utility of jointly modeling all SLU tasks. 
We investigated integrating dialog context in the joint model ( Joint E2E model with context  in Table 1) using formulation described in section 3 and observe a signi cant improvement in performance, particularly for intent, dialog acts, and speaker role 
denti cation, providing evidence that our proposed methodology can effectively encode the context. We further observe a performance gain using  order agnostic training , as de ned in Eq. 
9, which con rms our hypothesis that the ordering of SLU tags while training can impact model performance. 
While our order agnostic training framework increases the training time as it requires the computation of CTC loss over all possible permutations of SLU tags (Eq. 9), this increase in training time is not very signi cant. 
We also experimented with  previous SLU tag condition  (i.e. attending to r1:c 1in Eq. 5) using only utterance-level annotated tags, i.e., dialog act, emotion, and speaker role, to encode context. 
This model achieves a performance gain in intent classi cation and dialog act prediction with comparable results on predicting other SLU tasks. We plan to further investigate the utility of SLU tags from previous spoken utterances in future work. 
Our joint E2E model with context also achieved similar performance to  Separate E2E model with context [18] with no knowledge distillation from a text-based system, although the two models have different data preparation setups. 
To better understand our joint model, we also perform an ablation study in Table 2 and infer that using intermediate CTC loss can stabilize training and improve the performance of our joint model. 5.2. 
Using oracle context To understand the impact of errors in ASR transcripts , we compute results of our joint E2E model with oracle context, i.e., using ground truth transcripts as  s1:c 1in Eq. 5. 
Table 3 shows only a slight increase in performance compared with the model that uses ASR transcripts to encode context, indicating that it is robust to ASR errors. 4The Word Error Rate (WER) of ASR transcripts is 11.7. 
5Task-speci c baselines are executed one after other in sequential order.Order Training (%) Inference (%) Sample Training Utterance (sr, ic, da, er) 35.0 40.1 i ve ordered your replacement debit card (da, ic, sr, er) 27.9 36. 
0 uh well james my name is jennifer jones and i would like to reset my password (ic, sr, da, er) 16.4 6.8 thank you for calling have a great day (ic, da, sr, er) 14.5 7.5 you too bye (sr, da, ic, er) 6.2 9. 
5 hello this is harper valley national bank my name is patricia how can i help you today Table 4 : Results showcasing the optimal order of SLU tags found during training using Eq. 9 and predicted order of SLU tags during inference. 
We report sample training utterance for each tag order. 5.3. Predicted ordering of SLU tags We analyze the optimal order of SLU tags found during training using Eq. 
9 in Table 4 and observe that optimal permutation sequence are among only 5 of 4! possible permutations of SLU tags. We compare it to the tag order predicted by our joint E2E model with context and order agnostic training. 
The predicted order of SLU tags during inference has a similar trend to the optimal order of SLU tags found during training, with (sr, ic, da, er) and (da, ic, sr, er) being the two most common orders. 
Further the training utterances with optimal order (sr, da, ic, er) are usually spoken at the start of the conversation and are mainly greetings, as shown in Table 4. 
We hypothesize that this tag order is optimal for these utterances as it is challenging to detect the intent of the conversation from these greeting utterances, and hence intent is predicted after the model decodes dialogue act and speaker role tags. 
This gives model the ability to incorporate the dependency on these tags to better extract the intent of current utterance. Our analysis is similar for other tag orders and we infer that we can validate the optimal tag order found for training utterances. 
After a more  ne-grained analysis, we observe that test utterances that are similar to training utterances with optimal tag order are also predicted by the joint model in the same tag order during inference. 
This  nding provides initial evidence to the hypothesis that the joint model learns to automatically predict in the optimal tag order during inference using order agnostic training. 
Based on this interesting insight and performance gains observed in Table 1, we recommend future studies on jointly modeling different SLU tasks to incorporate order agnostic training in their framework. 
We propose a novel model architecture that can jointly model intent classi cation, dialogue act prediction, speaker role identi cation and emotion recognition with full integration of dialog history in spoken conversations. 
Our results show that the joint model achieves comparable performance to task-speci c models with the additional bene ts of low latency and lightweight inference. 
Our joint model can also successfully capture dialog context to improve the prediction performance of all SLU tasks signi cantly. We experimentally con rm that order agnostic training can further enhance performance. 
In future work, we plan to explore E2E integration of dialog context as well as knowledge distillation from a text-based system. 
This work used the Extreme Science and Engineering Discovery Environment (XSEDE) [42], which is supported by NSF grant number ACI-1548562. 
Speci cally, it used the Bridges system [43], which is supported by NSF award number ACI-1445606, at the Pittsburgh Supercomputing Center (PSC).8. REFERENCES [1] R. De Mori, F. Bechet, D. Hakkani-Tur, et al. 
,  Spoken language understanding,  IEEE Signal Processing Magazine , vol. 25, no. 3, [2] J. F. Allen, D. K. Byron, M. Dzikovska, et al. ,  Toward conversational human-computer interaction,  AI magazine , vol. 22, no. 4, pp. 27 27, [3] B. Agrawal, M. 
M  uller, M. Radfar, et al. ,  Tie your embeddings down: Cross-modal latent spaces for end-to-end spoken language understanding,  arXiv preprint arXiv:2011.09044 , 2020. [4] S. Cha, W. Hou, H. Jung, et al. 
,  Speak or chat with me: End-to-end spoken language understanding with  exible inputs,  arXiv , 2021. [5] A. Bhargava, A. Celikyilmaz, D. Hakkani-T  ur,et al. ,  Easy contextual intent prediction and slot detection,  in Proc. ICASSP , 2013, [6] P. 
Xu and R. Sarikaya,  Contextual domain classi cation in spoken language understanding systems using recurrent neural network,  in Proc. ICASSP , 2014, pp. 136 140. [7] P. Colombo, E. Chapuis, M. Manica, et al. 
,  Guiding attention in sequence-to-sequence models for dialogue act prediction,  in Proceedings of the AAAI Conference on Arti cial Intelligence , vol. 34, [8] C. Bothe, C. Weber, S. Magg, et al. 
,  A context-based approach for dialogue act recognition using simple recurrent neural networks,  arXiv preprint arXiv:1805.06280 , 2018. [9] V . Raheja and J. 
Tetreault,  Dialogue act classi cation with contextaware self-attention,  arXiv preprint arXiv:1904.02594 , 2019. [10] S. Kim, S. Dalmia, and F. Metze,  Gated embeddings in e2e speech recognition for conversational-context fusion,  in Proc. 
ACL , 2019, [11]  ,  Cross-Attention End-to-End ASR for Two-Party Conversations, in Proc. Interspeech , 2019, pp. 4380 4384. [12] T. Hori, N. Moritz, C. Hori, et al. ,  Transformer-based long-context end-to-end speech recognition.,  in Proc. 
Interspeech , 2020, pp. 5011 [13] Y .-N. Chen, D. Hakkani-T  ur, G. T  ur,et al. ,  End-to-end memory networks with knowledge carryover for multi-turn spoken language understanding., in Proc. Interspeech , 2016, pp. 3245 3249. [14] C. Sankar, S. 
Subramanian, C. Pal, et al. ,  Do neural dialog systems use the conversation history effectively? an empirical study,  arXiv preprint arXiv:1906.01603 , 2019. [15] A. Bapna, G. Tur, D. Hakkani-Tur, et al. 
,  Sequential dialogue context modeling for spoken language understanding,  arXiv preprint arXiv:1705.03455 , 2017. [16] V . Vukoti  c, C. Raymond, and G. 
Gravier,  A step beyond local observations with a dialog aware bidirectional gru network for spoken language understanding,  in Proc. Interspeech , 2016, pp. 3241 3244. [17] J. Ganhotra, S. Thomas, H.-K. J. Kuo, et al. 
,  Integrating dialog history into end-to-end spoken language understanding systems,  arXiv preprint arXiv:2108.08405 , 2021. [18] V . Sunder, S. Thomas, H.-K. J. Kuo, et al. 
,  Towards end-to-end integration of dialog history for improved spoken language understanding, in Proc. ICASSP , 2022, pp. 7497 7501. [19] N. Tomashenko, C. Raymond, A. Caubri `ere,et al. 
,  Dialogue history integration into end-to-end signal-to-concept spoken language understanding systems,  in Proc. ICASSP , 2020, pp. 8509 8513. [20] S. Arora, S. Dalmia, X. Chang, et al. 
,  Two-pass low latency endto-end spoken language understanding,  in Proc. Interspeech , 2022, [21] T. O Malley, A. Narayanan, Q. Wang, et al. 
,  A conformer-based asr frontend for joint acoustic echo cancellation, speech enhancement and speech separation,  in Proc. ASRU , 2021, pp. 304 311.[22] W. Zhang, C. Boeddeker, S. Watanabe, et al. 
,  End-to-end dereverberation, beamforming, and speech recognition with improved numerical stability and advanced frontend,  in Proc. ICASSP , 2021, pp. 6898 [23] S.-Y . Chang, R. Prabhavalkar, Y . He, et al. 
,  Joint endpointing and decoding with end-to-end models,  in Proc. ICASSP , 2019, pp. 5626 [24] J. Lee and S. Watanabe,  Intermediate loss regularization for CTCbased speech recognition,  in Proc. ICASSP , 2021, pp. 6224 6228. [25] M. Wu, J. Nafziger, A. 
Scodary, et al. ,  Harpervalleybank: A domainspeci c spoken dialog corpus,  arXiv preprint arXiv:2010.13929 , [26] S. Arora, S. Dalmia, P. Denisov, et al. ,  Espnet-slu: Advancing spoken language understanding through espnet,  in Proc. 
ICASSP , 2022, [27] J. Devlin, M. Chang, K. Lee, et al. ,  BERT: pre-training of deep bidirectional transformers for language understanding,  in Proc. NAACLHLT, 2019, pp. 4171 4186. [28] S. Kim, T. Hori, and S. 
Watanabe,  Joint CTC-attention based end-toend speech recognition using multi-task learning,  in Proc. ICASSP , [29] C. Weng, D. Yu, M. L. Seltzer, et al. 
,  Deep neural networks for single-channel multi-talker speech recognition,  IEEE/ACM Transactions on Audio, Speech, and Language Processing , vol. 23, no. 10, [30] J. R. Hershey, Z. Chen, J. Le Roux, et al. 
,  Deep clustering: Discriminative embeddings for segmentation and separation,  in Proc. ICASSP , 2016, pp. 31 35. [31] D. Yu, M. Kolb k, Z.-H. Tan, et al. 
,  Permutation invariant training of deep models for speaker-independent multi-talker speech separation, inProc. ICASSP , 2017, pp. 241 245. [32] X. Chang, Y . Qian, K. Yu, et al. 
,  End-to-end monaural multi-speaker asr system without pretraining,  in Proc. ICASSP , 2019, pp. 6256 [33] A. Deoras, R. Sarikaya, G. Tur, et al. ,  Joint decoding for speech recognition and semantic tagging,  in Proc. Interspeech , 2012, [34] A. 
Paszke, S. Gross, F. Massa, et al. ,  Pytorch: An imperative style, high-performance deep learning library,  Proc. NeurIPS , vol. 32, [35] S. Watanabe, T. Hori, S. Karita, et al. ,  ESPnet: End-to-end speech processing toolkit,  in Proc. 
Interspeech , 2018, pp. 2207 2211. [36] A. Gulati, J. Qin, C. Chiu, et al. ,  Conformer: Convolution-augmented transformer for speech recognition,  in Proc. Interspeech , 2020, [37] P. Guo, F. Boyer, X. Chang, et al. 
,  Recent developments on espnet toolkit boosted by conformer,  in Proc. ICASSP , 2021, pp. 5874 [38] S. Chen, C. Wang, Z. Chen, et al. ,  WavLM: Large-scale selfsupervised pre-training for full stack speech processing,  arXiv preprint arXiv:2110. 
13900 , 2021. [39] A. Vaswani, N. Shazeer, N. Parmar, et al. ,  Attention is all you need, Proc. NeurIPS , vol. 30, pp. 5998 6008, 2017. [40] S. Karita, N. Chen, T. Hayashi, et al. 
,  A comparative study on transformer vs RNN in speech applications,  in Proc. ASRU , 2019, [41] T. Wolf, L. Debut, V . Sanh, et al. ,  Transformers: State-of-the-art natural language processing,  in Proc. EMNLP , 2020, pp. 38 45. [42] J. Towns, T. 
Cockerill, M. Dahan, et al. ,  XSEDE: Accelerating scienti c discovery,  Computing in Science & Engineering , vol. 16, no. 5, pp. 62 74, 2014.[43] N. A. Nystrom, M. J. Levine, R. Z. Roskies, et al. 
,  Bridges: A uniquely  exible HPC resource for new communities and data analytics, in Proc. XSEDE Conference: Scienti c Advancements Enabled by Enhanced Cyberinfrastructure , 2015. 