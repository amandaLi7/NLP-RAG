{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import regex\n",
    "\n",
    "\n",
    "# Normalization and score functions from SQuAD evaluation script https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
    "def normalize_answer(s: str) -> str:\n",
    "    def remove_articles(text):\n",
    "        return regex.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def em(prediction, ground_truth, normalize_fn):\n",
    "    return float(normalize_fn(prediction) == normalize_fn(ground_truth))\n",
    "\n",
    "\n",
    "def f1_and_recall(prediction, ground_truth, normalize_fn):\n",
    "    prediction_tokens = normalize_fn(prediction).split()\n",
    "    ground_truth_tokens = normalize_fn(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0, 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, recall\n",
    "\n",
    "# returns the f1 score and recall score for one question/answer\n",
    "def f1_recall_score(prediction, ground_truths, normalize_fn: Callable[[str], str] = lambda x: x):\n",
    "    result = [f1_and_recall(prediction, gt, normalize_fn) for gt in ground_truths]\n",
    "    unzip = list(zip(*result))\n",
    "    return max(unzip[0]), max(unzip[1])\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truths, normalize_fn: Callable[[str], str] = lambda x: x):\n",
    "    return max([em(prediction, gt, normalize_fn) for gt in ground_truths])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "exact_score = exact_match_score('hello', ['hi', 'hello hello', 'hello <> '], normalize_answer)\n",
    "print(exact_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.5, 1.0)\n"
     ]
    }
   ],
   "source": [
    "variable_name = f1_recall_score('hello bye garbage', ['hi', 'hello hello', 'hello <> ', 'bye'], normalize_answer)\n",
    "print(variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns exact match probability over all answers, f1 score average, and recall score average over all Q/A pairs\n",
    "def total_score(predictions_file, ground_truths_file):\n",
    "    reference_answers = open(ground_truths_file, 'r')\n",
    "    ref = reference_answers.readlines()\n",
    "\n",
    "    rag_answers = open(predictions_file, 'r')\n",
    "    rag = rag_answers.readlines()\n",
    "    assert(len(rag) == len(ref))\n",
    "\n",
    "    exact_match_sum = 0.0\n",
    "    f1_sum = 0.0\n",
    "    recall_sum = 0.0\n",
    "    for pred, truth in zip(rag, ref):\n",
    "        ground_truths = truth.split(';')\n",
    "        exact_match_sum += exact_match_score(pred, ground_truths, normalize_answer)\n",
    "        f1, recall = f1_recall_score(pred, ground_truths, normalize_answer)\n",
    "        f1_sum += f1\n",
    "        recall_sum += recall\n",
    "\n",
    "    return exact_match_sum/len(rag), f1_sum/len(rag), recall_sum/len(rag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 1.0, 1.0)\n"
     ]
    }
   ],
   "source": [
    "print(total_score('system_outputs/system_output1.txt', 'data/test/reference_answers.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
