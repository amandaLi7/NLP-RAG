{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "executionInfo": {
     "elapsed": 13570,
     "status": "ok",
     "timestamp": 1710110042978,
     "user": {
      "displayName": "Vashisth Tiwari",
      "userId": "14058204908965748495"
     },
     "user_tz": 240
    },
    "id": "VbUAaUUtf1mg",
    "outputId": "f8419cd9-3b78-43bf-cc73-772c118d60fe",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy==1.26.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.26.4)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.26.4'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# !pip install ctransformers[cuda]\n",
    "!pip3 install numpy==1.26.4\n",
    "import numpy as np\n",
    "np.__version__\n",
    "\n",
    "# restart after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "s96leWz22vbg",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "# from dotenv import load_dotenv\n",
    "# load_dotenv('.env')\n",
    "# hf_api = os.getenv('HF_API')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Mar 13 17:24:22 2024       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.104.12             Driver Version: 535.104.12   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  Tesla T4                       On  | 00000000:00:1E.0 Off |                    0 |\n",
      "| N/A   25C    P8               9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|  No running processes found                                                           |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 18031,
     "status": "ok",
     "timestamp": 1709884858907,
     "user": {
      "displayName": "Vashisth Tiwari",
      "userId": "14058204908965748495"
     },
     "user_tz": 300
    },
    "id": "s-iSASxn3CUu",
    "outputId": "6c5d8662-e6ed-45db-fcea-54ea512ec45d",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.38.0\n",
      "  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.38.0)\n",
      "  Downloading huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.38.0)\n",
      "  Downloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.0)\n",
      "  Downloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.38.0)\n",
      "  Downloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.38.0) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.38.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.38.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.38.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.38.0) (2023.11.17)\n",
      "Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m346.4/346.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m774.0/774.0 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m31.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m44.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.21.4 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.0\n",
      "Collecting unstructured\n",
      "  Downloading unstructured-0.12.6-py3-none-any.whl.metadata (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ragatouille\n",
      "  Downloading ragatouille-0.0.7.post10-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting backoff==2.2.1 (from unstructured)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (4.12.3)\n",
      "Collecting certifi==2024.2.2 (from unstructured)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting chardet==5.2.0 (from unstructured)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (3.3.2)\n",
      "Requirement already satisfied: click==8.1.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (8.1.7)\n",
      "Requirement already satisfied: dataclasses-json==0.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (0.6.4)\n",
      "Collecting dataclasses-json-speakeasy==0.5.11 (from unstructured)\n",
      "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting emoji==2.10.1 (from unstructured)\n",
      "  Downloading emoji-2.10.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting filetype==1.2.0 (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: idna==3.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (3.6)\n",
      "Requirement already satisfied: joblib==1.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (1.3.2)\n",
      "Collecting jsonpath-python==1.0.6 (from unstructured)\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting langdetect==1.0.9 (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m20.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lxml==5.1.0 (from unstructured)\n",
      "  Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting marshmallow==3.20.2 (from unstructured)\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (1.0.0)\n",
      "Collecting nltk==3.8.1 (from unstructured)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Requirement already satisfied: numpy==1.26.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (1.26.4)\n",
      "Requirement already satisfied: packaging==23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (23.2)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (2.8.2)\n",
      "Collecting python-iso639==2024.2.7 (from unstructured)\n",
      "  Downloading python_iso639-2024.2.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting python-magic==0.4.27 (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting rapidfuzz==3.6.1 (from unstructured)\n",
      "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: regex==2023.12.25 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (2023.12.25)\n",
      "Requirement already satisfied: requests==2.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (2.31.0)\n",
      "Requirement already satisfied: six==1.16.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (1.16.0)\n",
      "Requirement already satisfied: soupsieve==2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (2.5)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (0.9.0)\n",
      "Collecting tqdm==4.66.2 (from unstructured)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions==4.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect==0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (0.9.0)\n",
      "Collecting unstructured-client==0.18.0 (from unstructured)\n",
      "  Downloading unstructured_client-0.18.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: urllib3==1.26.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (1.26.18)\n",
      "Collecting wrapt==1.16.0 (from unstructured)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting aiohttp==3.9.1 (from ragatouille)\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting colbert-ai==0.2.19 (from ragatouille)\n",
      "  Downloading colbert-ai-0.2.19.tar.gz (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting faiss-cpu<2.0.0,>=1.7.4 (from ragatouille)\n",
      "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ragatouille) (0.1.12)\n",
      "Requirement already satisfied: langchain_core<0.2.0,>=0.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ragatouille) (0.1.31)\n",
      "Collecting llama-index<0.10.0,>=0.9.24 (from ragatouille)\n",
      "  Downloading llama_index-0.9.48-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting onnx<2.0.0,>=1.15.0 (from ragatouille)\n",
      "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting ruff<0.2.0,>=0.1.9 (from ragatouille)\n",
      "  Downloading ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: sentence-transformers<3.0.0,>=2.2.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ragatouille) (2.5.1)\n",
      "Collecting srsly==2.4.8 (from ragatouille)\n",
      "  Downloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ragatouille) (2.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ragatouille) (4.38.0)\n",
      "Collecting voyager<3.0.0,>=2.0.2 (from ragatouille)\n",
      "  Downloading voyager-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (4.0.3)\n",
      "Collecting bitarray (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting datasets (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: flask in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (3.0.1)\n",
      "Collecting git-python (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading git_python-1.0.3-py2.py3-none-any.whl.metadata (331 bytes)\n",
      "Collecting python-dotenv (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting ninja (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (1.12.0)\n",
      "Requirement already satisfied: ujson in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (5.9.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.3 (from srsly==2.4.8->ragatouille)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.0.28)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.28 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.0.28)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.1.24)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.6.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (4.2.0)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (2023.12.2)\n",
      "Collecting httpx (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (3.2.1)\n",
      "Collecting openai>=1.1.0 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (1.5.3)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (4.25.2)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.21.4)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (10.2.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.1->ragatouille) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.1->ragatouille) (1.12)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.1->ragatouille) (3.1.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (1.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.0->ragatouille) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.0->ragatouille) (3.9.15)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (2.16.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.0->ragatouille) (3.0.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (15.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.3.8)\n",
      "Collecting xxhash (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.70.16)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (3.0.1)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (1.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch<3.0.0,>=2.0.1->ragatouille) (2.1.4)\n",
      "Collecting gitpython (from git-python->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->llama-index<0.10.0,>=0.9.24->ragatouille) (2023.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch<3.0.0,>=2.0.1->ragatouille) (1.3.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython->git-python->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Downloading unstructured-0.12.6-py3-none-any.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Downloading chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.4/199.4 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
      "Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m53.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.0/8.0 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m18.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m28.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Downloading unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
      "Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m80.3/80.3 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading ragatouille-0.0.7.post10-py3-none-any.whl (35 kB)\n",
      "Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m18.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.0/493.0 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index-0.9.48-py3-none-any.whl (15.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m37.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading voyager-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m26.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: langdetect, colbert-ai\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=4d6c5bf2946cb56a823f0716718cd349a08d7c861527cff2785d28c41f6d49bc\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "  Building wheel for colbert-ai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for colbert-ai: filename=colbert_ai-0.2.19-py3-none-any.whl size=114761 sha256=7d1e0787b0b06c321fcea78fe3b78c88e82f6a5a41adeadee619350344479692\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/90/b9/63/d4fc276c73c42ef7fc1065a26cf87e5a1cf56ef6498cbfbe5d\n",
      "Successfully built langdetect colbert-ai\n",
      "Installing collected packages: ninja, filetype, dirtyjson, bitarray, xxhash, wrapt, voyager, tqdm, smmap, ruff, rapidfuzz, python-magic, python-iso639, python-dotenv, pyarrow-hotfix, onnx, marshmallow, lxml, langdetect, jsonpath-python, h11, faiss-cpu, emoji, distro, chardet, certifi, catalogue, backoff, srsly, nltk, httpcore, gitdb, deprecated, dataclasses-json-speakeasy, aiohttp, unstructured-client, tiktoken, httpx, gitpython, unstructured, openai, git-python, datasets, llama-index, colbert-ai, ragatouille\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.66.1\n",
      "    Uninstalling tqdm-4.66.1:\n",
      "      Successfully uninstalled tqdm-4.66.1\n",
      "  Attempting uninstall: onnx\n",
      "    Found existing installation: onnx 1.13.1\n",
      "    Uninstalling onnx-1.13.1:\n",
      "      Successfully uninstalled onnx-1.13.1\n",
      "  Attempting uninstall: marshmallow\n",
      "    Found existing installation: marshmallow 3.21.1\n",
      "    Uninstalling marshmallow-3.21.1:\n",
      "      Successfully uninstalled marshmallow-3.21.1\n",
      "  Attempting uninstall: certifi\n",
      "    Found existing installation: certifi 2023.11.17\n",
      "    Uninstalling certifi-2023.11.17:\n",
      "      Successfully uninstalled certifi-2023.11.17\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.9.3\n",
      "    Uninstalling aiohttp-3.9.3:\n",
      "      Successfully uninstalled aiohttp-3.9.3\n",
      "Successfully installed aiohttp-3.9.1 backoff-2.2.1 bitarray-2.9.2 catalogue-2.0.10 certifi-2024.2.2 chardet-5.2.0 colbert-ai-0.2.19 dataclasses-json-speakeasy-0.5.11 datasets-2.18.0 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 emoji-2.10.1 faiss-cpu-1.8.0 filetype-1.2.0 git-python-1.0.3 gitdb-4.0.11 gitpython-3.1.42 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 jsonpath-python-1.0.6 langdetect-1.0.9 llama-index-0.9.48 lxml-5.1.0 marshmallow-3.20.2 ninja-1.11.1.1 nltk-3.8.1 onnx-1.15.0 openai-1.13.3 pyarrow-hotfix-0.6 python-dotenv-1.0.1 python-iso639-2024.2.7 python-magic-0.4.27 ragatouille-0.0.7.post10 rapidfuzz-3.6.1 ruff-0.1.15 smmap-5.0.1 srsly-2.4.8 tiktoken-0.6.0 tqdm-4.66.2 unstructured-0.12.6 unstructured-client-0.18.0 voyager-2.0.2 wrapt-1.16.0 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.38.0\n",
    "!pip install -q torch accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl\n",
    "!pip install unstructured ragatouille\n",
    "# reranker\n",
    "from ragatouille import RAGPretrainedModel\n",
    "# !nvcc --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 275993,
     "status": "ok",
     "timestamp": 1710110355792,
     "user": {
      "displayName": "Vashisth Tiwari",
      "userId": "14058204908965748495"
     },
     "user_tz": 240
    },
    "id": "-hsCAcjxIP19",
    "outputId": "128ca964-5cf2-4f00-cfa4-2ec234a65bdd",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting llama-cpp-python\n",
      "  Downloading llama_cpp_python-0.2.56.tar.gz (36.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.9/36.9 MB\u001b[0m \u001b[31m272.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Installing backend dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting typing-extensions>=4.5.0 (from llama-cpp-python)\n",
      "  Downloading typing_extensions-4.10.0-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting numpy>=1.20.0 (from llama-cpp-python)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m315.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n",
      "  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting jinja2>=2.11.3 (from llama-cpp-python)\n",
      "  Downloading Jinja2-3.1.3-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n",
      "  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
      "Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m260.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m383.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m383.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n",
      "Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
      "Building wheels for collected packages: llama-cpp-python\n",
      "  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.56-cp310-cp310-manylinux_2_26_x86_64.whl size=22630186 sha256=34e1d2f36b761e8d687bfce0d753db1043790cc70f82eb6974d00706e18f11e0\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-7a43h70t/wheels/e5/09/9d/c413053f6258cb2546cc792418c595e276f9efd5db31a80377\n",
      "Successfully built llama-cpp-python\n",
      "Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.9.0\n",
      "    Uninstalling typing_extensions-4.9.0:\n",
      "      Successfully uninstalled typing_extensions-4.9.0\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "  Attempting uninstall: MarkupSafe\n",
      "    Found existing installation: MarkupSafe 2.1.4\n",
      "    Uninstalling MarkupSafe-2.1.4:\n",
      "      Successfully uninstalled MarkupSafe-2.1.4\n",
      "  Attempting uninstall: jinja2\n",
      "    Found existing installation: Jinja2 3.1.3\n",
      "    Uninstalling Jinja2-3.1.3:\n",
      "      Successfully uninstalled Jinja2-3.1.3\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "unstructured 0.12.6 requires typing-extensions==4.9.0, but you have typing-extensions 4.10.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.3 llama-cpp-python-0.2.56 numpy-1.26.4 typing-extensions-4.10.0\n"
     ]
    }
   ],
   "source": [
    "# this will take time :c\n",
    "!CUDACXX=/usr/local/cuda-12.1/bin/nvcc CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all-major\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir --force-reinstall --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "4ItWW1Es3OX_",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix colab error: https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "CB6DVShA2vbh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "GrqalAohmDnp",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "m1Dm1uHk2vbh",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "3ti-G7vMdASy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pip list --outdated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pO3SVhxSshpo"
   },
   "source": [
    "# Specify the model/versions, etc.\n",
    "## also specify the knowledge base file and the reference answer directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365,
     "referenced_widgets": [
      "800014039af04d66adc94131a8ab568e",
      "f741891d16434e959971b6331ab7a09e",
      "20619dd814a245cb96eed1ffd6d23793",
      "f91289ae253446ed90b7c9260d85ac2c",
      "3088fb3c8cc74d7c97d9cbacc5b20c03",
      "234ab5cd38b14aec8426fcc28dfc4952",
      "b8d486c46bbc4a6abdb3b0af3dbc4c04",
      "be42d78fa32c44f4ba983327b2f2b0ae",
      "d696cc22200743caaaa0d70de5f29eaf",
      "2d7b6a6ffb3849ec9828e24f4e193840",
      "718e83bf80b74bbb9e28a9a2473ab8db",
      "9561e20747684a1a878a637d00c946fa",
      "b5cfff7bf253443682d8051953740ad2",
      "d2b20654e6aa49359b678e198cc55a73",
      "5fca46bd83bd4f95ae25eecf342ff20f",
      "63a47d5282db441a8e5732ab3266af33",
      "8d567a74de4843a4b50292512c7d0ebe",
      "e4710695ec874eada402065fe800edd5",
      "9b8ebc99d0ed4d9fb6f873f06a638318",
      "2750e13775ad4cc5a1ca97abd23a7fa4",
      "fc979bda6f3c486e8108945371fee26e",
      "5de0a6a267a24518ac2857b47c4d2d4e",
      "fc6ba90ca08747e1bace0a55d1eeed55",
      "5e9df76d9eb04c3cacc9caf4edacb913",
      "091d085339a14a0ab5a23a60b24508f6",
      "1191b9f6ca7e491c91a54648d9a95740",
      "b106e671bf2040ff85286339391cacce",
      "25d444efba4c4f8ea2a15c4a197e83fa",
      "fa2721e59a714068a0a5918cd7ec3246",
      "8041af99466c478ab32dcb07ff597d41",
      "ec25bd06a00e4e2cab1b9f62905449ca",
      "d3e5436855af42a5b2f170b44388e4a8",
      "195ac8d683264e9999a419b94e60bdc1",
      "08e5c13a33c442d69db6a4e5cb1aefd8",
      "486f19f393194214820b2e4719793822",
      "82baa3fb82e24827b49b6bda22f72d78",
      "a2c29ede70564d5b879b4084453d753f",
      "bb8b2e9c41ea45dfa6fe7dff8a934fa2",
      "6257ec67ee0043adab5a9e24a4f709a8",
      "df62dad6b54444659f23d7cca3ddaa70",
      "2c938e4d55734c248ffb292415aa5f5b",
      "5582c1c8145b4fe786ba967188680aa7",
      "b5693df9d1cf458e9c7dabfba2331102",
      "4c7a193e4d3349dc8c2ff58c7137e77e",
      "c1225405706548aa86f466345ba78795",
      "f9124d67aef64019a437baa8eef8e54e",
      "7a6d685007704814a51a027b060ba1b0",
      "c87f3b6818934457af1827e01f4924ba",
      "cfab7d69b2634082805f101090dcbd04",
      "9553bb69e7c5479ebbf73787d3fc8b14",
      "0eae622975df4cfaaa2e54cde12fecc2",
      "b599c45a81df44619d92a3ac8ad41b0f",
      "c2cf8f815146436e86373dd6f02e94aa",
      "b02abbcd98d94732961735247946d2d4",
      "666d42ef1a74402fb13ea79db36c1287",
      "c28b40c65c1f450988dfb834ccbf1526",
      "1257cd90321540d8b5530c35b672ac38",
      "365523a881584181973972b7df86c3eb",
      "6023241ffbf740ec871828d6276cccb8",
      "863ed2e3ac1e4670b7850cf4deafc867",
      "890443b9c63d42b597b1fce37ed2612f",
      "e7bc53149d98433785d628cd3783cbdb",
      "3a1808fea5884186924c50c1d44cdb2b",
      "6cb72ab4081a44fd8462b7d58da7236c",
      "d9d79532e89447e1ae295ea296d33dbf",
      "4663ab81441947f7bd1b86a08d82a6b2",
      "b14e572db35545c58c413b5313c230d0",
      "26f3de4a37ed42c783b8cc8496a8830e",
      "25ea5fce0d51425ca66647c86a69054d",
      "1d5d56be1f3e4e06ad05b7a61c073773",
      "0e6e63ccb1164deab11b438e7059d69f",
      "e5ff1078ea2d454ba88d92b978bee13f",
      "97d0ee251daa487db840ba561a36d4a8",
      "572f1df5c7904ccf939d45356997f7a7",
      "abe064fd2d2943cc9658631022e7dfbe",
      "37bbe9f296674040a1e4e6f414645aee",
      "0567916024824a5cb153ee96ae90bcfd"
     ]
    },
    "executionInfo": {
     "elapsed": 6034,
     "status": "ok",
     "timestamp": 1710110390648,
     "user": {
      "displayName": "Vashisth Tiwari",
      "userId": "14058204908965748495"
     },
     "user_tz": 240
    },
    "id": "SscMCDYU2vbh",
    "outputId": "83e0ecb7-0a38-489a-ec55-a9c7d22fc51f",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1277d973fa74d958e55f71f6faf3994",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "artifact.metadata:   0%|          | 0.00/1.63k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd2845de7c0348978110f53e06f29c94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a698376dc3b4178b3c7b456f2d05647",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f9bf535a854b13bd3dd16d528164e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/405 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "209bb86a0dee44aaa460252b9b2555ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04ba1f1a34b845b99570d496c46acc76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "79fee611cb504de49e6881e348dcae60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EMBEDDING_MODEL = \"thenlper/gte-base\"\n",
    "RERANKER_MODEL = \"colbert-ir/colbertv2.0\"\n",
    "RERANKER = RAGPretrainedModel.from_pretrained(RERANKER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 337,
     "referenced_widgets": [
      "cb37e6988db144ac9e6b615b71705c3d",
      "58f1227b067740c99856d28d1b3535f0",
      "a68946ee535e4693955f8292b47c5b6c",
      "72f6e2507a5c49338610f22446087687",
      "f7afe5336f9645bbaa3da495dad04c6f",
      "f4c9a624ca3342adbdda81d0c505d540",
      "7b9187f7562945ba813032f73d5fed96",
      "755d31fc51614b78a6798a1f45d66a29",
      "46b7f140d4134097a8f6c7dd41d9acd3",
      "784f6571b15c4ac19ca2e1b1a913f318",
      "42c591eaee9f42f8ad965372898e3bdd",
      "7a550bf2dc964d0caafc8aed54d187a6",
      "5bf549707f604a99a4743166718cd720",
      "bac9a59f6a1244d8a6e37795e0383e30",
      "a113bb38b1f7458cae5e7825d6a65b3c",
      "cda8adeebae9484c9399422c870aa7cf",
      "b524e9e4f75a45aba71a8a2b68557700",
      "fdbe596b18164389a51d860b24f3b15b",
      "e55c826128994d0a8b67885f40574fae",
      "836f49316d114d37846143df795ba15a",
      "1ad12046944a468297e3c982794251ef",
      "c0d3f59319544daa858e76bf490ac735",
      "f8c3f5363e824e338cf4b7ea0f09b92a",
      "6aaf81405fa24b0e93a858d609b85036",
      "4977730998754c6d82831b47a647b428",
      "9dc189ff36034264b081ca782c0bc1ed",
      "91b7390bb1d94bd6be84496ed73052ee",
      "7e04af96adae41a6b05ecc7bfe5a9fd9",
      "b13c2b7166784d0f8d8a1ee231fe3235",
      "761b0c7bec0f4ea98fff4469f3e147e1",
      "2f225874a7174aae9695b521d33eb563",
      "0e167716cc6941e2aaa5e85a476f1b1a",
      "9ce3da5248a34146945e40a660ed455d",
      "6584fa0193f0433ead447e60d2c82607",
      "47d8987c94434ddb89281035f30e9d4f",
      "cf1bd11c82b44b99ac40e8abbec3785f",
      "7e069a6942cb4cb09ef6aa3661844f88",
      "7e0b008fb7294404a73b8341c1230125",
      "45c9710ebd6e45ddaa72f349380c5e16",
      "9defae4a31724cc1b6c0128b76bae404",
      "3332c9a35d2e40ac860233ebfb8c64c7",
      "e0686b63c9374bcda05c70aeffbcda03",
      "f5553c33eefd47eea49e0e3901bf51f5",
      "fc2843b06e0f4b6ca2a7d1d5fcde23d6",
      "7b70f0bd5409433e93719cd05441e473",
      "83665867accd4853b6efe68a06f7f403",
      "66b76f1f830445e98fb9c1acc1683cde",
      "26f97033c9e5479da8dca6f7dc642ece",
      "6da0a949eaf1499094fa4e264e1929ca",
      "0b946716322e43bb9f4213087c6ee1de",
      "c3cb074231f84c169f31afa3baf84fc9",
      "aee8fac5931844c5a88f651e7f854362",
      "e39705238d474d5a99f0a49832b408ed",
      "45d616297f6d42fc89f2b28c6fc10c90",
      "58bdc5e07c694b4f9fc4e6612c1de4c5",
      "0d0fbbe470054b0b879c1a62c411dc0c",
      "6efb365326c24c339085be28cc203fdc",
      "df4976d8d7ec47e4986f5e9c0395c3a9",
      "e249235d4bb14478a802d3fc362fe215",
      "ea2f3d5138cb4d3eb4b5f50122c42634",
      "0bce7925eb1f4323a482bb90467bbc66",
      "392ee34639024892a83a85f6878b60ef",
      "ba96d1c92aeb4a9584f700869c028ace",
      "c8c4de2dd87b409ab14e3bbd4f8ff777",
      "355092a75fa445ddb58c3c0e6c7c87f7",
      "a77de46019434b2182d8ace90729411d",
      "3f54b0320e054ef4ab7cc33ca97ad75c",
      "13c4cd250f4c49169902a77e3b76727c",
      "9100526f824f4e2298b22f43461c6daa",
      "5e7028e3f8f6469086b2b94066dc9526",
      "6901a356aa2c4361953ba9b44065710b",
      "b215c0274af9444c9265aa576d33f6d5",
      "f46f3a7030a34d3894d97214fb03e1c2",
      "34f2229c854f4695b5aba5d5afe772f5",
      "38344f64512b421aa12ac82fcf4b195d",
      "11e9dc55ddd64166a23b389ef4115bc5",
      "090fd1cdc0694112b57d28e833932350",
      "0c73d54adcb74546b98a15ff898640d2",
      "8cd319e80b28462698e8adb196302802",
      "4016322235e5422687997b6c942b8684",
      "90efeff073a541e3ace582146fd5fd45",
      "4da4cc126514421ebb1cd20eff5544f9",
      "255d439785a54b9b8f37be9730c5b9f9",
      "e3ea567b814a4d949a018e39137923fd",
      "ea9b31c529f0482c99ac5ede1b0c2437",
      "2f08477187154be899c3a1378e703260",
      "183c14e615d6471c9fe4b9a56d13f85a",
      "e15406eedc3b4b10b5d8c32d2ba5f1cd",
      "f82609221b714ae58214a6f6745526b3",
      "6f038781e3a5498a9823999548de18fd",
      "8110d80f4fe94d499834b799c85667ad",
      "e7816626014f47fbb744f707990f3c0b",
      "65617f6c0a3f40f1bb7198eadb9757da",
      "1efc8ad79138462e97c1bf5f00d8a210",
      "4e8321573083413d80d545eb711dabcb",
      "ca2e4e27247949fd96c09ca0084eb8d5",
      "ab3716a57cb1461ca4b89d3e2af60aa1",
      "e7096ed061374902935f3363540caa56",
      "3e583707d8f7457bb3a5433440f3a6cb",
      "200a894d245143b59503f70bbe099655",
      "e68e4e022b7946139920753c55012db2",
      "1d9cc219294748d7a4d06a0ef220d034",
      "61055f305b414a9995dd0093c8dd085b",
      "6793b20c68004ef6806c0719d850ee07",
      "45045e6a12e041e1b2b2713ff59ef938",
      "0f57c3f8ee4d42b3b02aba300b4fe271",
      "bfd5578f1cc3406b9b6e49540918dffd",
      "0fab8e8ae3fd4f47b43ee00df8a80deb",
      "f4388ee32ed9434ea5e756c22fb61002",
      "eac3132f4d9349cfa6f1422686ae7bd5"
     ]
    },
    "executionInfo": {
     "elapsed": 5437,
     "status": "ok",
     "timestamp": 1710110400718,
     "user": {
      "displayName": "Vashisth Tiwari",
      "userId": "14058204908965748495"
     },
     "user_tz": 240
    },
    "id": "zliNdIDo2vbh",
    "outputId": "7302e1a2-7bee-4b0e-8fcd-074783531201",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b148287d39f24f71b46c1fe1a975e85d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dd98b7d6cfa4ab78a25ae1add6aba51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06084062245e481a96f4113cdeefb0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88924bd5bdb4049817c4c22d3b03c82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/618 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c418044188a4692a0c90e6c160c30f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/219M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e444b000f7849f4809429ff61e26a9b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdcb9ff4ccee46728c4b5f12dd62d356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10177ce68ea54471916d45a10968a86f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476c2b40d30644b99fbe212757197ede",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a146669f01f424e8e7c5c80383b1870",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EMBEDDING_MODEL_NAME = EMBEDDING_MODEL\n",
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL_NAME,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 181,
     "status": "ok",
     "timestamp": 1710111573183,
     "user": {
      "displayName": "Vashisth Tiwari",
      "userId": "14058204908965748495"
     },
     "user_tz": 240
    },
    "id": "JIaPbE2th0I5",
    "outputId": "98ab622e-0a7f-4d29-f1ba-af5c6360d978",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/SageMaker\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "0jF7haihucN6",
    "tags": []
   },
   "outputs": [],
   "source": [
    "FAISS_FILE = 'faiss_index_final_new'\n",
    "# num_retrieved_docs: int = 5\n",
    "# num_docs_final: int = 3\n",
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.load_local(FAISS_FILE, embedding_model, allow_dangerous_deserialization = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "pHsA8wuz4eE7",
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82022742eaa04652b15400ce9d28c56c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "llama-2-7b-chat.Q4_K_M.gguf:   0%|          | 0.00/4.08G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_path = \"TheBloke/Llama-2-7B-Chat-GGUF\"\n",
    "model_basename = \"llama-2-7b-chat.Q4_K_M.gguf\"\n",
    "model_path = hf_hub_download(repo_id=model_path, filename=model_basename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i_27tccjshpp"
   },
   "source": [
    "# Reader LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3662,
     "status": "ok",
     "timestamp": 1710110638040,
     "user": {
      "displayName": "Vashisth Tiwari",
      "userId": "14058204908965748495"
     },
     "user_tz": 240
    },
    "id": "7H5oLKT72vbi",
    "outputId": "68c9a4ec-8d72-46c7-994a-55322496e435",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ggml_init_cublas: GGML_CUDA_FORCE_MMQ:   no\n",
      "ggml_init_cublas: CUDA_USE_TENSOR_CORES: yes\n",
      "ggml_init_cublas: found 1 CUDA devices:\n",
      "  Device 0: Tesla T4, compute capability 7.5, VMM: yes\n"
     ]
    }
   ],
   "source": [
    "# https://www.reddit.com/r/LocalLLaMA/comments/1343bgz/what_model_parameters_is_everyone_using/\n",
    "# temperature\t0.7\n",
    "# repetition_penalty=\t1.176\n",
    "# top_k\t= 40\n",
    "# top_p= 0.1\n",
    "\n",
    "# Initialize Large Language Model for answer generation\n",
    "llm_answer_gen = LlamaCpp(\n",
    "streaming = True,\n",
    "model_path = model_path,\n",
    "temperature=.2,\n",
    "n_gpu_layers=-1,\n",
    "top_p=.1,\n",
    "top_k\t= 40,\n",
    "repetition_penalty=\t1.176,\n",
    "verbose=False,\n",
    "n_ctx=2400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1710113142107,
     "user": {
      "displayName": "Vashisth Tiwari",
      "userId": "14058204908965748495"
     },
     "user_tz": 240
    },
    "id": "l8bKX1rXFQBv",
    "outputId": "9d39a40d-28e7-4025-b401-052cc9b2f1c3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "llm_answer_gen.verbose = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TCp6KhE0LXh4"
   },
   "source": [
    "# RAG Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "id": "VhwgRYB467Gt",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# link: https://python.langchain.com/docs/use_cases/chatbots/retrieval\n",
    "\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "from langchain_core.messages import HumanMessage\n",
    "\n",
    "# https://stackoverflow.com/questions/76551067/how-to-create-a-langchain-doc-from-an-str\n",
    "\n",
    "def answer_llama_new(\n",
    "    question: str,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: RAGPretrainedModel,\n",
    "    num_retrieved_docs: int = 10,\n",
    "    num_docs_final: int = 3,\n",
    "    llm: LlamaCpp = llm_answer_gen,\n",
    "    ):\n",
    "\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    relevant_docs_acquired = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    relevant_doc_id = [doc.metadata['source'] for doc in relevant_docs_acquired]\n",
    "\n",
    "    if reranker:\n",
    "        print(\"=> Reranking documents...\")\n",
    "        relevant_docs = [doc.page_content for doc in relevant_docs_acquired]\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "\n",
    "        relevant_docs = [LangchainDocument(page_content=doc[\"content\"],\n",
    "                                           metadata={'source': relevant_doc_id[ doc['result_index']] } ) for doc in relevant_docs]\n",
    "\n",
    "        relevant_docs_content = [doc.page_content for doc in relevant_docs]\n",
    "        relevant_doc_id = [doc.metadata for doc in relevant_docs]\n",
    "\n",
    "    else:\n",
    "        relevant_docs_content = [doc.page_content for doc in relevant_docs_acquired]\n",
    "        relevant_docs = relevant_docs_acquired\n",
    "\n",
    "    relevant_docs_content = relevant_docs_content[:num_docs_final]\n",
    "    relevant_docs_indexed = relevant_docs[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"\\nDocument {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs_content)])\n",
    "\n",
    "    # context_and_question = f\"Please provide concise answers. Extract the most relevant information from the given context while keeping the responses as short as possible. Include multiple correct answers if necessary. If no relevant context, then say 'I dont know'.\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\nAgain we just want main key points in the answers. For example, if the question is 'What is the biggest planet in the solar system?'; the answer should be 'Jupiter'\\n.Answer:\"\n",
    "    \n",
    "    context_and_question = f\"\"\"### Instruction: Your task is to give short answers to the question based on the given context.\n",
    "    \n",
    "    Given the context: \n",
    "    {context}\n",
    "    ---------------\n",
    "    Answer the following question concisely using keywords/key information from the context:\n",
    "    {question}\n",
    "    \n",
    "    Here are the guidelines for answering:\n",
    "    - Identify relevant details to answer the query\n",
    "    - Try to just give the answer without repeating words from the question in the answer\n",
    "    - The answer is simply name, place, time, unless asked for more complicated questions\n",
    "    - Provide a short, precise answer with keywords only; the shorter and more accurate the answer the better\n",
    "    - If the question refers to ‘this semester’, use Spring 2024” and “If the question refers to ‘last semester’, use Fall 202\n",
    "    - You will likley find course numbers in the QA format, but if not it is the number before the title of the class (Number  title)\n",
    "    - For dates about schedule the format is 'Date  Description of the event\n",
    "    - I do not need the explanations or the context in the answer.\n",
    "    - Do not repeat the question in your response; just output the answer.\n",
    "\n",
    "    Now answer the questions based on the context, query, and following the guidelines.\n",
    "   \n",
    "    ### Response:\n",
    "    \"\"\"\n",
    "    # context_and_question = f\"\"\"Please provide concise answers.\n",
    "    # Extract the most relevant information from the given context while keeping the responses as short as possible.\n",
    "    # Include multiple correct answers if necessary.\n",
    "    # If no relevant context, then say 'I dont know'.\n",
    "    # \\n\\nContext:\\n{context}\\n\\nQuestion:\\n{question}\\n\\n\n",
    "    # We just want main key points in the answers.\n",
    "    # For example, if the question is 'What is the biggest planet in the solar system?'; the answer should be 'Jupiter'\n",
    "    # \\n.Answer:\"\"\"\n",
    "\n",
    "\n",
    "#     context_and_question = f\"\"\"### Instruction: Please provide concise answers. Extract the most relevant information from the given context while keeping the responses as short as possible.\n",
    "#     Include multiple correct answers if necessary.\n",
    "    \n",
    "#     Here are the guidelines for answering:\n",
    "#      - Identify relevant details to answer the query\n",
    "#      - Try to just give the answer without repeating words from the question in the answer\n",
    "#      - The answer is simply name, place, time, unless asked for more complicated questions\n",
    "#      - Provide a short, precise answer with keywords only; the shorter and more accurate the answer the better\n",
    "#      - Do not repeat a version of the question in the answer.\n",
    "#      - If the question refers to ‘this semester’, use Spring 2024” and “If the question refers to ‘last semester’, use Fall 202\n",
    "#      - You will likley find course numbers in the QA format, but if not it is the number before the title of the class (Number  title)\n",
    "#      - For dates about schedule the format is 'Date  Description of the event\n",
    "#      - I do not need the explanations or the context in the answer.\n",
    "    \n",
    "#     Context: {context}\n",
    "    \n",
    "#     Answer the following question concisely using keywords/key information from the context:\n",
    "#     Question: {question}\n",
    "    \n",
    "#     We just want main key points in the answers. For example, if the question is 'What is the biggest planet in the solar system?'; the answer should be 'Jupiter'\n",
    "    \n",
    "#     ### Response:\n",
    "#     \"\"\"\n",
    "    print(\"=> Generating Answers...\")\n",
    "    answer = llm.invoke(context_and_question)\n",
    "\n",
    "    if answer is not None:\n",
    "        new_answer = answer.split(': ', 1)\n",
    "        if len(new_answer) > 1:\n",
    "            answer = new_answer[1]\n",
    "        else:\n",
    "            answer = new_answer[0]\n",
    "    # print(f\"Question: {question}\")\n",
    "    \n",
    "    forbidden_list = [\"According to the provided context, \", \"Based on the provided context,\", '\\n']\n",
    "    for phrase in forbidden_list:\n",
    "        answer = answer.replace(phrase, \" \")\n",
    "    answer = answer.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").replace(\"\\n\", \"\").strip()\n",
    "    # print(f\"Answer: {answer}\")\n",
    "    \n",
    "    return answer, relevant_docs_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5VaFc5mg2vbk"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_answer(question,llm=llm_answer_gen, return_relevant_docs = False):\n",
    "    answer, relevant_docs = answer_llama_new(question, KNOWLEDGE_VECTOR_DATABASE,\n",
    "                            reranker=RERANKER, llm = llm)\n",
    "    if return_relevant_docs:\n",
    "        return answer, relevant_docs\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# give the paths\n",
    "QUESTIONS_FILE = 'questions_test_set.txt'\n",
    "OUTPUT_FILE = 'test_set_llama-newinst_final_final_final.txt'\n",
    "\n",
    "# FAISS_FILE = '../faiss_index_author_papers_natural_language' # it's actually a folder but whatever\n",
    "FAISS_FILE = 'faiss_index_total_final_new' # it's actually a folder but whatever\n",
    "\n",
    "EMBEDDING_MODEL = \"thenlper/gte-base\" # make sure this matches whatever was used to create the doc embeddings\n",
    "GENERATOR_MODEL = \"google/flan-t5-large\"\n",
    "RERANKER_MODEL = \"colbert-ir/colbertv2.0\"\n",
    "\n",
    "RERANKER = RAGPretrainedModel.from_pretrained(RERANKER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_answers_all(qfile, afile):\n",
    "    with open(qfile, 'r') as questions_file, open(afile, \"a\") as ans_file:\n",
    "        questions = questions_file.readlines()\n",
    "        for q in questions:\n",
    "            ans = generate_answer(q.strip())  # Remove newline character\n",
    "            ans_file.write(ans + '\\n')\n",
    "            ans_file.flush()  # Flush buffer to ensure content is written immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 53.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 286.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 322.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 298.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 310.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 576.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 337.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 296.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 71.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 280.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 311.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 359.79999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 272.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 314.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 289.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 65.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 333.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 318.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 557.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 362.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 268.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 306.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 333.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 279.99999999999994 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 207.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 565.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 223.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 218.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 257.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 300.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 352.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 252.29999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 239.49999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 302.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 261.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 288.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 302.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 247.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 307.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 335.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 58.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 221.79999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 232.19999999999996 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 258.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 257.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 302.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 322.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 367.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 47.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 304.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 281.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 412.69999999999993 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 595.5999999999999 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 205.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 47.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 222.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 303.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 233.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 324.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 288.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 259.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 224.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 327.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 225.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 327.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 327.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 241.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 54.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 304.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 279.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 366.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 292.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 310.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 56.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 307.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 254.39999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 48.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 247.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 238.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n",
      "WARNING! Your documents have duplicate entries!  This will slow down calculation and may yield subpar results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 319.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 257.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 49.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 322.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 256.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 60.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 333.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 298.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 322.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 59.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 327.29999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 574.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 252.29999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 60.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 278.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 214.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 70.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 47.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 350.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 26.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 65.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 329.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 338.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 381.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 31.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 263.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 65.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 253.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 347.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 240.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 310.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 299.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 304.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 350.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 288.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 318.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 315.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 314.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 335.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 300.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 338.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 338.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 331.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 321.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 355.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 380.8999999999999 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 240.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 321.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 333.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 305.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 277.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 294.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 332.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 312.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 273.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 350.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 331.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 54.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 273.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 506.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 330.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 55.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 417.89999999999986 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 249.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 332.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 349.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 70.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 341.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 340.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 46.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 197.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 305.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 315.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 305.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 298.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 347.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 258.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 340.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 309.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 65.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 334.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 349.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 48.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 289.79999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 367.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 349.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 237.49999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 253.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 336.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 345.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 381.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 359.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 66.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 372.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 285.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 310.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 193.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 296.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 351.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 45.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 309.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 313.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 304.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 333.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 65.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 436.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 279.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 579.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 333.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 296.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 292.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 315.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 298.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 41.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 328.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 359.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 70.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 348.79999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 338.29999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 318.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 333.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 55.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 391.3999999999999 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 300.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 324.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 294.79999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 328.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 333.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 332.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 273.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 253.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 54.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 51.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 288.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 347.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 71.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 229.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 216.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 276.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 311.29999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 332.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 346.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 367.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 305.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 335.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 319.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 308.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 276.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 210.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.52it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 332.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 329.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 318.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 308.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 307.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 266.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 344.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 335.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 55.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 257.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 53.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 331.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 356.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 298.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 318.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 311.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 350.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 251.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 66.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 471.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 315.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 57.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 326.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 344.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 312.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 302.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 321.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 388.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 296.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 375.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 336.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 371.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 334.29999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 296.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 274.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 302.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 279.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 58.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 375.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 367.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 311.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 339.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 278.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 238.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n",
      "WARNING! Your documents have duplicate entries!  This will slow down calculation and may yield subpar results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 309.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 48.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 289.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 350.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 331.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 53.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 232.89999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 54.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 303.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 66.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 344.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 318.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 315.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 323.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 47.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 366.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 276.99999999999994 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 300.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 59.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 226.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 257.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 51.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 343.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 318.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 335.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 330.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 310.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 329.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 279.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 304.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 314.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 308.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 598.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 322.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 330.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 280.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 328.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 308.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 315.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 66.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 247.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 311.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 296.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 295.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 358.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 304.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 347.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 259.29999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 329.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 314.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.05it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 225.89999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 285.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 262.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 714.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 293.49999999999994 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 301.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 329.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 277.79999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 358.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 326.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 297.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 348.49999999999994 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 324.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 302.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 232.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 311.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 297.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 315.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 48.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 322.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 247.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 228.89999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 286.79999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 332.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.59it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 65.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 306.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 333.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 35.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 261.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 335.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 302.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 31.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 321.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.80it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 323.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 264.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 368.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 412.69999999999993 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.44it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 301.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 356.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 296.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 57.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 329.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 273.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 218.99999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 273.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 344.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 265.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 363.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 43.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 368.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 280.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 367.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 46.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 65.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 329.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 359.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.61it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 356.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 329.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 232.39999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 16.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.91it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 65.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 299.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 349.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.70it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 202.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 239.49999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 356.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 60.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 352.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 238.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n",
      "WARNING! Your documents have duplicate entries!  This will slow down calculation and may yield subpar results\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 237.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 304.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 33.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 257.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 267.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 314.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 288.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 313.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 387.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 402.1999999999999 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 67.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.67it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 288.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 304.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 353.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 373.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 66.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 350.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 35.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 62.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 673.6999999999999 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 290.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 341.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 301.29999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 514.5999999999999 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.31it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 273.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 268.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 47.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 53.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 237.79999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 302.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 330.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 246.29999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 331.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.84it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 238.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 14.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 310.79999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 323.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.81it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 347.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.37it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 247.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 334.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.32it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 301.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 331.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 293.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 61.48it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 659.1999999999999 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 257.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 273.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 221.49999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 15.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 64.23it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 300.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 274.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.40it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.26it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 369.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.99it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 54.65it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 301.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 320.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 321.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 670.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 380.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 292.3 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 335.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  8.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.69it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 289.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 77.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 257.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 52.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 63.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 52.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.45it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 252.29999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 380.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 247.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.96it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 247.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 318.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.75it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 11.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 403.09999999999985 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 265.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 332.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.46it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 297.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 313.09999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 259.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 329.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 332.5 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 55.50it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 253.1 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 308.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 332.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 298.7 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 268.79999999999995 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 300.59999999999997 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 10.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 252.29999999999998 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 53.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 254.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 13.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 73.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.9 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00,  9.85it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating Answers...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generate_answers_all(QUESTIONS_FILE, OUTPUT_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qH40Baoa7MNC",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # specify output csv \n",
    "# input_file = 'qa_gold-llama-1-shot-no_chat_template.csv'\n",
    "# output_file = 'llama-1-shot-no_chat_template-output.csv'\n",
    "# csv_input_dir = f'csv_qa_gold/{input_file}'\n",
    "# csv_output_dir = f'csv_qa_gold/{output_file}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def initialize_model_answer_column(df):\n",
    "#     if 'ModelAnswer' not in df.columns:\n",
    "#         df['ModelAnswer'] = None\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import glob\n",
    "# import os\n",
    "# import pandas as pd\n",
    "\n",
    "# directory_csv = 'csv_qa_gold'\n",
    "# csv_files = ['csv_qa_gold/test_combined.csv']\n",
    "\n",
    "# # read in the csv files in the directory and concatenate\n",
    "# df_total = pd.DataFrame()\n",
    "# for file in csv_files:\n",
    "#     df = pd.read_csv(file)\n",
    "#     print(df.shape)\n",
    "#     df_total = pd.concat([df_total, df], axis=0)\n",
    "# print(df_total.shape)\n",
    "# df_total = initialize_model_answer_column(df_total)\n",
    "# df_total.to_csv(csv_input_dir, index=False) \n",
    "# # doing this so the original is untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_total.iloc[10:20, :] # please check that excel does not fuck up for row 15,16 the November 2006 to Nov-06 or some other format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def generate_answer(question,llm, return_relevant_docs = False):\n",
    "#     answer, relevant_docs = answer_llama_new(question, KNOWLEDGE_VECTOR_DATABASE,\n",
    "#                             reranker=RERANKER, llm = llm)\n",
    "#     if return_relevant_docs:\n",
    "#         return answer, relevant_docs\n",
    "#     return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "97CmQOe4shpp"
   },
   "outputs": [],
   "source": [
    "# # Function to run over a list of questions and return the answers\n",
    "# # Define the function to process the CSV and add model answers\n",
    "\n",
    "# def generate_answers_and_save(csv_path, llm):\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     if 'ModelAnswer' not in df.columns:\n",
    "#         df['ModelAnswer'] = ''\n",
    "#     for index, row in df.iterrows():\n",
    "#         ModelAnswer = generate_answer(row['Question'],llm=llm)\n",
    "#         df.at[index, 'ModelAnswer'] = ModelAnswer\n",
    "#         df.to_csv(csv_output_dir, index=False)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 43.69it/s]\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       7.17 ms /    12 runs   (    0.60 ms per token,  1672.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10044.01 ms /   979 tokens (   10.26 ms per token,    97.47 tokens per second)\n",
      "llama_print_timings:        eval time =     272.85 ms /    11 runs   (   24.80 ms per token,    40.32 tokens per second)\n",
      "llama_print_timings:       total time =   10603.53 ms /   990 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 236.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.24it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.81 ms /    23 runs   (    0.60 ms per token,  1665.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9807.40 ms /  1024 tokens (    9.58 ms per token,   104.41 tokens per second)\n",
      "llama_print_timings:        eval time =     576.37 ms /    22 runs   (   26.20 ms per token,    38.17 tokens per second)\n",
      "llama_print_timings:       total time =   10712.94 ms /  1046 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 236.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.43it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.16 ms /    22 runs   (    0.60 ms per token,  1671.35 tokens per second)\n",
      "llama_print_timings: prompt eval time =     544.89 ms /    56 tokens (    9.73 ms per token,   102.77 tokens per second)\n",
      "llama_print_timings:        eval time =     551.73 ms /    21 runs   (   26.27 ms per token,    38.06 tokens per second)\n",
      "llama_print_timings:       total time =    1184.31 ms /    77 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 290.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.68it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.28 ms /    17 runs   (    0.60 ms per token,  1653.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5811.24 ms /   589 tokens (    9.87 ms per token,   101.36 tokens per second)\n",
      "llama_print_timings:        eval time =     422.49 ms /    16 runs   (   26.41 ms per token,    37.87 tokens per second)\n",
      "llama_print_timings:       total time =    6437.37 ms /   605 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 236.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.32it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      19.93 ms /    33 runs   (    0.60 ms per token,  1655.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4625.19 ms /   471 tokens (    9.82 ms per token,   101.83 tokens per second)\n",
      "llama_print_timings:        eval time =     837.14 ms /    32 runs   (   26.16 ms per token,    38.23 tokens per second)\n",
      "llama_print_timings:       total time =    5692.86 ms /   503 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 327.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.56it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.19 ms /    22 runs   (    0.60 ms per token,  1667.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17986.78 ms /  1871 tokens (    9.61 ms per token,   104.02 tokens per second)\n",
      "llama_print_timings:        eval time =     573.75 ms /    21 runs   (   27.32 ms per token,    36.60 tokens per second)\n",
      "llama_print_timings:       total time =   19093.97 ms /  1892 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 271.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.98it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      18.63 ms /    31 runs   (    0.60 ms per token,  1664.25 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14840.51 ms /  1560 tokens (    9.51 ms per token,   105.12 tokens per second)\n",
      "llama_print_timings:        eval time =     819.23 ms /    31 runs   (   26.43 ms per token,    37.84 tokens per second)\n",
      "llama_print_timings:       total time =   16146.31 ms /  1591 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 271.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.87it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      16.74 ms /    28 runs   (    0.60 ms per token,  1672.44 tokens per second)\n",
      "llama_print_timings: prompt eval time =     607.11 ms /    61 tokens (    9.95 ms per token,   100.48 tokens per second)\n",
      "llama_print_timings:        eval time =     715.17 ms /    27 runs   (   26.49 ms per token,    37.75 tokens per second)\n",
      "llama_print_timings:       total time =    1433.82 ms /    88 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 236.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.52it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      17.41 ms /    29 runs   (    0.60 ms per token,  1665.61 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13635.05 ms /  1440 tokens (    9.47 ms per token,   105.61 tokens per second)\n",
      "llama_print_timings:        eval time =     759.90 ms /    29 runs   (   26.20 ms per token,    38.16 tokens per second)\n",
      "llama_print_timings:       total time =   14848.02 ms /  1469 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 271.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.03it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       4.85 ms /     8 runs   (    0.61 ms per token,  1650.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13559.90 ms /  1432 tokens (    9.47 ms per token,   105.61 tokens per second)\n",
      "llama_print_timings:        eval time =     184.94 ms /     7 runs   (   26.42 ms per token,    37.85 tokens per second)\n",
      "llama_print_timings:       total time =   14123.91 ms /  1439 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.35it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       1.22 ms /     2 runs   (    0.61 ms per token,  1636.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11556.80 ms /  1216 tokens (    9.50 ms per token,   105.22 tokens per second)\n",
      "llama_print_timings:        eval time =      27.10 ms /     1 runs   (   27.10 ms per token,    36.90 tokens per second)\n",
      "llama_print_timings:       total time =   11888.47 ms /  1217 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 309.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.19it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       9.06 ms /    15 runs   (    0.60 ms per token,  1655.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12729.90 ms /  1347 tokens (    9.45 ms per token,   105.81 tokens per second)\n",
      "llama_print_timings:        eval time =     363.16 ms /    14 runs   (   25.94 ms per token,    38.55 tokens per second)\n",
      "llama_print_timings:       total time =   13480.22 ms /  1361 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 324.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.16it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      16.77 ms /    28 runs   (    0.60 ms per token,  1669.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12834.80 ms /  1356 tokens (    9.47 ms per token,   105.65 tokens per second)\n",
      "llama_print_timings:        eval time =     701.63 ms /    27 runs   (   25.99 ms per token,    38.48 tokens per second)\n",
      "llama_print_timings:       total time =   13971.59 ms /  1383 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 296.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.98it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       5.37 ms /     9 runs   (    0.60 ms per token,  1675.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14765.51 ms /  1550 tokens (    9.53 ms per token,   104.97 tokens per second)\n",
      "llama_print_timings:        eval time =     212.25 ms /     8 runs   (   26.53 ms per token,    37.69 tokens per second)\n",
      "llama_print_timings:       total time =   15399.04 ms /  1558 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 258.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.72it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.80 ms /    23 runs   (    0.60 ms per token,  1667.15 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12753.65 ms /  1351 tokens (    9.44 ms per token,   105.93 tokens per second)\n",
      "llama_print_timings:        eval time =     570.35 ms /    22 runs   (   25.92 ms per token,    38.57 tokens per second)\n",
      "llama_print_timings:       total time =   13742.11 ms /  1373 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.23it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.74 ms /    18 runs   (    0.60 ms per token,  1675.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6134.33 ms /   628 tokens (    9.77 ms per token,   102.37 tokens per second)\n",
      "llama_print_timings:        eval time =     439.57 ms /    17 runs   (   25.86 ms per token,    38.67 tokens per second)\n",
      "llama_print_timings:       total time =    6791.84 ms /   645 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 299.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.15it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      12.61 ms /    21 runs   (    0.60 ms per token,  1666.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3820.73 ms /   396 tokens (    9.65 ms per token,   103.65 tokens per second)\n",
      "llama_print_timings:        eval time =     501.80 ms /    20 runs   (   25.09 ms per token,    39.86 tokens per second)\n",
      "llama_print_timings:       total time =    4492.20 ms /   416 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 312.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.28it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       8.31 ms /    14 runs   (    0.59 ms per token,  1684.72 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12678.98 ms /  1344 tokens (    9.43 ms per token,   106.00 tokens per second)\n",
      "llama_print_timings:        eval time =     337.11 ms /    13 runs   (   25.93 ms per token,    38.56 tokens per second)\n",
      "llama_print_timings:       total time =   13399.15 ms /  1357 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 294.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.18it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       9.60 ms /    16 runs   (    0.60 ms per token,  1667.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14585.19 ms /  1534 tokens (    9.51 ms per token,   105.18 tokens per second)\n",
      "llama_print_timings:        eval time =     400.44 ms /    15 runs   (   26.70 ms per token,    37.46 tokens per second)\n",
      "llama_print_timings:       total time =   15425.41 ms /  1549 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 359.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.26it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.23 ms /    22 runs   (    0.60 ms per token,  1662.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18056.32 ms /  1868 tokens (    9.67 ms per token,   103.45 tokens per second)\n",
      "llama_print_timings:        eval time =     571.61 ms /    21 runs   (   27.22 ms per token,    36.74 tokens per second)\n",
      "llama_print_timings:       total time =   19176.84 ms /  1889 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 320.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.96it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      48.07 ms /    80 runs   (    0.60 ms per token,  1664.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12586.29 ms /  1267 tokens (    9.93 ms per token,   100.67 tokens per second)\n",
      "llama_print_timings:        eval time =    2163.03 ms /    79 runs   (   27.38 ms per token,    36.52 tokens per second)\n",
      "llama_print_timings:       total time =   15365.78 ms /  1346 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.18it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       7.81 ms /    13 runs   (    0.60 ms per token,  1664.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17173.09 ms /  1784 tokens (    9.63 ms per token,   103.88 tokens per second)\n",
      "llama_print_timings:        eval time =     354.55 ms /    13 runs   (   27.27 ms per token,    36.67 tokens per second)\n",
      "llama_print_timings:       total time =   18029.50 ms /  1797 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 313.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.02it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      14.95 ms /    25 runs   (    0.60 ms per token,  1672.69 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15601.88 ms /  1630 tokens (    9.57 ms per token,   104.47 tokens per second)\n",
      "llama_print_timings:        eval time =     641.03 ms /    24 runs   (   26.71 ms per token,    37.44 tokens per second)\n",
      "llama_print_timings:       total time =   16742.96 ms /  1654 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 289.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.06it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      20.96 ms /    35 runs   (    0.60 ms per token,  1670.09 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16057.01 ms /  1675 tokens (    9.59 ms per token,   104.32 tokens per second)\n",
      "llama_print_timings:        eval time =     911.76 ms /    34 runs   (   26.82 ms per token,    37.29 tokens per second)\n",
      "llama_print_timings:       total time =   17516.41 ms /  1709 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 301.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.09it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       8.99 ms /    15 runs   (    0.60 ms per token,  1669.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15599.46 ms /  1630 tokens (    9.57 ms per token,   104.49 tokens per second)\n",
      "llama_print_timings:        eval time =     376.85 ms /    14 runs   (   26.92 ms per token,    37.15 tokens per second)\n",
      "llama_print_timings:       total time =   16445.15 ms /  1644 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 313.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.16it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       3.59 ms /     6 runs   (    0.60 ms per token,  1673.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17074.41 ms /  1775 tokens (    9.62 ms per token,   103.96 tokens per second)\n",
      "llama_print_timings:        eval time =     138.84 ms /     5 runs   (   27.77 ms per token,    36.01 tokens per second)\n",
      "llama_print_timings:       total time =   17685.21 ms /  1780 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 307.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.02it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      21.55 ms /    36 runs   (    0.60 ms per token,  1670.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14923.97 ms /  1565 tokens (    9.54 ms per token,   104.86 tokens per second)\n",
      "llama_print_timings:        eval time =     924.87 ms /    35 runs   (   26.42 ms per token,    37.84 tokens per second)\n",
      "llama_print_timings:       total time =   16369.95 ms /  1600 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 307.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.11it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       8.44 ms /    14 runs   (    0.60 ms per token,  1659.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14440.81 ms /  1516 tokens (    9.53 ms per token,   104.98 tokens per second)\n",
      "llama_print_timings:        eval time =     343.23 ms /    13 runs   (   26.40 ms per token,    37.88 tokens per second)\n",
      "llama_print_timings:       total time =   15220.08 ms /  1529 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 307.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.11it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.78 ms /    18 runs   (    0.60 ms per token,  1669.60 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15625.57 ms /  1636 tokens (    9.55 ms per token,   104.70 tokens per second)\n",
      "llama_print_timings:        eval time =     455.83 ms /    17 runs   (   26.81 ms per token,    37.29 tokens per second)\n",
      "llama_print_timings:       total time =   16558.04 ms /  1653 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 190.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 34.94it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       7.29 ms /    12 runs   (    0.61 ms per token,  1646.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6044.25 ms /   662 tokens (    9.13 ms per token,   109.53 tokens per second)\n",
      "llama_print_timings:        eval time =     264.85 ms /    11 runs   (   24.08 ms per token,    41.53 tokens per second)\n",
      "llama_print_timings:       total time =    6517.17 ms /   673 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 284.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.07it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.76 ms /    23 runs   (    0.60 ms per token,  1672.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19420.29 ms /  2000 tokens (    9.71 ms per token,   102.99 tokens per second)\n",
      "llama_print_timings:        eval time =     612.89 ms /    22 runs   (   27.86 ms per token,    35.90 tokens per second)\n",
      "llama_print_timings:       total time =   20631.15 ms /  2022 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 307.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.09it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      17.96 ms /    30 runs   (    0.60 ms per token,  1670.01 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14466.83 ms /  1520 tokens (    9.52 ms per token,   105.07 tokens per second)\n",
      "llama_print_timings:        eval time =     762.79 ms /    29 runs   (   26.30 ms per token,    38.02 tokens per second)\n",
      "llama_print_timings:       total time =   15722.43 ms /  1549 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 315.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.07it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      12.73 ms /    21 runs   (    0.61 ms per token,  1649.78 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15925.83 ms /  1663 tokens (    9.58 ms per token,   104.42 tokens per second)\n",
      "llama_print_timings:        eval time =     539.73 ms /    20 runs   (   26.99 ms per token,    37.06 tokens per second)\n",
      "llama_print_timings:       total time =   16969.69 ms /  1683 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 359.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.44it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       7.84 ms /    13 runs   (    0.60 ms per token,  1658.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18628.18 ms /  1922 tokens (    9.69 ms per token,   103.18 tokens per second)\n",
      "llama_print_timings:        eval time =     331.54 ms /    12 runs   (   27.63 ms per token,    36.19 tokens per second)\n",
      "llama_print_timings:       total time =   19499.09 ms /  1934 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 359.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.26it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      14.39 ms /    24 runs   (    0.60 ms per token,  1667.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18738.99 ms /  1933 tokens (    9.69 ms per token,   103.15 tokens per second)\n",
      "llama_print_timings:        eval time =     632.83 ms /    23 runs   (   27.51 ms per token,    36.34 tokens per second)\n",
      "llama_print_timings:       total time =   19954.73 ms /  1956 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 359.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.28it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      49.27 ms /    82 runs   (    0.60 ms per token,  1664.47 tokens per second)\n",
      "llama_print_timings: prompt eval time =    9766.34 ms /   992 tokens (    9.85 ms per token,   101.57 tokens per second)\n",
      "llama_print_timings:        eval time =    2165.69 ms /    81 runs   (   26.74 ms per token,    37.40 tokens per second)\n",
      "llama_print_timings:       total time =   12483.21 ms /  1073 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 359.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.18it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       5.41 ms /     9 runs   (    0.60 ms per token,  1663.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18396.67 ms /  1903 tokens (    9.67 ms per token,   103.44 tokens per second)\n",
      "llama_print_timings:        eval time =     221.21 ms /     8 runs   (   27.65 ms per token,    36.17 tokens per second)\n",
      "llama_print_timings:       total time =   19142.15 ms /  1911 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 320.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 21.91it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.53 ms /    22 runs   (    0.62 ms per token,  1625.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10277.71 ms /  1047 tokens (    9.82 ms per token,   101.87 tokens per second)\n",
      "llama_print_timings:        eval time =     562.54 ms /    21 runs   (   26.79 ms per token,    37.33 tokens per second)\n",
      "llama_print_timings:       total time =   11202.84 ms /  1068 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 359.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.44it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      26.32 ms /    44 runs   (    0.60 ms per token,  1671.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17981.05 ms /  1864 tokens (    9.65 ms per token,   103.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1204.84 ms /    44 runs   (   27.38 ms per token,    36.52 tokens per second)\n",
      "llama_print_timings:       total time =   19848.54 ms /  1908 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 323.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.73it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      31.91 ms /    53 runs   (    0.60 ms per token,  1661.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12392.61 ms /  1314 tokens (    9.43 ms per token,   106.03 tokens per second)\n",
      "llama_print_timings:        eval time =    1339.91 ms /    52 runs   (   25.77 ms per token,    38.81 tokens per second)\n",
      "llama_print_timings:       total time =   14271.91 ms /  1366 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 340.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.75it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      18.05 ms /    30 runs   (    0.60 ms per token,  1662.33 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14376.17 ms /  1514 tokens (    9.50 ms per token,   105.31 tokens per second)\n",
      "llama_print_timings:        eval time =     765.19 ms /    29 runs   (   26.39 ms per token,    37.90 tokens per second)\n",
      "llama_print_timings:       total time =   15647.80 ms /  1543 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 69.77it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      16.78 ms /    28 runs   (    0.60 ms per token,  1668.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1762.39 ms /   199 tokens (    8.86 ms per token,   112.91 tokens per second)\n",
      "llama_print_timings:        eval time =     612.03 ms /    27 runs   (   22.67 ms per token,    44.12 tokens per second)\n",
      "llama_print_timings:       total time =    2519.11 ms /   226 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 335.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.51it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.83 ms /    18 runs   (    0.60 ms per token,  1662.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12134.99 ms /  1290 tokens (    9.41 ms per token,   106.30 tokens per second)\n",
      "llama_print_timings:        eval time =     437.11 ms /    17 runs   (   25.71 ms per token,    38.89 tokens per second)\n",
      "llama_print_timings:       total time =   12975.70 ms /  1307 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 290.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.27it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      21.76 ms /    36 runs   (    0.60 ms per token,  1654.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10115.75 ms /  1084 tokens (    9.33 ms per token,   107.16 tokens per second)\n",
      "llama_print_timings:        eval time =     876.40 ms /    35 runs   (   25.04 ms per token,    39.94 tokens per second)\n",
      "llama_print_timings:       total time =   11405.70 ms /  1119 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 268.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.56it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      43.48 ms /    72 runs   (    0.60 ms per token,  1655.97 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12814.38 ms /  1358 tokens (    9.44 ms per token,   105.97 tokens per second)\n",
      "llama_print_timings:        eval time =    1845.14 ms /    71 runs   (   25.99 ms per token,    38.48 tokens per second)\n",
      "llama_print_timings:       total time =   15278.59 ms /  1429 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.80it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       4.77 ms /     8 runs   (    0.60 ms per token,  1675.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2204.83 ms /   248 tokens (    8.89 ms per token,   112.48 tokens per second)\n",
      "llama_print_timings:        eval time =     160.36 ms /     7 runs   (   22.91 ms per token,    43.65 tokens per second)\n",
      "llama_print_timings:       total time =    2457.30 ms /   255 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 342.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.66it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.92 ms /    23 runs   (    0.61 ms per token,  1651.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15807.13 ms /  1656 tokens (    9.55 ms per token,   104.76 tokens per second)\n",
      "llama_print_timings:        eval time =     612.45 ms /    23 runs   (   26.63 ms per token,    37.55 tokens per second)\n",
      "llama_print_timings:       total time =   16944.29 ms /  1679 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 291.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.27it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       3.60 ms /     6 runs   (    0.60 ms per token,  1668.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10698.49 ms /  1144 tokens (    9.35 ms per token,   106.93 tokens per second)\n",
      "llama_print_timings:        eval time =     128.22 ms /     5 runs   (   25.64 ms per token,    39.00 tokens per second)\n",
      "llama_print_timings:       total time =   11151.71 ms /  1149 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 342.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.48it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.28 ms /    17 runs   (    0.60 ms per token,  1653.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16155.51 ms /  1688 tokens (    9.57 ms per token,   104.48 tokens per second)\n",
      "llama_print_timings:        eval time =     427.16 ms /    16 runs   (   26.70 ms per token,    37.46 tokens per second)\n",
      "llama_print_timings:       total time =   17096.29 ms /  1704 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 250.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.10it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      16.36 ms /    27 runs   (    0.61 ms per token,  1649.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12562.71 ms /  1330 tokens (    9.45 ms per token,   105.87 tokens per second)\n",
      "llama_print_timings:        eval time =     671.57 ms /    26 runs   (   25.83 ms per token,    38.72 tokens per second)\n",
      "llama_print_timings:       total time =   13691.31 ms /  1356 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 317.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.92it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      14.93 ms /    25 runs   (    0.60 ms per token,  1674.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =     859.69 ms /    90 tokens (    9.55 ms per token,   104.69 tokens per second)\n",
      "llama_print_timings:        eval time =     582.66 ms /    24 runs   (   24.28 ms per token,    41.19 tokens per second)\n",
      "llama_print_timings:       total time =    1551.22 ms /   114 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 250.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.69it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      15.68 ms /    26 runs   (    0.60 ms per token,  1658.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12569.14 ms /  1333 tokens (    9.43 ms per token,   106.05 tokens per second)\n",
      "llama_print_timings:        eval time =     645.92 ms /    25 runs   (   25.84 ms per token,    38.70 tokens per second)\n",
      "llama_print_timings:       total time =   13664.43 ms /  1358 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 311.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.20it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      41.62 ms /    69 runs   (    0.60 ms per token,  1657.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11789.09 ms /  1256 tokens (    9.39 ms per token,   106.54 tokens per second)\n",
      "llama_print_timings:        eval time =    1742.57 ms /    68 runs   (   25.63 ms per token,    39.02 tokens per second)\n",
      "llama_print_timings:       total time =   14121.80 ms /  1324 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 278.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.13it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      19.19 ms /    32 runs   (    0.60 ms per token,  1667.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7267.74 ms /   752 tokens (    9.66 ms per token,   103.47 tokens per second)\n",
      "llama_print_timings:        eval time =     799.94 ms /    31 runs   (   25.80 ms per token,    38.75 tokens per second)\n",
      "llama_print_timings:       total time =    8384.15 ms /   783 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 331.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.36it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      11.47 ms /    19 runs   (    0.60 ms per token,  1656.50 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11164.71 ms /  1191 tokens (    9.37 ms per token,   106.68 tokens per second)\n",
      "llama_print_timings:        eval time =     459.38 ms /    18 runs   (   25.52 ms per token,    39.18 tokens per second)\n",
      "llama_print_timings:       total time =   12013.32 ms /  1209 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 306.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.11it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      15.06 ms /    25 runs   (    0.60 ms per token,  1659.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14223.75 ms /  1498 tokens (    9.50 ms per token,   105.32 tokens per second)\n",
      "llama_print_timings:        eval time =     631.29 ms /    24 runs   (   26.30 ms per token,    38.02 tokens per second)\n",
      "llama_print_timings:       total time =   15352.69 ms /  1522 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 385.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.24it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      20.37 ms /    34 runs   (    0.60 ms per token,  1669.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17784.22 ms /  1843 tokens (    9.65 ms per token,   103.63 tokens per second)\n",
      "llama_print_timings:        eval time =     899.71 ms /    33 runs   (   27.26 ms per token,    36.68 tokens per second)\n",
      "llama_print_timings:       total time =   19305.07 ms /  1876 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 332.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.11it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =     140.31 ms /   233 runs   (    0.60 ms per token,  1660.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16927.93 ms /  1759 tokens (    9.62 ms per token,   103.91 tokens per second)\n",
      "llama_print_timings:        eval time =    6308.24 ms /   232 runs   (   27.19 ms per token,    36.78 tokens per second)\n",
      "llama_print_timings:       total time =   24625.83 ms /  1991 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 273.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.86it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      12.07 ms /    20 runs   (    0.60 ms per token,  1656.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   14702.21 ms /  1542 tokens (    9.53 ms per token,   104.88 tokens per second)\n",
      "llama_print_timings:        eval time =     505.39 ms /    19 runs   (   26.60 ms per token,    37.59 tokens per second)\n",
      "llama_print_timings:       total time =   15700.28 ms /  1561 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 344.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.07it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      57.23 ms /    95 runs   (    0.60 ms per token,  1660.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15157.64 ms /  1589 tokens (    9.54 ms per token,   104.83 tokens per second)\n",
      "llama_print_timings:        eval time =    2505.04 ms /    94 runs   (   26.65 ms per token,    37.52 tokens per second)\n",
      "llama_print_timings:       total time =   18450.69 ms /  1683 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 341.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.69it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      54.90 ms /    91 runs   (    0.60 ms per token,  1657.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12124.08 ms /  1288 tokens (    9.41 ms per token,   106.23 tokens per second)\n",
      "llama_print_timings:        eval time =    2347.40 ms /    91 runs   (   25.80 ms per token,    38.77 tokens per second)\n",
      "llama_print_timings:       total time =   15154.69 ms /  1379 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 323.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.25it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =     124.44 ms /   207 runs   (    0.60 ms per token,  1663.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15130.62 ms /  1584 tokens (    9.55 ms per token,   104.69 tokens per second)\n",
      "llama_print_timings:        eval time =    5534.94 ms /   207 runs   (   26.74 ms per token,    37.40 tokens per second)\n",
      "llama_print_timings:       total time =   21901.06 ms /  1791 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.47it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      45.67 ms /    76 runs   (    0.60 ms per token,  1664.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1921.08 ms /   216 tokens (    8.89 ms per token,   112.44 tokens per second)\n",
      "llama_print_timings:        eval time =    1724.12 ms /    76 runs   (   22.69 ms per token,    44.08 tokens per second)\n",
      "llama_print_timings:       total time =    3965.71 ms /   292 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 43.52it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      55.10 ms /    91 runs   (    0.61 ms per token,  1651.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13317.78 ms /  1406 tokens (    9.47 ms per token,   105.57 tokens per second)\n",
      "llama_print_timings:        eval time =    2355.74 ms /    90 runs   (   26.17 ms per token,    38.20 tokens per second)\n",
      "llama_print_timings:       total time =   16401.94 ms /  1496 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.00it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      27.48 ms /    46 runs   (    0.60 ms per token,  1673.76 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2331.34 ms /   262 tokens (    8.90 ms per token,   112.38 tokens per second)\n",
      "llama_print_timings:        eval time =    1020.78 ms /    45 runs   (   22.68 ms per token,    44.08 tokens per second)\n",
      "llama_print_timings:       total time =    3582.94 ms /   307 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 38.71it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       7.19 ms /    12 runs   (    0.60 ms per token,  1669.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    7190.49 ms /   784 tokens (    9.17 ms per token,   109.03 tokens per second)\n",
      "llama_print_timings:        eval time =     291.87 ms /    12 runs   (   24.32 ms per token,    41.11 tokens per second)\n",
      "llama_print_timings:       total time =    7738.68 ms /   796 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.11it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      14.50 ms /    24 runs   (    0.60 ms per token,  1655.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1414.39 ms /   160 tokens (    8.84 ms per token,   113.12 tokens per second)\n",
      "llama_print_timings:        eval time =     537.96 ms /    24 runs   (   22.41 ms per token,    44.61 tokens per second)\n",
      "llama_print_timings:       total time =    2078.88 ms /   184 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 356.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.81it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      19.88 ms /    33 runs   (    0.60 ms per token,  1659.71 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18356.24 ms /  1900 tokens (    9.66 ms per token,   103.51 tokens per second)\n",
      "llama_print_timings:        eval time =     886.02 ms /    32 runs   (   27.69 ms per token,    36.12 tokens per second)\n",
      "llama_print_timings:       total time =   19884.61 ms /  1932 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 327.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.06it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      19.42 ms /    32 runs   (    0.61 ms per token,  1647.96 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16601.62 ms /  1728 tokens (    9.61 ms per token,   104.09 tokens per second)\n",
      "llama_print_timings:        eval time =     860.87 ms /    32 runs   (   26.90 ms per token,    37.17 tokens per second)\n",
      "llama_print_timings:       total time =   18054.94 ms /  1760 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 313.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.72it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      69.33 ms /   114 runs   (    0.61 ms per token,  1644.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15510.48 ms /  1624 tokens (    9.55 ms per token,   104.70 tokens per second)\n",
      "llama_print_timings:        eval time =    3042.23 ms /   114 runs   (   26.69 ms per token,    37.47 tokens per second)\n",
      "llama_print_timings:       total time =   19433.06 ms /  1738 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 266.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.48it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      19.35 ms /    32 runs   (    0.60 ms per token,  1653.83 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19501.58 ms /  2003 tokens (    9.74 ms per token,   102.71 tokens per second)\n",
      "llama_print_timings:        eval time =     858.47 ms /    31 runs   (   27.69 ms per token,    36.11 tokens per second)\n",
      "llama_print_timings:       total time =   21036.20 ms /  2034 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 292.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.27it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       8.46 ms /    14 runs   (    0.60 ms per token,  1654.26 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12491.31 ms /  1322 tokens (    9.45 ms per token,   105.83 tokens per second)\n",
      "llama_print_timings:        eval time =     335.14 ms /    13 runs   (   25.78 ms per token,    38.79 tokens per second)\n",
      "llama_print_timings:       total time =   13241.11 ms /  1335 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 271.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.96it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       5.42 ms /     9 runs   (    0.60 ms per token,  1659.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12823.76 ms /  1355 tokens (    9.46 ms per token,   105.66 tokens per second)\n",
      "llama_print_timings:        eval time =     208.87 ms /     8 runs   (   26.11 ms per token,    38.30 tokens per second)\n",
      "llama_print_timings:       total time =   13435.40 ms /  1363 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.26it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.84 ms /    18 runs   (    0.60 ms per token,  1659.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1491.99 ms /   168 tokens (    8.88 ms per token,   112.60 tokens per second)\n",
      "llama_print_timings:        eval time =     404.09 ms /    18 runs   (   22.45 ms per token,    44.54 tokens per second)\n",
      "llama_print_timings:       total time =    2003.89 ms /   186 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 345.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.60it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.26 ms /    22 runs   (    0.60 ms per token,  1658.75 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17882.28 ms /  1852 tokens (    9.66 ms per token,   103.57 tokens per second)\n",
      "llama_print_timings:        eval time =     575.55 ms /    21 runs   (   27.41 ms per token,    36.49 tokens per second)\n",
      "llama_print_timings:       total time =   19048.71 ms /  1873 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 333.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.01it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      15.58 ms /    26 runs   (    0.60 ms per token,  1668.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16873.78 ms /  1757 tokens (    9.60 ms per token,   104.13 tokens per second)\n",
      "llama_print_timings:        eval time =     678.01 ms /    25 runs   (   27.12 ms per token,    36.87 tokens per second)\n",
      "llama_print_timings:       total time =   18131.62 ms /  1782 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 82.10it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      22.83 ms /    38 runs   (    0.60 ms per token,  1664.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1508.17 ms /   170 tokens (    8.87 ms per token,   112.72 tokens per second)\n",
      "llama_print_timings:        eval time =     830.55 ms /    37 runs   (   22.45 ms per token,    44.55 tokens per second)\n",
      "llama_print_timings:       total time =    2515.71 ms /   207 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 358.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.70it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      18.05 ms /    30 runs   (    0.60 ms per token,  1661.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15186.60 ms /  1592 tokens (    9.54 ms per token,   104.83 tokens per second)\n",
      "llama_print_timings:        eval time =     767.86 ms /    29 runs   (   26.48 ms per token,    37.77 tokens per second)\n",
      "llama_print_timings:       total time =   16501.07 ms /  1621 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 324.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.29it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      25.89 ms /    43 runs   (    0.60 ms per token,  1661.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15020.18 ms /  1572 tokens (    9.55 ms per token,   104.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1116.11 ms /    42 runs   (   26.57 ms per token,    37.63 tokens per second)\n",
      "llama_print_timings:       total time =   16724.91 ms /  1614 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 234.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.19it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      27.99 ms /    46 runs   (    0.61 ms per token,  1643.56 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16147.98 ms /  1683 tokens (    9.59 ms per token,   104.22 tokens per second)\n",
      "llama_print_timings:        eval time =    1207.81 ms /    45 runs   (   26.84 ms per token,    37.26 tokens per second)\n",
      "llama_print_timings:       total time =   17991.66 ms /  1728 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 301.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.05it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      34.88 ms /    58 runs   (    0.60 ms per token,  1663.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18632.78 ms /  1918 tokens (    9.71 ms per token,   102.94 tokens per second)\n",
      "llama_print_timings:        eval time =    1568.68 ms /    57 runs   (   27.52 ms per token,    36.34 tokens per second)\n",
      "llama_print_timings:       total time =   20946.48 ms /  1975 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.75it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      18.07 ms /    30 runs   (    0.60 ms per token,  1660.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1535.45 ms /   172 tokens (    8.93 ms per token,   112.02 tokens per second)\n",
      "llama_print_timings:        eval time =     651.29 ms /    29 runs   (   22.46 ms per token,    44.53 tokens per second)\n",
      "llama_print_timings:       total time =    2336.90 ms /   201 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.14it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       1.20 ms /     2 runs   (    0.60 ms per token,  1673.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1383.44 ms /   156 tokens (    8.87 ms per token,   112.76 tokens per second)\n",
      "llama_print_timings:        eval time =      23.41 ms /     1 runs   (   23.41 ms per token,    42.72 tokens per second)\n",
      "llama_print_timings:       total time =    1454.95 ms /   157 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.36it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      19.89 ms /    33 runs   (    0.60 ms per token,  1659.46 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1652.42 ms /   186 tokens (    8.88 ms per token,   112.56 tokens per second)\n",
      "llama_print_timings:        eval time =     721.73 ms /    32 runs   (   22.55 ms per token,    44.34 tokens per second)\n",
      "llama_print_timings:       total time =    2539.42 ms /   218 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.02it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       5.44 ms /     9 runs   (    0.60 ms per token,  1653.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1691.43 ms /   192 tokens (    8.81 ms per token,   113.51 tokens per second)\n",
      "llama_print_timings:        eval time =     207.62 ms /     9 runs   (   23.07 ms per token,    43.35 tokens per second)\n",
      "llama_print_timings:       total time =    1982.04 ms /   201 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 359.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.28it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      17.35 ms /    29 runs   (    0.60 ms per token,  1671.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1403.32 ms /   159 tokens (    8.83 ms per token,   113.30 tokens per second)\n",
      "llama_print_timings:        eval time =     625.25 ms /    28 runs   (   22.33 ms per token,    44.78 tokens per second)\n",
      "llama_print_timings:       total time =    2166.79 ms /   187 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.09it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.82 ms /    23 runs   (    0.60 ms per token,  1663.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1245.51 ms /   141 tokens (    8.83 ms per token,   113.21 tokens per second)\n",
      "llama_print_timings:        eval time =     490.83 ms /    22 runs   (   22.31 ms per token,    44.82 tokens per second)\n",
      "llama_print_timings:       total time =    1851.67 ms /   163 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 285.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.29it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      16.95 ms /    28 runs   (    0.61 ms per token,  1652.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17195.71 ms /  1791 tokens (    9.60 ms per token,   104.15 tokens per second)\n",
      "llama_print_timings:        eval time =     732.65 ms /    27 runs   (   27.14 ms per token,    36.85 tokens per second)\n",
      "llama_print_timings:       total time =   18526.32 ms /  1818 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.67it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      50.56 ms /    84 runs   (    0.60 ms per token,  1661.49 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2808.27 ms /   314 tokens (    8.94 ms per token,   111.81 tokens per second)\n",
      "llama_print_timings:        eval time =    1901.62 ms /    83 runs   (   22.91 ms per token,    43.65 tokens per second)\n",
      "llama_print_timings:       total time =    5089.25 ms /   397 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 306.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.15it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      12.64 ms /    21 runs   (    0.60 ms per token,  1661.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16057.72 ms /  1679 tokens (    9.56 ms per token,   104.56 tokens per second)\n",
      "llama_print_timings:        eval time =     537.78 ms /    20 runs   (   26.89 ms per token,    37.19 tokens per second)\n",
      "llama_print_timings:       total time =   17136.54 ms /  1699 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 264.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.07it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      11.01 ms /    18 runs   (    0.61 ms per token,  1634.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15937.81 ms /  1663 tokens (    9.58 ms per token,   104.34 tokens per second)\n",
      "llama_print_timings:        eval time =     457.39 ms /    17 runs   (   26.91 ms per token,    37.17 tokens per second)\n",
      "llama_print_timings:       total time =   16921.57 ms /  1680 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 312.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.07it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      59.31 ms /    98 runs   (    0.61 ms per token,  1652.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15810.91 ms /  1650 tokens (    9.58 ms per token,   104.36 tokens per second)\n",
      "llama_print_timings:        eval time =    2596.09 ms /    97 runs   (   26.76 ms per token,    37.36 tokens per second)\n",
      "llama_print_timings:       total time =   19247.72 ms /  1747 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.32it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      30.32 ms /    48 runs   (    0.63 ms per token,  1582.90 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16115.67 ms /  1680 tokens (    9.59 ms per token,   104.25 tokens per second)\n",
      "llama_print_timings:        eval time =    1292.08 ms /    48 runs   (   26.92 ms per token,    37.15 tokens per second)\n",
      "llama_print_timings:       total time =   18068.20 ms /  1728 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 633.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.31it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      45.21 ms /    75 runs   (    0.60 ms per token,  1658.93 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17299.40 ms /  1796 tokens (    9.63 ms per token,   103.82 tokens per second)\n",
      "llama_print_timings:        eval time =    2008.23 ms /    74 runs   (   27.14 ms per token,    36.85 tokens per second)\n",
      "llama_print_timings:       total time =   20086.58 ms /  1870 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 324.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.71it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.83 ms /    18 runs   (    0.60 ms per token,  1662.66 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15503.11 ms /  1621 tokens (    9.56 ms per token,   104.56 tokens per second)\n",
      "llama_print_timings:        eval time =     453.90 ms /    17 runs   (   26.70 ms per token,    37.45 tokens per second)\n",
      "llama_print_timings:       total time =   16470.01 ms /  1638 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 308.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.16it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      15.65 ms /    26 runs   (    0.60 ms per token,  1661.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15611.30 ms /  1632 tokens (    9.57 ms per token,   104.54 tokens per second)\n",
      "llama_print_timings:        eval time =     671.65 ms /    25 runs   (   26.87 ms per token,    37.22 tokens per second)\n",
      "llama_print_timings:       total time =   16818.81 ms /  1657 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 306.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.92it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.76 ms /    18 runs   (    0.60 ms per token,  1672.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10524.76 ms /  1123 tokens (    9.37 ms per token,   106.70 tokens per second)\n",
      "llama_print_timings:        eval time =     430.70 ms /    17 runs   (   25.34 ms per token,    39.47 tokens per second)\n",
      "llama_print_timings:       total time =   11324.89 ms /  1140 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 331.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.37it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      16.20 ms /    27 runs   (    0.60 ms per token,  1667.08 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15646.37 ms /  1634 tokens (    9.58 ms per token,   104.43 tokens per second)\n",
      "llama_print_timings:        eval time =     696.91 ms /    26 runs   (   26.80 ms per token,    37.31 tokens per second)\n",
      "llama_print_timings:       total time =   16884.90 ms /  1660 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 367.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.72it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      41.69 ms /    69 runs   (    0.60 ms per token,  1654.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15258.50 ms /  1597 tokens (    9.55 ms per token,   104.66 tokens per second)\n",
      "llama_print_timings:        eval time =    1812.80 ms /    68 runs   (   26.66 ms per token,    37.51 tokens per second)\n",
      "llama_print_timings:       total time =   17763.25 ms /  1665 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.44it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      24.59 ms /    41 runs   (    0.60 ms per token,  1667.48 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1900.25 ms /   213 tokens (    8.92 ms per token,   112.09 tokens per second)\n",
      "llama_print_timings:        eval time =     907.14 ms /    40 runs   (   22.68 ms per token,    44.09 tokens per second)\n",
      "llama_print_timings:       total time =    3005.28 ms /   253 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 38.58it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      25.92 ms /    43 runs   (    0.60 ms per token,  1658.89 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8508.94 ms /   919 tokens (    9.26 ms per token,   108.00 tokens per second)\n",
      "llama_print_timings:        eval time =    1035.07 ms /    42 runs   (   24.64 ms per token,    40.58 tokens per second)\n",
      "llama_print_timings:       total time =    9951.36 ms /   961 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 448.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.73it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      96.46 ms /   160 runs   (    0.60 ms per token,  1658.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5051.46 ms /   554 tokens (    9.12 ms per token,   109.67 tokens per second)\n",
      "llama_print_timings:        eval time =    3787.66 ms /   159 runs   (   23.82 ms per token,    41.98 tokens per second)\n",
      "llama_print_timings:       total time =    9567.39 ms /   713 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.60it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      31.17 ms /    52 runs   (    0.60 ms per token,  1668.06 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1456.60 ms /   164 tokens (    8.88 ms per token,   112.59 tokens per second)\n",
      "llama_print_timings:        eval time =    1147.20 ms /    51 runs   (   22.49 ms per token,    44.46 tokens per second)\n",
      "llama_print_timings:       total time =    2825.90 ms /   215 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 518.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 27.47it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.94 ms /    23 runs   (    0.61 ms per token,  1650.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    5220.65 ms /   576 tokens (    9.06 ms per token,   110.33 tokens per second)\n",
      "llama_print_timings:        eval time =     519.26 ms /    22 runs   (   23.60 ms per token,    42.37 tokens per second)\n",
      "llama_print_timings:       total time =    5980.25 ms /   598 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.79it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       9.56 ms /    16 runs   (    0.60 ms per token,  1674.34 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1321.16 ms /   150 tokens (    8.81 ms per token,   113.54 tokens per second)\n",
      "llama_print_timings:        eval time =     336.28 ms /    15 runs   (   22.42 ms per token,    44.61 tokens per second)\n",
      "llama_print_timings:       total time =    1751.60 ms /   165 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 344.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 18.64it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =     153.92 ms /   256 runs   (    0.60 ms per token,  1663.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18023.63 ms /  1868 tokens (    9.65 ms per token,   103.64 tokens per second)\n",
      "llama_print_timings:        eval time =    7022.27 ms /   255 runs   (   27.54 ms per token,    36.31 tokens per second)\n",
      "llama_print_timings:       total time =   26541.94 ms /  2123 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 342.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.52it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       6.62 ms /    11 runs   (    0.60 ms per token,  1660.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   13982.28 ms /  1472 tokens (    9.50 ms per token,   105.28 tokens per second)\n",
      "llama_print_timings:        eval time =     291.74 ms /    11 runs   (   26.52 ms per token,    37.71 tokens per second)\n",
      "llama_print_timings:       total time =   14707.00 ms /  1483 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 309.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.67it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      18.10 ms /    30 runs   (    0.60 ms per token,  1657.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16886.40 ms /  1755 tokens (    9.62 ms per token,   103.93 tokens per second)\n",
      "llama_print_timings:        eval time =     780.34 ms /    29 runs   (   26.91 ms per token,    37.16 tokens per second)\n",
      "llama_print_timings:       total time =   18244.95 ms /  1784 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 509.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.68it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.21 ms /    17 runs   (    0.60 ms per token,  1664.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15560.20 ms /  1626 tokens (    9.57 ms per token,   104.50 tokens per second)\n",
      "llama_print_timings:        eval time =     426.91 ms /    16 runs   (   26.68 ms per token,    37.48 tokens per second)\n",
      "llama_print_timings:       total time =   16487.14 ms /  1642 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 38.71it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.90 ms /    18 runs   (    0.61 ms per token,  1650.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1164.92 ms /   120 tokens (    9.71 ms per token,   103.01 tokens per second)\n",
      "llama_print_timings:        eval time =     422.51 ms /    17 runs   (   24.85 ms per token,    40.24 tokens per second)\n",
      "llama_print_timings:       total time =    1681.41 ms /   137 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 283.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.09it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      13.73 ms /    23 runs   (    0.60 ms per token,  1675.53 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15955.78 ms /  1668 tokens (    9.57 ms per token,   104.54 tokens per second)\n",
      "llama_print_timings:        eval time =     593.43 ms /    22 runs   (   26.97 ms per token,    37.07 tokens per second)\n",
      "llama_print_timings:       total time =   17070.94 ms /  1690 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 313.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.84it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      20.98 ms /    35 runs   (    0.60 ms per token,  1668.02 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18431.02 ms /  1904 tokens (    9.68 ms per token,   103.30 tokens per second)\n",
      "llama_print_timings:        eval time =     931.55 ms /    34 runs   (   27.40 ms per token,    36.50 tokens per second)\n",
      "llama_print_timings:       total time =   19995.10 ms /  1938 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.70it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      35.76 ms /    60 runs   (    0.60 ms per token,  1678.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2600.50 ms /   290 tokens (    8.97 ms per token,   111.52 tokens per second)\n",
      "llama_print_timings:        eval time =    1342.26 ms /    59 runs   (   22.75 ms per token,    43.96 tokens per second)\n",
      "llama_print_timings:       total time =    4216.48 ms /   349 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.48it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       2.98 ms /     5 runs   (    0.60 ms per token,  1678.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2059.11 ms /   232 tokens (    8.88 ms per token,   112.67 tokens per second)\n",
      "llama_print_timings:        eval time =      93.54 ms /     4 runs   (   23.38 ms per token,    42.76 tokens per second)\n",
      "llama_print_timings:       total time =    2227.73 ms /   236 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.39it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      65.98 ms /   110 runs   (    0.60 ms per token,  1667.12 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2628.72 ms /   294 tokens (    8.94 ms per token,   111.84 tokens per second)\n",
      "llama_print_timings:        eval time =    2489.95 ms /   109 runs   (   22.84 ms per token,    43.78 tokens per second)\n",
      "llama_print_timings:       total time =    5571.05 ms /   403 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.54it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       9.04 ms /    15 runs   (    0.60 ms per token,  1658.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1987.01 ms /   223 tokens (    8.91 ms per token,   112.23 tokens per second)\n",
      "llama_print_timings:        eval time =     320.73 ms /    14 runs   (   22.91 ms per token,    43.65 tokens per second)\n",
      "llama_print_timings:       total time =    2417.98 ms /   237 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.36it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       2.96 ms /     5 runs   (    0.59 ms per token,  1688.05 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2029.86 ms /   227 tokens (    8.94 ms per token,   111.83 tokens per second)\n",
      "llama_print_timings:        eval time =      93.18 ms /     4 runs   (   23.30 ms per token,    42.93 tokens per second)\n",
      "llama_print_timings:       total time =    2197.40 ms /   231 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 367.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.36it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      21.64 ms /    36 runs   (    0.60 ms per token,  1663.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18345.29 ms /  1899 tokens (    9.66 ms per token,   103.51 tokens per second)\n",
      "llama_print_timings:        eval time =     959.00 ms /    35 runs   (   27.40 ms per token,    36.50 tokens per second)\n",
      "llama_print_timings:       total time =   19931.75 ms /  1934 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.81it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      76.27 ms /   127 runs   (    0.60 ms per token,  1665.16 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3036.06 ms /   338 tokens (    8.98 ms per token,   111.33 tokens per second)\n",
      "llama_print_timings:        eval time =    2896.99 ms /   126 runs   (   22.99 ms per token,    43.49 tokens per second)\n",
      "llama_print_timings:       total time =    6447.13 ms /   464 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.55it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      33.12 ms /    55 runs   (    0.60 ms per token,  1660.68 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3160.22 ms /   352 tokens (    8.98 ms per token,   111.38 tokens per second)\n",
      "llama_print_timings:        eval time =    1240.90 ms /    54 runs   (   22.98 ms per token,    43.52 tokens per second)\n",
      "llama_print_timings:       total time =    4676.07 ms /   406 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.06it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      30.52 ms /    51 runs   (    0.60 ms per token,  1671.14 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2114.58 ms /   237 tokens (    8.92 ms per token,   112.08 tokens per second)\n",
      "llama_print_timings:        eval time =    1129.70 ms /    50 runs   (   22.59 ms per token,    44.26 tokens per second)\n",
      "llama_print_timings:       total time =    3472.31 ms /   287 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 68.96it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      16.93 ms /    28 runs   (    0.60 ms per token,  1654.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1974.66 ms /   221 tokens (    8.94 ms per token,   111.92 tokens per second)\n",
      "llama_print_timings:        eval time =     611.00 ms /    27 runs   (   22.63 ms per token,    44.19 tokens per second)\n",
      "llama_print_timings:       total time =    2737.64 ms /   248 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.88it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      92.69 ms /   155 runs   (    0.60 ms per token,  1672.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2707.36 ms /   304 tokens (    8.91 ms per token,   112.29 tokens per second)\n",
      "llama_print_timings:        eval time =    3551.86 ms /   155 runs   (   22.92 ms per token,    43.64 tokens per second)\n",
      "llama_print_timings:       total time =    6867.08 ms /   459 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 214.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.37it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       3.62 ms /     6 runs   (    0.60 ms per token,  1658.37 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12905.00 ms /  1368 tokens (    9.43 ms per token,   106.01 tokens per second)\n",
      "llama_print_timings:        eval time =     157.18 ms /     6 runs   (   26.20 ms per token,    38.17 tokens per second)\n",
      "llama_print_timings:       total time =   13448.51 ms /  1374 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.21it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      82.92 ms /   138 runs   (    0.60 ms per token,  1664.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2791.53 ms /   312 tokens (    8.95 ms per token,   111.77 tokens per second)\n",
      "llama_print_timings:        eval time =    3140.42 ms /   137 runs   (   22.92 ms per token,    43.62 tokens per second)\n",
      "llama_print_timings:       total time =    6469.96 ms /   449 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.43it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      39.01 ms /    65 runs   (    0.60 ms per token,  1666.41 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3063.73 ms /   342 tokens (    8.96 ms per token,   111.63 tokens per second)\n",
      "llama_print_timings:        eval time =    1467.28 ms /    64 runs   (   22.93 ms per token,    43.62 tokens per second)\n",
      "llama_print_timings:       total time =    4839.25 ms /   406 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 325.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.82it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      36.05 ms /    60 runs   (    0.60 ms per token,  1664.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =   19025.70 ms /  1963 tokens (    9.69 ms per token,   103.18 tokens per second)\n",
      "llama_print_timings:        eval time =    1636.69 ms /    59 runs   (   27.74 ms per token,    36.05 tokens per second)\n",
      "llama_print_timings:       total time =   21402.88 ms /  2022 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 71.67it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      34.78 ms /    58 runs   (    0.60 ms per token,  1667.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3087.77 ms /   344 tokens (    8.98 ms per token,   111.41 tokens per second)\n",
      "llama_print_timings:        eval time =    1307.65 ms /    57 runs   (   22.94 ms per token,    43.59 tokens per second)\n",
      "llama_print_timings:       total time =    4677.09 ms /   401 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.80it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      22.10 ms /    37 runs   (    0.60 ms per token,  1674.28 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2129.20 ms /   238 tokens (    8.95 ms per token,   111.78 tokens per second)\n",
      "llama_print_timings:        eval time =     815.58 ms /    36 runs   (   22.66 ms per token,    44.14 tokens per second)\n",
      "llama_print_timings:       total time =    3127.57 ms /   274 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 294.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.28it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      32.97 ms /    55 runs   (    0.60 ms per token,  1668.13 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8176.99 ms /   886 tokens (    9.23 ms per token,   108.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1324.27 ms /    54 runs   (   24.52 ms per token,    40.78 tokens per second)\n",
      "llama_print_timings:       total time =    9920.23 ms /   940 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 84.49it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      39.70 ms /    66 runs   (    0.60 ms per token,  1662.64 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3054.57 ms /   341 tokens (    8.96 ms per token,   111.64 tokens per second)\n",
      "llama_print_timings:        eval time =    1490.83 ms /    65 runs   (   22.94 ms per token,    43.60 tokens per second)\n",
      "llama_print_timings:       total time =    4854.97 ms /   406 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 296.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.25it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       4.26 ms /     7 runs   (    0.61 ms per token,  1644.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15544.45 ms /  1627 tokens (    9.55 ms per token,   104.67 tokens per second)\n",
      "llama_print_timings:        eval time =     162.06 ms /     6 runs   (   27.01 ms per token,    37.02 tokens per second)\n",
      "llama_print_timings:       total time =   16169.71 ms /  1633 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 70.38it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      23.96 ms /    40 runs   (    0.60 ms per token,  1669.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2177.54 ms /   244 tokens (    8.92 ms per token,   112.05 tokens per second)\n",
      "llama_print_timings:        eval time =     883.29 ms /    39 runs   (   22.65 ms per token,    44.15 tokens per second)\n",
      "llama_print_timings:       total time =    3256.09 ms /   283 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.71it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      12.13 ms /    20 runs   (    0.61 ms per token,  1648.94 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2093.85 ms /   234 tokens (    8.95 ms per token,   111.76 tokens per second)\n",
      "llama_print_timings:        eval time =     429.13 ms /    19 runs   (   22.59 ms per token,    44.28 tokens per second)\n",
      "llama_print_timings:       total time =    2655.96 ms /   253 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 270.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 22.91it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       1.78 ms /     3 runs   (    0.59 ms per token,  1685.39 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2197.52 ms /   247 tokens (    8.90 ms per token,   112.40 tokens per second)\n",
      "llama_print_timings:        eval time =      47.97 ms /     2 runs   (   23.98 ms per token,    41.70 tokens per second)\n",
      "llama_print_timings:       total time =    2319.29 ms /   249 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.84it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      18.65 ms /    31 runs   (    0.60 ms per token,  1662.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1867.60 ms /   210 tokens (    8.89 ms per token,   112.44 tokens per second)\n",
      "llama_print_timings:        eval time =     676.60 ms /    30 runs   (   22.55 ms per token,    44.34 tokens per second)\n",
      "llama_print_timings:       total time =    2703.68 ms /   240 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 240.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.99it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      16.96 ms /    28 runs   (    0.61 ms per token,  1650.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16272.51 ms /  1701 tokens (    9.57 ms per token,   104.53 tokens per second)\n",
      "llama_print_timings:        eval time =     724.68 ms /    27 runs   (   26.84 ms per token,    37.26 tokens per second)\n",
      "llama_print_timings:       total time =   17564.96 ms /  1728 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 82.66it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      33.59 ms /    56 runs   (    0.60 ms per token,  1667.21 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3337.72 ms /   372 tokens (    8.97 ms per token,   111.45 tokens per second)\n",
      "llama_print_timings:        eval time =    1266.13 ms /    55 runs   (   23.02 ms per token,    43.44 tokens per second)\n",
      "llama_print_timings:       total time =    4884.70 ms /   427 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 286.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.25it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      34.12 ms /    57 runs   (    0.60 ms per token,  1670.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17390.64 ms /  1808 tokens (    9.62 ms per token,   103.96 tokens per second)\n",
      "llama_print_timings:        eval time =    1514.87 ms /    56 runs   (   27.05 ms per token,    36.97 tokens per second)\n",
      "llama_print_timings:       total time =   19585.54 ms /  1864 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 329.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.37it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      79.06 ms /   132 runs   (    0.60 ms per token,  1669.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =   11869.68 ms /  1259 tokens (    9.43 ms per token,   106.07 tokens per second)\n",
      "llama_print_timings:        eval time =    3360.57 ms /   131 runs   (   25.65 ms per token,    38.98 tokens per second)\n",
      "llama_print_timings:       total time =   16023.46 ms /  1390 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 38.91it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       3.00 ms /     5 runs   (    0.60 ms per token,  1666.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2554.83 ms /   286 tokens (    8.93 ms per token,   111.94 tokens per second)\n",
      "llama_print_timings:        eval time =      92.31 ms /     4 runs   (   23.08 ms per token,    43.33 tokens per second)\n",
      "llama_print_timings:       total time =    2737.01 ms /   290 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 78.52it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      26.43 ms /    44 runs   (    0.60 ms per token,  1664.77 tokens per second)\n",
      "llama_print_timings: prompt eval time =    4807.57 ms /   530 tokens (    9.07 ms per token,   110.24 tokens per second)\n",
      "llama_print_timings:        eval time =    1007.65 ms /    43 runs   (   23.43 ms per token,    42.67 tokens per second)\n",
      "llama_print_timings:       total time =    6098.08 ms /   573 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 253.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.87it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      31.18 ms /    52 runs   (    0.60 ms per token,  1667.63 tokens per second)\n",
      "llama_print_timings: prompt eval time =   18316.76 ms /  1895 tokens (    9.67 ms per token,   103.46 tokens per second)\n",
      "llama_print_timings:        eval time =    1398.37 ms /    51 runs   (   27.42 ms per token,    36.47 tokens per second)\n",
      "llama_print_timings:       total time =   20401.13 ms /  1946 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 82.63it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      11.41 ms /    19 runs   (    0.60 ms per token,  1664.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2184.49 ms /   245 tokens (    8.92 ms per token,   112.15 tokens per second)\n",
      "llama_print_timings:        eval time =     409.05 ms /    18 runs   (   22.72 ms per token,    44.00 tokens per second)\n",
      "llama_print_timings:       total time =    2719.73 ms /   263 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 192.40000000000003 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 35.32it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       3.56 ms /     6 runs   (    0.59 ms per token,  1685.87 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2263.52 ms /   254 tokens (    8.91 ms per token,   112.21 tokens per second)\n",
      "llama_print_timings:        eval time =     114.86 ms /     5 runs   (   22.97 ms per token,    43.53 tokens per second)\n",
      "llama_print_timings:       total time =    2463.76 ms /   259 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.12it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.18 ms /    17 runs   (    0.60 ms per token,  1670.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2490.47 ms /   280 tokens (    8.89 ms per token,   112.43 tokens per second)\n",
      "llama_print_timings:        eval time =     387.46 ms /    17 runs   (   22.79 ms per token,    43.88 tokens per second)\n",
      "llama_print_timings:       total time =    3015.91 ms /   297 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 250.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.11it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      60.09 ms /   100 runs   (    0.60 ms per token,  1664.31 tokens per second)\n",
      "llama_print_timings: prompt eval time =   15673.16 ms /  1642 tokens (    9.55 ms per token,   104.77 tokens per second)\n",
      "llama_print_timings:        eval time =    2648.66 ms /    99 runs   (   26.75 ms per token,    37.38 tokens per second)\n",
      "llama_print_timings:       total time =   19117.23 ms /  1741 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 277.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.15it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       4.20 ms /     7 runs   (    0.60 ms per token,  1666.67 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12026.16 ms /  1279 tokens (    9.40 ms per token,   106.35 tokens per second)\n",
      "llama_print_timings:        eval time =     155.74 ms /     6 runs   (   25.96 ms per token,    38.53 tokens per second)\n",
      "llama_print_timings:       total time =   12548.12 ms /  1285 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 219.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.44it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      28.70 ms /    48 runs   (    0.60 ms per token,  1672.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12474.26 ms /  1323 tokens (    9.43 ms per token,   106.06 tokens per second)\n",
      "llama_print_timings:        eval time =    1209.31 ms /    47 runs   (   25.73 ms per token,    38.87 tokens per second)\n",
      "llama_print_timings:       total time =   14204.15 ms /  1370 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 304.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.36it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       6.55 ms /    11 runs   (    0.60 ms per token,  1678.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    8112.53 ms /   880 tokens (    9.22 ms per token,   108.47 tokens per second)\n",
      "llama_print_timings:        eval time =     245.91 ms /    10 runs   (   24.59 ms per token,    40.67 tokens per second)\n",
      "llama_print_timings:       total time =    8626.22 ms /   890 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 215.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 30.49it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       9.56 ms /    16 runs   (    0.60 ms per token,  1673.99 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2122.83 ms /   238 tokens (    8.92 ms per token,   112.11 tokens per second)\n",
      "llama_print_timings:        eval time =     339.41 ms /    15 runs   (   22.63 ms per token,    44.19 tokens per second)\n",
      "llama_print_timings:       total time =    2575.23 ms /   253 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 75.57it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      57.55 ms /    96 runs   (    0.60 ms per token,  1668.00 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2418.39 ms /   272 tokens (    8.89 ms per token,   112.47 tokens per second)\n",
      "llama_print_timings:        eval time =    2163.35 ms /    95 runs   (   22.77 ms per token,    43.91 tokens per second)\n",
      "llama_print_timings:       total time =    4984.78 ms /   367 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 70.13it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      81.68 ms /   137 runs   (    0.60 ms per token,  1677.30 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2607.62 ms /   291 tokens (    8.96 ms per token,   111.60 tokens per second)\n",
      "llama_print_timings:        eval time =    3104.84 ms /   136 runs   (   22.83 ms per token,    43.80 tokens per second)\n",
      "llama_print_timings:       total time =    6239.91 ms /   427 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.84it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      34.12 ms /    57 runs   (    0.60 ms per token,  1670.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2062.39 ms /   232 tokens (    8.89 ms per token,   112.49 tokens per second)\n",
      "llama_print_timings:        eval time =    1285.92 ms /    57 runs   (   22.56 ms per token,    44.33 tokens per second)\n",
      "llama_print_timings:       total time =    3592.97 ms /   289 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.64it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      48.73 ms /    81 runs   (    0.60 ms per token,  1662.22 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3171.83 ms /   354 tokens (    8.96 ms per token,   111.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1833.33 ms /    80 runs   (   22.92 ms per token,    43.64 tokens per second)\n",
      "llama_print_timings:       total time =    5363.71 ms /   434 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.16it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      33.97 ms /    57 runs   (    0.60 ms per token,  1677.95 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2989.21 ms /   335 tokens (    8.92 ms per token,   112.07 tokens per second)\n",
      "llama_print_timings:        eval time =    1281.03 ms /    56 runs   (   22.88 ms per token,    43.71 tokens per second)\n",
      "llama_print_timings:       total time =    4538.77 ms /   391 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 308.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.05it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      20.64 ms /    34 runs   (    0.61 ms per token,  1647.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =   17540.97 ms /  1826 tokens (    9.61 ms per token,   104.10 tokens per second)\n",
      "llama_print_timings:        eval time =     898.73 ms /    33 runs   (   27.23 ms per token,    36.72 tokens per second)\n",
      "llama_print_timings:       total time =   19075.91 ms /  1859 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.74it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      67.38 ms /   112 runs   (    0.60 ms per token,  1662.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3125.67 ms /   349 tokens (    8.96 ms per token,   111.66 tokens per second)\n",
      "llama_print_timings:        eval time =    2554.55 ms /   111 runs   (   23.01 ms per token,    43.45 tokens per second)\n",
      "llama_print_timings:       total time =    6165.04 ms /   460 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.96it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      42.39 ms /    71 runs   (    0.60 ms per token,  1674.88 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3565.76 ms /   398 tokens (    8.96 ms per token,   111.62 tokens per second)\n",
      "llama_print_timings:        eval time =    1616.74 ms /    70 runs   (   23.10 ms per token,    43.30 tokens per second)\n",
      "llama_print_timings:       total time =    5516.04 ms /   468 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 275.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 23.06it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      30.28 ms /    50 runs   (    0.61 ms per token,  1651.04 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16288.40 ms /  1703 tokens (    9.56 ms per token,   104.55 tokens per second)\n",
      "llama_print_timings:        eval time =    1312.41 ms /    49 runs   (   26.78 ms per token,    37.34 tokens per second)\n",
      "llama_print_timings:       total time =   18254.68 ms /  1752 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 208.6 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 35.50it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      20.56 ms /    34 runs   (    0.60 ms per token,  1653.86 tokens per second)\n",
      "llama_print_timings: prompt eval time =   16696.19 ms /  1739 tokens (    9.60 ms per token,   104.16 tokens per second)\n",
      "llama_print_timings:        eval time =     889.72 ms /    33 runs   (   26.96 ms per token,    37.09 tokens per second)\n",
      "llama_print_timings:       total time =   18184.15 ms /  1772 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 321.2 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.77it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       2.41 ms /     4 runs   (    0.60 ms per token,  1663.20 tokens per second)\n",
      "llama_print_timings: prompt eval time =    6928.69 ms /   756 tokens (    9.16 ms per token,   109.11 tokens per second)\n",
      "llama_print_timings:        eval time =      75.60 ms /     3 runs   (   25.20 ms per token,    39.68 tokens per second)\n",
      "llama_print_timings:       total time =    7216.54 ms /   759 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.80it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      57.97 ms /    97 runs   (    0.60 ms per token,  1673.42 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3134.19 ms /   349 tokens (    8.98 ms per token,   111.35 tokens per second)\n",
      "llama_print_timings:        eval time =    2205.05 ms /    96 runs   (   22.97 ms per token,    43.54 tokens per second)\n",
      "llama_print_timings:       total time =    5752.05 ms /   445 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.97it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      43.25 ms /    72 runs   (    0.60 ms per token,  1664.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3510.07 ms /   391 tokens (    8.98 ms per token,   111.39 tokens per second)\n",
      "llama_print_timings:        eval time =    1638.75 ms /    71 runs   (   23.08 ms per token,    43.33 tokens per second)\n",
      "llama_print_timings:       total time =    5488.14 ms /   462 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.73it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      36.71 ms /    61 runs   (    0.60 ms per token,  1661.81 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2477.51 ms /   278 tokens (    8.91 ms per token,   112.21 tokens per second)\n",
      "llama_print_timings:        eval time =    1361.66 ms /    60 runs   (   22.69 ms per token,    44.06 tokens per second)\n",
      "llama_print_timings:       total time =    4109.90 ms /   338 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 242.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 24.92it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       4.22 ms /     7 runs   (    0.60 ms per token,  1657.59 tokens per second)\n",
      "llama_print_timings: prompt eval time =   10662.24 ms /  1143 tokens (    9.33 ms per token,   107.20 tokens per second)\n",
      "llama_print_timings:        eval time =     152.50 ms /     6 runs   (   25.42 ms per token,    39.34 tokens per second)\n",
      "llama_print_timings:       total time =   11139.04 ms /  1149 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 68.44it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      81.00 ms /   135 runs   (    0.60 ms per token,  1666.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3357.00 ms /   375 tokens (    8.95 ms per token,   111.71 tokens per second)\n",
      "llama_print_timings:        eval time =    3094.11 ms /   134 runs   (   23.09 ms per token,    43.31 tokens per second)\n",
      "llama_print_timings:       total time =    7000.85 ms /   509 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.71it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      42.54 ms /    71 runs   (    0.60 ms per token,  1668.98 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2060.22 ms /   232 tokens (    8.88 ms per token,   112.61 tokens per second)\n",
      "llama_print_timings:        eval time =    1581.66 ms /    70 runs   (   22.60 ms per token,    44.26 tokens per second)\n",
      "llama_print_timings:       total time =    3936.36 ms /   302 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 43.72it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      19.73 ms /    33 runs   (    0.60 ms per token,  1672.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2240.58 ms /   251 tokens (    8.93 ms per token,   112.02 tokens per second)\n",
      "llama_print_timings:        eval time =     722.70 ms /    32 runs   (   22.58 ms per token,    44.28 tokens per second)\n",
      "llama_print_timings:       total time =    3136.24 ms /   283 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 84.31it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      31.96 ms /    53 runs   (    0.60 ms per token,  1658.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2905.66 ms /   325 tokens (    8.94 ms per token,   111.85 tokens per second)\n",
      "llama_print_timings:        eval time =    1187.57 ms /    52 runs   (   22.84 ms per token,    43.79 tokens per second)\n",
      "llama_print_timings:       total time =    4353.06 ms /   377 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.26it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      35.43 ms /    59 runs   (    0.60 ms per token,  1665.40 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2207.39 ms /   248 tokens (    8.90 ms per token,   112.35 tokens per second)\n",
      "llama_print_timings:        eval time =    1335.16 ms /    59 runs   (   22.63 ms per token,    44.19 tokens per second)\n",
      "llama_print_timings:       total time =    3803.47 ms /   307 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 76.57it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       9.60 ms /    16 runs   (    0.60 ms per token,  1667.54 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1950.33 ms /   219 tokens (    8.91 ms per token,   112.29 tokens per second)\n",
      "llama_print_timings:        eval time =     339.75 ms /    15 runs   (   22.65 ms per token,    44.15 tokens per second)\n",
      "llama_print_timings:       total time =    2398.61 ms /   234 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 80.38it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      91.71 ms /   153 runs   (    0.60 ms per token,  1668.38 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3040.93 ms /   340 tokens (    8.94 ms per token,   111.81 tokens per second)\n",
      "llama_print_timings:        eval time =    3494.20 ms /   152 runs   (   22.99 ms per token,    43.50 tokens per second)\n",
      "llama_print_timings:       total time =    7142.13 ms /   492 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 65.78it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      18.60 ms /    31 runs   (    0.60 ms per token,  1666.58 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3849.68 ms /   428 tokens (    8.99 ms per token,   111.18 tokens per second)\n",
      "llama_print_timings:        eval time =     694.77 ms /    30 runs   (   23.16 ms per token,    43.18 tokens per second)\n",
      "llama_print_timings:       total time =    4755.71 ms /   458 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 83.52it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       5.36 ms /     9 runs   (    0.60 ms per token,  1678.17 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2906.70 ms /   326 tokens (    8.92 ms per token,   112.15 tokens per second)\n",
      "llama_print_timings:        eval time =     186.81 ms /     8 runs   (   23.35 ms per token,    42.83 tokens per second)\n",
      "llama_print_timings:       total time =    3207.24 ms /   334 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "Your documents are roughly 329.4 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 19.24it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =       7.79 ms /    13 runs   (    0.60 ms per token,  1667.74 tokens per second)\n",
      "llama_print_timings: prompt eval time =   12340.28 ms /  1311 tokens (    9.41 ms per token,   106.24 tokens per second)\n",
      "llama_print_timings:        eval time =     309.89 ms /    12 runs   (   25.82 ms per token,    38.72 tokens per second)\n",
      "llama_print_timings:       total time =   13047.74 ms /  1323 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 82.38it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.23 ms /    17 runs   (    0.60 ms per token,  1661.62 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2061.44 ms /   232 tokens (    8.89 ms per token,   112.54 tokens per second)\n",
      "llama_print_timings:        eval time =     364.95 ms /    16 runs   (   22.81 ms per token,    43.84 tokens per second)\n",
      "llama_print_timings:       total time =    2542.21 ms /   248 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 84.49it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      24.56 ms /    41 runs   (    0.60 ms per token,  1669.45 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2060.44 ms /   232 tokens (    8.88 ms per token,   112.60 tokens per second)\n",
      "llama_print_timings:        eval time =     926.48 ms /    41 runs   (   22.60 ms per token,    44.25 tokens per second)\n",
      "llama_print_timings:       total time =    3187.35 ms /   273 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 82.85it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      96.16 ms /   159 runs   (    0.60 ms per token,  1653.51 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2272.04 ms /   256 tokens (    8.88 ms per token,   112.67 tokens per second)\n",
      "llama_print_timings:        eval time =    3598.48 ms /   158 runs   (   22.78 ms per token,    43.91 tokens per second)\n",
      "llama_print_timings:       total time =    6500.60 ms /   414 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.57it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      44.51 ms /    74 runs   (    0.60 ms per token,  1662.55 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2336.79 ms /   263 tokens (    8.89 ms per token,   112.55 tokens per second)\n",
      "llama_print_timings:        eval time =    1656.96 ms /    73 runs   (   22.70 ms per token,    44.06 tokens per second)\n",
      "llama_print_timings:       total time =    4317.43 ms /   336 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 84.49it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      20.44 ms /    34 runs   (    0.60 ms per token,  1663.32 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2203.94 ms /   248 tokens (    8.89 ms per token,   112.53 tokens per second)\n",
      "llama_print_timings:        eval time =     747.82 ms /    33 runs   (   22.66 ms per token,    44.13 tokens per second)\n",
      "llama_print_timings:       total time =    3132.47 ms /   281 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 67.68it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      91.94 ms /   152 runs   (    0.60 ms per token,  1653.29 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2926.19 ms /   328 tokens (    8.92 ms per token,   112.09 tokens per second)\n",
      "llama_print_timings:        eval time =    3472.77 ms /   151 runs   (   23.00 ms per token,    43.48 tokens per second)\n",
      "llama_print_timings:       total time =    7027.42 ms /   479 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 81.57it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      25.73 ms /    43 runs   (    0.60 ms per token,  1671.07 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2733.97 ms /   306 tokens (    8.93 ms per token,   111.93 tokens per second)\n",
      "llama_print_timings:        eval time =     960.46 ms /    42 runs   (   22.87 ms per token,    43.73 tokens per second)\n",
      "llama_print_timings:       total time =    3920.76 ms /   348 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 79.28it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      47.01 ms /    78 runs   (    0.60 ms per token,  1659.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2604.40 ms /   291 tokens (    8.95 ms per token,   111.73 tokens per second)\n",
      "llama_print_timings:        eval time =    1758.28 ms /    77 runs   (   22.83 ms per token,    43.79 tokens per second)\n",
      "llama_print_timings:       total time =    4713.53 ms /   368 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 82.19it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      61.85 ms /   103 runs   (    0.60 ms per token,  1665.43 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3187.23 ms /   357 tokens (    8.93 ms per token,   112.01 tokens per second)\n",
      "llama_print_timings:        eval time =    2349.51 ms /   102 runs   (   23.03 ms per token,    43.41 tokens per second)\n",
      "llama_print_timings:       total time =    5981.45 ms /   459 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 82.67it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      20.97 ms /    35 runs   (    0.60 ms per token,  1668.73 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2633.50 ms /   296 tokens (    8.90 ms per token,   112.40 tokens per second)\n",
      "llama_print_timings:        eval time =     774.31 ms /    34 runs   (   22.77 ms per token,    43.91 tokens per second)\n",
      "llama_print_timings:       total time =    3604.71 ms /   330 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 82.85it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =     118.12 ms /   197 runs   (    0.60 ms per token,  1667.82 tokens per second)\n",
      "llama_print_timings: prompt eval time =    3175.39 ms /   355 tokens (    8.94 ms per token,   111.80 tokens per second)\n",
      "llama_print_timings:        eval time =    4523.22 ms /   196 runs   (   23.08 ms per token,    43.33 tokens per second)\n",
      "llama_print_timings:       total time =    8476.43 ms /   551 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 70.86it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      11.49 ms /    19 runs   (    0.60 ms per token,  1654.19 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2228.31 ms /   250 tokens (    8.91 ms per token,   112.19 tokens per second)\n",
      "llama_print_timings:        eval time =     407.27 ms /    18 runs   (   22.63 ms per token,    44.20 tokens per second)\n",
      "llama_print_timings:       total time =    2779.03 ms /   268 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 82.19it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.21 ms /    17 runs   (    0.60 ms per token,  1665.36 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2186.33 ms /   246 tokens (    8.89 ms per token,   112.52 tokens per second)\n",
      "llama_print_timings:        eval time =     363.33 ms /    16 runs   (   22.71 ms per token,    44.04 tokens per second)\n",
      "llama_print_timings:       total time =    2669.42 ms /   262 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 84.65it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      10.18 ms /    17 runs   (    0.60 ms per token,  1670.11 tokens per second)\n",
      "llama_print_timings: prompt eval time =    1660.92 ms /   187 tokens (    8.88 ms per token,   112.59 tokens per second)\n",
      "llama_print_timings:        eval time =     362.48 ms /    16 runs   (   22.66 ms per token,    44.14 tokens per second)\n",
      "llama_print_timings:       total time =    2127.38 ms /   203 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 85.08it/s]\n",
      "Llama.generate: prefix-match hit\n",
      "\n",
      "llama_print_timings:        load time =    1215.36 ms\n",
      "llama_print_timings:      sample time =      55.37 ms /    92 runs   (    0.60 ms per token,  1661.52 tokens per second)\n",
      "llama_print_timings: prompt eval time =    2195.73 ms /   248 tokens (    8.85 ms per token,   112.95 tokens per second)\n",
      "llama_print_timings:        eval time =    2058.25 ms /    91 runs   (   22.62 ms per token,    44.21 tokens per second)\n",
      "llama_print_timings:       total time =    4625.13 ms /   339 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>ModelAnswer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>webpages</td>\n",
       "      <td>Do students in Kiltie have to memorize music?</td>\n",
       "      <td>No</td>\n",
       "      <td>No, students do not have to memorize music.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>webpages</td>\n",
       "      <td>Do I need to email my home address to the Kiltie Band Director if I'm interested in joining?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes, you should email your home address to the Kiltie Band Director if you are interested in joining.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>webpages</td>\n",
       "      <td>Do I need to audition to join the Kiltie Band?</td>\n",
       "      <td>No</td>\n",
       "      <td>No, any member of the campus community with music experience is able to join the Kiltie Band!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>webpages</td>\n",
       "      <td>Where does the Kiltie Band rehearse?</td>\n",
       "      <td>CUC Studio Theater</td>\n",
       "      <td>The Kiltie Band rehearses in the CUC Studio Theater.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>webpages</td>\n",
       "      <td>When should I get an instrument from the Kiltie Band if I need one?</td>\n",
       "      <td>4:30p.m. before the first rehearsal</td>\n",
       "      <td>The Kiltie Band has instruments available for borrowing starting at 4:30 p.m. before the first rehearsal on Monday.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Category  \\\n",
       "0  webpages   \n",
       "1  webpages   \n",
       "2  webpages   \n",
       "3  webpages   \n",
       "4  webpages   \n",
       "\n",
       "                                                                                       Question  \\\n",
       "0                                                 Do students in Kiltie have to memorize music?   \n",
       "1  Do I need to email my home address to the Kiltie Band Director if I'm interested in joining?   \n",
       "2                                                Do I need to audition to join the Kiltie Band?   \n",
       "3                                                          Where does the Kiltie Band rehearse?   \n",
       "4                           When should I get an instrument from the Kiltie Band if I need one?   \n",
       "\n",
       "                                Answer  \\\n",
       "0                                   No   \n",
       "1                                  Yes   \n",
       "2                                   No   \n",
       "3                   CUC Studio Theater   \n",
       "4  4:30p.m. before the first rehearsal   \n",
       "\n",
       "                                                                                                           ModelAnswer  \n",
       "0                                                                          No, students do not have to memorize music.  \n",
       "1                Yes, you should email your home address to the Kiltie Band Director if you are interested in joining.  \n",
       "2                        No, any member of the campus community with music experience is able to join the Kiltie Band!  \n",
       "3                                                                 The Kiltie Band rehearses in the CUC Studio Theater.  \n",
       "4  The Kiltie Band has instruments available for borrowing starting at 4:30 p.m. before the first rehearsal on Monday.  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df_with_model_answers = generate_answers_and_save(csv_path=csv_input_dir, llm = llm_answer_gen)\n",
    "# print(df_with_model_answers.shape)\n",
    "# df_with_model_answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: 0.08900523560209424\n",
      "F1 score: 0.2469942301836472\n",
      "Recall score: 0.48594194350185055\n"
     ]
    }
   ],
   "source": [
    "# # with additional extraction prompt after generation without prompt\n",
    "# type(df_with_model_answers['ModelAnswer'][0]),type(df_with_model_answers['Answer'][0])\n",
    "# df_with_model_answers = df_with_model_answers.astype(str)  # Convert columns to string type\n",
    "\n",
    "# from evaluation_csv import total_score_csv\n",
    "# exact_match, f1_score, recall_score = total_score_csv(df_with_model_answers['ModelAnswer'], df_with_model_answers['Answer'])\n",
    "# print(f'Exact match: {exact_match}')\n",
    "# print(f'F1 score: {f1_score}')\n",
    "# print(f'Recall score: {recall_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "drdoGkF3wja4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: webpages\n",
      "Exact match: 0.07407407407407407\n",
      "F1 score: 0.2991502362824957\n",
      "Recall score: 0.6361828568334417\n",
      "\n",
      "\n",
      "Category: tabular_webpages\n",
      "Exact match: 0.0\n",
      "F1 score: 0.09635171805447396\n",
      "Recall score: 0.44166666666666665\n",
      "\n",
      "\n",
      "Category: other_pdf\n",
      "Exact match: 0.0\n",
      "F1 score: 0.154057882798292\n",
      "Recall score: 0.2581262939958592\n",
      "\n",
      "\n",
      "Category: papers_pdf\n",
      "Exact match: 0.0\n",
      "F1 score: 0.038841342988808425\n",
      "Recall score: 0.07936507936507937\n",
      "\n",
      "\n",
      "Category: schedule_pdf\n",
      "Exact match: 0.0\n",
      "F1 score: 0.12626262626262627\n",
      "Recall score: 0.1212121212121212\n",
      "\n",
      "\n",
      "Category: jsons\n",
      "Exact match: 0.18571428571428572\n",
      "F1 score: 0.34399769336092967\n",
      "Recall score: 0.6141290166834544\n",
      "\n",
      "\n",
      "Category: json_hard\n",
      "Exact match: 0.0\n",
      "F1 score: 0.019620958751393535\n",
      "Recall score: 0.05795454545454546\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for category in df_with_model_answers['Category'].unique():\n",
    "#     print(f'Category: {category}')\n",
    "#     df_category = df_with_model_answers[df_with_model_answers['Category'] == category]\n",
    "#     exact_match, f1_score, recall_score = total_score_csv(df_category['ModelAnswer'], df_category['Answer'])\n",
    "#     print(f'Exact match: {exact_match}')\n",
    "#     print(f'F1 score: {f1_score}')\n",
    "#     print(f'Recall score: {recall_score}')\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "D2T8to5h39kK",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_with_model_answers.to_csv('../llama-no-temp-one-shot-output.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display(df_with_model_answers.head(60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0567916024824a5cb153ee96ae90bcfd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "08e5c13a33c442d69db6a4e5cb1aefd8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_486f19f393194214820b2e4719793822",
       "IPY_MODEL_82baa3fb82e24827b49b6bda22f72d78",
       "IPY_MODEL_a2c29ede70564d5b879b4084453d753f"
      ],
      "layout": "IPY_MODEL_bb8b2e9c41ea45dfa6fe7dff8a934fa2"
     }
    },
    "090fd1cdc0694112b57d28e833932350": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "091d085339a14a0ab5a23a60b24508f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8041af99466c478ab32dcb07ff597d41",
      "max": 438349816,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ec25bd06a00e4e2cab1b9f62905449ca",
      "value": 438349816
     }
    },
    "0b946716322e43bb9f4213087c6ee1de": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0bce7925eb1f4323a482bb90467bbc66": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0c73d54adcb74546b98a15ff898640d2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_8cd319e80b28462698e8adb196302802",
       "IPY_MODEL_4016322235e5422687997b6c942b8684",
       "IPY_MODEL_90efeff073a541e3ace582146fd5fd45"
      ],
      "layout": "IPY_MODEL_4da4cc126514421ebb1cd20eff5544f9"
     }
    },
    "0d0fbbe470054b0b879c1a62c411dc0c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6efb365326c24c339085be28cc203fdc",
       "IPY_MODEL_df4976d8d7ec47e4986f5e9c0395c3a9",
       "IPY_MODEL_e249235d4bb14478a802d3fc362fe215"
      ],
      "layout": "IPY_MODEL_ea2f3d5138cb4d3eb4b5f50122c42634"
     }
    },
    "0e167716cc6941e2aaa5e85a476f1b1a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0e6e63ccb1164deab11b438e7059d69f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "0eae622975df4cfaaa2e54cde12fecc2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0f57c3f8ee4d42b3b02aba300b4fe271": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "0fab8e8ae3fd4f47b43ee00df8a80deb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "1191b9f6ca7e491c91a54648d9a95740": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d3e5436855af42a5b2f170b44388e4a8",
      "placeholder": "​",
      "style": "IPY_MODEL_195ac8d683264e9999a419b94e60bdc1",
      "value": " 438M/438M [00:02&lt;00:00, 233MB/s]"
     }
    },
    "11e9dc55ddd64166a23b389ef4115bc5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1257cd90321540d8b5530c35b672ac38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_890443b9c63d42b597b1fce37ed2612f",
      "placeholder": "​",
      "style": "IPY_MODEL_e7bc53149d98433785d628cd3783cbdb",
      "value": "tokenizer.json: 100%"
     }
    },
    "13c4cd250f4c49169902a77e3b76727c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b215c0274af9444c9265aa576d33f6d5",
      "placeholder": "​",
      "style": "IPY_MODEL_f46f3a7030a34d3894d97214fb03e1c2",
      "value": "vocab.txt: 100%"
     }
    },
    "183c14e615d6471c9fe4b9a56d13f85a": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "195ac8d683264e9999a419b94e60bdc1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "1ad12046944a468297e3c982794251ef": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "1d5d56be1f3e4e06ad05b7a61c073773": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_37bbe9f296674040a1e4e6f414645aee",
      "placeholder": "​",
      "style": "IPY_MODEL_0567916024824a5cb153ee96ae90bcfd",
      "value": " 112/112 [00:00&lt;00:00, 6.75kB/s]"
     }
    },
    "1d9cc219294748d7a4d06a0ef220d034": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_bfd5578f1cc3406b9b6e49540918dffd",
      "max": 190,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_0fab8e8ae3fd4f47b43ee00df8a80deb",
      "value": 190
     }
    },
    "1efc8ad79138462e97c1bf5f00d8a210": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "200a894d245143b59503f70bbe099655": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_e68e4e022b7946139920753c55012db2",
       "IPY_MODEL_1d9cc219294748d7a4d06a0ef220d034",
       "IPY_MODEL_61055f305b414a9995dd0093c8dd085b"
      ],
      "layout": "IPY_MODEL_6793b20c68004ef6806c0719d850ee07"
     }
    },
    "20619dd814a245cb96eed1ffd6d23793": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_be42d78fa32c44f4ba983327b2f2b0ae",
      "max": 1633,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d696cc22200743caaaa0d70de5f29eaf",
      "value": 1633
     }
    },
    "234ab5cd38b14aec8426fcc28dfc4952": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "255d439785a54b9b8f37be9730c5b9f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25d444efba4c4f8ea2a15c4a197e83fa": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25ea5fce0d51425ca66647c86a69054d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_572f1df5c7904ccf939d45356997f7a7",
      "max": 112,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_abe064fd2d2943cc9658631022e7dfbe",
      "value": 112
     }
    },
    "26f3de4a37ed42c783b8cc8496a8830e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e5ff1078ea2d454ba88d92b978bee13f",
      "placeholder": "​",
      "style": "IPY_MODEL_97d0ee251daa487db840ba561a36d4a8",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "26f97033c9e5479da8dca6f7dc642ece": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45d616297f6d42fc89f2b28c6fc10c90",
      "placeholder": "​",
      "style": "IPY_MODEL_58bdc5e07c694b4f9fc4e6612c1de4c5",
      "value": " 219M/219M [00:01&lt;00:00, 174MB/s]"
     }
    },
    "2750e13775ad4cc5a1ca97abd23a7fa4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2c938e4d55734c248ffb292415aa5f5b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2d7b6a6ffb3849ec9828e24f4e193840": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "2f08477187154be899c3a1378e703260": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "2f225874a7174aae9695b521d33eb563": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "3088fb3c8cc74d7c97d9cbacc5b20c03": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3332c9a35d2e40ac860233ebfb8c64c7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "34f2229c854f4695b5aba5d5afe772f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "355092a75fa445ddb58c3c0e6c7c87f7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "365523a881584181973972b7df86c3eb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3a1808fea5884186924c50c1d44cdb2b",
      "max": 466081,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_6cb72ab4081a44fd8462b7d58da7236c",
      "value": 466081
     }
    },
    "37bbe9f296674040a1e4e6f414645aee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "38344f64512b421aa12ac82fcf4b195d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "392ee34639024892a83a85f6878b60ef": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3a1808fea5884186924c50c1d44cdb2b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3e583707d8f7457bb3a5433440f3a6cb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3f54b0320e054ef4ab7cc33ca97ad75c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_13c4cd250f4c49169902a77e3b76727c",
       "IPY_MODEL_9100526f824f4e2298b22f43461c6daa",
       "IPY_MODEL_5e7028e3f8f6469086b2b94066dc9526"
      ],
      "layout": "IPY_MODEL_6901a356aa2c4361953ba9b44065710b"
     }
    },
    "4016322235e5422687997b6c942b8684": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ea9b31c529f0482c99ac5ede1b0c2437",
      "max": 711661,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2f08477187154be899c3a1378e703260",
      "value": 711661
     }
    },
    "42c591eaee9f42f8ad965372898e3bdd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "45045e6a12e041e1b2b2713ff59ef938": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45c9710ebd6e45ddaa72f349380c5e16": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "45d616297f6d42fc89f2b28c6fc10c90": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4663ab81441947f7bd1b86a08d82a6b2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "46b7f140d4134097a8f6c7dd41d9acd3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "47d8987c94434ddb89281035f30e9d4f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45c9710ebd6e45ddaa72f349380c5e16",
      "placeholder": "​",
      "style": "IPY_MODEL_9defae4a31724cc1b6c0128b76bae404",
      "value": "config.json: 100%"
     }
    },
    "486f19f393194214820b2e4719793822": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6257ec67ee0043adab5a9e24a4f709a8",
      "placeholder": "​",
      "style": "IPY_MODEL_df62dad6b54444659f23d7cca3ddaa70",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "4977730998754c6d82831b47a647b428": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_761b0c7bec0f4ea98fff4469f3e147e1",
      "max": 57,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2f225874a7174aae9695b521d33eb563",
      "value": 57
     }
    },
    "4c7a193e4d3349dc8c2ff58c7137e77e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "4da4cc126514421ebb1cd20eff5544f9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4e8321573083413d80d545eb711dabcb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5582c1c8145b4fe786ba967188680aa7": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "572f1df5c7904ccf939d45356997f7a7": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "58bdc5e07c694b4f9fc4e6612c1de4c5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "58f1227b067740c99856d28d1b3535f0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4c9a624ca3342adbdda81d0c505d540",
      "placeholder": "​",
      "style": "IPY_MODEL_7b9187f7562945ba813032f73d5fed96",
      "value": "modules.json: 100%"
     }
    },
    "5bf549707f604a99a4743166718cd720": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b524e9e4f75a45aba71a8a2b68557700",
      "placeholder": "​",
      "style": "IPY_MODEL_fdbe596b18164389a51d860b24f3b15b",
      "value": "README.md: 100%"
     }
    },
    "5de0a6a267a24518ac2857b47c4d2d4e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "5e7028e3f8f6469086b2b94066dc9526": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_11e9dc55ddd64166a23b389ef4115bc5",
      "placeholder": "​",
      "style": "IPY_MODEL_090fd1cdc0694112b57d28e833932350",
      "value": " 232k/232k [00:00&lt;00:00, 1.78MB/s]"
     }
    },
    "5e9df76d9eb04c3cacc9caf4edacb913": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_25d444efba4c4f8ea2a15c4a197e83fa",
      "placeholder": "​",
      "style": "IPY_MODEL_fa2721e59a714068a0a5918cd7ec3246",
      "value": "model.safetensors: 100%"
     }
    },
    "5fca46bd83bd4f95ae25eecf342ff20f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fc979bda6f3c486e8108945371fee26e",
      "placeholder": "​",
      "style": "IPY_MODEL_5de0a6a267a24518ac2857b47c4d2d4e",
      "value": " 743/743 [00:00&lt;00:00, 31.0kB/s]"
     }
    },
    "6023241ffbf740ec871828d6276cccb8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d9d79532e89447e1ae295ea296d33dbf",
      "placeholder": "​",
      "style": "IPY_MODEL_4663ab81441947f7bd1b86a08d82a6b2",
      "value": " 466k/466k [00:00&lt;00:00, 2.32MB/s]"
     }
    },
    "61055f305b414a9995dd0093c8dd085b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f4388ee32ed9434ea5e756c22fb61002",
      "placeholder": "​",
      "style": "IPY_MODEL_eac3132f4d9349cfa6f1422686ae7bd5",
      "value": " 190/190 [00:00&lt;00:00, 11.7kB/s]"
     }
    },
    "6257ec67ee0043adab5a9e24a4f709a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "63a47d5282db441a8e5732ab3266af33": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "65617f6c0a3f40f1bb7198eadb9757da": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6584fa0193f0433ead447e60d2c82607": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_47d8987c94434ddb89281035f30e9d4f",
       "IPY_MODEL_cf1bd11c82b44b99ac40e8abbec3785f",
       "IPY_MODEL_7e069a6942cb4cb09ef6aa3661844f88"
      ],
      "layout": "IPY_MODEL_7e0b008fb7294404a73b8341c1230125"
     }
    },
    "666d42ef1a74402fb13ea79db36c1287": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "66b76f1f830445e98fb9c1acc1683cde": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_aee8fac5931844c5a88f651e7f854362",
      "max": 218990904,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e39705238d474d5a99f0a49832b408ed",
      "value": 218990904
     }
    },
    "6793b20c68004ef6806c0719d850ee07": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6901a356aa2c4361953ba9b44065710b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6aaf81405fa24b0e93a858d609b85036": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_7e04af96adae41a6b05ecc7bfe5a9fd9",
      "placeholder": "​",
      "style": "IPY_MODEL_b13c2b7166784d0f8d8a1ee231fe3235",
      "value": "sentence_bert_config.json: 100%"
     }
    },
    "6cb72ab4081a44fd8462b7d58da7236c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "6da0a949eaf1499094fa4e264e1929ca": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6efb365326c24c339085be28cc203fdc": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0bce7925eb1f4323a482bb90467bbc66",
      "placeholder": "​",
      "style": "IPY_MODEL_392ee34639024892a83a85f6878b60ef",
      "value": "tokenizer_config.json: 100%"
     }
    },
    "6f038781e3a5498a9823999548de18fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1efc8ad79138462e97c1bf5f00d8a210",
      "placeholder": "​",
      "style": "IPY_MODEL_4e8321573083413d80d545eb711dabcb",
      "value": "special_tokens_map.json: 100%"
     }
    },
    "718e83bf80b74bbb9e28a9a2473ab8db": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "72f6e2507a5c49338610f22446087687": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_784f6571b15c4ac19ca2e1b1a913f318",
      "placeholder": "​",
      "style": "IPY_MODEL_42c591eaee9f42f8ad965372898e3bdd",
      "value": " 385/385 [00:00&lt;00:00, 26.4kB/s]"
     }
    },
    "755d31fc51614b78a6798a1f45d66a29": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "761b0c7bec0f4ea98fff4469f3e147e1": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "784f6571b15c4ac19ca2e1b1a913f318": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7a550bf2dc964d0caafc8aed54d187a6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5bf549707f604a99a4743166718cd720",
       "IPY_MODEL_bac9a59f6a1244d8a6e37795e0383e30",
       "IPY_MODEL_a113bb38b1f7458cae5e7825d6a65b3c"
      ],
      "layout": "IPY_MODEL_cda8adeebae9484c9399422c870aa7cf"
     }
    },
    "7a6d685007704814a51a027b060ba1b0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b599c45a81df44619d92a3ac8ad41b0f",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c2cf8f815146436e86373dd6f02e94aa",
      "value": 231508
     }
    },
    "7b70f0bd5409433e93719cd05441e473": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_83665867accd4853b6efe68a06f7f403",
       "IPY_MODEL_66b76f1f830445e98fb9c1acc1683cde",
       "IPY_MODEL_26f97033c9e5479da8dca6f7dc642ece"
      ],
      "layout": "IPY_MODEL_6da0a949eaf1499094fa4e264e1929ca"
     }
    },
    "7b9187f7562945ba813032f73d5fed96": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "7e04af96adae41a6b05ecc7bfe5a9fd9": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "7e069a6942cb4cb09ef6aa3661844f88": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_f5553c33eefd47eea49e0e3901bf51f5",
      "placeholder": "​",
      "style": "IPY_MODEL_fc2843b06e0f4b6ca2a7d1d5fcde23d6",
      "value": " 618/618 [00:00&lt;00:00, 56.6kB/s]"
     }
    },
    "7e0b008fb7294404a73b8341c1230125": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "800014039af04d66adc94131a8ab568e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f741891d16434e959971b6331ab7a09e",
       "IPY_MODEL_20619dd814a245cb96eed1ffd6d23793",
       "IPY_MODEL_f91289ae253446ed90b7c9260d85ac2c"
      ],
      "layout": "IPY_MODEL_3088fb3c8cc74d7c97d9cbacc5b20c03"
     }
    },
    "8041af99466c478ab32dcb07ff597d41": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8110d80f4fe94d499834b799c85667ad": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ca2e4e27247949fd96c09ca0084eb8d5",
      "max": 125,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ab3716a57cb1461ca4b89d3e2af60aa1",
      "value": 125
     }
    },
    "82baa3fb82e24827b49b6bda22f72d78": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2c938e4d55734c248ffb292415aa5f5b",
      "max": 405,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_5582c1c8145b4fe786ba967188680aa7",
      "value": 405
     }
    },
    "83665867accd4853b6efe68a06f7f403": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0b946716322e43bb9f4213087c6ee1de",
      "placeholder": "​",
      "style": "IPY_MODEL_c3cb074231f84c169f31afa3baf84fc9",
      "value": "model.safetensors: 100%"
     }
    },
    "836f49316d114d37846143df795ba15a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "863ed2e3ac1e4670b7850cf4deafc867": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "890443b9c63d42b597b1fce37ed2612f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "8cd319e80b28462698e8adb196302802": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_255d439785a54b9b8f37be9730c5b9f9",
      "placeholder": "​",
      "style": "IPY_MODEL_e3ea567b814a4d949a018e39137923fd",
      "value": "tokenizer.json: 100%"
     }
    },
    "8d567a74de4843a4b50292512c7d0ebe": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "90efeff073a541e3ace582146fd5fd45": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_183c14e615d6471c9fe4b9a56d13f85a",
      "placeholder": "​",
      "style": "IPY_MODEL_e15406eedc3b4b10b5d8c32d2ba5f1cd",
      "value": " 712k/712k [00:00&lt;00:00, 3.68MB/s]"
     }
    },
    "9100526f824f4e2298b22f43461c6daa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_34f2229c854f4695b5aba5d5afe772f5",
      "max": 231508,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_38344f64512b421aa12ac82fcf4b195d",
      "value": 231508
     }
    },
    "91b7390bb1d94bd6be84496ed73052ee": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9553bb69e7c5479ebbf73787d3fc8b14": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9561e20747684a1a878a637d00c946fa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_b5cfff7bf253443682d8051953740ad2",
       "IPY_MODEL_d2b20654e6aa49359b678e198cc55a73",
       "IPY_MODEL_5fca46bd83bd4f95ae25eecf342ff20f"
      ],
      "layout": "IPY_MODEL_63a47d5282db441a8e5732ab3266af33"
     }
    },
    "97d0ee251daa487db840ba561a36d4a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9b8ebc99d0ed4d9fb6f873f06a638318": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "9ce3da5248a34146945e40a660ed455d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9dc189ff36034264b081ca782c0bc1ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_0e167716cc6941e2aaa5e85a476f1b1a",
      "placeholder": "​",
      "style": "IPY_MODEL_9ce3da5248a34146945e40a660ed455d",
      "value": " 57.0/57.0 [00:00&lt;00:00, 4.50kB/s]"
     }
    },
    "9defae4a31724cc1b6c0128b76bae404": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a113bb38b1f7458cae5e7825d6a65b3c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1ad12046944a468297e3c982794251ef",
      "placeholder": "​",
      "style": "IPY_MODEL_c0d3f59319544daa858e76bf490ac735",
      "value": " 68.1k/68.1k [00:00&lt;00:00, 4.12MB/s]"
     }
    },
    "a2c29ede70564d5b879b4084453d753f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b5693df9d1cf458e9c7dabfba2331102",
      "placeholder": "​",
      "style": "IPY_MODEL_4c7a193e4d3349dc8c2ff58c7137e77e",
      "value": " 405/405 [00:00&lt;00:00, 24.3kB/s]"
     }
    },
    "a68946ee535e4693955f8292b47c5b6c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_755d31fc51614b78a6798a1f45d66a29",
      "max": 385,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_46b7f140d4134097a8f6c7dd41d9acd3",
      "value": 385
     }
    },
    "a77de46019434b2182d8ace90729411d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ab3716a57cb1461ca4b89d3e2af60aa1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "abe064fd2d2943cc9658631022e7dfbe": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "aee8fac5931844c5a88f651e7f854362": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b02abbcd98d94732961735247946d2d4": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b106e671bf2040ff85286339391cacce": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b13c2b7166784d0f8d8a1ee231fe3235": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "b14e572db35545c58c413b5313c230d0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_26f3de4a37ed42c783b8cc8496a8830e",
       "IPY_MODEL_25ea5fce0d51425ca66647c86a69054d",
       "IPY_MODEL_1d5d56be1f3e4e06ad05b7a61c073773"
      ],
      "layout": "IPY_MODEL_0e6e63ccb1164deab11b438e7059d69f"
     }
    },
    "b215c0274af9444c9265aa576d33f6d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b524e9e4f75a45aba71a8a2b68557700": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5693df9d1cf458e9c7dabfba2331102": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b599c45a81df44619d92a3ac8ad41b0f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b5cfff7bf253443682d8051953740ad2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_8d567a74de4843a4b50292512c7d0ebe",
      "placeholder": "​",
      "style": "IPY_MODEL_e4710695ec874eada402065fe800edd5",
      "value": "config.json: 100%"
     }
    },
    "b8d486c46bbc4a6abdb3b0af3dbc4c04": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ba96d1c92aeb4a9584f700869c028ace": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bac9a59f6a1244d8a6e37795e0383e30": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e55c826128994d0a8b67885f40574fae",
      "max": 68075,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_836f49316d114d37846143df795ba15a",
      "value": 68075
     }
    },
    "bb8b2e9c41ea45dfa6fe7dff8a934fa2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "be42d78fa32c44f4ba983327b2f2b0ae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "bfd5578f1cc3406b9b6e49540918dffd": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c0d3f59319544daa858e76bf490ac735": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c1225405706548aa86f466345ba78795": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f9124d67aef64019a437baa8eef8e54e",
       "IPY_MODEL_7a6d685007704814a51a027b060ba1b0",
       "IPY_MODEL_c87f3b6818934457af1827e01f4924ba"
      ],
      "layout": "IPY_MODEL_cfab7d69b2634082805f101090dcbd04"
     }
    },
    "c28b40c65c1f450988dfb834ccbf1526": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_1257cd90321540d8b5530c35b672ac38",
       "IPY_MODEL_365523a881584181973972b7df86c3eb",
       "IPY_MODEL_6023241ffbf740ec871828d6276cccb8"
      ],
      "layout": "IPY_MODEL_863ed2e3ac1e4670b7850cf4deafc867"
     }
    },
    "c2cf8f815146436e86373dd6f02e94aa": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "c3cb074231f84c169f31afa3baf84fc9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "c87f3b6818934457af1827e01f4924ba": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_b02abbcd98d94732961735247946d2d4",
      "placeholder": "​",
      "style": "IPY_MODEL_666d42ef1a74402fb13ea79db36c1287",
      "value": " 232k/232k [00:00&lt;00:00, 3.33MB/s]"
     }
    },
    "c8c4de2dd87b409ab14e3bbd4f8ff777": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "ca2e4e27247949fd96c09ca0084eb8d5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb37e6988db144ac9e6b615b71705c3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_58f1227b067740c99856d28d1b3535f0",
       "IPY_MODEL_a68946ee535e4693955f8292b47c5b6c",
       "IPY_MODEL_72f6e2507a5c49338610f22446087687"
      ],
      "layout": "IPY_MODEL_f7afe5336f9645bbaa3da495dad04c6f"
     }
    },
    "cda8adeebae9484c9399422c870aa7cf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cf1bd11c82b44b99ac40e8abbec3785f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_3332c9a35d2e40ac860233ebfb8c64c7",
      "max": 618,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_e0686b63c9374bcda05c70aeffbcda03",
      "value": 618
     }
    },
    "cfab7d69b2634082805f101090dcbd04": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d2b20654e6aa49359b678e198cc55a73": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9b8ebc99d0ed4d9fb6f873f06a638318",
      "max": 743,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_2750e13775ad4cc5a1ca97abd23a7fa4",
      "value": 743
     }
    },
    "d3e5436855af42a5b2f170b44388e4a8": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "d696cc22200743caaaa0d70de5f29eaf": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "d9d79532e89447e1ae295ea296d33dbf": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "df4976d8d7ec47e4986f5e9c0395c3a9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_ba96d1c92aeb4a9584f700869c028ace",
      "max": 314,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_c8c4de2dd87b409ab14e3bbd4f8ff777",
      "value": 314
     }
    },
    "df62dad6b54444659f23d7cca3ddaa70": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e0686b63c9374bcda05c70aeffbcda03": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e15406eedc3b4b10b5d8c32d2ba5f1cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e249235d4bb14478a802d3fc362fe215": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_355092a75fa445ddb58c3c0e6c7c87f7",
      "placeholder": "​",
      "style": "IPY_MODEL_a77de46019434b2182d8ace90729411d",
      "value": " 314/314 [00:00&lt;00:00, 15.5kB/s]"
     }
    },
    "e39705238d474d5a99f0a49832b408ed": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "e3ea567b814a4d949a018e39137923fd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e4710695ec874eada402065fe800edd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e55c826128994d0a8b67885f40574fae": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e5ff1078ea2d454ba88d92b978bee13f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e68e4e022b7946139920753c55012db2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_45045e6a12e041e1b2b2713ff59ef938",
      "placeholder": "​",
      "style": "IPY_MODEL_0f57c3f8ee4d42b3b02aba300b4fe271",
      "value": "1_Pooling/config.json: 100%"
     }
    },
    "e7096ed061374902935f3363540caa56": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e7816626014f47fbb744f707990f3c0b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e7096ed061374902935f3363540caa56",
      "placeholder": "​",
      "style": "IPY_MODEL_3e583707d8f7457bb3a5433440f3a6cb",
      "value": " 125/125 [00:00&lt;00:00, 7.81kB/s]"
     }
    },
    "e7bc53149d98433785d628cd3783cbdb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ea2f3d5138cb4d3eb4b5f50122c42634": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ea9b31c529f0482c99ac5ede1b0c2437": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "eac3132f4d9349cfa6f1422686ae7bd5": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ec25bd06a00e4e2cab1b9f62905449ca": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "f4388ee32ed9434ea5e756c22fb61002": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f46f3a7030a34d3894d97214fb03e1c2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "f4c9a624ca3342adbdda81d0c505d540": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f5553c33eefd47eea49e0e3901bf51f5": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f741891d16434e959971b6331ab7a09e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_234ab5cd38b14aec8426fcc28dfc4952",
      "placeholder": "​",
      "style": "IPY_MODEL_b8d486c46bbc4a6abdb3b0af3dbc4c04",
      "value": "artifact.metadata: 100%"
     }
    },
    "f7afe5336f9645bbaa3da495dad04c6f": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "f82609221b714ae58214a6f6745526b3": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6f038781e3a5498a9823999548de18fd",
       "IPY_MODEL_8110d80f4fe94d499834b799c85667ad",
       "IPY_MODEL_e7816626014f47fbb744f707990f3c0b"
      ],
      "layout": "IPY_MODEL_65617f6c0a3f40f1bb7198eadb9757da"
     }
    },
    "f8c3f5363e824e338cf4b7ea0f09b92a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_6aaf81405fa24b0e93a858d609b85036",
       "IPY_MODEL_4977730998754c6d82831b47a647b428",
       "IPY_MODEL_9dc189ff36034264b081ca782c0bc1ed"
      ],
      "layout": "IPY_MODEL_91b7390bb1d94bd6be84496ed73052ee"
     }
    },
    "f9124d67aef64019a437baa8eef8e54e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9553bb69e7c5479ebbf73787d3fc8b14",
      "placeholder": "​",
      "style": "IPY_MODEL_0eae622975df4cfaaa2e54cde12fecc2",
      "value": "vocab.txt: 100%"
     }
    },
    "f91289ae253446ed90b7c9260d85ac2c": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_2d7b6a6ffb3849ec9828e24f4e193840",
      "placeholder": "​",
      "style": "IPY_MODEL_718e83bf80b74bbb9e28a9a2473ab8db",
      "value": " 1.63k/1.63k [00:00&lt;00:00, 59.6kB/s]"
     }
    },
    "fa2721e59a714068a0a5918cd7ec3246": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc2843b06e0f4b6ca2a7d1d5fcde23d6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "fc6ba90ca08747e1bace0a55d1eeed55": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_5e9df76d9eb04c3cacc9caf4edacb913",
       "IPY_MODEL_091d085339a14a0ab5a23a60b24508f6",
       "IPY_MODEL_1191b9f6ca7e491c91a54648d9a95740"
      ],
      "layout": "IPY_MODEL_b106e671bf2040ff85286339391cacce"
     }
    },
    "fc979bda6f3c486e8108945371fee26e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "fdbe596b18164389a51d860b24f3b15b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
