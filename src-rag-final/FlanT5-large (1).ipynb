{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUCaGdAj9-9F"
   },
   "source": [
    "Souce:\n",
    "- https://huggingface.co/learn/cookbook/en/advanced_rag\n",
    "- https://arc.net/l/quote/vntkseji"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pt_BRiBR2tKE"
   },
   "source": [
    "# Assumptions\n",
    "- the faiss_index embeddings are up to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "MrXGTQoSsiQv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv('.env')\n",
    "hf_api = os.getenv('HF_API')\n",
    "HUGGINGFACEHUB_API_TOKEN = hf_api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (1.26.3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'1.26.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "!pip3 install numpy\n",
    "import numpy as np\n",
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10559,
     "status": "ok",
     "timestamp": 1709617668179,
     "user": {
      "displayName": "Vashisth Tiwari",
      "userId": "14058204908965748495"
     },
     "user_tz": 300
    },
    "id": "AmPefRqf2tKF",
    "outputId": "3b5a2047-bdd6-4229-f014-c6ecc32edf69",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.38.0\n",
      "  Downloading transformers-4.38.0-py3-none-any.whl.metadata (131 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.1/131.1 kB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (3.13.1)\n",
      "Collecting huggingface-hub<1.0,>=0.19.3 (from transformers==4.38.0)\n",
      "  Using cached huggingface_hub-0.21.4-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (1.26.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (21.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers==4.38.0)\n",
      "  Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.0)\n",
      "  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers==4.38.0)\n",
      "  Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers==4.38.0) (4.66.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0) (2023.12.2)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0) (4.9.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from packaging>=20.0->transformers==4.38.0) (3.1.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.38.0) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.38.0) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.38.0) (1.26.18)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from requests->transformers==4.38.0) (2023.11.17)\n",
      "Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m36.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached huggingface_hub-0.21.4-py3-none-any.whl (346 kB)\n",
      "Using cached regex-2023.12.25-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
      "Using cached safetensors-0.4.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: safetensors, regex, huggingface-hub, tokenizers, transformers\n",
      "Successfully installed huggingface-hub-0.21.4 regex-2023.12.25 safetensors-0.4.2 tokenizers-0.15.2 transformers-4.38.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install transformers==4.38.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 39064,
     "status": "ok",
     "timestamp": 1709617707240,
     "user": {
      "displayName": "Vashisth Tiwari",
      "userId": "14058204908965748495"
     },
     "user_tz": 300
    },
    "id": "m9jP7QzjProY",
    "outputId": "dcd78147-a081-4a6c-d793-a3489d3d01d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -q torch accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 40514,
     "status": "ok",
     "timestamp": 1709617747751,
     "user": {
      "displayName": "Vashisth Tiwari",
      "userId": "14058204908965748495"
     },
     "user_tz": 300
    },
    "id": "EHEfmoGTRsGh",
    "outputId": "06607aec-1cfe-4660-8703-2506142a9807",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unstructured\n",
      "  Downloading unstructured-0.12.6-py3-none-any.whl.metadata (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.2/83.2 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting ragatouille\n",
      "  Downloading ragatouille-0.0.7.post10-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting backoff==2.2.1 (from unstructured)\n",
      "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: beautifulsoup4==4.12.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (4.12.3)\n",
      "Collecting certifi==2024.2.2 (from unstructured)\n",
      "  Using cached certifi-2024.2.2-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting chardet==5.2.0 (from unstructured)\n",
      "  Downloading chardet-5.2.0-py3-none-any.whl.metadata (3.4 kB)\n",
      "Requirement already satisfied: charset-normalizer==3.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (3.3.2)\n",
      "Requirement already satisfied: click==8.1.7 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (8.1.7)\n",
      "Requirement already satisfied: dataclasses-json==0.6.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (0.6.4)\n",
      "Collecting dataclasses-json-speakeasy==0.5.11 (from unstructured)\n",
      "  Downloading dataclasses_json_speakeasy-0.5.11-py3-none-any.whl.metadata (25 kB)\n",
      "Collecting emoji==2.10.1 (from unstructured)\n",
      "  Downloading emoji-2.10.1-py2.py3-none-any.whl.metadata (5.3 kB)\n",
      "Collecting filetype==1.2.0 (from unstructured)\n",
      "  Downloading filetype-1.2.0-py2.py3-none-any.whl.metadata (6.5 kB)\n",
      "Requirement already satisfied: idna==3.6 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (3.6)\n",
      "Requirement already satisfied: joblib==1.3.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (1.3.2)\n",
      "Collecting jsonpath-python==1.0.6 (from unstructured)\n",
      "  Downloading jsonpath_python-1.0.6-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting langdetect==1.0.9 (from unstructured)\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting lxml==5.1.0 (from unstructured)\n",
      "  Downloading lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.5 kB)\n",
      "Collecting marshmallow==3.20.2 (from unstructured)\n",
      "  Downloading marshmallow-3.20.2-py3-none-any.whl.metadata (7.5 kB)\n",
      "Requirement already satisfied: mypy-extensions==1.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (1.0.0)\n",
      "Collecting nltk==3.8.1 (from unstructured)\n",
      "  Downloading nltk-3.8.1-py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting numpy==1.26.4 (from unstructured)\n",
      "  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: packaging==23.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (23.2)\n",
      "Requirement already satisfied: python-dateutil==2.8.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (2.8.2)\n",
      "Collecting python-iso639==2024.2.7 (from unstructured)\n",
      "  Downloading python_iso639-2024.2.7-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting python-magic==0.4.27 (from unstructured)\n",
      "  Downloading python_magic-0.4.27-py2.py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting rapidfuzz==3.6.1 (from unstructured)\n",
      "  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: regex==2023.12.25 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (2023.12.25)\n",
      "Requirement already satisfied: requests==2.31.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (2.31.0)\n",
      "Requirement already satisfied: six==1.16.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (1.16.0)\n",
      "Requirement already satisfied: soupsieve==2.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (2.5)\n",
      "Requirement already satisfied: tabulate==0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (0.9.0)\n",
      "Collecting tqdm==4.66.2 (from unstructured)\n",
      "  Using cached tqdm-4.66.2-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: typing-extensions==4.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (4.9.0)\n",
      "Requirement already satisfied: typing-inspect==0.9.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (0.9.0)\n",
      "Collecting unstructured-client==0.18.0 (from unstructured)\n",
      "  Downloading unstructured_client-0.18.0-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: urllib3==1.26.18 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from unstructured) (1.26.18)\n",
      "Collecting wrapt==1.16.0 (from unstructured)\n",
      "  Downloading wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Collecting aiohttp==3.9.1 (from ragatouille)\n",
      "  Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.4 kB)\n",
      "Collecting colbert-ai==0.2.19 (from ragatouille)\n",
      "  Downloading colbert-ai-0.2.19.tar.gz (86 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting faiss-cpu<2.0.0,>=1.7.4 (from ragatouille)\n",
      "  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: langchain<0.2.0,>=0.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ragatouille) (0.1.11)\n",
      "Requirement already satisfied: langchain_core<0.2.0,>=0.1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ragatouille) (0.1.31)\n",
      "Collecting llama-index<0.10.0,>=0.9.24 (from ragatouille)\n",
      "  Downloading llama_index-0.9.48-py3-none-any.whl.metadata (8.4 kB)\n",
      "Collecting onnx<2.0.0,>=1.15.0 (from ragatouille)\n",
      "  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (15 kB)\n",
      "Collecting ruff<0.2.0,>=0.1.9 (from ragatouille)\n",
      "  Downloading ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (22 kB)\n",
      "Requirement already satisfied: sentence-transformers<3.0.0,>=2.2.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ragatouille) (2.5.1)\n",
      "Collecting srsly==2.4.8 (from ragatouille)\n",
      "  Downloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: torch<3.0.0,>=2.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ragatouille) (2.1.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from ragatouille) (4.38.0)\n",
      "Collecting voyager<3.0.0,>=2.0.2 (from ragatouille)\n",
      "  Downloading voyager-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (23.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (1.9.4)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (1.4.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from aiohttp==3.9.1->ragatouille) (4.0.3)\n",
      "Collecting bitarray (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
      "Collecting datasets (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading datasets-2.18.0-py3-none-any.whl.metadata (20 kB)\n",
      "Requirement already satisfied: flask in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (3.0.1)\n",
      "Collecting git-python (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading git_python-1.0.3-py2.py3-none-any.whl.metadata (331 bytes)\n",
      "Collecting python-dotenv (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting ninja (from colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl.metadata (5.3 kB)\n",
      "Requirement already satisfied: scipy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (1.12.0)\n",
      "Requirement already satisfied: ujson in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from colbert-ai==0.2.19->ragatouille) (5.9.0)\n",
      "Collecting catalogue<2.1.0,>=2.0.3 (from srsly==2.4.8->ragatouille)\n",
      "  Downloading catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (6.0.1)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.0.28)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (1.33)\n",
      "Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.0.28)\n",
      "Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.0.1)\n",
      "Requirement already satisfied: langsmith<0.2.0,>=0.1.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.1.24)\n",
      "Requirement already satisfied: pydantic<3,>=1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.6.0)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (8.2.3)\n",
      "Requirement already satisfied: anyio<5,>=3 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (4.2.0)\n",
      "Collecting deprecated>=1.2.9.3 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading Deprecated-1.2.14-py2.py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading dirtyjson-1.0.8-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (2023.12.2)\n",
      "Collecting httpx (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading httpx-0.27.0-py3-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (3.2.1)\n",
      "Collecting openai>=1.1.0 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading openai-1.13.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: pandas in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (1.5.3)\n",
      "Collecting tiktoken>=0.3.3 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (4.25.2)\n",
      "Requirement already satisfied: scikit-learn in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.4.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.21.4)\n",
      "Requirement already satisfied: Pillow in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (10.2.0)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.1->ragatouille) (3.13.1)\n",
      "Requirement already satisfied: sympy in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.1->ragatouille) (1.12)\n",
      "Requirement already satisfied: jinja2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from torch<3.0.0,>=2.0.1->ragatouille) (3.1.3)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.15.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.4.2)\n",
      "Requirement already satisfied: sniffio>=1.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (1.3.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (1.2.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.0->ragatouille) (2.4)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from langsmith<0.2.0,>=0.1.17->langchain<0.2.0,>=0.1.0->ragatouille) (3.9.15)\n",
      "Collecting distro<2,>=1.7.0 (from openai>=1.1.0->llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading distro-1.9.0-py3-none-any.whl.metadata (6.8 kB)\n",
      "Collecting httpcore==1.* (from httpx->llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading httpcore-1.0.4-py3-none-any.whl.metadata (20 kB)\n",
      "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index<0.10.0,>=0.9.24->ragatouille)\n",
      "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (2.16.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.0->ragatouille) (3.0.3)\n",
      "Requirement already satisfied: pyarrow>=12.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (15.0.0)\n",
      "Collecting pyarrow-hotfix (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading pyarrow_hotfix-0.6-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.3.8)\n",
      "Collecting xxhash (from datasets->colbert-ai==0.2.19->ragatouille)\n",
      "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
      "Requirement already satisfied: multiprocess in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.70.16)\n",
      "Requirement already satisfied: Werkzeug>=3.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (3.0.1)\n",
      "Requirement already satisfied: itsdangerous>=2.1.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.1.2)\n",
      "Requirement already satisfied: blinker>=1.6.2 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from flask->colbert-ai==0.2.19->ragatouille) (1.7.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from jinja2->torch<3.0.0,>=2.0.1->ragatouille) (2.1.4)\n",
      "Collecting gitpython (from git-python->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached GitPython-3.1.42-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from pandas->llama-index<0.10.0,>=0.9.24->ragatouille) (2023.4)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.2.0)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (from sympy->torch<3.0.0,>=2.0.1->ragatouille) (1.3.0)\n",
      "Collecting gitdb<5,>=4.0.1 (from gitpython->git-python->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached gitdb-4.0.11-py3-none-any.whl.metadata (1.2 kB)\n",
      "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille)\n",
      "  Using cached smmap-5.0.1-py3-none-any.whl.metadata (4.3 kB)\n",
      "Using cached unstructured-0.12.6-py3-none-any.whl (1.8 MB)\n",
      "Using cached backoff-2.2.1-py3-none-any.whl (15 kB)\n",
      "Using cached certifi-2024.2.2-py3-none-any.whl (163 kB)\n",
      "Using cached chardet-5.2.0-py3-none-any.whl (199 kB)\n",
      "Using cached dataclasses_json_speakeasy-0.5.11-py3-none-any.whl (28 kB)\n",
      "Using cached emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n",
      "Using cached filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n",
      "Using cached jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n",
      "Using cached lxml-5.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.0 MB)\n",
      "Using cached marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
      "Using cached nltk-3.8.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n",
      "Using cached python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n",
      "Using cached python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n",
      "Using cached rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "Using cached tqdm-4.66.2-py3-none-any.whl (78 kB)\n",
      "Using cached unstructured_client-0.18.0-py3-none-any.whl (21 kB)\n",
      "Using cached wrapt-1.16.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (80 kB)\n",
      "Downloading ragatouille-0.0.7.post10-py3-none-any.whl (35 kB)\n",
      "Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading srsly-2.4.8-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (493 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m493.0/493.0 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m23.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading llama_index-0.9.48-py3-none-any.whl (15.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hDownloading ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading voyager-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n",
      "Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n",
      "Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0meta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hDownloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n",
      "Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Downloading distro-1.9.0-py3-none-any.whl (20 kB)\n",
      "Using cached GitPython-3.1.42-py3-none-any.whl (195 kB)\n",
      "Downloading pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m34.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
      "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m11.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached smmap-5.0.1-py3-none-any.whl (24 kB)\n",
      "Building wheels for collected packages: langdetect, colbert-ai\n",
      "  Building wheel for langdetect (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=ef9312501713f305baa7555523aac8fd8711be74cde46bcaa2030fb758c54342\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n",
      "  Building wheel for colbert-ai (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for colbert-ai: filename=colbert_ai-0.2.19-py3-none-any.whl size=114761 sha256=6dae8c8425f302612df3e6668b4ca66be53701db3c4a03bc3eeffb9fc70bb612\n",
      "  Stored in directory: /home/ec2-user/.cache/pip/wheels/90/b9/63/d4fc276c73c42ef7fc1065a26cf87e5a1cf56ef6498cbfbe5d\n",
      "Successfully built langdetect colbert-ai\n",
      "\u001b[33mWARNING: Error parsing requirements for numpy: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/numpy-1.26.3.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Error parsing requirements for tqdm: [Errno 2] No such file or directory: '/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/tqdm-4.66.1.dist-info/METADATA'\u001b[0m\u001b[33m\n",
      "\u001b[0mInstalling collected packages: ninja, filetype, dirtyjson, bitarray, xxhash, wrapt, tqdm, smmap, ruff, rapidfuzz, python-magic, python-iso639, python-dotenv, pyarrow-hotfix, numpy, marshmallow, lxml, langdetect, jsonpath-python, h11, emoji, distro, chardet, certifi, catalogue, backoff, voyager, srsly, onnx, nltk, httpcore, gitdb, faiss-cpu, deprecated, dataclasses-json-speakeasy, aiohttp, unstructured-client, tiktoken, httpx, gitpython, unstructured, openai, git-python, datasets, llama-index, colbert-ai, ragatouille\n",
      "  Attempting uninstall: tqdm\n",
      "\u001b[33m    WARNING: No metadata found in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: tqdm 4.66.1\n",
      "    Can't uninstall 'tqdm'. No files were found to uninstall.\n",
      "  Attempting uninstall: numpy\n",
      "\u001b[33m    WARNING: No metadata found in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: numpy 1.26.3\n",
      "    Can't uninstall 'numpy'. No files were found to uninstall.\n",
      "  Attempting uninstall: marshmallow\n",
      "\u001b[33m    WARNING: No metadata found in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: marshmallow 3.21.1\n",
      "    Can't uninstall 'marshmallow'. No files were found to uninstall.\n",
      "  Attempting uninstall: certifi\n",
      "\u001b[33m    WARNING: No metadata found in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages\u001b[0m\u001b[33m\n",
      "\u001b[0m    Found existing installation: certifi 2023.11.17\n",
      "    Can't uninstall 'certifi'. No files were found to uninstall.\n",
      "  Attempting uninstall: onnx\n",
      "    Found existing installation: onnx 1.13.1\n",
      "    Uninstalling onnx-1.13.1:\n",
      "      Successfully uninstalled onnx-1.13.1\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.9.3\n",
      "    Uninstalling aiohttp-3.9.3:\n",
      "      Successfully uninstalled aiohttp-3.9.3\n",
      "Successfully installed aiohttp-3.9.1 backoff-2.2.1 bitarray-2.9.2 catalogue-2.0.10 certifi-2024.2.2 chardet-5.2.0 colbert-ai-0.2.19 dataclasses-json-speakeasy-0.5.11 datasets-2.18.0 deprecated-1.2.14 dirtyjson-1.0.8 distro-1.9.0 emoji-2.10.1 faiss-cpu-1.8.0 filetype-1.2.0 git-python-1.0.3 gitdb-4.0.11 gitpython-3.1.42 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 jsonpath-python-1.0.6 langdetect-1.0.9 llama-index-0.9.48 lxml-5.1.0 marshmallow-3.20.2 ninja-1.11.1.1 nltk-3.8.1 numpy-1.26.4 onnx-1.15.0 openai-1.13.3 pyarrow-hotfix-0.6 python-dotenv-1.0.1 python-iso639-2024.2.7 python-magic-0.4.27 ragatouille-0.0.7.post10 rapidfuzz-3.6.1 ruff-0.1.15 smmap-5.0.1 srsly-2.4.8 tiktoken-0.6.0 tqdm-4.66.2 unstructured-0.12.6 unstructured-client-0.18.0 voyager-2.0.2 wrapt-1.16.0 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install unstructured ragatouille\n",
    "# reranker\n",
    "from ragatouille import RAGPretrainedModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "dMOUdS4CaWO3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# fix colab error: https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n",
    "import locale\n",
    "def getpreferredencoding(do_setlocale = True):\n",
    "    return \"UTF-8\"\n",
    "locale.getpreferredencoding = getpreferredencoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "eoujYMwW9-9J",
    "tags": []
   },
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd\n",
    "from typing import Optional, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "pd.set_option(\n",
    "    \"display.max_colwidth\", None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "9DLqgmyvsiQw",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# langchain imports\n",
    "from langchain.docstore.document import Document as LangchainDocument\n",
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# hf imports\n",
    "from transformers import pipeline\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "\n",
    "# reranking\n",
    "\n",
    "from ragatouille import RAGPretrainedModel\n",
    "from transformers import Pipeline\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dcMNNt55siQx"
   },
   "source": [
    "# Specify the models/versions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "id": "FAnu3j0hsiQx",
    "outputId": "157e427f-6b36-4470-b46f-ed62c44e951d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# give the paths\n",
    "QUESTIONS_FILE = 'questions.txt'\n",
    "# OUTPUT_FILE = 'system_outputs/webpages.txt'\n",
    "\n",
    "# FAISS_FILE = '../faiss_index_author_papers_natural_language' # it's actually a folder but whatever\n",
    "FAISS_FILE = 'faiss_index_total_final_new' # it's actually a folder but whatever\n",
    "\n",
    "EMBEDDING_MODEL = \"thenlper/gte-base\" # make sure this matches whatever was used to create the doc embeddings\n",
    "GENERATOR_MODEL = \"google/flan-t5-large\"\n",
    "RERANKER_MODEL = \"colbert-ir/colbertv2.0\"\n",
    "\n",
    "RERANKER = RAGPretrainedModel.from_pretrained(RERANKER_MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentencepiece in /home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages (0.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "colab": {
     "referenced_widgets": [
      "7d8a9536491b4edbb9b45b1fca65a14a",
      "ea2d43edd2a949e1ba2c9288c77278c8",
      "1264f61a2602416c9707501077cc4ba3",
      "d6a60b7f0f7d42799c8b7a2adfe32b95",
      "3820631823774c52bacb10826096e1f7",
      "50f3e8a8bffa483a996bff8b9fc178e8",
      "af5956998d934d189568b5551262a3cc"
     ]
    },
    "id": "M9X3nGPFsiQx",
    "outputId": "866f2588-9d54-41e6-b513-38adee6529e5",
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "# initialize the LLM and its tokenizer, we are using Flan T5 Large for this\n",
    "tokenizer = T5Tokenizer.from_pretrained(GENERATOR_MODEL)\n",
    "model = T5ForConditionalGeneration.from_pretrained(GENERATOR_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kr6rN10U9-9J"
   },
   "source": [
    "### Load the knowledge base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "Z14HQGDx2tKI",
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_model = HuggingFaceEmbeddings(\n",
    "    model_name=EMBEDDING_MODEL,\n",
    "    multi_process=True,\n",
    "    model_kwargs={\"device\": \"cuda\"},\n",
    "    encode_kwargs={\"normalize_embeddings\": True},  #  True for cosine similarity\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "z5cSyyk_siQx",
    "tags": []
   },
   "outputs": [],
   "source": [
    "KNOWLEDGE_VECTOR_DATABASE = FAISS.load_local(FAISS_FILE, embedding_model, allow_dangerous_deserialization=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "JfDXZPNINnvq",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function to get the prediction and scores from the LLM, given a prompt\n",
    "def get_prediction_and_scores(prompt):\n",
    "    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "    outputs =  model.generate(input_ids, output_scores=True, return_dict_in_generate=True, max_length=100)\n",
    "                            #   skip_special_tokens=True)\n",
    "    generated_sequence = outputs.sequences[0]\n",
    "\n",
    "    # get the probability scores for each generated token\n",
    "    transition_scores = torch.exp(model.compute_transition_scores(\n",
    "        outputs.sequences, outputs.scores, normalize_logits=True\n",
    "        # , skip_special_tokens = True\n",
    "    )[0])\n",
    "    return tokenizer.decode(generated_sequence), generated_sequence, transition_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RlfHavRT9-9O"
   },
   "source": [
    "## Retrieval and Answer Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "BUQZfctcOkbT",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def flanT5_without_threshold(\n",
    "    question: str,\n",
    "    knowledge_index: FAISS,\n",
    "    reranker: Optional[RAGPretrainedModel] = None,\n",
    "    num_retrieved_docs: int = 10,\n",
    "    num_docs_final: int = 3\n",
    "    ):\n",
    "\n",
    "    print(\"=> Retrieving documents...\")\n",
    "    # Gather documents with retriever\n",
    "    relevant_docs_acquired = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n",
    "    # print(relevant_docs_acquired)\n",
    "    # print(relevant_docs_acquired)\n",
    "    if reranker:\n",
    "        print(\"=> Reranking documents...\")\n",
    "        relevant_docs = [doc.page_content for doc in relevant_docs_acquired]\n",
    "        # print(relevant_docs)\n",
    "        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n",
    "        \n",
    "        relevant_docs_content = [doc[\"content\"] for doc in relevant_docs]\n",
    "        relevant_doc_score = [doc[\"score\"] for doc in relevant_docs]\n",
    "\n",
    "    else:\n",
    "        relevant_docs_content = [doc.page_content for doc in relevant_docs_acquired]\n",
    "\n",
    "    relevant_docs_content = relevant_docs_content[:num_docs_final]\n",
    "    # relevant_doc_id = relevant_doc_id[:num_docs_final]\n",
    "    # relevant_doc_index = relevant_doc_index[:num_docs_final]\n",
    "\n",
    "    # Build the final prompt\n",
    "    context = \"\\nExtracted documents:\\n\"\n",
    "    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs_content)])\n",
    "\n",
    "    # context_and_question = f\"Keep your answers short and concise. If the text has date and time include the date, time both. If there are multiple right answers, include them all, but keep it short overall. If the question cannot be answered from the context, say 'I don't know'. \\n Given the below context:\\n{context}\\n\\n Answer the following \\n{question}\\n\"\n",
    "    context_and_question = f\"Keep your answers short and concise. If the text has date and time include the date, time both. If there are multiple right answers, include them all, but keep it short overall. \\n Given the below context:\\n{context}\\n\\n Answer the following \\n{question}\\n\"\n",
    "    # context_and_question = \"\"\"\n",
    "    # Answer the user's questions based on the below context. Please keep your answers short and concise. Only provide the answer itself.\"\n",
    "    # ------------\n",
    "    # {context}\n",
    "    # ------------\n",
    "    # Question: {question}\n",
    "    # Answer:\n",
    "    # \"\"\"\n",
    "\n",
    "    # Redact an answer\n",
    "    print(\"=> Generating answer...\")\n",
    "    generated_sequence, _, _ = get_prediction_and_scores(context_and_question)\n",
    "    # answer = f\"{question} {generated_sequence}\"\n",
    "\n",
    "    # removing the special tokens and padding\n",
    "    answer = generated_sequence.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").replace(\"\\n\", \"\").strip()\n",
    "\n",
    "    return answer, relevant_docs_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "reUcMJqkRi2m",
    "tags": []
   },
   "outputs": [],
   "source": [
    "user_query = 'Who taught 11737 Multilingual Natural Language Processing in fall 2023?'\n",
    "# user_query = 'Was carnegie the best man alive\"?'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HX130eMiRjXm",
    "outputId": "d415795c-6396-4ec6-adc6-5aaf236ba253",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n",
      "['Question: Is the course 11737 Multilingual Natural Language Processing. offered in  Fall 2023?\\nAnswer: Yes', '## TITLE\\nLearning Performance-Improving Code Edits\\nPhD thesis, Western University, 2022. Lewis Tunstall, Leandro V on Werra, and Thomas Wolf. Natural Language Processing with Transformers . \"O Reilly Media, Inc.\", 2022. Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama, Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals, Percy Liang, Jeff Dean, and William Fedus. Emergent Abilities of Large Language Models. arXiv preprint arXiv:2206.07682 , 2022a.\\n\\nJason Wei, Xuezhi Wang, Dale Schuurmans, Maarten Bosma, Fei Xia, Ed Chi, Quoc V Le, Denny Zhou, et al. Chain-of-thought prompting elicits reasoning in large language models. Advances in Neural Information Processing Systems , 35:24824 24837, 2022b.\\n\\nThomas Wolf, Lysandre Debut, Victor Sanh, Julien Chaumond, Clement Delangue, Anthony Moi, Pierric Cistac, Tim Rault, Remi Louf, Morgan Funtowicz, Joe Davison, Sam Shleifer, Patrick von Platen, Clara Ma, Yacine Jernite, Julien Plu, Canwen Xu, Teven Le Scao, Sylvain Gugger, Mariama Drame, Quentin Lhoest, and Alexander Rush. Transformers: State-of-the-art natural language processing. In Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations , pp.', 'Question: Who are the instructors of the course 11737 Multilingual Natural Language Processing.?\\nAnswer: Li', 'Question: What is the course number of Multilingual Natural Language Processing.?\\nAnswer: 11737', 'Question: What is the name of course number 11737?\\nAnswer: Multilingual Natural Language Processing.', '## TITLE\\nPlayGround Low Resource Machine Translation System for the 2023 AmericasNLP Shared Task\\nIn Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas , pages 255 264, Online. Association for Computational Linguistics. Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, and Ludwig Schmidt. 2022. Model soups: averaging weights of multiple finetuned models improves accuracy without increasing inference time.\\n\\nIn Proceedings of the 39th International Conference on Machine Learning , volume 162 ofProceedings of Machine Learning Research , pages Yige Xu, Xipeng Qiu, Ligao Zhou, and Xuanjing Huang. 2020. Improving BERT fine-tuning via self-ensemble and self-distillation. CoRR , Francis Zheng, Machel Reid, Edison Marrese-Taylor, and Yutaka Matsuo. 2021. Low-resource machine translation using cross-lingual language model pretraining.\\n\\nIn Proceedings of the First Workshop on Natural Language Processing for Indigenous Languages of the Americas , pages 234 240, Online. Association for Computational Linguistics.\\n\\n176', '## TITLE\\nALERT: Adapt Language Models to Reasoning Tasks\\nMikel Artetxe, Shruti Bhosale, Naman Goyal, Todor Mihaylov, Myle Ott, Sam Shleifer, Xi Victoria Lin, Jingfei Du, Srinivasan Iyer, Ramakanth Pasunuru, et al. 2021. Efficient large scale language modeling with mixtures of experts. arXiv preprint Jason Baumgartner, Savvas Zannettou, Brian Keegan, Megan Squire, and Jeremy Blackburn. 2020. The pushshift reddit dataset. In Proceedings ofthe international AAAI conference onweb andsocial media, volume 14, pages 830 839.\\n\\nYonatan Bisk, Rowan Zellers, Ronan Le Bras, Jianfeng Gao, and Yejin Choi. 2020. Piqa: Reasoning about physical commonsense in natural language. InThirty-Fourth AAAI Conference onArtificial Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020a. Language models are few-shot learners. Advances inneural information processing systems, 33:1877 1901.\\n\\nTom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. 2020b. Language models are few-shot learners. Advances inneural information processing systems, 33:1877 1901. Oana-Maria Camburu, Tim Rockt schel, Thomas Lukasiewicz, and Phil Blunsom. 2018. e-snli: Natural language inference with natural language explanations. Advances inNeural Information Processing Zeming Chen and Qiyue Gao. 2022.', '## TITLE\\nTrain Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages\\nThe decline of Native Language in Canada . Stanford University Press. Guillaume Klein, Yoon Kim, Yuntian Deng, Jean Senellart, and Alexander Rush. 2017. Opennmt: Opensource toolkit for neural machine translation. Proceedings of the 55th annual meeting of the Association for Computational Linguistics, System Demonstrations , pages 67 72.Donald E Knuth. 1991. 3: 16 Bible texts illuminated . Sai Koneru, Danni Liu, and Jan Niehues. 2022. Costeffective training in low-resource neural machine translation.\\n\\narXiv preprint arXiv:2201.05700 . Zehui Lin, Xiao Pan, Mingxuan Wang, Xipeng Qiu, Jiangtao Feng, Hao Zhou, and Lei Li. 2020. Pretraining multilingual neural machine translation by leveraging alignment information. Proceedings of the 25th Conference on Empirical Methods in Natural Language Processing . Ming Liu, Wray Buntine, and Gholamreza Haffari. 2018. Learning to actively learn neural machine translation.\\n\\nIn Proceedings of the 22nd Conference on Computational Natural Language Learning , Yinhan Liu, Jiatao Gu, Naman Goyal, Xian Li, Sergey Edunov, Marjan Ghazvininejad, Mike Lewis, and Luke Zettlemoyer. 2020. Multilingual denoising pre-training for neural machine translation. Transactions of the Association for Computational Linguistics, Sameen Maruf, Fahimeh Saleh, and Gholamreza Haffari. 2019. A survey on document-level machine translation: Methods and evaluation.\\n\\nACM Computing Thomas Mayer and Michael Cysouw. 2014. Creating a massively parallel bible corpus. Oceania , Akiva Miura, Graham Neubig, Michael Paul, and Satoshi Nakamura. 2016. Selecting syntactic, nonredundant segments in active learning for machine translation. In Proceedings of the the 15th Conference of the North American Chapter of the Association for Computational Linguistics on Human Language Technologies , pages 20 29.', '## TITLE\\nDo All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\\nSmith, and Tao Yu. 2023. Selective annotation makes language models better few-shot learners. In Proc. of ICLR . Jimin Sun, Patrick Fernandes, Xinyi Wang, and Graham Neubig. 2023. A multi-dimensional evaluation of tokenizer-free multilingual pretrained models. In Findings of the Association for Computational Linguistics: EACL 2023 , pages 1725 1735, Dubrovnik, Croatia. Association for Computational Linguistics. Tianxiang Sun, Yunfan Shao, Hong Qian, Xuanjing Huang, and Xipeng Qiu. 2022.\\n\\nBlack-box tuning for language-model-as-a-service. In Proceedings of Zeerak Talat, Joachim Bingel, and Isabelle Augenstein. 2021. Disembodied machine learning: On the illusion of objectivity in nlp. ArXiv , abs/2101.11974. Zeerak Talat, Aur lie N v ol, Stella Biderman, Miruna Clinciu, Manan Dey, Shayne Longpre, Sasha Luccioni, Maraim Masoud, Margaret Mitchell, Dragomir Radev, Shanya Sharma, Arjun Subramonian, Jaesung Tae, Samson Tan, Deepak Tunuguntla, and Oskar Van Der Wal. 2022.\\n\\nYou reap what you sow: On the challenges of bias evaluation under multilingual settings. InProceedings of BigScience Episode #5   Workshop on Challenges & Perspectives in Creating Large Language Models , pages 26 41, virtual+Dublin. Association for Computational Linguistics. Yi Tay, Vinh Q. Tran, Sebastian Ruder, Jai Gupta, Hyung Won Chung, Dara Bahri, Zhen Qin, Simon Baumgartner, Cong Yu, and Donald Metzler. 2022. Charformer: Fast character transformers via gradientbased subword tokenization.\\n\\nIn International Conference on Learning Representations . NLLB Team, Marta R.', '## TITLE\\nMitigating Societal Harms in Large Language Models\\nNo copyright issues are expected as we will use open-source material. 4 General Information Sachin Kumar is a sixth year PhD candidate at the Language Technologies Institute, School of Computer Science at CMU. Sachin s research tackles critical technical problems in core language generation with deep learning, such as open-vocabulary generation, detection and demotion of spurious confounders, and controllable generation.28Vidhisha Balachandran (she/her) is a fourth-year Ph.D.\\n\\nstudent at the Language Technologies Institute, School of Computer Science at CMU. Her current research focuses on building interpretable and reliable NLP models with a focus on summarization, factuality, and KB-based reasoning. Lucille Njoo (she/her) is a second-year PhD student at the Paul G. Allen School of Computer Science and Engineering at the University of Washington. She works in the intersection of NLP, ethics, and computational social science, working on identifying societal harms in NLP models.\\n\\nAntonios Anastasopoulos (he/him) is an Assistant Professor at the Department of Computer Science at George Mason University, USA. His research focuses on NLP for local and low-resource languages and varieties, cross-lingual learning and multilinguality, and cross-lingual fairness. Yulia Tsvetkov (she/her) is an Assistant Professor at the Paul G. Allen School of Computer Science and Engineering at the University of Washington, USA.\\n\\nHer research focuses on computational ethics, multilingual NLP, and machine learning for NLP. She developed a course on Computational Ethics in NLP and is teaching it at both undergraduate and graduate levels since 2017, and she is a co-chair of the ACL Ethics Committee. 4.2 Audience and Pre-Requisites We expect participants from a wide array of backgrounds, including researchers, engineers, and end users of NLP technologies. Based on prior iterations of the tutorial, we expect an audience size of 50-100.']\n",
      "Your documents are roughly 261.8 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n",
      " Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 12.21it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answer, relevant_docs = flanT5_without_threshold(\n",
    "    user_query, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KKCo8SEpRj3i",
    "outputId": "f064de33-4077-435f-d77e-2e02c12c12f8",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Answer==================================\n",
      "3\n",
      "Li\n"
     ]
    }
   ],
   "source": [
    "print(\"==================================Answer==================================\")\n",
    "print(len(relevant_docs))\n",
    "print(f\"{answer}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "th4u-kqLRoik",
    "outputId": "fffeab8d-de47-43c5-d2ac-3def6903d71a",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================Source docs==================================\n",
      "Document ------------------------------------------------------------\n",
      "Question: Is the course 11737 Multilingual Natural Language Processing. offered in  Fall 2023?\n",
      "Answer: Yes\n",
      "Document ------------------------------------------------------------\n",
      "Question: Who are the instructors of the course 11737 Multilingual Natural Language Processing.?\n",
      "Answer: Li\n",
      "Document ------------------------------------------------------------\n",
      "Question: What is the name of course number 11737?\n",
      "Answer: Multilingual Natural Language Processing.\n"
     ]
    }
   ],
   "source": [
    "print(\"==================================Source docs==================================\")\n",
    "for  doc in (relevant_docs):\n",
    "    print(f\"Document ------------------------------------------------------------\")\n",
    "    print(f'{doc}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xEsiqSCesiQy"
   },
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# specify output csv \n",
    "input_file = 'qa_gold-t5-large.csv'\n",
    "output_file = 'flan-t5-large-output-new.csv'\n",
    "csv_input_dir = f'csv_qa_gold/{input_file}'\n",
    "csv_output_dir = f'csv_qa_gold/{output_file}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def initialize_model_answer_column(df):\n",
    "    if 'ModelAnswer' not in df.columns:\n",
    "        df['ModelAnswer'] = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191, 3)\n",
      "(191, 3)\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory_csv = 'csv_qa_gold'\n",
    "csv_files = ['csv_qa_gold/test_combined.csv']\n",
    "\n",
    "# read in the csv files in the directory and concatenate\n",
    "df_total = pd.DataFrame()\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    print(df.shape)\n",
    "    df_total = pd.concat([df_total, df], axis=0)\n",
    "print(df_total.shape)\n",
    "df_total = initialize_model_answer_column(df_total)\n",
    "df_total.to_csv(csv_input_dir, index=False) \n",
    "# doing this so the original is untouched"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Category</th>\n",
       "      <th>Question</th>\n",
       "      <th>Answer</th>\n",
       "      <th>ModelAnswer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>webpages</td>\n",
       "      <td>Was the Scottish terrier a familiar figure at CMU before it officially became a mascot?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>webpages</td>\n",
       "      <td>Is there an official mascot costume of Scotty?</td>\n",
       "      <td>Yes</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>webpages</td>\n",
       "      <td>Who led the effort to make Carnegie Mellon's mascot the Scottish Terrier?</td>\n",
       "      <td>Susan Bassett and Jennifer Church</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>webpages</td>\n",
       "      <td>Where did Scotty debug as CMU's official mascot?</td>\n",
       "      <td>at a football game</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>webpages</td>\n",
       "      <td>Who did CMU partner with to create the Scotty mascot graphics?</td>\n",
       "      <td>SME Branding</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>webpages</td>\n",
       "      <td>What month and year was the Mascot Identity Task Force formed?</td>\n",
       "      <td>November 2006</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>webpages</td>\n",
       "      <td>When did Scotty debut as CMU's official mascot?</td>\n",
       "      <td>November 2007</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>webpages</td>\n",
       "      <td>What characteristics does the Scottish terrier represent?</td>\n",
       "      <td>determined, thoughtful, strength, power, agility in a small package</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>webpages</td>\n",
       "      <td>Where did the athletic director of CMU graduate from?</td>\n",
       "      <td>Brandeis</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>webpages</td>\n",
       "      <td>When was Carnegie Technical Schools founded?</td>\n",
       "      <td>1900</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Category  \\\n",
       "10  webpages   \n",
       "11  webpages   \n",
       "12  webpages   \n",
       "13  webpages   \n",
       "14  webpages   \n",
       "15  webpages   \n",
       "16  webpages   \n",
       "17  webpages   \n",
       "18  webpages   \n",
       "19  webpages   \n",
       "\n",
       "                                                                                   Question  \\\n",
       "10  Was the Scottish terrier a familiar figure at CMU before it officially became a mascot?   \n",
       "11                                           Is there an official mascot costume of Scotty?   \n",
       "12                Who led the effort to make Carnegie Mellon's mascot the Scottish Terrier?   \n",
       "13                                         Where did Scotty debug as CMU's official mascot?   \n",
       "14                           Who did CMU partner with to create the Scotty mascot graphics?   \n",
       "15                           What month and year was the Mascot Identity Task Force formed?   \n",
       "16                                          When did Scotty debut as CMU's official mascot?   \n",
       "17                                What characteristics does the Scottish terrier represent?   \n",
       "18                                    Where did the athletic director of CMU graduate from?   \n",
       "19                                             When was Carnegie Technical Schools founded?   \n",
       "\n",
       "                                                                 Answer  \\\n",
       "10                                                                  Yes   \n",
       "11                                                                  Yes   \n",
       "12                                    Susan Bassett and Jennifer Church   \n",
       "13                                                   at a football game   \n",
       "14                                                         SME Branding   \n",
       "15                                                        November 2006   \n",
       "16                                                        November 2007   \n",
       "17  determined, thoughtful, strength, power, agility in a small package   \n",
       "18                                                             Brandeis   \n",
       "19                                                                 1900   \n",
       "\n",
       "   ModelAnswer  \n",
       "10        None  \n",
       "11        None  \n",
       "12        None  \n",
       "13        None  \n",
       "14        None  \n",
       "15        None  \n",
       "16        None  \n",
       "17        None  \n",
       "18        None  \n",
       "19        None  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_total.iloc[10:20, :] # please check that excel does not fuck up for row 15,16 the November 2006 to Nov-06 or some other format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "id": "aS3tl8emsiQy",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate_answer(question):\n",
    "    answer, _ = flanT5_without_threshold(\n",
    "        question, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER\n",
    "    )\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# def generate_answer(question, return_relevant_docs = False, llm = llm_answer_gen):\n",
    "#     answer, relevant_docs = answer_llama_new(question, KNOWLEDGE_VECTOR_DATABASE,\n",
    "#                             reranker=RERANKER, llm = llm)\n",
    "#     if return_relevant_docs:\n",
    "#         return answer, relevant_docs\n",
    "#     return answer\n",
    "OUTPUT_FILE= 'flan-t5-large-output-new-embeddings.txt'\n",
    "# note that this overwrites previously generated answers to the answer file\n",
    "def generate_answers_all(qfile, afile):\n",
    "    with open(qfile, 'r') as questions_file, open(afile, \"a\") as ans_file:\n",
    "        questions = questions_file.readlines()\n",
    "        for q in questions:\n",
    "            ans = generate_answer(q.strip())  # Remove newline character\n",
    "            ans_file.write(ans + '\\n')\n",
    "            ans_file.flush()  # Flush buffer to ensure content is written immediately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define the function to process the CSV and add model answers\n",
    "# def generate_answers_and_save(csv_path):\n",
    "#     df = pd.read_csv(csv_path)\n",
    "#     if 'ModelAnswer' not in df.columns:\n",
    "#         df['ModelAnswer'] = ''\n",
    "#     for index, row in df.iterrows():\n",
    "#         ModelAnswer = generate_answer(row['Question'])\n",
    "#         df.at[index, 'ModelAnswer'] = ModelAnswer\n",
    "#         df.to_csv(csv_output_dir, index=False)\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 25.72it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 20.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 72.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 74.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n",
      "=> Reranking documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1/1 [00:00<00:00, 51.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Generating answer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> Retrieving documents...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process SpawnProcess-210:\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/multiprocessing/process.py\", line 314, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/multiprocessing/process.py\", line 108, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 514, in _encode_multi_process_worker\n",
      "    embeddings = model.encode(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py\", line 350, in encode\n",
      "    out_features = self.forward(features)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/container.py\", line 215, in forward\n",
      "    input = module(input)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sentence_transformers/models/Transformer.py\", line 98, in forward\n",
      "    output_states = self.auto_model(**trans_features, return_dict=False)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 1006, in forward\n",
      "    embedding_output = self.embeddings(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py\", line 232, in forward\n",
      "    inputs_embeds = self.word_embeddings(input_ids)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1518, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 1527, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/modules/sparse.py\", line 162, in forward\n",
      "    return F.embedding(\n",
      "  File \"/home/ec2-user/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/torch/nn/functional.py\", line 2233, in embedding\n",
      "    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[110], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# df_with_model_answers = generate_answers_and_save(csv_path=csv_input_dir)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df_with_model_answers \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answers_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mQUESTIONS_FILE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOUTPUT_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(df_with_model_answers\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      4\u001b[0m df_with_model_answers\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[0;32mIn[107], line 13\u001b[0m, in \u001b[0;36mgenerate_answers_all\u001b[0;34m(qfile, afile)\u001b[0m\n\u001b[1;32m     11\u001b[0m questions \u001b[38;5;241m=\u001b[39m questions_file\u001b[38;5;241m.\u001b[39mreadlines()\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m q \u001b[38;5;129;01min\u001b[39;00m questions:\n\u001b[0;32m---> 13\u001b[0m     ans \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_answer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrip\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Remove newline character\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     ans_file\u001b[38;5;241m.\u001b[39mwrite(ans \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m     ans_file\u001b[38;5;241m.\u001b[39mflush()\n",
      "Cell \u001b[0;32mIn[106], line 2\u001b[0m, in \u001b[0;36mgenerate_answer\u001b[0;34m(question)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_answer\u001b[39m(question):\n\u001b[0;32m----> 2\u001b[0m     answer, _ \u001b[38;5;241m=\u001b[39m \u001b[43mflanT5_without_threshold\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mKNOWLEDGE_VECTOR_DATABASE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreranker\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mRERANKER\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m answer\n",
      "Cell \u001b[0;32mIn[82], line 11\u001b[0m, in \u001b[0;36mflanT5_without_threshold\u001b[0;34m(question, knowledge_index, reranker, num_retrieved_docs, num_docs_final)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=> Retrieving documents...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Gather documents with retriever\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m relevant_docs_acquired \u001b[38;5;241m=\u001b[39m \u001b[43mknowledge_index\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquestion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_retrieved_docs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# print(relevant_docs_acquired)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# print(relevant_docs_acquired)\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m reranker:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:530\u001b[0m, in \u001b[0;36mFAISS.similarity_search\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search\u001b[39m(\n\u001b[1;32m    511\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    512\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    516\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    517\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Document]:\n\u001b[1;32m    518\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    519\u001b[0m \n\u001b[1;32m    520\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    528\u001b[0m \u001b[38;5;124;03m        List of Documents most similar to the query.\u001b[39;00m\n\u001b[1;32m    529\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 530\u001b[0m     docs_and_scores \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_score\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    531\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mfilter\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfetch_k\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfetch_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    532\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    533\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [doc \u001b[38;5;28;01mfor\u001b[39;00m doc, _ \u001b[38;5;129;01min\u001b[39;00m docs_and_scores]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:402\u001b[0m, in \u001b[0;36mFAISS.similarity_search_with_score\u001b[0;34m(self, query, k, filter, fetch_k, **kwargs)\u001b[0m\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msimilarity_search_with_score\u001b[39m(\n\u001b[1;32m    379\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    380\u001b[0m     query: \u001b[38;5;28mstr\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    384\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    385\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[Tuple[Document, \u001b[38;5;28mfloat\u001b[39m]]:\n\u001b[1;32m    386\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return docs most similar to query.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m \n\u001b[1;32m    388\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    400\u001b[0m \u001b[38;5;124;03m        L2 distance in float. Lower score represents more similarity.\u001b[39;00m\n\u001b[1;32m    401\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 402\u001b[0m     embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_embed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    403\u001b[0m     docs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msimilarity_search_with_score_by_vector(\n\u001b[1;32m    404\u001b[0m         embedding,\n\u001b[1;32m    405\u001b[0m         k,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    408\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    409\u001b[0m     )\n\u001b[1;32m    410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m docs\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_community/vectorstores/faiss.py:154\u001b[0m, in \u001b[0;36mFAISS._embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_embed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    153\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function, Embeddings):\n\u001b[0;32m--> 154\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membedding_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_query\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    155\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    156\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membedding_function(text)\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py:108\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_query\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21membed_query\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[\u001b[38;5;28mfloat\u001b[39m]:\n\u001b[1;32m    100\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Compute query embeddings using a HuggingFace transformer model.\u001b[39;00m\n\u001b[1;32m    101\u001b[0m \n\u001b[1;32m    102\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m        Embeddings for the text.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/langchain_community/embeddings/huggingface.py:90\u001b[0m, in \u001b[0;36mHuggingFaceEmbeddings.embed_documents\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmulti_process:\n\u001b[1;32m     89\u001b[0m     pool \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclient\u001b[38;5;241m.\u001b[39mstart_multi_process_pool()\n\u001b[0;32m---> 90\u001b[0m     embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencode_multi_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpool\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     91\u001b[0m     sentence_transformers\u001b[38;5;241m.\u001b[39mSentenceTransformer\u001b[38;5;241m.\u001b[39mstop_multi_process_pool(pool)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:502\u001b[0m, in \u001b[0;36mSentenceTransformer.encode_multi_process\u001b[0;34m(self, sentences, pool, prompt_name, prompt, batch_size, chunk_size, normalize_embeddings)\u001b[0m\n\u001b[1;32m    499\u001b[0m     last_chunk_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    501\u001b[0m output_queue \u001b[38;5;241m=\u001b[39m pool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 502\u001b[0m results_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([output_queue\u001b[38;5;241m.\u001b[39mget() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(last_chunk_id)], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    503\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([result[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results_list])\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:502\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    499\u001b[0m     last_chunk_id \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    501\u001b[0m output_queue \u001b[38;5;241m=\u001b[39m pool[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m--> 502\u001b[0m results_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m([\u001b[43moutput_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(last_chunk_id)], key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    503\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([result[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results_list])\n\u001b[1;32m    504\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/multiprocessing/queues.py:103\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m block \u001b[38;5;129;01mand\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock:\n\u001b[0;32m--> 103\u001b[0m         res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sem\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    105\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/multiprocessing/connection.py:216\u001b[0m, in \u001b[0;36m_ConnectionBase.recv_bytes\u001b[0;34m(self, maxlength)\u001b[0m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m maxlength \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m maxlength \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    215\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnegative maxlength\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 216\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv_bytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaxlength\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m buf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bad_message_length()\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/multiprocessing/connection.py:414\u001b[0m, in \u001b[0;36mConnection._recv_bytes\u001b[0;34m(self, maxsize)\u001b[0m\n\u001b[1;32m    413\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_recv_bytes\u001b[39m(\u001b[38;5;28mself\u001b[39m, maxsize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 414\u001b[0m     buf \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_recv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    415\u001b[0m     size, \u001b[38;5;241m=\u001b[39m struct\u001b[38;5;241m.\u001b[39munpack(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m!i\u001b[39m\u001b[38;5;124m\"\u001b[39m, buf\u001b[38;5;241m.\u001b[39mgetvalue())\n\u001b[1;32m    416\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m size \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m~/anaconda3/envs/pytorch_p310/lib/python3.10/multiprocessing/connection.py:379\u001b[0m, in \u001b[0;36mConnection._recv\u001b[0;34m(self, size, read)\u001b[0m\n\u001b[1;32m    377\u001b[0m remaining \u001b[38;5;241m=\u001b[39m size\n\u001b[1;32m    378\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m remaining \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 379\u001b[0m     chunk \u001b[38;5;241m=\u001b[39m \u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mremaining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    380\u001b[0m     n \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(chunk)\n\u001b[1;32m    381\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# df_with_model_answers = generate_answers_and_save(csv_path=csv_input_dir)\n",
    "df_with_model_answers = generate_answers_all(QUESTIONS_FILE, OUTPUT_FILE)\n",
    "print(df_with_model_answers.shape)\n",
    "df_with_model_answers.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "id": "3k1mPLZAsiQy",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact match: 0.04712041884816754\n",
      "F1 score: 0.19903977874921847\n",
      "Recall score: 0.45991834195224224\n"
     ]
    }
   ],
   "source": [
    "# with additional extraction prompt after generation without prompt\n",
    "df = pd.read_csv('csv_qa_gold/llama-no-temp-output.csv')\n",
    "type(df['ModelAnswer'][0]),type(df['Answer'][0])\n",
    "df = df.astype(str)  # Convert columns to string type\n",
    "\n",
    "from evaluation_csv import total_score_csv\n",
    "exact_match, f1_score, recall_score = total_score_csv(df['ModelAnswer'], df['Answer'])\n",
    "print(f'Exact match: {exact_match}')\n",
    "print(f'F1 score: {f1_score}')\n",
    "print(f'Recall score: {recall_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Category: webpages\n",
      "Exact match: 0.09259259259259259\n",
      "F1 score: 0.31618458605389715\n",
      "Recall score: 0.6935956790123456\n",
      "\n",
      "\n",
      "Category: tabular_webpages\n",
      "Exact match: 0.0\n",
      "F1 score: 0.14436440295815298\n",
      "Recall score: 0.46610576923076924\n",
      "\n",
      "\n",
      "Category: other_pdf\n",
      "Exact match: 0.0\n",
      "F1 score: 0.13257874211960932\n",
      "Recall score: 0.2637853692201518\n",
      "\n",
      "\n",
      "Category: papers_pdf\n",
      "Exact match: 0.0\n",
      "F1 score: 0.0574977817213842\n",
      "Recall score: 0.22222222222222224\n",
      "\n",
      "\n",
      "Category: schedule_pdf\n",
      "Exact match: 0.0\n",
      "F1 score: 0.015810276679841896\n",
      "Recall score: 0.045454545454545456\n",
      "\n",
      "\n",
      "Category: jsons\n",
      "Exact match: 0.05714285714285714\n",
      "F1 score: 0.2100834755820694\n",
      "Recall score: 0.49030412753234165\n",
      "\n",
      "\n",
      "Category: json_hard\n",
      "Exact match: 0.0\n",
      "F1 score: 0.030124777183600716\n",
      "Recall score: 0.048863636363636366\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for category in df['Category'].unique():\n",
    "    print(f'Category: {category}')\n",
    "    df_category = df[df['Category'] == category]\n",
    "    exact_match, f1_score, recall_score = total_score_csv(df_category['ModelAnswer'], df_category['Answer'])\n",
    "    print(f'Exact match: {exact_match}')\n",
    "    print(f'F1 score: {f1_score}')\n",
    "    print(f'Recall score: {recall_score}')\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
