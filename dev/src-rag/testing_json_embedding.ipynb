{"cells":[{"cell_type":"markdown","metadata":{"id":"hUCaGdAj9-9F"},"source":["Souce:\n","- https://huggingface.co/learn/cookbook/en/advanced_rag\n","- https://arc.net/l/quote/vntkseji"]},{"cell_type":"markdown","metadata":{"id":"pt_BRiBR2tKE"},"source":["# Assumptions\n","- the faiss_index embeddings are up to date"]},{"cell_type":"code","execution_count":37,"metadata":{"id":"MrXGTQoSsiQv"},"outputs":[],"source":["import os\n","from dotenv import load_dotenv\n","load_dotenv('.env')\n","hf_api = os.getenv('HF_API')\n","HUGGINGFACEHUB_API_TOKEN = hf_api"]},{"cell_type":"code","execution_count":38,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10559,"status":"ok","timestamp":1709617668179,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"AmPefRqf2tKF","outputId":"3b5a2047-bdd6-4229-f014-c6ecc32edf69"},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers==4.38.0 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (4.38.0)\n","Requirement already satisfied: filelock in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from transformers==4.38.0) (3.0.12)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from transformers==4.38.0) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from transformers==4.38.0) (1.26.3)\n","Requirement already satisfied: packaging>=20.0 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from transformers==4.38.0) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from transformers==4.38.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from transformers==4.38.0) (2023.12.25)\n","Requirement already satisfied: requests in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from transformers==4.38.0) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from transformers==4.38.0) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from transformers==4.38.0) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from transformers==4.38.0) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0) (2023.10.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0) (4.9.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from requests->transformers==4.38.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from requests->transformers==4.38.0) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from requests->transformers==4.38.0) (2.1.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages (from requests->transformers==4.38.0) (2024.2.2)\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["pip install transformers==4.38.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39064,"status":"ok","timestamp":1709617707240,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"m9jP7QzjProY","outputId":"dcd78147-a081-4a6c-d793-a3489d3d01d3"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m806.2/806.2 kB\u001b[0m \u001b[31m43.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m26.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m252.6/252.6 kB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.7/65.7 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q torch accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":40514,"status":"ok","timestamp":1709617747751,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"EHEfmoGTRsGh","outputId":"06607aec-1cfe-4660-8703-2506142a9807"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting unstructured\n","  Downloading unstructured-0.12.5-py3-none-any.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: chardet in /usr/local/lib/python3.10/dist-packages (from unstructured) (5.2.0)\n","Collecting filetype (from unstructured)\n","  Downloading filetype-1.2.0-py2.py3-none-any.whl (19 kB)\n","Collecting python-magic (from unstructured)\n","  Downloading python_magic-0.4.27-py2.py3-none-any.whl (13 kB)\n","Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.9.4)\n","Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (from unstructured) (3.8.1)\n","Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from unstructured) (2.31.0)\n","Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.12.3)\n","Collecting emoji (from unstructured)\n","  Downloading emoji-2.10.1-py2.py3-none-any.whl (421 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m421.5/421.5 kB\u001b[0m \u001b[31m35.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: dataclasses-json in /usr/local/lib/python3.10/dist-packages (from unstructured) (0.6.4)\n","Collecting python-iso639 (from unstructured)\n","  Downloading python_iso639-2024.2.7-py3-none-any.whl (274 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m274.7/274.7 kB\u001b[0m \u001b[31m23.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting langdetect (from unstructured)\n","  Downloading langdetect-1.0.9.tar.gz (981 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.25.2)\n","Collecting rapidfuzz (from unstructured)\n","  Downloading rapidfuzz-3.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m37.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting backoff (from unstructured)\n","  Downloading backoff-2.2.1-py3-none-any.whl (15 kB)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from unstructured) (4.10.0)\n","Collecting unstructured-client>=0.15.1 (from unstructured)\n","  Downloading unstructured_client-0.21.0-py3-none-any.whl (24 kB)\n","Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from unstructured) (1.14.1)\n","Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2024.2.2)\n","Requirement already satisfied: charset-normalizer>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.3.2)\n","Requirement already satisfied: idna>=3.4 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.6)\n","Collecting jsonpath-python>=1.0.6 (from unstructured-client>=0.15.1->unstructured)\n","  Downloading jsonpath_python-1.0.6-py3-none-any.whl (7.6 kB)\n","Requirement already satisfied: marshmallow>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (3.21.1)\n","Requirement already satisfied: mypy-extensions>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (1.0.0)\n","Requirement already satisfied: packaging>=23.1 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (23.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2.8.2)\n","Requirement already satisfied: six>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (1.16.0)\n","Requirement already satisfied: typing-inspect>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (0.9.0)\n","Requirement already satisfied: urllib3>=1.26.18 in /usr/local/lib/python3.10/dist-packages (from unstructured-client>=0.15.1->unstructured) (2.0.7)\n","Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4->unstructured) (2.5)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (1.3.2)\n","Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (2023.12.25)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk->unstructured) (4.66.2)\n","Building wheels for collected packages: langdetect\n","  Building wheel for langdetect (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993225 sha256=e34b4c3f81cf6c136e17b9c1087784325d08297d693f46620b901f823eacc1a2\n","  Stored in directory: /root/.cache/pip/wheels/95/03/7d/59ea870c70ce4e5a370638b5462a7711ab78fba2f655d05106\n","Successfully built langdetect\n","Installing collected packages: filetype, rapidfuzz, python-magic, python-iso639, langdetect, jsonpath-python, emoji, backoff, unstructured-client, unstructured\n","Successfully installed backoff-2.2.1 emoji-2.10.1 filetype-1.2.0 jsonpath-python-1.0.6 langdetect-1.0.9 python-iso639-2024.2.7 python-magic-0.4.27 rapidfuzz-3.6.1 unstructured-0.12.5 unstructured-client-0.21.0\n"]}],"source":["!pip install unstructured ragatouille\n","# reranker\n","from ragatouille import RAGPretrainedModel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":42243,"status":"ok","timestamp":1709617789990,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"NjDsXe5NaJ4y","outputId":"f557b365-6f37-47ce-ea39-e89886cb3b50"},"outputs":[{"name":"stdout","output_type":"stream","text":["Collecting ragatouille\n","  Downloading ragatouille-0.0.7.post9-py3-none-any.whl (35 kB)\n","Collecting aiohttp==3.9.1 (from ragatouille)\n","  Downloading aiohttp-3.9.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting colbert-ai==0.2.19 (from ragatouille)\n","  Downloading colbert-ai-0.2.19.tar.gz (86 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.7/86.7 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting faiss-cpu<2.0.0,>=1.7.4 (from ragatouille)\n","  Downloading faiss_cpu-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.0 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.0/27.0 MB\u001b[0m \u001b[31m28.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: langchain<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (0.1.10)\n","Requirement already satisfied: langchain_core<0.2.0,>=0.1.4 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (0.1.29)\n","Collecting llama-index<0.10.0,>=0.9.24 (from ragatouille)\n","  Downloading llama_index-0.9.48-py3-none-any.whl (15.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting onnx<2.0.0,>=1.15.0 (from ragatouille)\n","  Downloading onnx-1.15.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting ruff<0.2.0,>=0.1.9 (from ragatouille)\n","  Downloading ruff-0.1.15-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: sentence-transformers<3.0.0,>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (2.5.1)\n","Requirement already satisfied: srsly==2.4.8 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (2.4.8)\n","Requirement already satisfied: torch<3.0.0,>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (2.1.0+cu121)\n","Requirement already satisfied: transformers<5.0.0,>=4.36.2 in /usr/local/lib/python3.10/dist-packages (from ragatouille) (4.38.0)\n","Collecting voyager<3.0.0,>=2.0.2 (from ragatouille)\n","  Downloading voyager-2.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.1 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.1/4.1 MB\u001b[0m \u001b[31m59.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (1.9.4)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp==3.9.1->ragatouille) (4.0.3)\n","Collecting bitarray (from colbert-ai==0.2.19->ragatouille)\n","  Downloading bitarray-2.9.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.3/288.3 kB\u001b[0m \u001b[31m27.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting datasets (from colbert-ai==0.2.19->ragatouille)\n","  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m35.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (2.2.5)\n","Collecting git-python (from colbert-ai==0.2.19->ragatouille)\n","  Downloading git_python-1.0.3-py2.py3-none-any.whl (1.9 kB)\n","Collecting python-dotenv (from colbert-ai==0.2.19->ragatouille)\n","  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n","Collecting ninja (from colbert-ai==0.2.19->ragatouille)\n","  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (1.11.4)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from colbert-ai==0.2.19->ragatouille) (4.66.2)\n","Collecting ujson (from colbert-ai==0.2.19->ragatouille)\n","  Downloading ujson-5.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.2/53.2 kB\u001b[0m \u001b[31m7.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: catalogue<2.1.0,>=2.0.3 in /usr/local/lib/python3.10/dist-packages (from srsly==2.4.8->ragatouille) (2.0.10)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from faiss-cpu<2.0.0,>=1.7.4->ragatouille) (1.25.2)\n","Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (6.0.1)\n","Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.0.27)\n","Requirement already satisfied: dataclasses-json<0.7,>=0.5.7 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.6.4)\n","Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (1.33)\n","Requirement already satisfied: langchain-community<0.1,>=0.0.25 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.0.25)\n","Requirement already satisfied: langchain-text-splitters<0.1,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.0.1)\n","Requirement already satisfied: langsmith<0.2.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (0.1.19)\n","Requirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.6.3)\n","Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (2.31.0)\n","Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain<0.2.0,>=0.1.0->ragatouille) (8.2.3)\n","Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (3.7.1)\n","Requirement already satisfied: packaging<24.0,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain_core<0.2.0,>=0.1.4->ragatouille) (23.2)\n","Collecting deprecated>=1.2.9.3 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n","  Downloading Deprecated-1.2.14-py2.py3-none-any.whl (9.6 kB)\n","Collecting dirtyjson<2.0.0,>=1.0.8 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n","  Downloading dirtyjson-1.0.8-py3-none-any.whl (25 kB)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (2023.6.0)\n","Collecting httpx (from llama-index<0.10.0,>=0.9.24->ragatouille)\n","  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (1.6.0)\n","Requirement already satisfied: networkx>=3.0 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (3.2.1)\n","Requirement already satisfied: nltk<4.0.0,>=3.8.1 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (3.8.1)\n","Collecting openai>=1.1.0 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n","  Downloading openai-1.13.3-py3-none-any.whl (227 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m24.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (1.5.3)\n","Collecting tiktoken>=0.3.3 (from llama-index<0.10.0,>=0.9.24->ragatouille)\n","  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (4.10.0)\n","Requirement already satisfied: typing-inspect>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from llama-index<0.10.0,>=0.9.24->ragatouille) (0.9.0)\n","Requirement already satisfied: protobuf>=3.20.2 in /usr/local/lib/python3.10/dist-packages (from onnx<2.0.0,>=1.15.0->ragatouille) (3.20.3)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (1.2.2)\n","Requirement already satisfied: huggingface-hub>=0.15.1 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (0.20.3)\n","Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers<3.0.0,>=2.2.2->ragatouille) (9.4.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->ragatouille) (3.13.1)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->ragatouille) (1.12)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->ragatouille) (3.1.3)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<3.0.0,>=2.0.1->ragatouille) (2.1.0)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (2023.12.25)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers<5.0.0,>=4.36.2->ragatouille) (0.4.2)\n","Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (3.6)\n","Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (1.3.1)\n","Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain_core<0.2.0,>=0.1.4->ragatouille) (1.2.0)\n","Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in /usr/local/lib/python3.10/dist-packages (from dataclasses-json<0.7,>=0.5.7->langchain<0.2.0,>=0.1.0->ragatouille) (3.21.1)\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from deprecated>=1.2.9.3->llama-index<0.10.0,>=0.9.24->ragatouille) (1.14.1)\n","Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.10/dist-packages (from jsonpatch<2.0,>=1.33->langchain<0.2.0,>=0.1.0->ragatouille) (2.4)\n","Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.10/dist-packages (from langsmith<0.2.0,>=0.1.0->langchain<0.2.0,>=0.1.0->ragatouille) (3.9.15)\n","Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index<0.10.0,>=0.9.24->ragatouille) (8.1.7)\n","Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk<4.0.0,>=3.8.1->llama-index<0.10.0,>=0.9.24->ragatouille) (1.3.2)\n","Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai>=1.1.0->llama-index<0.10.0,>=0.9.24->ragatouille) (1.7.0)\n","Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx->llama-index<0.10.0,>=0.9.24->ragatouille) (2024.2.2)\n","Collecting httpcore==1.* (from httpx->llama-index<0.10.0,>=0.9.24->ragatouille)\n","  Downloading httpcore-1.0.4-py3-none-any.whl (77 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.8/77.8 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting h11<0.15,>=0.13 (from httpcore==1.*->httpx->llama-index<0.10.0,>=0.9.24->ragatouille)\n","  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (0.6.0)\n","Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain<0.2.0,>=0.1.0->ragatouille) (2.16.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (3.3.2)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain<0.2.0,>=0.1.0->ragatouille) (2.0.7)\n","Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain<0.2.0,>=0.1.0->ragatouille) (3.0.3)\n","Requirement already satisfied: mypy-extensions>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from typing-inspect>=0.8.0->llama-index<0.10.0,>=0.9.24->ragatouille) (1.0.0)\n","Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (14.0.2)\n","Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (0.6)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->colbert-ai==0.2.19->ragatouille)\n","  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->colbert-ai==0.2.19->ragatouille) (3.4.1)\n","Collecting multiprocess (from datasets->colbert-ai==0.2.19->ragatouille)\n","  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (3.0.1)\n","Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->colbert-ai==0.2.19->ragatouille) (2.1.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch<3.0.0,>=2.0.1->ragatouille) (2.1.5)\n","Collecting gitpython (from git-python->colbert-ai==0.2.19->ragatouille)\n","  Downloading GitPython-3.1.42-py3-none-any.whl (195 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m195.4/195.4 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index<0.10.0,>=0.9.24->ragatouille) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->llama-index<0.10.0,>=0.9.24->ragatouille) (2023.4)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers<3.0.0,>=2.2.2->ragatouille) (3.3.0)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<3.0.0,>=2.0.1->ragatouille) (1.3.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->llama-index<0.10.0,>=0.9.24->ragatouille) (1.16.0)\n","Collecting gitdb<5,>=4.0.1 (from gitpython->git-python->colbert-ai==0.2.19->ragatouille)\n","  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython->git-python->colbert-ai==0.2.19->ragatouille)\n","  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n","Building wheels for collected packages: colbert-ai\n","  Building wheel for colbert-ai (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for colbert-ai: filename=colbert_ai-0.2.19-py3-none-any.whl size=114761 sha256=341f3b353452b88c4fa6ed3d696c8437b26f92e0920c07149e1983e11c1e0486\n","  Stored in directory: /root/.cache/pip/wheels/90/b9/63/d4fc276c73c42ef7fc1065a26cf87e5a1cf56ef6498cbfbe5d\n","Successfully built colbert-ai\n","Installing collected packages: ninja, dirtyjson, bitarray, voyager, ujson, smmap, ruff, python-dotenv, onnx, h11, faiss-cpu, dill, deprecated, tiktoken, multiprocess, httpcore, gitdb, aiohttp, httpx, gitpython, openai, git-python, datasets, llama-index, colbert-ai, ragatouille\n","  Attempting uninstall: aiohttp\n","    Found existing installation: aiohttp 3.9.3\n","    Uninstalling aiohttp-3.9.3:\n","      Successfully uninstalled aiohttp-3.9.3\n","Successfully installed aiohttp-3.9.1 bitarray-2.9.2 colbert-ai-0.2.19 datasets-2.18.0 deprecated-1.2.14 dill-0.3.8 dirtyjson-1.0.8 faiss-cpu-1.8.0 git-python-1.0.3 gitdb-4.0.11 gitpython-3.1.42 h11-0.14.0 httpcore-1.0.4 httpx-0.27.0 llama-index-0.9.48 multiprocess-0.70.16 ninja-1.11.1.1 onnx-1.15.0 openai-1.13.3 python-dotenv-1.0.1 ragatouille-0.0.7.post9 ruff-0.1.15 smmap-5.0.1 tiktoken-0.6.0 ujson-5.9.0 voyager-2.0.2\n"]}],"source":["!pip3 install numpy\n","import numpy as np\n","np.__version__"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"dMOUdS4CaWO3"},"outputs":[],"source":["# fix colab error: https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n","import locale\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"eoujYMwW9-9J"},"outputs":[],"source":["from tqdm.notebook import tqdm\n","import pandas as pd\n","from typing import Optional, List, Tuple\n","import matplotlib.pyplot as plt\n","pd.set_option(\n","    \"display.max_colwidth\", None\n",")"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"9DLqgmyvsiQw"},"outputs":[],"source":["# Imports\n","import os\n","import pandas as pd\n","\n","# langchain imports\n","from langchain.docstore.document import Document as LangchainDocument\n","from langchain_community.document_loaders import DirectoryLoader\n","from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.vectorstores import FAISS\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","\n","# hf imports\n","from transformers import pipeline\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","# reranking\n","\n","from ragatouille import RAGPretrainedModel\n","from transformers import Pipeline"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15161,"status":"ok","timestamp":1709617909742,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"Nqq1MHWNQo7K","outputId":"dd1bb3d7-2412-4464-a4a1-7c58deaccd8e"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":559,"status":"ok","timestamp":1709617913932,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"dQ5-vC--Q6Bw","outputId":"c6e541ce-ec56-405c-a35a-cec23f160429"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/ANLP/NLP-RAG/src-rag\n"," database.py\t __pycache__    qa.txt\t       'RAG T5+ Normal Flare.ipynb'\n"," evaluation.py\t qa_gen.ipynb   questions.txt   setup.txt\n"]}],"source":["%cd drive/MyDrive/ANLP/NLP-RAG/src-rag\n","!ls"]},{"cell_type":"markdown","metadata":{"id":"dcMNNt55siQx"},"source":["# Specify the models/versions"]},{"cell_type":"code","execution_count":39,"metadata":{"id":"FAnu3j0hsiQx","outputId":"157e427f-6b36-4470-b46f-ed62c44e951d"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages/torch/cuda/amp/grad_scaler.py:125: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n","  warnings.warn(\n"]}],"source":["# give the paths\n","QUESTIONS_FILE = 'data/test/questions_webpages.txt'\n","# OUTPUT_FILE = 'system_outputs/webpages.txt'\n","\n","FAISS_FILE_JSON = '../faiss_index_author_papers_json' # it's actually a folder but whatever\n","FAISS_FILE = '../faiss_index_author_papers_lang' # it's actually a folder but whatever\n","\n","EMBEDDING_MODEL = \"thenlper/gte-base\" # make sure this matches whatever was used to create the doc embeddings\n","GENERATOR_MODEL = \"google/flan-t5-large\"\n","RERANKER_MODEL = \"colbert-ir/colbertv2.0\"\n","\n","RERANKER = RAGPretrainedModel.from_pretrained(RERANKER_MODEL)"]},{"cell_type":"code","execution_count":40,"metadata":{"colab":{"referenced_widgets":["7d8a9536491b4edbb9b45b1fca65a14a","ea2d43edd2a949e1ba2c9288c77278c8","1264f61a2602416c9707501077cc4ba3","d6a60b7f0f7d42799c8b7a2adfe32b95","3820631823774c52bacb10826096e1f7","50f3e8a8bffa483a996bff8b9fc178e8","af5956998d934d189568b5551262a3cc"]},"id":"M9X3nGPFsiQx","outputId":"866f2588-9d54-41e6-b513-38adee6529e5"},"outputs":[{"name":"stderr","output_type":"stream","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"]}],"source":["# initialize the LLM and its tokenizer, we are using Flan T5 Large for this\n","tokenizer = T5Tokenizer.from_pretrained(GENERATOR_MODEL)\n","model = T5ForConditionalGeneration.from_pretrained(GENERATOR_MODEL)"]},{"cell_type":"markdown","metadata":{"id":"Kr6rN10U9-9J"},"source":["### Load the knowledge base"]},{"cell_type":"code","execution_count":41,"metadata":{"id":"Z14HQGDx2tKI"},"outputs":[],"source":["embedding_model = HuggingFaceEmbeddings(\n","    model_name=EMBEDDING_MODEL,\n","    multi_process=True,\n","    # model_kwargs={\"device\": \"cuda\"},\n","    encode_kwargs={\"normalize_embeddings\": True},  #  True for cosine similarity\n","    )"]},{"cell_type":"code","execution_count":42,"metadata":{"id":"z5cSyyk_siQx"},"outputs":[],"source":["KNOWLEDGE_VECTOR_DATABASE_JSON = FAISS.load_local(FAISS_FILE_JSON, embedding_model)\n","KNOWLEDGE_VECTOR_DATABASE = FAISS.load_local(FAISS_FILE, embedding_model)"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"JfDXZPNINnvq"},"outputs":[],"source":["# function to get the prediction and scores from the LLM, given a prompt\n","def get_prediction_and_scores(prompt):\n","    input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","    outputs =  model.generate(input_ids, output_scores=True, return_dict_in_generate=True, max_length=100)\n","                            #   skip_special_tokens=True)\n","    generated_sequence = outputs.sequences[0]\n","\n","    # get the probability scores for each generated token\n","    transition_scores = torch.exp(model.compute_transition_scores(\n","        outputs.sequences, outputs.scores, normalize_logits=True\n","        # , skip_special_tokens = True\n","    )[0])\n","    return tokenizer.decode(generated_sequence), generated_sequence, transition_scores"]},{"cell_type":"markdown","metadata":{"id":"RlfHavRT9-9O"},"source":["## Retrieval and Answer Generation"]},{"cell_type":"code","execution_count":50,"metadata":{"id":"BUQZfctcOkbT"},"outputs":[],"source":["def flanT5_without_threshold(\n","    question: str,\n","    knowledge_index: FAISS,\n","    reranker: Optional[RAGPretrainedModel] = None,\n","    num_retrieved_docs: int = 5,\n","    num_docs_final: int = 3\n","    ):\n","\n","    print(\"=> Retrieving documents...\")\n","    # Gather documents with retriever\n","    relevant_docs_acquired = knowledge_index.similarity_search(query=question, k=num_retrieved_docs)\n","    # print(relevant_docs_acquired)\n","    # print(relevant_docs_acquired)\n","    if reranker:\n","        print(\"=> Reranking documents...\")\n","        relevant_docs = [doc.page_content for doc in relevant_docs_acquired]\n","        relevant_docs = reranker.rerank(question, relevant_docs, k=num_docs_final)\n","        # print(relevant_docs)\n","        relevant_docs_content = [doc[\"content\"] for doc in relevant_docs]\n","        relevant_doc_score = [doc[\"score\"] for doc in relevant_docs]\n","\n","    else:\n","        relevant_docs_content = [doc.page_content for doc in relevant_docs_acquired]\n","\n","    relevant_docs_content = relevant_docs_content[:num_docs_final]\n","    # relevant_doc_id = relevant_doc_id[:num_docs_final]\n","    # relevant_doc_index = relevant_doc_index[:num_docs_final]\n","\n","    # Build the final prompt\n","    context = \"\\nExtracted documents:\\n\"\n","    context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs_content)])\n","\n","    context_and_question = f\"Keep your answers short and concise. If the text has date and time include the date, time both. If there are multiple right answers, include them all, but keep it short overall. \\n Given the below context:\\n{context}\\n\\n Answer the following \\n{question}\\n\"\n","\n","    # context_and_question = \"\"\"\n","    # Answer the user's questions based on the below context. Please keep your answers short and concise. Only provide the answer itself.\"\n","    # ------------\n","    # {context}\n","    # ------------\n","    # Question: {question}\n","    # Answer:\n","    # \"\"\"\n","\n","    # Redact an answer\n","    print(\"=> Generating answer...\")\n","    generated_sequence, _, _ = get_prediction_and_scores(context_and_question)\n","    # answer = f\"{question} {generated_sequence}\"\n","\n","    # removing the special tokens and padding\n","    answer = generated_sequence.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n","    return answer, relevant_docs_content"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"reUcMJqkRi2m"},"outputs":[],"source":["# user_query = 'What venue was the paper \"The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics\" published in?'\n","user_query =' What is the paper ID of the paper Screen Correspondence: Mapping Interchangeable Elements between UIs?'"]},{"cell_type":"code","execution_count":57,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HX130eMiRjXm","outputId":"d415795c-6396-4ec6-adc6-5aaf236ba253"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> Retrieving documents...\n"]},{"name":"stderr","output_type":"stream","text":["/Users/vashisth/anaconda3/envs/llama_hw/lib/python3.11/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["=> Reranking documents...\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00,  4.45it/s]\n"]},{"name":"stdout","output_type":"stream","text":["=> Generating answer...\n","==================================Answer==================================\n","3\n","0e84679cf0945a2868245ba2be68c90453e48f2e\n"]}],"source":["answer, relevant_docs = flanT5_without_threshold(\n","    user_query, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER\n",")\n","print(\"==================================Answer==================================\")\n","print(len(relevant_docs))\n","print(f\"{answer}\")"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KKCo8SEpRj3i","outputId":"f064de33-4077-435f-d77e-2e02c12c12f8"},"outputs":[{"name":"stdout","output_type":"stream","text":["=> Retrieving documents...\n","=> Reranking documents...\n","Your documents are roughly 307.0 tokens long at the 90th percentile! This is quite long and might slow down reranking!\n"," Provide fewer documents, build smaller chunks or run on GPU if it takes too long for your needs!\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00,  1.29it/s]\n"]},{"name":"stdout","output_type":"stream","text":["=> Generating answer...\n","==================================Answer==================================\n","3\n","98abc6de98a24d599cf009a9670eaa5c97cba9bb\n"]}],"source":["answer, relevant_docs = flanT5_without_threshold(\n","    user_query, KNOWLEDGE_VECTOR_DATABASE_JSON, reranker=RERANKER\n",")\n","print(\"==================================Answer==================================\")\n","print(len(relevant_docs))\n","print(f\"{answer}\")"]},{"cell_type":"code","execution_count":20,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"th4u-kqLRoik","outputId":"fffeab8d-de47-43c5-d2ac-3def6903d71a"},"outputs":[{"name":"stdout","output_type":"stream","text":["==================================Source docs==================================\n","Documet ------------------------------------------------------------\n","Question: What is the semantic scholar author name of Graham Neubig?\n","Answer: Graham Neubig\n","Notes: ##Author: Graham Neubig, ##Title: Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction\n","Documet ------------------------------------------------------------\n","Question: What is the author ID of Graham Neubig?\n","Answer: 1700325\n","Notes: ##Author: Graham Neubig, ##Title: Cross-Modal Fine-Tuning: Align then Refine\n","Documet ------------------------------------------------------------\n","Question: What is the semantic scholar author name of Graham Neubig?\n","Answer: Graham Neubig\n","Notes: ##Author: Graham Neubig, ##Title: User-Centric Evaluation of OCR Systems for Kwak’wala\n"]}],"source":["print(\"==================================Source docs==================================\")\n","for  doc in (relevant_docs):\n","    print(f\"Documet ------------------------------------------------------------\")\n","    print(f'{doc}')"]},{"cell_type":"markdown","metadata":{"id":"xEsiqSCesiQy"},"source":["## Evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aS3tl8emsiQy"},"outputs":[],"source":["def generate_answer(question):\n","    answer, _ = flanT5_without_threshold(\n","        question, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER\n","    )\n","    return answer\n","\n","OUTPUT_FILE_WITHOUT_THRESHOLD= 'system_outputs/webpages_no_threshold.txt'\n","\n","# note that this overwrites previously generated answers to the answer file\n","def generate_answers_all(qfile, afile):\n","    questions_file = open(qfile, 'r')\n","    questions = questions_file.readlines()\n","    ans_file = open(afile, \"w+\")\n","    for q in questions:\n","        ans = generate_answer(q)\n","        ans_file.write(ans + '\\n')\n","    questions_file.close()\n","    ans_file.close()\n","\n","generate_answers_all(QUESTIONS_FILE, OUTPUT_FILE_WITHOUT_THRESHOLD)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3k1mPLZAsiQy"},"outputs":[],"source":["from evaluation import total_score\n","\n","print(total_score(OUTPUT_FILE_WITHOUT_THRESHOLD, '../data/test/reference_answers.txt'))"]},{"cell_type":"markdown","metadata":{"id":"jMzFR9rGsiQy"},"source":["---\n","# Leave for now the normal one works just fine"]},{"cell_type":"markdown","metadata":{"id":"iUDUM3Bbt9Hm"},"source":["## Flare\n","- Source: https://ayushtues.medium.com/flare-advanced-rag-implemented-from-scratch-07ca75c89800\n","- essentially an extra acceptance step\n","- </s>\n","  -  is the seperation token (shows the end of a sentence. When we say that we break the generation)\n","  -  if you dont want this remove the if statement where we 'break'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8H2wxME7siQy"},"outputs":[],"source":["# # function to get the prediction and scores from the LLM, given a prompt\n","# def get_prediction_and_score_flare(prompt):\n","#     input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n","#     outputs =  model.generate(input_ids, output_scores=True, return_dict_in_generate=True, max_length=200)\n","#     generated_sequence = outputs.sequences[0]\n","\n","#     # get the probability scores for each generated token\n","#     transition_scores = torch.exp(model.compute_transition_scores(\n","#         outputs.sequences, outputs.scores, normalize_logits=True\n","#     )[0])\n","#     return tokenizer.decode(generated_sequence), generated_sequence, transition_scores"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zxmrk3PPsiQy"},"outputs":[],"source":["# def flanT5_with_threshold(\n","#     input_text: str,\n","#     knowledge_index: FAISS,\n","#     reranker: Optional[RAGPretrainedModel] = None,\n","#     num_retrieved_docs: int = 5,\n","#     num_docs_final: int = 3,\n","#     threshold = .1\n","#     ):\n","\n","#     relevant_docs = None\n","\n","#     while True: # breaks when you have a separation token in the generated sequence\n","\n","#         generated_sequence, tokens, scores = get_prediction_and_scores(input_text)\n","\n","#         if torch.min(scores)< threshold:\n","\n","#             # new query = high confidence tokens\n","#             confident_tokens = tokens[torch.where(scores>threshold)]\n","#             confident_query = tokenizer.decode(confident_tokens)\n","\n","#             # Gather documents with retriever\n","#             relevant_docs_acquired = knowledge_index.similarity_search(query=confident_query, k=num_retrieved_docs)\n","#             # print(relevant_d|ocs_acquired)\n","#             # print(relevant_docs_acquired)\n","#             if reranker:\n","#                 print(\"=> Reranking documents...\")\n","#                 relevant_docs = [doc.page_content for doc in relevant_docs_acquired]\n","\n","#                 relevant_docs = reranker.rerank(confident_query, relevant_docs, k=num_docs_final)\n","#                 # print(relevant_docs)\n","\n","#                 relevant_docs_content = [doc[\"content\"] for doc in relevant_docs]\n","#                 # relevant_doc_score = [doc[\"score\"] for doc in relevant_docs]\n","\n","#             else:\n","#                 relevant_docs_content = [doc.page_content for doc in relevant_docs_acquired]\n","\n","#             relevant_docs_content = relevant_docs_content[:num_docs_final]\n","#             # relevant_doc_score = relevant_doc_score[:num_docs_final]\n","\n","#             # Build the final prompt\n","#             context = \"\\nExtracted documents:\\n\"\n","#             context += \"\".join([f\"Document {str(i)}:::\\n\" + doc for i, doc in enumerate(relevant_docs_content)])\n","\n","#             # new_input_text = f\"Answer the user's questions based on the below context. Keep your answers short and concise.\\n------------<context>\\n{context}\\n</context>------------\\n<question> Here is the question\\n{input_text}\\n</question>\\n\"\n","\n","#             new_input_text = f\"Keep your answers short and concise. If there are multiple right answers, include them all, but keep it short overall. \\n Given the below context:\\n{context}\\n\\n Answer the following \\n{input_text}\\n\"\n","\n","#             # Redact an answer\n","#             print(\"=> Generating answer...\")\n","#             generated_sequence, seq, _ = get_prediction_and_scores(new_input_text)\n","\n","#             if \"</s>\" in generated_sequence:\n","#                 input_text = tokenizer.decode(seq, skip_special_tokens=True)\n","#                 break\n","\n","#         else: # tokens are already high confidence\n","#             if \"</s>\" in generated_sequence:\n","#                 input_text = tokenizer.decode(tokens, skip_special_tokens=True)\n","#                 break\n","\n","#     # print(relevant_docs_content)\n","#     answer = input_text.replace(\"<pad>\", \"\").replace(\"</s>\", \"\").strip()\n","\n","#     if relevant_docs is None:\n","#         return answer, 'docs not needed'\n","#         # 'id = None', 'index = None'\n","#     return answer, relevant_docs_content"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PL8E1xVJw766"},"outputs":[],"source":["# # user_query = 'What is the Buggy race schedule this year?'\n","# user_query = 'What is the safety gear required by all buggy drivers?'\n","# answer, relevant_docs = flanT5_with_threshold(\n","#     user_query, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER\n","# )"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dko_REQ9xDXF","outputId":"24775901-bc7b-4cc9-c1c2-4d1306bc7e3f"},"outputs":[{"name":"stdout","output_type":"stream","text":["==================================Answer==================================\n","hat and gloves\n"]}],"source":["# print(\"==================================Answer==================================\")\n","# print(answer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-sd3UO1yx4J","outputId":"b2852135-42d4-4de3-ff63-d91f6fd93b26"},"outputs":[{"name":"stdout","output_type":"stream","text":["3\n","This manuscript describes the work that has been completed for domain characterization as an early step toward developing standardized PRO measures to evaluate these important outcomes specific to upper extremity transplantation.\n","\n","## AUTHORNAME\n","\n","Lori S. Levin\n","\n","## JOURNAL\n","\n","{'volume': '13', 'name': 'Frontiers in Psychology'}\n","\n","## FIELDSOFSTUDY\n","\n","['Medicine']\n","\n","## URL\n","\n","https://www.semanticscholar.org/paper/52a97ad16605c18e23c9750a388a26a9cdf12200\n","\n","## YEAR\n","\n","2023\n","\n","## TLDR\n","\n","Qualitative work with experts, clinicians, and patients has identified several domains of QOL that are unique to individuals who have received upper extremity transplants, which are distinct from topics covered by existing PRO measures.\n","\n","## VENUE\n","\n","Frontiers in Psychology\n","\n","on, grip strength, pain, and the various activities of\n","\n","daily living that require manual motor function and dexterity to\n","\n","complete. Thus, we believe that assessment of functional ability is\n","\n","best left to existing measures, such as SCI-FI Fine Motor, SCI-FI\n","\n","Self-Care, PROMIS Upper Extremity ( Jette et al., 2012   Tulsky\n","\n","et al., 2012   Kaat et al., 2019 ), or Neuro-QoL Upper ExtremityFine\n","\n","Motor ( Cella et al., 2012   Gershon et al., 2012 ).\n","\n","In contrast, satisfaction  with hand functioning as experienced\n","\n","by UE transplant recipients appears to merit a new HRQOL content\n","\n","domain, as difficulties and frustrations with the responsivity and\n","\n","ease of movement of the transplanted limbs/hands are distinct for\n","\n","this population, where capabilities improve gradually with\n","\n","treatment and nerve regrowth or sometimes not at all. Likewise,\n","\n","the challenges with sensation and aesthetic satisfaction are also\n","\n","unique to UE transplant. Although there are other clinical groups\n","This manuscript describes the work that has been completed for domain characterization as an early step toward developing standardized PRO measures to evaluate these important outcomes specific to upper extremity transplantation.\n","\n","## AUTHORNAME\n","\n","Lori S. Levin\n","\n","## JOURNAL\n","\n","{'volume': '13', 'name': 'Frontiers in Psychology'}\n","\n","## FIELDSOFSTUDY\n","\n","['Medicine']\n","\n","## URL\n","\n","https://www.semanticscholar.org/paper/52a97ad16605c18e23c9750a388a26a9cdf12200\n","\n","## YEAR\n","\n","2023\n","\n","## TLDR\n","\n","Qualitative work with experts, clinicians, and patients has identified several domains of QOL that are unique to individuals who have received upper extremity transplants, which are distinct from topics covered by existing PRO measures.\n","\n","## VENUE\n","\n","Frontiers in Psychology\n","\n","or tertiary goal. Stakeholders also explained that\n","\n","the aesthetic aspects of the transplants were expected to change\n","\n","over time, as follow-up procedures (e.g., debulking) could be done\n","\n","to improve aesthetics.\n","\n","It has been always important for me to have new real hands\n","\n","and not plastic or silicone hands. UE transplant recipient\n","\n","They try to match on skin color as well as [donor] sex.. but\n","\n","there's often a big size discrepancy in the arms  because what's\n","\n","left of [the recipient s] native arm is often very shrunken and\n","\n","small, and then you're transplanting   a normal size forearm.\n","\n","UE transplant expert stakeholder clinician\n","\n","To assess recipients  satisfaction with the external\n","\n","appearance of the transplant, we designed the Satisfaction with\n","\n","Hand Aesthetics domain. Specific subtopics covered in the\n","\n","stakeholder discussions included skin tone of the transplant,\n","\n","size of the transplant, fingernail appearance, forearm bulk, scar\n","## YEAR\n","\n","2023\n","\n","## TLDR\n","\n","Qualitative work with experts, clinicians, and patients has identified several domains of QOL that are unique to individuals who have received upper extremity transplants, which are distinct from topics covered by existing PRO measures.\n","\n","## VENUE\n","\n","Frontiers in Psychology\n","\n","cially relevant sense and was closely tied to the desire for\n","\n","improved social functioning after the transplant.\n","\n","Sensation though is so important. And I can t reinforce that\n","\n","enough as it relates to relationships with those that you love.\n","\n","Your spouse and your children, especially for those that have\n","\n","young children. Hooks don t have any value with young children,\n","\n","and electric hands don t have value with children. UE\n","\n","transplant expert stakeholder\n","\n","I can feel what I touch, I can feel if it is hot or if it is cold, if it is\n","\n","soft, or if it is itchy or anything, and   that is something that is\n","\n","very important for me, and it goes with the fact that I can like\n","\n","touch somebody. So, for example, my boyfriend, I can  put my\n","\n","hand on him and I can touch him or feel him or touch his hair\n","\n","or things like that  that really matters for me currently. UE\n","\n","In response to these stakeholder comments, two HRQOL\n","\n","content domains were developed on the topic of sensation. First,\n","\n","the Hand Function  Sensation domain was designed to evaluate\n","\n","recipients  ability to perceive a variety of sensations in the\n","\n","transplanted limb/hand. These included, for example, light\n","\n","pressure, touch, textures, temperature, and pain. The second\n","\n","domain developed was Satisfaction with Sensation. This domain\n","\n","was designed to assess recipients  satisfaction with their ability to\n","\n","perceive sensation with the transplant, including social touch.\n","\n","These two domains were conceptualized as discrete because\n","\n","stakeholders acknowledged that recipients  degree of satisfaction\n","\n","may not correlate directly with the amount of sensory function\n","\n","they have in the UE transplant. Stakeholders described how\n"]}],"source":["# # print(\"=========================Relevant Documents===========================\")\n","# print(len(relevant_docs))\n","# for i in relevant_docs:\n","#     print(i)"]},{"cell_type":"markdown","metadata":{"id":"zdx4_Ptp2tKT"},"source":["## Generate Answers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPFTPldT0y05"},"outputs":[],"source":["# def generate_answer(question):\n","#     answer, _ = flanT5_with_threshold(\n","#         question, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER\n","#     )\n","#     return answer\n","\n","# OUTPUT_FILE_THRESHOLD= 'system_outputs/webpages_threshold.txt'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fhMx-4pq2tKT"},"outputs":[],"source":["# def generate_answer(question):\n","#     answer, _ = flanT5_with_threshold(\n","#         question, KNOWLEDGE_VECTOR_DATABASE, reranker=RERANKER\n","#     )\n","#     return answer\n","\n","\n","# generate_answers_all(QUESTIONS_FILE, OUTPUT_FILE_THRESHOLD)\n"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{}}},"nbformat":4,"nbformat_minor":0}
