{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "from typing import Callable\n",
    "\n",
    "import numpy as np\n",
    "import regex\n",
    "import pandas as pd\n",
    "\n",
    "# Normalization and score functions from SQuAD evaluation script https://worksheets.codalab.org/rest/bundles/0x6b567e1cf2e041ec80d7098f031c5c9e/contents/blob/\n",
    "def normalize_answer(s: str) -> str:\n",
    "    def remove_articles(text):\n",
    "        return regex.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
    "\n",
    "    def white_space_fix(text):\n",
    "        return \" \".join(text.split())\n",
    "\n",
    "    def remove_punc(text):\n",
    "        exclude = set(string.punctuation)\n",
    "        return \"\".join(ch for ch in text if ch not in exclude)\n",
    "\n",
    "    def lower(text):\n",
    "        return text.lower()\n",
    "\n",
    "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "\n",
    "def em(prediction, ground_truth, normalize_fn):\n",
    "    return float(normalize_fn(prediction) == normalize_fn(ground_truth))\n",
    "\n",
    "\n",
    "def f1_and_recall(prediction, ground_truth, normalize_fn):\n",
    "    prediction_tokens = normalize_fn(prediction).split()\n",
    "    ground_truth_tokens = normalize_fn(ground_truth).split()\n",
    "    common = Counter(prediction_tokens) & Counter(ground_truth_tokens)\n",
    "    num_same = sum(common.values())\n",
    "\n",
    "    if num_same == 0:\n",
    "        return 0, 0\n",
    "    precision = 1.0 * num_same / len(prediction_tokens)\n",
    "    recall = 1.0 * num_same / len(ground_truth_tokens)\n",
    "    f1 = (2 * precision * recall) / (precision + recall)\n",
    "    return f1, recall\n",
    "\n",
    "# returns the f1 score and recall score for one question/answer\n",
    "def f1_recall_score(prediction, ground_truths, normalize_fn: Callable[[str], str] = lambda x: x):\n",
    "    result = [f1_and_recall(prediction, gt, normalize_fn) for gt in ground_truths]\n",
    "    unzip = list(zip(*result))\n",
    "    return max(unzip[0]), max(unzip[1])\n",
    "\n",
    "\n",
    "def exact_match_score(prediction, ground_truths, normalize_fn: Callable[[str], str] = lambda x: x):\n",
    "    return max([em(prediction, gt, normalize_fn) for gt in ground_truths])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'float' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mat various times\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# exact_score = max([em(pred, g, normalize_answer) for g in gt])\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m exact_score \u001b[38;5;241m=\u001b[39m \u001b[43mem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize_answer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(exact_score)\n",
      "\u001b[0;31mTypeError\u001b[0m: 'float' object is not callable"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# variable_name = f1_recall_score('hello bye garbage', ['hi', 'hello hello', 'hello <> ', 'bye'], normalize_answer)\n",
    "# print(variable_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns exact match probability over all answers, f1 score average, and recall score average over all Q/A pairs\n",
    "# def total_score(predictions_file, ground_truths_file):\n",
    "#     reference_answers = open(ground_truths_file, 'r')\n",
    "#     ref = reference_answers.readlines()\n",
    "\n",
    "#     rag_answers = open(predictions_file, 'r')\n",
    "#     rag = rag_answers.readlines()\n",
    "#     assert(len(rag) == len(ref))\n",
    "\n",
    "#     exact_match_sum = 0.0\n",
    "#     f1_sum = 0.0\n",
    "#     recall_sum = 0.0\n",
    "#     for pred, truth in zip(rag, ref):\n",
    "#         ground_truths = truth.split(';')\n",
    "#         exact_match_sum += exact_match_score(pred, ground_truths, normalize_answer)\n",
    "#         f1, recall = f1_recall_score(pred, ground_truths, normalize_answer)\n",
    "#         f1_sum += f1\n",
    "#         recall_sum += recall\n",
    "\n",
    "#     return exact_match_sum/len(rag), f1_sum/len(rag), recall_sum/len(rag)\n",
    "\n",
    "def total_score(predictions, ground_truths):\n",
    "    assert(len(predictions) == len(ground_truths))\n",
    "\n",
    "    exact_match_sum = 0.0\n",
    "    f1_sum = 0.0\n",
    "    recall_sum = 0.0\n",
    "    for pred, truth in zip(predictions, ground_truths):\n",
    "        ground_truths = truth.split(';')\n",
    "        exact_match_sum += exact_match_score(pred, ground_truths, normalize_answer)\n",
    "        f1, recall = f1_recall_score(pred, ground_truths, normalize_answer)\n",
    "        f1_sum += f1\n",
    "        recall_sum += recall\n",
    "\n",
    "    return exact_match_sum/len(predictions), f1_sum/len(predictions), recall_sum/len(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(total_score('system_outputs/system_output1.txt', 'data/test/reference_answers.txt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/gr/18p82fyj7fvggfzjjlt4_0rc0000gn/T/ipykernel_23503/1814787191.py:37: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_metrics = pd.concat([df_metrics, pd.DataFrame([metrics])], ignore_index=True)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Category</th>\n",
       "      <th>EM</th>\n",
       "      <th>F1</th>\n",
       "      <th>Recall</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>flan-t5-large-no-context</td>\n",
       "      <td>webpages</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.119</td>\n",
       "      <td>0.126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>flan-t5-large-no-context</td>\n",
       "      <td>tabular_webpages</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.021</td>\n",
       "      <td>0.031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>flan-t5-large-no-context</td>\n",
       "      <td>other_pdf</td>\n",
       "      <td>0.043</td>\n",
       "      <td>0.109</td>\n",
       "      <td>0.091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>flan-t5-large-no-context</td>\n",
       "      <td>papers_pdf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>flan-t5-large-no-context</td>\n",
       "      <td>schedule_pdf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.040</td>\n",
       "      <td>0.030</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>flan-t5-large-no-context</td>\n",
       "      <td>jsons</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.015</td>\n",
       "      <td>0.013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>flan-t5-large-no-context</td>\n",
       "      <td>all</td>\n",
       "      <td>0.028</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.059</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>flan-t5-xlarge-no-context</td>\n",
       "      <td>webpages</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.069</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>flan-t5-xlarge-no-context</td>\n",
       "      <td>tabular_webpages</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.060</td>\n",
       "      <td>0.062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>flan-t5-xlarge-no-context</td>\n",
       "      <td>other_pdf</td>\n",
       "      <td>0.130</td>\n",
       "      <td>0.185</td>\n",
       "      <td>0.176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>flan-t5-xlarge-no-context</td>\n",
       "      <td>papers_pdf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.071</td>\n",
       "      <td>0.048</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>flan-t5-xlarge-no-context</td>\n",
       "      <td>schedule_pdf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.068</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>flan-t5-xlarge-no-context</td>\n",
       "      <td>jsons</td>\n",
       "      <td>0.014</td>\n",
       "      <td>0.042</td>\n",
       "      <td>0.044</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>flan-t5-xlarge-no-context</td>\n",
       "      <td>all</td>\n",
       "      <td>0.033</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>llama-no-temp-one-shot-no-context</td>\n",
       "      <td>webpages</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>llama-no-temp-one-shot-no-context</td>\n",
       "      <td>tabular_webpages</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.067</td>\n",
       "      <td>0.246</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>llama-no-temp-one-shot-no-context</td>\n",
       "      <td>other_pdf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.091</td>\n",
       "      <td>0.261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>llama-no-temp-one-shot-no-context</td>\n",
       "      <td>papers_pdf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.016</td>\n",
       "      <td>0.063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>llama-no-temp-one-shot-no-context</td>\n",
       "      <td>schedule_pdf</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>llama-no-temp-one-shot-no-context</td>\n",
       "      <td>jsons</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>llama-no-temp-one-shot-no-context</td>\n",
       "      <td>all</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.057</td>\n",
       "      <td>0.143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                Model          Category     EM     F1  Recall\n",
       "0            flan-t5-large-no-context          webpages  0.074  0.119   0.126\n",
       "1            flan-t5-large-no-context  tabular_webpages  0.000  0.021   0.031\n",
       "2            flan-t5-large-no-context         other_pdf  0.043  0.109   0.091\n",
       "3            flan-t5-large-no-context        papers_pdf  0.000  0.000   0.000\n",
       "4            flan-t5-large-no-context      schedule_pdf  0.000  0.040   0.030\n",
       "5            flan-t5-large-no-context             jsons  0.000  0.015   0.013\n",
       "6            flan-t5-large-no-context               all  0.028  0.059   0.059\n",
       "7           flan-t5-xlarge-no-context          webpages  0.037  0.069   0.074\n",
       "8           flan-t5-xlarge-no-context  tabular_webpages  0.000  0.060   0.062\n",
       "9           flan-t5-xlarge-no-context         other_pdf  0.130  0.185   0.176\n",
       "10          flan-t5-xlarge-no-context        papers_pdf  0.000  0.071   0.048\n",
       "11          flan-t5-xlarge-no-context      schedule_pdf  0.000  0.091   0.068\n",
       "12          flan-t5-xlarge-no-context             jsons  0.014  0.042   0.044\n",
       "13          flan-t5-xlarge-no-context               all  0.033  0.074   0.073\n",
       "14  llama-no-temp-one-shot-no-context          webpages  0.000  0.065   0.180\n",
       "15  llama-no-temp-one-shot-no-context  tabular_webpages  0.000  0.067   0.246\n",
       "16  llama-no-temp-one-shot-no-context         other_pdf  0.000  0.091   0.261\n",
       "17  llama-no-temp-one-shot-no-context        papers_pdf  0.000  0.016   0.063\n",
       "18  llama-no-temp-one-shot-no-context      schedule_pdf  0.000  0.034   0.061\n",
       "19  llama-no-temp-one-shot-no-context             jsons  0.000  0.044   0.074\n",
       "20  llama-no-temp-one-shot-no-context               all  0.000  0.057   0.143"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "paths = [\n",
    "    # 'system_outputs/flan-t5-large-output.csv',\n",
    "    # 'system_outputs/flan-t5-xlarge-output.csv', \n",
    "    # 'system_outputs/llama-no-temp-one-shot-output.csv',\n",
    "    # 'system_outputs/llama-no-temp-output.csv',\n",
    "    # 'system_outputs/Mistral-output.csv',\n",
    "    'system_outputs/flan-t5-large-no-context.csv',\n",
    "    'system_outputs/flan-t5-xlarge-no-context.csv',\n",
    "    'system_outputs/llama-no-temp-one-shot-no-context-output.csv',\n",
    "]\n",
    "\n",
    "df_metrics = pd.DataFrame(columns=['Model', 'Category', 'EM', 'F1', 'Recall'])\n",
    "\n",
    "for path in paths:\n",
    "    df = pd.read_csv(path)\n",
    "    df = df[~(df['Category'] == 'json_hard')]\n",
    "    # display(df)\n",
    "\n",
    "    if path == 'system_outputs/flan-t5-large-no-context.csv':\n",
    "        model = 'flan-t5-large-no-context'\n",
    "    elif path == 'system_outputs/flan-t5-xlarge-no-context.csv':\n",
    "        model = 'flan-t5-xlarge-no-context'\n",
    "    else:\n",
    "        model = path[15:-11]\n",
    "    categories = df['Category'].unique().tolist() + ['all']\n",
    "    for category in categories:\n",
    "        if category == 'all':\n",
    "            cat_df = df\n",
    "        else:\n",
    "            cat_df = df[df['Category'] == category]\n",
    "        em_score, f1, recall = total_score(cat_df['ModelAnswer'], cat_df['Answer'])\n",
    "        em_score = round(em_score, 3)\n",
    "        f1 = round(f1, 3)\n",
    "        recall = round(recall, 3)\n",
    "        metrics = {'Model': model, 'Category': category, 'EM': em_score, \n",
    "                   'F1': f1, 'Recall': recall}\n",
    "        df_metrics = pd.concat([df_metrics, pd.DataFrame([metrics])], ignore_index=True)\n",
    "    \n",
    "display(df_metrics)\n",
    "\n",
    "df_metrics.to_csv('system_outputs/all_metrics.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.547 0.717 0.733\n"
     ]
    }
   ],
   "source": [
    "gold_answer_paths = [\n",
    "    'IAA_files/AQAA.txt',\n",
    "    'IAA_files/EQEA.txt',\n",
    "    'IAA_files/VQVA.txt',\n",
    "]\n",
    "\n",
    "annotator_answer_paths = [\n",
    "    'IAA_files/AQEA.txt',\n",
    "    'IAA_files/EQAA.txt',\n",
    "    'IAA_files/VQAA.txt',\n",
    "]\n",
    "\n",
    "# Concat all gold answers\n",
    "gold_answers = []\n",
    "for path in gold_answer_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        gold_answers += f.readlines()\n",
    "\n",
    "# Concat all annotator answers\n",
    "annotator_answers = []\n",
    "for path in annotator_answer_paths:\n",
    "    with open(path, 'r') as f:\n",
    "        annotator_answers += f.readlines()\n",
    "\n",
    "em_score, f1, recall = total_score(annotator_answers, gold_answers)\n",
    "em_score = round(em_score, 3)\n",
    "f1 = round(f1, 3)\n",
    "recall = round(recall, 3)\n",
    "\n",
    "print(em_score, f1, recall)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
