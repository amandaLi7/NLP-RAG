{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semanticscholar import SemanticScholar\n",
    "import csv\n",
    "import requests\n",
    "import os\n",
    "API_KEY = os.getenv('S2APIKEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "from requests import Session\n",
    "from typing import Generator, Union\n",
    "\n",
    "import urllib3\n",
    "urllib3.disable_warnings()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "output_dir = \"papers\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "download_errors = \"ignore\" \n",
    "\n",
    "extra_fields = [\"keywords\", \"topics\", \"tldr\", \"citations\"]\n",
    "\n",
    "seen_paper_ids = set()\n",
    "\n",
    "authors = [\n",
    "    (\"First Last1\", [\"John Doe\", \"Jane Smith\"]),  # Replace with your list\n",
    "    (\"Second Last2\", [\"Another Author\"]),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_paper(url, filename):\n",
    "    try:\n",
    "        response = requests.get(url, stream=True)\n",
    "        if response.status_code == 200:\n",
    "            with open(os.path.join(output_dir, filename), \"wb\") as f:\n",
    "                for chunk in response.iter_content(chunk_size=1024):\n",
    "                    if chunk:\n",
    "                        f.write(chunk)\n",
    "        else:\n",
    "            print(f\"PDF download failed for {url} ({response.status_code})\")\n",
    "    except Exception as e:\n",
    "        if download_errors == \"raise\":\n",
    "            raise e\n",
    "        elif download_errors == \"warn\":\n",
    "            print(f\"Error downloading PDF: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_paper(author_name, paper_id, paper_details, seen_paper_ids, fields):\n",
    "    \"\"\"Processes a paper and writes data to a CSV file.\n",
    "\n",
    "    Args:\n",
    "        author_name (str): Name of the author.\n",
    "        paper_id (str): ID of the paper.\n",
    "        paper_details (dict): Dictionary containing paper details from the API.\n",
    "        seen_paper_ids (set): Set of processed paper IDs to avoid duplicates.\n",
    "        fields (list): List of fields to extract from the paper details.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "\n",
    "    if paper_id in seen_paper_ids:\n",
    "        return\n",
    "\n",
    "    seen_paper_ids.add(paper_id)\n",
    "\n",
    "    data = {}\n",
    "    for field in fields:\n",
    "        data[field] = paper_details.get(field)\n",
    "\n",
    "    # Extract desired data:\n",
    "    author_full_name = paper_details.get(\"authors\", [{\"name\": author_name}])[0][\"name\"]\n",
    "    title = paper_details.get(\"title\")\n",
    "    publication_date = paper_details.get(\"year\")\n",
    "    url = paper_details.get(\"url\")\n",
    "    abstract = paper_details.get(\"abstract\")\n",
    "    citation_count = paper_details.get(\"citationCount\")\n",
    "    influential_citation_count = paper_details.get(\"influentialCitationCount\")\n",
    "    open_access = paper_details.get(\"isOpenAccess\")\n",
    "    open_access_pdf = paper_details.get(\"openAccessPdf\")\n",
    "    fields_of_study = paper_details.get(\"fieldsOfStudy\")\n",
    "    s2_fields_of_study = paper_details.get(\"s2FieldsOfStudy\")\n",
    "    publication_types = paper_details.get(\"publicationTypes\")\n",
    "    journal = paper_details.get(\"journal\")\n",
    "    tldr = paper_details.get(\"tldr\")\n",
    "\n",
    "    # Prepare data for CSV:\n",
    "    data[\"author_name\"] = author_full_name\n",
    "    data[\"paper_id\"] = paper_id\n",
    "    data[\"title\"] = title\n",
    "    data[\"hindex\"] = data.get(\"hIndex\")  # Assume this comes from author details\n",
    "    data[\"semantic_scholar_url\"] = url\n",
    "    data[\"affiliations\"] = \", \".join([affiliation[\"name\"] for affiliation in data.get(\"affiliations\", [])])\n",
    "    data[\"citation_count\"] = citation_count\n",
    "    data[\"publication_types\"] = \", \".join(publication_types)\n",
    "    data[\"influential_citation_count\"] = influential_citation_count\n",
    "    data[\"publication_venue\"] = journal.get(\"name\")\n",
    "    data[\"citation_count_of_the_paper\"] = citation_count\n",
    "    data[\"field_of_study\"] = \", \".join(fields_of_study)\n",
    "    data[\"s2_fields_of_study\"] = \", \".join(s2_fields_of_study)\n",
    "    data[\"journal\"] = journal.get(\"name\")\n",
    "    data[\"abstract\"] = abstract\n",
    "    data[\"tldr\"] = tldr\n",
    "    data[\"publication_date\"] = publication_date\n",
    "\n",
    "    # Download PDF if open access and not already downloaded:\n",
    "    if open_access and open_access_pdf:\n",
    "        pdf_path = download_paper(paper_id, \"papers\")\n",
    "        if pdf_path:\n",
    "            data[\"pdf_path\"] = pdf_path\n",
    "\n",
    "    # Write data to CSV:\n",
    "    with open(\"papers.csv\", \"a\", newline=\"\") as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=fields + [\"author_name\", \"paper_id\", \"pdf_path\"])\n",
    "        writer.writerow(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_author(author_name, all_authors):\n",
    "    try:\n",
    "        # Use API key from environment variable\n",
    "        sch = SemanticScholar(api_key=API_KEY)\n",
    "\n",
    "        # Find author and retrieve ID\n",
    "        author_profile = sch.search_author(query=author_name)\n",
    "        author_id = author_profile[0].id\n",
    "\n",
    "        # Get author publications\n",
    "        author_papers = sch.get_author_papers(author_id)\n",
    "\n",
    "        for paper in author_papers:\n",
    "            try:\n",
    "                paper_details = sch.paper_by_id(paper['paperId'])\n",
    "\n",
    "                process_paper(author_name, all_authors, paper['paperId'], paper_details)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing paper {paper['paperId']}: {e}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        if download_errors == \"raise\":\n",
    "            raise e\n",
    "        elif download_errors == \"warn\":\n",
    "            print(f\"Error processing author {author_name}: {e}\")\n",
    "        else:\n",
    "            print(f\"Author not found: {author_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "sch = SemanticScholar(api_key=API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out = sch.search_author(query=\"Graham Neubig\")\n",
    "author_id = out[0].authorId\n",
    "url = out[0].url\n",
    "paper_count = out[0].paperCount\n",
    "# write similar code for citationCount, hIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paperId', 'externalIds', 'corpusId', 'publicationVenue', 'url', 'title', 'abstract', 'venue', 'year', 'referenceCount', 'citationCount', 'influentialCitationCount', 'isOpenAccess', 'openAccessPdf', 'fieldsOfStudy', 's2FieldsOfStudy', 'publicationTypes', 'publicationDate', 'journal', 'authors'])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out[0].papers[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Author' object has no attribute 'tldr'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[96], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtldr\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Author' object has no attribute 'tldr'"
     ]
    }
   ],
   "source": [
    "out[0].tldr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Response [200]>"
      ]
     },
     "execution_count": 190,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_name = \"Graham Neubig\"\n",
    "search_response = requests.get(\n",
    "        'https://api.semanticscholar.org/graph/v1/author/search',\n",
    "        headers=headers,\n",
    "        params={'query': author_name}\n",
    "    )\n",
    "search_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paperId', 'externalIds', 'corpusId', 'publicationVenue', 'url', 'title', 'abstract', 'venue', 'year', 'referenceCount', 'citationCount', 'influentialCitationCount', 'isOpenAccess', 'openAccessPdf', 'fieldsOfStudy', 's2FieldsOfStudy', 'publicationTypes', 'publicationDate', 'journal', 'authors'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out[0].papers[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'sb'"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 's'\n",
    "a = a+'b'\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved JSON response to author_1700325_data.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "# Fetch API key from environment variables\n",
    "api_key = API_KEY\n",
    "if not api_key:\n",
    "    raise EnvironmentError(\"S2_API_KEY environment variable not set.\")\n",
    "\n",
    "# Set up the headers with the API key\n",
    "headers = {\n",
    "    \"x-api-key\": api_key\n",
    "}\n",
    "\n",
    "# Specify the author_id and the fields you want to fetch\n",
    "author_id = \"1700325\"\n",
    "fields = \"authorId,name,url,hIndex,affiliations,paperCount,citationCount\"\n",
    "fields +=\",papers.paperId,papers.title,papers.year,papers.url,papers.abstract,papers.authors\"\n",
    "fields += \",papers.externalIds,papers.isOpenAccess,papers.openAccessPdf,papers.fieldsOfStudy\"\n",
    "fields += \",papers.influentialCitationCount,papers.journal\"\n",
    "# Make the GET request to fetch author details with the specified fields\n",
    "response = requests.get(\n",
    "    f'https://api.semanticscholar.org/graph/v1/author/{author_id}',\n",
    "    headers=headers,\n",
    "    params={'fields': fields}\n",
    ")\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Create the directory for raw output if it doesn't exist\n",
    "    os.makedirs('raw_output_semantic', exist_ok=True)\n",
    "    \n",
    "    # Define the path for the JSON file\n",
    "    json_file_path = os.path.join(f'author_{author_id}_data.json')\n",
    "    \n",
    "    # Save the JSON response\n",
    "    with open(json_file_path, 'w') as json_file:\n",
    "        json.dump(response.json(), json_file, indent=2)\n",
    "        print(f\"Saved JSON response to {json_file_path}\")\n",
    "    \n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    if response.text:\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "    else:\n",
    "        print(\"No additional error information is provided.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to data/paper/processed_output/filtered_output_Graham Neubig_1700325.json\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "import os\n",
    "\n",
    "def fetch_and_process_author_papers(author_name, year, save_raw=False):\n",
    "    if not API_KEY:\n",
    "        raise EnvironmentError(\"S2_API_KEY environment variable not set.\")\n",
    "\n",
    "    # Set up the headers with the API key\n",
    "    headers = {\n",
    "        \"x-api-key\": API_KEY\n",
    "    }\n",
    "\n",
    "    # First, find the author ID using the search endpoint\n",
    "    search_response = requests.get(\n",
    "        'https://api.semanticscholar.org/graph/v1/author/search',\n",
    "        headers=headers,\n",
    "        params={'query': author_name}\n",
    "    )\n",
    "    if search_response.status_code != 200:\n",
    "        raise Exception(f\"Error finding author ID: {search_response.status_code}\")\n",
    "\n",
    "    author_id = search_response.json()['data'][0]['authorId']\n",
    "\n",
    "    # Specify the fields you want to fetch\n",
    "    fields = \"authorId,name,url,hIndex,affiliations,paperCount,citationCount\"\n",
    "    fields += \",papers.paperId,papers.title,papers.year,papers.url,papers.abstract,papers.authors\"\n",
    "    fields += \",papers.externalIds,papers.isOpenAccess,papers.openAccessPdf,papers.fieldsOfStudy\"\n",
    "    fields += \",papers.influentialCitationCount,papers.journal\"\n",
    "\n",
    "    # Make the GET request to fetch author details with the specified fields\n",
    "    response = requests.get(\n",
    "        f'https://api.semanticscholar.org/graph/v1/author/{author_id}',\n",
    "        headers=headers,\n",
    "        params={'fields': fields}\n",
    "    )\n",
    "\n",
    "    # Check if the request was successful\n",
    "    if response.status_code == 200:\n",
    "        author_data = response.json()\n",
    "\n",
    "        # Define paths for the raw and processed data\n",
    "        raw_data_dir = 'data/paper/raw_output'\n",
    "        processed_data_dir = 'data/paper/processed_output'\n",
    "        raw_file_name = f'raw_output_{author_name}_{author_id}.json'\n",
    "        processed_file_name = f'filtered_output_{author_name}_{author_id}.json'\n",
    "\n",
    "        # Optionally save the raw output\n",
    "        if save_raw:\n",
    "            os.makedirs(raw_data_dir, exist_ok=True)\n",
    "            raw_json_file_path = os.path.join(raw_data_dir, raw_file_name)\n",
    "            with open(raw_json_file_path, 'w') as raw_file:\n",
    "                json.dump(author_data, raw_file, indent=2)\n",
    "                print(f\"Saved raw data to {raw_json_file_path}\")\n",
    "\n",
    "        # Process and filter the data\n",
    "        filtered_papers = [\n",
    "            paper for paper in author_data.get('papers', [])\n",
    "            if str(paper.get('year')) == str(year) and paper.get('isOpenAccess')\n",
    "        ]\n",
    "\n",
    "        # Save the processed data\n",
    "        os.makedirs(processed_data_dir, exist_ok=True)\n",
    "        processed_json_file_path = os.path.join(processed_data_dir, processed_file_name)\n",
    "        with open(processed_json_file_path, 'w') as processed_file:\n",
    "            json.dump(filtered_papers, processed_file, indent=2)\n",
    "            print(f\"Saved processed data to {processed_json_file_path}\")\n",
    "    else:\n",
    "        print(f\"Error fetching author data: {response.status_code}\")\n",
    "        if response.text:\n",
    "            print(json.dumps(response.json(), indent=2))\n",
    "        else:\n",
    "            print(\"No additional error information is provided.\")\n",
    "\n",
    "# Usage\n",
    "# Replace 'Graham Neubig' with the actual author's name and '2023' with the target year\n",
    "fetch_and_process_author_papers('Graham Neubig', 2023, save_raw=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved processed data to processed_output_semantic/author_1700325_processed_data.json\n"
     ]
    }
   ],
   "source": [
    "# Now, let's process the saved JSON data\n",
    "def process_saved_data(json_file_path):\n",
    "    # Create the directory for processed output if it doesn't exist\n",
    "    processed_output_dir = 'processed_output_semantic'\n",
    "    os.makedirs(processed_output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load the JSON data for processing\n",
    "    with open(json_file_path, 'r') as json_file:\n",
    "        author_data = json.load(json_file)\n",
    "        \n",
    "        # Filter for open access papers from the year 2023\n",
    "        filtered_papers = [\n",
    "            paper for paper in author_data.get('papers', [])\n",
    "            if (paper.get('year') == \"2023\")is True\n",
    "        ]\n",
    "        \n",
    "        # Define the path for the processed JSON file\n",
    "        processed_json_file_path = os.path.join(processed_output_dir, f'author_{author_id}_processed_data.json')\n",
    "        \n",
    "        # Save the filtered papers\n",
    "        with open(processed_json_file_path, 'w') as processed_json_file:\n",
    "            json.dump(filtered_papers, processed_json_file, indent=2)\n",
    "            print(f\"Saved processed data to {processed_json_file_path}\")\n",
    "\n",
    "# Call the processing function if the raw data was successfully saved\n",
    "if response.status_code == 200:\n",
    "    process_saved_data(json_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(json_file_path, 'r') as json_file:\n",
    "    author_data = json.load(json_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "author_data.get('papers')[0].get('isOpenAccess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_papers = [\n",
    "    paper for paper in author_data.get('papers', [])\n",
    "    if str(paper.get('year')) == \"2023\" and paper.get('isOpenAccess')\n",
    "]\n",
    "len(filtered_papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-Modal Fine-Tuning: Align then Refine\n",
      "ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages\n",
      "GlobalBench: A Benchmark for Global Progress in Natural Language Processing\n",
      "Learning Performance-Improving Code Edits\n",
      "CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code\n",
      "Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction\n",
      "User-Centric Evaluation of OCR Systems for Kwak’wala\n",
      "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning\n",
      "A Gold Standard Dataset for the Reviewer Assignment Problem\n",
      "SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing\n",
      "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\n",
      "FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios\n",
      "Active Retrieval Augmented Generation\n",
      "Large Language Models Enable Few-Shot Clustering\n",
      "Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach\n",
      "Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting\n",
      "Why do Nearest Neighbor Language Models Work?\n",
      "Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity\n",
      "DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions\n",
      "Multi-lingual and Multi-cultural Figurative Language Understanding\n",
      "It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk\n",
      "Unlimiformer: Long-Range Transformers with Unlimited Length Input\n",
      "WebArena: A Realistic Web Environment for Building Autonomous Agents\n",
      "Prompt2Model: Generating Deployable Models from Natural Language Instructions\n",
      "Computational Language Acquisition with Theory of Mind\n",
      "Improving Factuality of Abstractive Summarization via Contrastive Reward Learning\n",
      "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation\n"
     ]
    }
   ],
   "source": [
    "for paper in author_data.get('papers'):\n",
    "    if(paper.get('year')) ==2023 and paper.get('isOpenAccess') == True:\n",
    "        print(paper.get('title'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved raw data to raw_output_semantic/author_1700325_data.json\n",
      "Saved processed data to processed_output_semantic/author_1700325_processed_data.json\n",
      "Number of filtered papers: 27\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "# Assuming `response` is the response object from the requests call\n",
    "if response.status_code == 200:\n",
    "    author_data = response.json()\n",
    "\n",
    "    # Directory where the raw and processed data will be saved\n",
    "    raw_data_dir = 'raw_output_semantic'\n",
    "    processed_data_dir = 'processed_output_semantic'\n",
    "\n",
    "    # Create directories if they don't exist\n",
    "    os.makedirs(raw_data_dir, exist_ok=True)\n",
    "    os.makedirs(processed_data_dir, exist_ok=True)\n",
    "\n",
    "    # Define the path for the raw JSON file and save the raw response\n",
    "    raw_json_file_path = os.path.join(raw_data_dir, f'author_{author_id}_data.json')\n",
    "    with open(raw_json_file_path, 'w') as raw_file:\n",
    "        json.dump(author_data, raw_file, indent=2)\n",
    "        print(f\"Saved raw data to {raw_json_file_path}\")\n",
    "    \n",
    "    # Now let's filter the papers\n",
    "    filtered_papers = [\n",
    "        paper for paper in author_data.get('papers', [])\n",
    "        if str(paper.get('year')) == \"2023\" and paper.get('isOpenAccess')\n",
    "    ]\n",
    "\n",
    "    # Define the path for the processed JSON file and save the processed data\n",
    "    processed_json_file_path = os.path.join(processed_data_dir, f'author_{author_id}_processed_data.json')\n",
    "    with open(processed_json_file_path, 'w') as processed_file:\n",
    "        json.dump(filtered_papers, processed_file, indent=2)\n",
    "        print(f\"Saved processed data to {processed_json_file_path}\")\n",
    "\n",
    "    # Print the number of filtered papers for verification\n",
    "    print(f\"Number of filtered papers: {len(filtered_papers)}\")\n",
    "else:\n",
    "    print(f\"Error: {response.status_code}\")\n",
    "    if response.text:\n",
    "        print(json.dumps(response.json(), indent=2))\n",
    "    else:\n",
    "        print(\"No additional error information is provided.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['paperId', 'externalIds', 'corpusId', 'publicationVenue', 'url', 'title', 'abstract', 'venue', 'year', 'referenceCount', 'citationCount', 'influentialCitationCount', 'isOpenAccess', 'openAccessPdf', 'fieldsOfStudy', 's2FieldsOfStudy', 'publicationTypes', 'publicationDate', 'journal', 'authors'])"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "out[0].papers[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DBLP': ['Graham Neubig']}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "key = x[1]\n",
    "value = out[0].__getattribute__(key)\n",
    "value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'dict_keys' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlist\u001b[39m(out[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())[i])\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;28mprint\u001b[39m( out[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mlist(out[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys())[i]\u001b[38;5;241m.\u001b[39mkeys() )\n",
      "\u001b[0;31mTypeError\u001b[0m: 'dict_keys' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "for i in raout[0].keys():\n",
    "    print(list(out[0].keys())[i])\n",
    "    print( out[0].list(out[0].keys())[i].keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1232796"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file = open(\"log.txt\", \"w\")\n",
    "file.write(str(out[0]) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# out = sch.search_author(query=\"Graham Neubig\")\n",
    "# then out[0] is a dict with keys dict_keys(['authorId', 'externalIds', 'url', 'name', 'affiliations', 'homepage', 'paperCount', 'citationCount', 'hIndex', 'papers'])\n",
    "#out[0].papers[0] has dict_keys(['paperId', 'externalIds', 'corpusId', 'publicationVenue', 'url', 'title', 'abstract', 'venue', 'year', 'referenceCount', 'citationCount', 'influentialCitationCount', 'isOpenAccess', 'openAccessPdf', 'fieldsOfStudy', 's2FieldsOfStudy', 'publicationTypes', 'publicationDate', 'journal', 'authors'])\n",
    "\n",
    "# out[0].papers[0].externalIds.keys() has dict_keys(['ArXiv', 'DBLP', 'DOI', 'CorpusId'])\n",
    "# out[0].papers[0].publicationVenue.keys()dict_keys(['id', 'name', 'alternate_names', 'issn', 'url'])\n",
    "# out[0].papers[0].journal.keys() dict_keys(['volume', 'name'])\n",
    "# out[0].papers[0].authors[0] is a list of dicts \n",
    "# [{'authorId': '2279677197', 'name': 'Abhika Mishra'}, {'authorId': '35584853', 'name': 'Akari Asai'}, {'authorId': '143820870', 'name': 'Vidhisha Balachandran'}]\n",
    "# if AttributeError for any of the out.atrributes then just skip that in adding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "year\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llama_hw",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
