{"cells":[{"cell_type":"markdown","metadata":{"id":"jvckNkk2H-Uf"},"source":[]},{"cell_type":"code","source":["!pip freeze"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RJN0zbCuCSn3","executionInfo":{"status":"ok","timestamp":1710018349156,"user_tz":300,"elapsed":3091,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}},"outputId":"e87e38f6-ac9a-42c9-d612-3b54fc62c328"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["absl-py==1.4.0\n","aiohttp==3.9.3\n","aiosignal==1.3.1\n","alabaster==0.7.16\n","albumentations==1.3.1\n","altair==4.2.2\n","annotated-types==0.6.0\n","anyio==3.7.1\n","appdirs==1.4.4\n","argon2-cffi==23.1.0\n","argon2-cffi-bindings==21.2.0\n","array-record==0.5.0\n","arviz==0.15.1\n","astropy==5.3.4\n","astunparse==1.6.3\n","async-timeout==4.0.3\n","atpublic==4.0\n","attrs==23.2.0\n","audioread==3.0.1\n","autograd==1.6.2\n","Babel==2.14.0\n","backcall==0.2.0\n","beautifulsoup4==4.12.3\n","bidict==0.23.1\n","bigframes==0.22.0\n","bleach==6.1.0\n","blinker==1.4\n","blis==0.7.11\n","blosc2==2.0.0\n","bokeh==3.3.4\n","bqplot==0.12.43\n","branca==0.7.1\n","build==1.1.1\n","CacheControl==0.14.0\n","cachetools==5.3.3\n","catalogue==2.0.10\n","certifi==2024.2.2\n","cffi==1.16.0\n","chardet==5.2.0\n","charset-normalizer==3.3.2\n","chex==0.1.85\n","click==8.1.7\n","click-plugins==1.1.1\n","cligj==0.7.2\n","cloudpathlib==0.16.0\n","cloudpickle==2.2.1\n","cmake==3.27.9\n","cmdstanpy==1.2.1\n","colorcet==3.1.0\n","colorlover==0.3.0\n","colour==0.1.5\n","community==1.0.0b1\n","confection==0.1.4\n","cons==0.4.6\n","contextlib2==21.6.0\n","contourpy==1.2.0\n","cryptography==42.0.5\n","cufflinks==0.17.3\n","cupy-cuda12x==12.2.0\n","cvxopt==1.3.2\n","cvxpy==1.3.3\n","cycler==0.12.1\n","cymem==2.0.8\n","Cython==3.0.9\n","dask==2023.8.1\n","datascience==0.17.6\n","db-dtypes==1.2.0\n","dbus-python==1.2.18\n","debugpy==1.6.6\n","decorator==4.4.2\n","defusedxml==0.7.1\n","distributed==2023.8.1\n","distro==1.7.0\n","dlib==19.24.2\n","dm-tree==0.1.8\n","docutils==0.18.1\n","dopamine-rl==4.0.6\n","duckdb==0.9.2\n","earthengine-api==0.1.392\n","easydict==1.13\n","ecos==2.0.13\n","editdistance==0.6.2\n","eerepr==0.0.4\n","en-core-web-sm @ https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl#sha256=86cc141f63942d4b2c5fcee06630fd6f904788d2f0ab005cce45aadb8fb73889\n","entrypoints==0.4\n","et-xmlfile==1.1.0\n","etils==1.7.0\n","etuples==0.3.9\n","exceptiongroup==1.2.0\n","fastai==2.7.14\n","fastcore==1.5.29\n","fastdownload==0.0.7\n","fastjsonschema==2.19.1\n","fastprogress==1.0.3\n","fastrlock==0.8.2\n","filelock==3.13.1\n","fiona==1.9.5\n","firebase-admin==5.3.0\n","Flask==2.2.5\n","flatbuffers==23.5.26\n","flax==0.8.1\n","folium==0.14.0\n","fonttools==4.49.0\n","frozendict==2.4.0\n","frozenlist==1.4.1\n","fsspec==2023.6.0\n","future==0.18.3\n","gast==0.5.4\n","gcsfs==2023.6.0\n","GDAL==3.6.4\n","gdown==4.7.3\n","geemap==0.32.0\n","gensim==4.3.2\n","geocoder==1.38.1\n","geographiclib==2.0\n","geopandas==0.13.2\n","geopy==2.3.0\n","gin-config==0.5.0\n","glob2==0.7\n","google==2.0.3\n","google-ai-generativelanguage==0.4.0\n","google-api-core==2.11.1\n","google-api-python-client==2.84.0\n","google-auth==2.27.0\n","google-auth-httplib2==0.1.1\n","google-auth-oauthlib==1.2.0\n","google-cloud-aiplatform==1.43.0\n","google-cloud-bigquery==3.12.0\n","google-cloud-bigquery-connection==1.12.1\n","google-cloud-bigquery-storage==2.24.0\n","google-cloud-core==2.3.3\n","google-cloud-datastore==2.15.2\n","google-cloud-firestore==2.11.1\n","google-cloud-functions==1.13.3\n","google-cloud-iam==2.14.3\n","google-cloud-language==2.13.3\n","google-cloud-resource-manager==1.12.3\n","google-cloud-storage==2.8.0\n","google-cloud-translate==3.11.3\n","google-colab @ file:///colabtools/dist/google-colab-1.0.0.tar.gz#sha256=e916d4e7c3ba6158df864a2e03852211d8fab20abb3db5205b865eedf4be9799\n","google-crc32c==1.5.0\n","google-generativeai==0.3.2\n","google-pasta==0.2.0\n","google-resumable-media==2.7.0\n","googleapis-common-protos==1.62.0\n","googledrivedownloader==0.4\n","graphviz==0.20.1\n","greenlet==3.0.3\n","grpc-google-iam-v1==0.13.0\n","grpcio==1.62.0\n","grpcio-status==1.48.2\n","gspread==3.4.2\n","gspread-dataframe==3.3.1\n","gym==0.25.2\n","gym-notices==0.0.8\n","h5netcdf==1.3.0\n","h5py==3.9.0\n","holidays==0.44\n","holoviews==1.17.1\n","html5lib==1.1\n","httpimport==1.3.1\n","httplib2==0.22.0\n","huggingface-hub==0.20.3\n","humanize==4.7.0\n","hyperopt==0.2.7\n","ibis-framework==7.1.0\n","idna==3.6\n","imageio==2.31.6\n","imageio-ffmpeg==0.4.9\n","imagesize==1.4.1\n","imbalanced-learn==0.10.1\n","imgaug==0.4.0\n","importlib-metadata==7.0.1\n","importlib_resources==6.1.2\n","imutils==0.5.4\n","inflect==7.0.0\n","iniconfig==2.0.0\n","intel-openmp==2023.2.3\n","ipyevents==2.0.2\n","ipyfilechooser==0.6.0\n","ipykernel==5.5.6\n","ipyleaflet==0.18.2\n","ipython==7.34.0\n","ipython-genutils==0.2.0\n","ipython-sql==0.5.0\n","ipytree==0.2.2\n","ipywidgets==7.7.1\n","itsdangerous==2.1.2\n","jax==0.4.23\n","jaxlib @ https://storage.googleapis.com/jax-releases/cuda12/jaxlib-0.4.23+cuda12.cudnn89-cp310-cp310-manylinux2014_x86_64.whl#sha256=8e42000672599e7ec0ea7f551acfcc95dcdd0e22b05a1d1f12f97b56a9fce4a8\n","jeepney==0.7.1\n","jieba==0.42.1\n","Jinja2==3.1.3\n","joblib==1.3.2\n","jsonpickle==3.0.3\n","jsonschema==4.19.2\n","jsonschema-specifications==2023.12.1\n","jupyter-client==6.1.12\n","jupyter-console==6.1.0\n","jupyter-server==1.24.0\n","jupyter_core==5.7.1\n","jupyterlab_pygments==0.3.0\n","jupyterlab_widgets==3.0.10\n","kaggle==1.5.16\n","kagglehub==0.2.0\n","keras==2.15.0\n","keyring==23.5.0\n","kiwisolver==1.4.5\n","langcodes==3.3.0\n","launchpadlib==1.10.16\n","lazr.restfulclient==0.14.4\n","lazr.uri==1.0.6\n","lazy_loader==0.3\n","libclang==16.0.6\n","librosa==0.10.1\n","lightgbm==4.1.0\n","linkify-it-py==2.0.3\n","llvmlite==0.41.1\n","locket==1.0.0\n","logical-unification==0.4.6\n","lxml==4.9.4\n","malloy==2023.1067\n","Markdown==3.5.2\n","markdown-it-py==3.0.0\n","MarkupSafe==2.1.5\n","matplotlib==3.7.1\n","matplotlib-inline==0.1.6\n","matplotlib-venn==0.11.10\n","mdit-py-plugins==0.4.0\n","mdurl==0.1.2\n","miniKanren==1.0.3\n","missingno==0.5.2\n","mistune==0.8.4\n","mizani==0.9.3\n","mkl==2023.2.0\n","ml-dtypes==0.2.0\n","mlxtend==0.22.0\n","more-itertools==10.1.0\n","moviepy==1.0.3\n","mpmath==1.3.0\n","msgpack==1.0.8\n","multidict==6.0.5\n","multipledispatch==1.0.0\n","multitasking==0.0.11\n","murmurhash==1.0.10\n","music21==9.1.0\n","natsort==8.4.0\n","nbclassic==1.0.0\n","nbclient==0.9.0\n","nbconvert==6.5.4\n","nbformat==5.9.2\n","nest-asyncio==1.6.0\n","networkx==3.2.1\n","nibabel==4.0.2\n","nltk==3.8.1\n","notebook==6.5.5\n","notebook_shim==0.2.4\n","numba==0.58.1\n","numexpr==2.9.0\n","numpy==1.25.2\n","oauth2client==4.1.3\n","oauthlib==3.2.2\n","opencv-contrib-python==4.8.0.76\n","opencv-python==4.8.0.76\n","opencv-python-headless==4.9.0.80\n","openpyxl==3.1.2\n","opt-einsum==3.3.0\n","optax==0.1.9\n","orbax-checkpoint==0.4.4\n","osqp==0.6.2.post8\n","packaging==23.2\n","pandas==1.5.3\n","pandas-datareader==0.10.0\n","pandas-gbq==0.19.2\n","pandas-stubs==1.5.3.230304\n","pandocfilters==1.5.1\n","panel==1.3.8\n","param==2.0.2\n","parso==0.8.3\n","parsy==2.1\n","partd==1.4.1\n","pathlib==1.0.1\n","patsy==0.5.6\n","peewee==3.17.1\n","pexpect==4.9.0\n","pickleshare==0.7.5\n","Pillow==9.4.0\n","pins==0.8.4\n","pip-tools==6.13.0\n","platformdirs==4.2.0\n","plotly==5.15.0\n","plotnine==0.12.4\n","pluggy==1.4.0\n","polars==0.20.2\n","pooch==1.8.1\n","portpicker==1.5.2\n","prefetch-generator==1.0.3\n","preshed==3.0.9\n","prettytable==3.10.0\n","proglog==0.1.10\n","progressbar2==4.2.0\n","prometheus_client==0.20.0\n","promise==2.3\n","prompt-toolkit==3.0.43\n","prophet==1.1.5\n","proto-plus==1.23.0\n","protobuf==3.20.3\n","psutil==5.9.5\n","psycopg2==2.9.9\n","ptyprocess==0.7.0\n","py-cpuinfo==9.0.0\n","py4j==0.10.9.7\n","pyarrow==14.0.2\n","pyarrow-hotfix==0.6\n","pyasn1==0.5.1\n","pyasn1-modules==0.3.0\n","pycocotools==2.0.7\n","pycparser==2.21\n","pydantic==2.6.3\n","pydantic_core==2.16.3\n","pydata-google-auth==1.8.2\n","pydot==1.4.2\n","pydot-ng==2.0.0\n","pydotplus==2.0.2\n","PyDrive==1.3.1\n","PyDrive2==1.6.3\n","pyerfa==2.0.1.1\n","pygame==2.5.2\n","Pygments==2.16.1\n","PyGObject==3.42.1\n","PyJWT==2.3.0\n","pymc==5.10.4\n","pymystem3==0.2.0\n","PyOpenGL==3.1.7\n","pyOpenSSL==24.0.0\n","pyparsing==3.1.1\n","pyperclip==1.8.2\n","pyproj==3.6.1\n","pyproject_hooks==1.0.0\n","pyshp==2.3.1\n","PySocks==1.7.1\n","pytensor==2.18.6\n","pytest==7.4.4\n","python-apt @ file:///backend-container/containers/python_apt-0.0.0-cp310-cp310-linux_x86_64.whl#sha256=b209c7165d6061963abe611492f8c91c3bcef4b7a6600f966bab58900c63fefa\n","python-box==7.1.1\n","python-dateutil==2.8.2\n","python-louvain==0.16\n","python-slugify==8.0.4\n","python-utils==3.8.2\n","pytz==2023.4\n","pyviz_comms==3.0.1\n","PyWavelets==1.5.0\n","PyYAML==6.0.1\n","pyzmq==23.2.1\n","qdldl==0.1.7.post0\n","qudida==0.0.4\n","ratelim==0.1.6\n","referencing==0.33.0\n","regex==2023.12.25\n","requests==2.31.0\n","requests-oauthlib==1.3.1\n","requirements-parser==0.5.0\n","rich==13.7.1\n","rpds-py==0.18.0\n","rpy2==3.4.2\n","rsa==4.9\n","safetensors==0.4.2\n","scikit-image==0.19.3\n","scikit-learn==1.2.2\n","scipy==1.11.4\n","scooby==0.9.2\n","scs==3.2.4.post1\n","seaborn==0.13.1\n","SecretStorage==3.3.1\n","Send2Trash==1.8.2\n","sentencepiece==0.1.99\n","shapely==2.0.3\n","six==1.16.0\n","sklearn-pandas==2.2.0\n","smart-open==6.4.0\n","sniffio==1.3.1\n","snowballstemmer==2.2.0\n","sortedcontainers==2.4.0\n","soundfile==0.12.1\n","soupsieve==2.5\n","soxr==0.3.7\n","spacy==3.7.4\n","spacy-legacy==3.0.12\n","spacy-loggers==1.0.5\n","Sphinx==5.0.2\n","sphinxcontrib-applehelp==1.0.8\n","sphinxcontrib-devhelp==1.0.6\n","sphinxcontrib-htmlhelp==2.0.5\n","sphinxcontrib-jsmath==1.0.1\n","sphinxcontrib-qthelp==1.0.7\n","sphinxcontrib-serializinghtml==1.1.10\n","SQLAlchemy==2.0.28\n","sqlglot==19.9.0\n","sqlparse==0.4.4\n","srsly==2.4.8\n","stanio==0.3.0\n","statsmodels==0.14.1\n","sympy==1.12\n","tables==3.8.0\n","tabulate==0.9.0\n","tbb==2021.11.0\n","tblib==3.0.0\n","tenacity==8.2.3\n","tensorboard==2.15.2\n","tensorboard-data-server==0.7.2\n","tensorflow==2.15.0\n","tensorflow-datasets==4.9.4\n","tensorflow-estimator==2.15.0\n","tensorflow-gcs-config==2.15.0\n","tensorflow-hub==0.16.1\n","tensorflow-io-gcs-filesystem==0.36.0\n","tensorflow-metadata==1.14.0\n","tensorflow-probability==0.23.0\n","tensorstore==0.1.45\n","termcolor==2.4.0\n","terminado==0.18.0\n","text-unidecode==1.3\n","textblob==0.17.1\n","tf-keras==2.15.0\n","tf-slim==1.1.0\n","thinc==8.2.3\n","threadpoolctl==3.3.0\n","tifffile==2024.2.12\n","tinycss2==1.2.1\n","tokenizers==0.15.2\n","toml==0.10.2\n","tomli==2.0.1\n","toolz==0.12.1\n","torch @ https://download.pytorch.org/whl/cu121/torch-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=0d4e8c52a1fcf5ed6cfc256d9a370fcf4360958fc79d0b08a51d55e70914df46\n","torchaudio @ https://download.pytorch.org/whl/cu121/torchaudio-2.1.0%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=676bda4042734eda99bc59b2d7f761f345d3cde0cad492ad34e3aefde688c6d8\n","torchdata==0.7.0\n","torchsummary==1.5.1\n","torchtext==0.16.0\n","torchvision @ https://download.pytorch.org/whl/cu121/torchvision-0.16.0%2Bcu121-cp310-cp310-linux_x86_64.whl#sha256=e76e78d0ad43636c9884b3084ffaea8a8b61f21129fbfa456a5fe734f0affea9\n","tornado==6.3.3\n","tqdm==4.66.2\n","traitlets==5.7.1\n","traittypes==0.2.1\n","transformers==4.38.2\n","triton==2.1.0\n","tweepy==4.14.0\n","typer==0.9.0\n","types-pytz==2024.1.0.20240203\n","types-setuptools==69.1.0.20240302\n","typing_extensions==4.10.0\n","tzlocal==5.2\n","uc-micro-py==1.0.3\n","uritemplate==4.1.1\n","urllib3==2.0.7\n","vega-datasets==0.9.0\n","wadllib==1.3.6\n","wasabi==1.1.2\n","wcwidth==0.2.13\n","weasel==0.3.4\n","webcolors==1.13\n","webencodings==0.5.1\n","websocket-client==1.7.0\n","Werkzeug==3.0.1\n","widgetsnbextension==3.6.6\n","wordcloud==1.9.3\n","wrapt==1.14.1\n","xarray==2023.7.0\n","xarray-einstats==0.7.0\n","xgboost==2.0.3\n","xlrd==2.0.1\n","xxhash==3.4.1\n","xyzservices==2023.10.1\n","yarl==1.9.4\n","yellowbrick==1.5\n","yfinance==0.2.37\n","zict==3.0.0\n","zipp==3.17.0\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":847,"status":"ok","timestamp":1710019171795,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"_hOLtFMk5Da8","outputId":"b922f878-c072-4c47-d73f-3ae1999d61f3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":210,"status":"ok","timestamp":1710019172004,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"mY6P-Gdd5E7A","outputId":"a02309d1-2add-4b9c-fe6c-1f7e0be2c17a"},"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ANLP/NLP-RAG/src-rag\n","database.py    FlanT5.ipynb\t\t LlamaCcp.ipynb\t\t      __pycache__\n","evaluation.py  json_csv_embedding.ipynb  Llama_qa_gen_pipeline.ipynb  setup.txt\n"]}],"source":["%cd drive/MyDrive/ANLP/NLP-RAG/src-rag\n","!ls"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s96leWz22vbg"},"outputs":[],"source":["import os\n","# from dotenv import load_dotenv\n","# load_dotenv('.env')\n","# hf_api = os.getenv('HF_API')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18031,"status":"ok","timestamp":1709884858907,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"s-iSASxn3CUu","outputId":"6c5d8662-e6ed-45db-fcea-54ea512ec45d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers==4.38.0\n","  Downloading transformers-4.38.0-py3-none-any.whl (8.5 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.5/8.5 MB\u001b[0m \u001b[31m31.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0) (1.25.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0) (2023.12.25)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers==4.38.0) (4.66.2)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.0) (4.10.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers==4.38.0) (2024.2.2)\n","Installing collected packages: transformers\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.38.2\n","    Uninstalling transformers-4.38.2:\n","      Successfully uninstalled transformers-4.38.2\n","Successfully installed transformers-4.38.0\n"]}],"source":["pip install transformers==4.38.0"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":27114,"status":"ok","timestamp":1710018637738,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"PzQ5HViT2-PN","outputId":"6168f0e1-cf09-4cdf-c9db-1a59707f48ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m102.2/102.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m807.5/807.5 kB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.5/156.5 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.5/85.5 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m256.9/256.9 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.6/66.6 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h"]}],"source":["!pip install -q torch accelerate bitsandbytes langchain sentence-transformers faiss-gpu openpyxl"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mH-fC5LR3N4J"},"outputs":[],"source":["# !pip install unstructured ragatouille\n","# # reranker\n","# from ragatouille import RAGPretrainedModel"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":270208,"status":"ok","timestamp":1709885221775,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"-hsCAcjxIP19","outputId":"21f028bd-2730-47a7-b466-65ee2c58c292"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.2.55.tar.gz (36.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m167.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Collecting typing-extensions>=4.5.0 (from llama-cpp-python)\n","  Downloading typing_extensions-4.10.0-py3-none-any.whl (33 kB)\n","Collecting numpy>=1.20.0 (from llama-cpp-python)\n","  Downloading numpy-1.26.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.2 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.2/18.2 MB\u001b[0m \u001b[31m293.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m121.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting jinja2>=2.11.3 (from llama-cpp-python)\n","  Downloading Jinja2-3.1.3-py3-none-any.whl (133 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m133.2/133.2 kB\u001b[0m \u001b[31m334.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting MarkupSafe>=2.0 (from jinja2>=2.11.3->llama-cpp-python)\n","  Downloading MarkupSafe-2.1.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n","Building wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.2.55-cp310-cp310-manylinux_2_35_x86_64.whl size=22408779 sha256=d0b6cfd3faf886a1baaa1831d306ad125e364f5d0bcfba13c98ff3ff1161e901\n","  Stored in directory: /tmp/pip-ephem-wheel-cache-17ahi3kh/wheels/35/c9/bb/019dbfeef119ab5c29f76574b76070afa7b7755ccfbb3ee226\n","Successfully built llama-cpp-python\n","Installing collected packages: typing-extensions, numpy, MarkupSafe, diskcache, jinja2, llama-cpp-python\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing_extensions 4.10.0\n","    Uninstalling typing_extensions-4.10.0:\n","      Successfully uninstalled typing_extensions-4.10.0\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.25.2\n","    Uninstalling numpy-1.25.2:\n","      Successfully uninstalled numpy-1.25.2\n","  Attempting uninstall: MarkupSafe\n","    Found existing installation: MarkupSafe 2.1.5\n","    Uninstalling MarkupSafe-2.1.5:\n","      Successfully uninstalled MarkupSafe-2.1.5\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 3.1.3\n","    Uninstalling Jinja2-3.1.3:\n","      Successfully uninstalled Jinja2-3.1.3\n","Successfully installed MarkupSafe-2.1.5 diskcache-5.6.3 jinja2-3.1.3 llama-cpp-python-0.2.55 numpy-1.26.4 typing-extensions-4.10.0\n"]}],"source":["# # this will take time :c\n","# !CUDACXX=/usr/local/cuda-12/bin/nvcc CMAKE_ARGS=\"-DLLAMA_CUBLAS=on -DCMAKE_CUDA_ARCHITECTURES=all-major\" FORCE_CMAKE=1 pip install llama-cpp-python --no-cache-dir --force-reinstall --upgrade"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"__VQGn_rnfLD"},"outputs":[],"source":["# !pip install ctransformers[cuda]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":73},"executionInfo":{"elapsed":8595,"status":"ok","timestamp":1710019202610,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"eChKQNF8eOKk","outputId":"63348bc8-c8a8-4ef2-bae5-d4216241baba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: numpy==1.26.4 in /usr/local/lib/python3.10/dist-packages (1.26.4)\n"]},{"output_type":"execute_result","data":{"text/plain":["'1.26.4'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["!pip3 install numpy==1.26.4\n","import numpy as np\n","np.__version__"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4ItWW1Es3OX_"},"outputs":[],"source":["# fix colab error: https://stackoverflow.com/questions/56081324/why-are-google-colab-shell-commands-not-working\n","import locale\n","def getpreferredencoding(do_setlocale = True):\n","    return \"UTF-8\"\n","locale.getpreferredencoding = getpreferredencoding"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CB6DVShA2vbh"},"outputs":[],"source":["from langchain.text_splitter import RecursiveCharacterTextSplitter\n","from langchain.document_loaders import PyPDFLoader, DirectoryLoader\n","from langchain.docstore.document import Document as LangchainDocument\n","from langchain.chains.summarize import load_summarize_chain\n","from langchain.chains import RetrievalQA\n","from langchain.embeddings import HuggingFaceEmbeddings\n","from langchain.vectorstores import Chroma\n","from langchain.llms import LlamaCpp\n","from langchain.prompts import PromptTemplate, ChatPromptTemplate\n","import os\n","import pandas as pd"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GrqalAohmDnp"},"outputs":[],"source":["from huggingface_hub import hf_hub_download\n","from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m1Dm1uHk2vbh"},"outputs":[],"source":["from langchain.vectorstores import FAISS\n","from langchain_community.embeddings import HuggingFaceEmbeddings\n","from tqdm.notebook import tqdm\n","import pandas as pd\n","from typing import Optional, List, Tuple\n","import matplotlib.pyplot as plt\n","pd.set_option(\n","    \"display.max_colwidth\", None\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3ti-G7vMdASy"},"outputs":[],"source":["# pip list --outdated"]},{"cell_type":"markdown","metadata":{"id":"pO3SVhxSshpo"},"source":["# Specify the model/versions, etc.\n","## also specify the knowledge base file and the reference answer directory"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SscMCDYU2vbh"},"outputs":[],"source":["QUESTIONS_FILE = '../data/test/questions.txt' # change when we have all the test ones / for the category wise performance analysis\n","EMBEDDING_MODEL = \"thenlper/gte-base\"\n","RERANKER_MODEL = \"colbert-ir/colbertv2.0\"\n","# RERANKER = RAGPretrainedModel.from_pretrained(RERANKER_MODEL)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":461,"referenced_widgets":["73381c9bc58b41febb95b6cf451ec0a5","70ce8f12e7564712bc8dd21490addac7","4405c5fcad3f4b56b1012602149f85b4","049db641804e4662a86758ae1ab97c20","2b7e1f7357524494a8708d80b2d0b03b","204311f4b93f4ca2adbc75e799dcc269","ab210c96464541d9a49ed1eecdd2a482","bb79d53db4914eabbec43ba4f6d393c2","2fae61055efd40aaace631946908d9c0","5327b2ab06fc497882a192ff80572dd1","bfb6cf42ae2a4223a67d450d04fa58d6","c9275b2e658044cf8255babb574bbfaa","d55b45a257d54d3eb379310d25ee65ed","3aea1e0a6677492980b427cbc9e91ab3","2babd835e39f4d79ae45e0706da0fd28","971d978505954d98b3338bd7d64cf08c","c6ac0f6cbf8e44f894f8f745a241acdf","c46177cdfb694c308523aa1ff9ebef50","0b79308aa8d2448897e850c21fc15154","e5b636ea02fe4f93a929a36ff77375ad","df4855de7e3449a8b600b6585c834c94","86233655364345cdbfc5bba2d0fea91e","394a1323cb0943638187472da7cad0c0","eb59a54810cc4e33ad1f662accb181cd","337c4ca8dac441e08a24aca235436524","12bc787071f14b5aac2c943ec1dfeb53","c5470d77d157400580dc79658b53d7f4","4bf6f09713e64a07ab676042cb921928","9c484f8cc5114ab88bc7816dc3d2a552","3eccb3e57ac14719abdb4778a78c73e9","70efbe1306954d1788372cf5f3ef5091","3db41973588045c58f6759d7001fffbc","1d9f595c7cf0484baf373d9fb60e6c0c","c1396d674b3b42f18924bc088c7eeb65","c8ef58c234744846ba38dbdceb6b77fa","fb50d42f326b44dbb51680b07edf4403","31f006c985384e7aa3c78ebe4f1ecdab","23972042c8df4a7aba905e37ce4f6113","ccc02de561354c6ab0daed49b95ca23e","fcdbb160558149508fa50fed1d385ae7","5ef0cef9099f4ff585a18b37c020c3ed","90b1477470054fd68210622e22437ee7","89d13559cd554c8b8994154a440f3415","c14994d191e443999287a6456275c92a","b82501f97a664f919648c68528e98732","65f936e9b8bd4fadaac87005e0541c39","9903794c798c498c80f96da246427448","f23807ad890a40c485173939461d6ac5","e78590a6925545e7ac37632853869b1c","c8626b94dc404c519826cc20a20f4adc","66f50fb62e9e4c559e905360f02caded","b29ddfcdfe5547fea7b53c7ddd518082","99967e20e9d3441794323949472f025c","4976707ee6644b5fa66e5ace7c3edfee","77c2a046ac3d48b5b625c0c4cfc2c10c","aee5ea91e79147b3bff12ed9996e086a","af1934d6b9df4ea1bae2c7a9b81e84f5","3b4df1d228574aa78ee1c729c2463807","a75e7d4dddaa4151ae035a04ee798c5b","f7f49188d5da4a4caf8478fc69fd89a5","23c7160fe5bd4b539f58d2d7d5dab63b","2b2afcf3d68b4d48b5255db1b857dcd3","9e85535974874e54814cbf9520eb54f0","c106f61ad3c44b9281b658ada1c4ea55","661459366fe0423e919f913a4d83a31c","272ad2985f3d41ababb9644e34501301","6ba5845fabd64d0d82a63d524c861bde","06368b4be61243249c4c2d5551ad3434","b7e7d4ea149b4a4a90bb9abd56bc8aa4","17ad3c546c9c4d76b5446ee460e42631","2d3e3cba9e9d4ab6916e8cf49b1b302d","335cf7a69dd04ecdb0abcb03ccf68fc5","90e1eb2004a44e0480ccbf3aa44d7f12","7e178f71bcb44093b4604efa62749e1d","a6e37f23bb8e4cef98fdf56cdb9219cc","bd47c89371f7419a828b9552d3fc5de5","3353e8f58c2a48929f0b06d540396197","8b64eda4f1254744a3249420ee24d80b","18726390fbb94e2ebfafdbf115e5b2f2","f8c86673b1414202a4e461cd0895beaf","ef4ee6174ba945c9b2bec5ae6566bfbb","d1a454f2490b453fa5eda3cb2d55ce13","541e038f2839435fa5a47c020e004aed","7c16f94e804142cc8ca538d7608244aa","c95ab0557ca4460fb74fefcf8da40066","58e0f55eedd34c99baf104381909ccce","c62abfa1fc264123a2a73995c7bebf6f","857a7e8b5bf645f0a729883d3bfe7371","9712385ae82f4fbfa04ea909fd46258f","f9094c8b4bcb4744a52cacb3c334b85d","d3b546d4c3d94e0ebe90f874fd76e776","7c49a27b11e94c07b80333f52fc826b5","bf6508f46fe3454d9bc3af6ee31e8bdf","d3757ac836ef4d9b93239f4455a29684","0307710609fd4f2b832f87cfa9be60f8","1b9803e22b754d52a1aacbb297c8948c","f87b7f4b74854c88bde127d01b9fbeb5","93ef878d55b84249b2d51ae7434036b8","dda1a81e97ed46c58604cbe5ba335fac","153ffc6c9db347a98e0b6ae582769cc9","fa6ffcd7180c45a588f9e7c5860828ed","fa787bd9fedc4d7a89384f33e853ce64","95aaaa54fc184ca6912737fed003347e","7e1af9ed45b14ecf95435b9eff217397","0c3a1d08e4f14df2a36fe132349e33bc","3d16a72cda2b4097a8742b21d2172275","cd944b63c4f944f5aae18c76118c803f","4af9843f83214d55902799b5fb6665df","6484c7ab71bd4b5e8286feed649b393e","532d9ecf84304085b0f1e092da85b1a1"]},"executionInfo":{"elapsed":11534,"status":"ok","timestamp":1710019231765,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"},"user_tz":300},"id":"zliNdIDo2vbh","outputId":"83eb29f8-8b90-465b-89e3-5bb94184a963"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["modules.json:   0%|          | 0.00/385 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"73381c9bc58b41febb95b6cf451ec0a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["README.md:   0%|          | 0.00/68.1k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9275b2e658044cf8255babb574bbfaa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["sentence_bert_config.json:   0%|          | 0.00/57.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"394a1323cb0943638187472da7cad0c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/618 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1396d674b3b42f18924bc088c7eeb65"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/219M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b82501f97a664f919648c68528e98732"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/314 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aee5ea91e79147b3bff12ed9996e086a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ba5845fabd64d0d82a63d524c861bde"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/712k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8b64eda4f1254744a3249420ee24d80b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9712385ae82f4fbfa04ea909fd46258f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"153ffc6c9db347a98e0b6ae582769cc9"}},"metadata":{}}],"source":["EMBEDDING_MODEL_NAME = EMBEDDING_MODEL\n","embedding_model = HuggingFaceEmbeddings(\n","    model_name=EMBEDDING_MODEL_NAME,\n","    multi_process=True,\n","    model_kwargs={\"device\": \"cuda\"},\n","    encode_kwargs={\"normalize_embeddings\": True},\n",")"]},{"cell_type":"code","source":["from langchain_community.document_loaders import TextLoader, CSVLoader,JSONLoader\n","from database import split_documents, create_db\n","import glob\n","directory_jsons = '../data/paper_jsons/'\n","json_files = glob.glob(os.path.join(directory_jsons, '**', '*.json'), recursive=True)"],"metadata":{"id":"2wAOvYG5F67r","executionInfo":{"status":"ok","timestamp":1710024164755,"user_tz":300,"elapsed":1,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}}},"execution_count":120,"outputs":[]},{"cell_type":"code","source":["# works but if we want to retain the formatting the Jsonloader is better\n","loader = DirectoryLoader(directory_jsons, glob=\"**/*.json\",use_multithreading = True, loader_cls=TextLoader)\n","docs_json = loader.load()"],"metadata":{"id":"IIegqg0EFz0-","executionInfo":{"status":"ok","timestamp":1710024165547,"user_tz":300,"elapsed":146,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}}},"execution_count":121,"outputs":[]},{"cell_type":"code","source":["# !pip install jq"],"metadata":{"id":"khRHEnrLPSUk","executionInfo":{"status":"ok","timestamp":1710024166884,"user_tz":300,"elapsed":1,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}}},"execution_count":122,"outputs":[]},{"cell_type":"code","source":["# docs_json = []\n","# for file_path in json_files:\n","#   loader = JSONLoader(\n","#     file_path=file_path,\n","#     jq_schema='.content',\n","#     text_content=False,\n","#     json_lines=True)\n","\n","# data = loader.load()\n"],"metadata":{"id":"mmpV8lsDOUzG","executionInfo":{"status":"ok","timestamp":1710024168022,"user_tz":300,"elapsed":1,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}}},"execution_count":123,"outputs":[]},{"cell_type":"code","source":["len(docs_json), type(docs_json)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fiq7a5FtJSRf","executionInfo":{"status":"ok","timestamp":1710026606122,"user_tz":300,"elapsed":216,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}},"outputId":"58bba5eb-610c-4e76-f9ff-4f20e062ce19"},"execution_count":136,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(50, list)"]},"metadata":{},"execution_count":136}]},{"cell_type":"code","source":["docs_json_processed = split_documents(\n","        chunk_size = 512,\n","        chunk_overlap = 50,\n","        knowledge_base = docs_json,\n","        tokenizer_name=EMBEDDING_MODEL_NAME,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4QVFBEkFR8QU","executionInfo":{"status":"ok","timestamp":1710026614360,"user_tz":300,"elapsed":6136,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}},"outputId":"cd6f7af6-2f61-41d6-9dc5-b9775400f2d7"},"execution_count":137,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (854 > 512). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","source":["db_json = create_db(docs_json) # 512 giving (854 > 512) error"],"metadata":{"id":"bMCw4ZIWGShX","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710023619303,"user_tz":300,"elapsed":41447,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}},"outputId":"e8b0439d-02e5-4a1d-94a5-8b4c31b7e65e"},"execution_count":100,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (854 > 512). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","source":["db_json.save_local(\"../faiss_index_json\")"],"metadata":{"id":"kIZILUlMHbzl"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Running on processed jsons"],"metadata":{"id":"5qJKDygUHsNJ"}},{"cell_type":"code","source":["from langchain_community.document_loaders import TextLoader, CSVLoader\n","from database import split_documents, create_db\n","directory_csv = '../data/papers_metadata_csv/'\n","csv_files = glob.glob(os.path.join(directory_csv, '**', '*.csv'), recursive=True)"],"metadata":{"id":"0amD9JzIHr0q","executionInfo":{"status":"ok","timestamp":1710023058905,"user_tz":300,"elapsed":179,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}}},"execution_count":81,"outputs":[]},{"cell_type":"code","source":["len(csv_files)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"d1bciltGROXj","executionInfo":{"status":"ok","timestamp":1710023059761,"user_tz":300,"elapsed":163,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}},"outputId":"73a3689b-8394-4a98-913c-aedc20292ed6"},"execution_count":82,"outputs":[{"output_type":"execute_result","data":{"text/plain":["337"]},"metadata":{},"execution_count":82}]},{"cell_type":"code","source":["loader_csv = DirectoryLoader(directory_csv, glob=\"**/*.csv\",\n","                             use_multithreading = True,\n","                             loader_cls=CSVLoader,\n","                            #  loader_kwargs = csv_args\n","                             )\n","docs_csv = loader_csv.load()\n","docs_csv"],"metadata":{"id":"R5UQC3AwHpc_","executionInfo":{"status":"ok","timestamp":1710024238309,"user_tz":300,"elapsed":2362,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"ac99e46c-2506-49dd-bd22-76762b774aed"},"execution_count":126,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='Question: What is the name of this paper?\\nAnswer: Subject-driven Text-to-Image Generation via Apprenticeship Learning\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning', metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of William Cohen?\\nAnswer: 50056360\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning', metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of William Cohen?\\nAnswer: 88\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning', metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of William Cohen?\\nAnswer: William W. Cohen\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning', metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of William Cohen?\\nAnswer: https://www.semanticscholar.org/author/50056360\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning', metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of William Cohen?\\nAnswer: Google\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning', metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Subject-driven Text-to-Image Generation via Apprenticeship Learning?\\nAnswer: 83b8e18488d8f31dd017ec0b26531cef4b635b36\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning', metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Subject-driven Text-to-Image Generation via Apprenticeship Learning?\\nAnswer: {'DBLP': 'journals/corr/abs-2304-00186', 'ArXiv': '2304.00186', 'DOI': '10.48550/arXiv.2304.00186', 'CorpusId': 257913352}\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Subject-driven Text-to-Image Generation via Apprenticeship Learning?\\nAnswer: https://www.semanticscholar.org/paper/83b8e18488d8f31dd017ec0b26531cef4b635b36\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning', metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Subject-driven Text-to-Image Generation via Apprenticeship Learning'?\\nAnswer: Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with in-context learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by apprenticeship learning, where a single apprentice model is learned from data generated by a massive number of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train a massive number of expert models, each specializing in a different subject. The apprentice model SuTI then learns to imitate the behavior of these fine-tuned experts. SuTI can generate high-quality and customized subject-specific images 20x faster than optimization-based SoTA methods. On the challenging DreamBench and DreamBench-v2, our human evaluation shows that SuTI significantly outperforms existing models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen and DreamBooth, especially on the subject and text alignment aspects.\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Subject-driven Text-to-Image Generation via Apprenticeship Learning' published?\\nAnswer: arXiv.org\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Subject-driven Text-to-Image Generation via Apprenticeship Learning' published?\\nAnswer: 2023\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Subject-driven Text-to-Image Generation via Apprenticeship Learning'?\\nAnswer: 44\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Subject-driven Text-to-Image Generation via Apprenticeship Learning' have?\\nAnswer: 55\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Subject-driven Text-to-Image Generation via Apprenticeship Learning' have?\\nAnswer: 55\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Subject-driven Text-to-Image Generation via Apprenticeship Learning' have?\\nAnswer: 6\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Subject-driven Text-to-Image Generation via Apprenticeship Learning' open access?\\nAnswer: Yes\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Subject-driven Text-to-Image Generation via Apprenticeship Learning'?\\nAnswer: https://arxiv.org/pdf/2304.00186\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Subject-driven Text-to-Image Generation via Apprenticeship Learning'?\\nAnswer: Computer Science\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Subject-driven Text-to-Image Generation via Apprenticeship Learning'?\\nAnswer: ArXiv, volume: abs/2304.00186; ArXiv\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Subject-driven Text-to-Image Generation via Apprenticeship Learning'?\\nAnswer: Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, William W. Cohen\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Subject-driven Text-to-Image Generation via Apprenticeship Learning'?\\nAnswer: Human evaluation shows that SuTI significantly outperforms existing models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen and DreamBooth, especially on the subject and text alignment aspects.\\nNotes: ##Author: William Cohen, ##Title: Subject-driven Text-to-Image Generation via Apprenticeship Learning\", metadata={'source': '../data/papers_metadata_csv/83b8e18488d8f31dd017ec0b26531cef4b635b36.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Shinji Watanabe?\\nAnswer: 1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Shinji Watanabe?\\nAnswer: 67\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: https://www.semanticscholar.org/author/1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Shinji Watanabe?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study?\\nAnswer: bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study?\\nAnswer: {'DBLP': 'journals/corr/abs-2309-15800', 'ArXiv': '2309.15800', 'DOI': '10.48550/arXiv.2309.15800', 'CorpusId': 263152563}\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study?\\nAnswer: https://www.semanticscholar.org/paper/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study', metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study'?\\nAnswer: Speech signals, typically sampled at rates in the tens of thousands per second, contain redundancies, evoking inefficiencies in sequence modeling. High-dimensional speech features such as spectrograms are often used as the input for the subsequent model. However, they can still be redundant. Recent investigations proposed the use of discrete speech units derived from self-supervised learning representations, which significantly compresses the size of speech data. Applying various methods, such as de-duplication and subword modeling, can further compress the speech sequence length. Hence, training time is significantly reduced while retaining notable performance. In this study, we undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models. Experiments on 12 automatic speech recognition, 3 speech translation, and 1 spoken language understanding corpora demonstrate that discrete units achieve reasonably good results in almost all the settings. We intend to release our configurations and trained models to foster future research efforts.\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study' published?\\nAnswer: arXiv.org\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study' published?\\nAnswer: 2023\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study'?\\nAnswer: 46\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study' have?\\nAnswer: 2\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study' have?\\nAnswer: 2\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study' open access?\\nAnswer: Yes\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study'?\\nAnswer: https://arxiv.org/pdf/2309.15800\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study'?\\nAnswer: Computer Science, Engineering\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study'?\\nAnswer: ArXiv, volume: abs/2309.15800; ArXiv\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study'?\\nAnswer: Xuankai Chang, Brian Yan, Kwanghee Choi, Jee-weon Jung, Yichen Lu, Soumi Maiti, Roshan Sharma, Jiatong Shi, Jinchuan Tian, Shinji Watanabe, Yuya Fujita, Takashi Maekaku, Pengcheng Guo, Yao-Fei Cheng, Pavel Denisov, Kohei Saijo, Hsiu-Hsuan Wang\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study'?\\nAnswer: This study undertake a comprehensive and systematic exploration into the application of discrete units within end-to-end speech processing models, demonstrating that discrete units achieve reasonably good results in almost all the settings.\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study\", metadata={'source': '../data/papers_metadata_csv/bb4c59fc93d5be6b3d85dfde9d08e3dab80db9b7.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Speech collage: code-switched audio generation by collaging monolingual corpora\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora', metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Shinji Watanabe?\\nAnswer: 1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora', metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Shinji Watanabe?\\nAnswer: 67\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora', metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora', metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: https://www.semanticscholar.org/author/1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora', metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Shinji Watanabe?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora', metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Speech collage: code-switched audio generation by collaging monolingual corpora?\\nAnswer: fa5ebb425c57f6c4f1c36a7200ef1da867346e8c\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora', metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Speech collage: code-switched audio generation by collaging monolingual corpora?\\nAnswer: {'DBLP': 'journals/corr/abs-2309-15674', 'ArXiv': '2309.15674', 'DOI': '10.48550/arXiv.2309.15674', 'CorpusId': 263152081}\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Speech collage: code-switched audio generation by collaging monolingual corpora?\\nAnswer: https://www.semanticscholar.org/paper/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora', metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Speech collage: code-switched audio generation by collaging monolingual corpora'?\\nAnswer: Designing effective automatic speech recognition (ASR) systems for Code-Switching (CS) often depends on the availability of the transcribed CS resources. To address data scarcity, this paper introduces Speech Collage, a method that synthesizes CS data from monolingual corpora by splicing audio segments. We further improve the smoothness quality of audio generation using an overlap-add approach. We investigate the impact of generated data on speech recognition in two scenarios: using in-domain CS text and a zero-shot approach with synthesized CS text. Empirical results highlight up to 34.4% and 16.2% relative reductions in Mixed-Error Rate and Word-Error Rate for in-domain and zero-shot scenarios, respectively. Lastly, we demonstrate that CS augmentation bolsters the model's code-switching inclination and reduces its monolingual bias.\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Speech collage: code-switched audio generation by collaging monolingual corpora' published?\\nAnswer: arXiv.org\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Speech collage: code-switched audio generation by collaging monolingual corpora' published?\\nAnswer: 2023\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Speech collage: code-switched audio generation by collaging monolingual corpora'?\\nAnswer: 40\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Speech collage: code-switched audio generation by collaging monolingual corpora' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Speech collage: code-switched audio generation by collaging monolingual corpora' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Speech collage: code-switched audio generation by collaging monolingual corpora' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Speech collage: code-switched audio generation by collaging monolingual corpora' open access?\\nAnswer: Yes\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Speech collage: code-switched audio generation by collaging monolingual corpora'?\\nAnswer: https://arxiv.org/pdf/2309.15674\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Speech collage: code-switched audio generation by collaging monolingual corpora'?\\nAnswer: Computer Science, Engineering\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Speech collage: code-switched audio generation by collaging monolingual corpora'?\\nAnswer: ArXiv, volume: abs/2309.15674; ArXiv\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Speech collage: code-switched audio generation by collaging monolingual corpora'?\\nAnswer: A. Hussein, Dorsa Zeinali, Ondrej Klejch, Matthew Wiesner, Brian Yan, Shammur A. Chowdhury, Ahmed Ali, Shinji Watanabe, S. Khudanpur\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Speech collage: code-switched audio generation by collaging monolingual corpora'?\\nAnswer: Speech Collage is introduced, a method that synthesizes CS data from monolingual corpora by splicing audio segments that improves the smoothness quality of audio generation using an overlap-add approach and demonstrates that CS augmentation bolsters the model's code-switching inclination and reduces itsmonolingual bias.\\nNotes: ##Author: Shinji Watanabe, ##Title: Speech collage: code-switched audio generation by collaging monolingual corpora\", metadata={'source': '../data/papers_metadata_csv/fa5ebb425c57f6c4f1c36a7200ef1da867346e8c.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System', metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Daphne Ippolito?\\nAnswer: 7975935\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System', metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Daphne Ippolito?\\nAnswer: 25\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System', metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Daphne Ippolito?\\nAnswer: Daphne Ippolito\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System', metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Daphne Ippolito?\\nAnswer: https://www.semanticscholar.org/author/7975935\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System', metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Daphne Ippolito?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System', metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System?\\nAnswer: 03fb535de5cfcf435705a079334ac60f501226ab\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System', metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System?\\nAnswer: {'DBLP': 'journals/corr/abs-2309-04858', 'ACL': '2023.inlg-main.28', 'ArXiv': '2309.04858', 'DOI': '10.48550/arXiv.2309.04858', 'CorpusId': 261681722}\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System?\\nAnswer: https://www.semanticscholar.org/paper/03fb535de5cfcf435705a079334ac60f501226ab\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System', metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System'?\\nAnswer: Neural language models are increasingly deployed into APIs and websites that allow a user to pass in a prompt and receive generated text. Many of these systems do not reveal generation parameters. In this paper, we present methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling). Our ability to discover which decoding strategy was used has implications for detecting generated text. Additionally, the process of discovering the decoding strategy can reveal biases caused by selecting decoding settings which severely truncate a model’s predicted distributions. We perform our attack on several families of open-source language models, as well as on production systems (e.g., ChatGPT).\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System' published?\\nAnswer: International Conference on Natural Language Generation\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System' published?\\nAnswer: 2023\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System'?\\nAnswer: 15\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System' have?\\nAnswer: 3\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System' have?\\nAnswer: 3\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System' have?\\nAnswer: 0\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System' open access?\\nAnswer: Yes\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System'?\\nAnswer: https://arxiv.org/pdf/2309.04858\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System'?\\nAnswer: Computer Science\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System'?\\nAnswer: International Conference on Natural Language Generation, pages: 396-406; International Conference on Natural Language Generation\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System'?\\nAnswer: Daphne Ippolito, Nicholas Carlini, Katherine Lee, Milad Nasr, Yun William Yu\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System'?\\nAnswer: Methods to reverse-engineer the decoding method used to generate text (i.e., top-_k_ or nucleus sampling) are presented, which has implications for detecting generated text.\\nNotes: ##Author: Daphne Ippolito, ##Title: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System\", metadata={'source': '../data/papers_metadata_csv/03fb535de5cfcf435705a079334ac60f501226ab.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Making Scalable Meta Learning Practical\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical', metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Emma Strubell?\\nAnswer: 2268272\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical', metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Emma Strubell?\\nAnswer: 17\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical', metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Emma Strubell?\\nAnswer: Emma Strubell\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical', metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Emma Strubell?\\nAnswer: https://www.semanticscholar.org/author/2268272\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical', metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Emma Strubell?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical', metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Making Scalable Meta Learning Practical?\\nAnswer: a815c3209e7baff4466dbf6e129129511f842b7e\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical', metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Making Scalable Meta Learning Practical?\\nAnswer: {'ArXiv': '2310.05674', 'DBLP': 'journals/corr/abs-2310-05674', 'DOI': '10.48550/arXiv.2310.05674', 'CorpusId': 263830616}\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Making Scalable Meta Learning Practical?\\nAnswer: https://www.semanticscholar.org/paper/a815c3209e7baff4466dbf6e129129511f842b7e\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical', metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Making Scalable Meta Learning Practical'?\\nAnswer: Despite its flexibility to learn diverse inductive biases in machine learning programs, meta learning (i.e., learning to learn) has long been recognized to suffer from poor scalability due to its tremendous compute/memory costs, training instability, and a lack of efficient distributed training support. In this work, we focus on making scalable meta learning practical by introducing SAMA, which combines advances in both implicit differentiation algorithms and systems. Specifically, SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients. Evaluated on multiple large-scale meta learning benchmarks, SAMA showcases up to 1.7/4.8x increase in throughput and 2.0/3.8x decrease in memory consumption respectively on single-/multi-GPU setups compared to other baseline meta learning algorithms. Furthermore, we show that SAMA-based data optimization leads to consistent improvements in text classification accuracy with BERT and RoBERTa large language models, and achieves state-of-the-art results in both small- and large-scale data pruning on image classification tasks, demonstrating the practical applicability of scalable meta learning across language and vision domains.\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Making Scalable Meta Learning Practical' published?\\nAnswer: arXiv.org\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Making Scalable Meta Learning Practical' published?\\nAnswer: 2023\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Making Scalable Meta Learning Practical'?\\nAnswer: 72\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Making Scalable Meta Learning Practical' have?\\nAnswer: 1\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Making Scalable Meta Learning Practical' have?\\nAnswer: 1\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Making Scalable Meta Learning Practical' have?\\nAnswer: 0\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Making Scalable Meta Learning Practical' open access?\\nAnswer: Yes\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Making Scalable Meta Learning Practical'?\\nAnswer: https://arxiv.org/pdf/2310.05674\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Making Scalable Meta Learning Practical'?\\nAnswer: Computer Science\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Making Scalable Meta Learning Practical'?\\nAnswer: ArXiv, volume: abs/2310.05674; ArXiv\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Making Scalable Meta Learning Practical'?\\nAnswer: Sang Keun Choe, Sanket Vaibhav Mehta, Hwijeen Ahn, W. Neiswanger, Pengtao Xie, Emma Strubell, Eric P. Xing\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Making Scalable Meta Learning Practical'?\\nAnswer: SAMA is designed to flexibly support a broad range of adaptive optimizers in the base level of meta learning programs, while reducing computational burden by avoiding explicit computation of second-order gradient information, and exploiting efficient distributed training techniques implemented for first-order gradients.\\nNotes: ##Author: Emma Strubell, ##Title: Making Scalable Meta Learning Practical\", metadata={'source': '../data/papers_metadata_csv/a815c3209e7baff4466dbf6e129129511f842b7e.csv', 'row': 21}),\n"," Document(page_content=\"Question: What is the name of this paper?\\nAnswer: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 0}),\n"," Document(page_content=\"Question: What is the author ID of Yiming Yang?\\nAnswer: 46286308\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 1}),\n"," Document(page_content=\"Question: What is the H-index of Yiming Yang?\\nAnswer: 17\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 2}),\n"," Document(page_content=\"Question: What is the semantic scholar author name of Yiming Yang?\\nAnswer: Yiming Yang\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 3}),\n"," Document(page_content=\"Question: What is the semantic scholar author name of Yiming Yang?\\nAnswer: https://www.semanticscholar.org/author/46286308\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 4}),\n"," Document(page_content=\"Question: What is the affiliation of Yiming Yang?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 5}),\n"," Document(page_content=\"Question: What is the paper ID of the paper Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs?\\nAnswer: b4a6c010724f0459c9791018e34a982cf96987cf\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs?\\nAnswer: {'ArXiv': '2305.11860', 'DBLP': 'conf/emnlp/AggarwalY23', 'DOI': '10.18653/v1/2023.emnlp-main.761', 'CorpusId': 258823191}\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 7}),\n"," Document(page_content=\"Question: What is the URL of the paper Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs?\\nAnswer: https://www.semanticscholar.org/paper/b4a6c010724f0459c9791018e34a982cf96987cf\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs'?\\nAnswer: A popular approach for improving the correctness of output from large language models (LLMs) is Self-Consistency - poll the LLM multiple times and output the most frequent solution. Existing Self-Consistency techniques always generate a constant number of samples per question, where a better approach will be to non-uniformly distribute the available budget based on the amount of agreement in the samples generated so far. In response, we introduce Adaptive-Consistency, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion. Our experiments over 17 reasoning and code generation datasets and three LLMs demonstrate that Adaptive-Consistency reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%. Our code and data are available at https://www.sample-step-by-step.info\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs' published?\\nAnswer: Conference on Empirical Methods in Natural Language Processing\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs' published?\\nAnswer: 2023\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs'?\\nAnswer: 54\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs' have?\\nAnswer: 9\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs' have?\\nAnswer: 9\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs' have?\\nAnswer: 1\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs' open access?\\nAnswer: Yes\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs'?\\nAnswer: https://aclanthology.org/2023.emnlp-main.761.pdf\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs'?\\nAnswer: Computer Science\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs'?\\nAnswer: Conference on Empirical Methods in Natural Language Processing, pages: 12375-12396; Conference on Empirical Methods in Natural Language Processing\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs'?\\nAnswer: Pranjal Aggarwal, Aman Madaan, Yiming Yang, Mausam\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs'?\\nAnswer: Adaptive-Consistency is introduced, a cost-efficient, model-agnostic technique that dynamically adjusts the number of samples per question using a lightweight stopping criterion that reduces sample budget by up to 7.9 times with an average accuracy drop of less than 0.1%.\\nNotes: ##Author: Yiming Yang, ##Title: Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs\", metadata={'source': '../data/papers_metadata_csv/b4a6c010724f0459c9791018e34a982cf96987cf.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation', metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Graham Neubig?\\nAnswer: 1700325\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation', metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Graham Neubig?\\nAnswer: 75\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation', metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Graham Neubig?\\nAnswer: Graham Neubig\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation', metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Graham Neubig?\\nAnswer: https://www.semanticscholar.org/author/1700325\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation', metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Graham Neubig?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation', metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation?\\nAnswer: 74b05bba46db21e589a2cc0f916f81069b0368ef\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation', metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation?\\nAnswer: {'DBLP': 'journals/corr/abs-2305-00955', 'ArXiv': '2305.00955', 'DOI': '10.48550/arXiv.2305.00955', 'CorpusId': 258426970}\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation?\\nAnswer: https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation', metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation'?\\nAnswer: Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation' published?\\nAnswer: arXiv.org\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation' published?\\nAnswer: 2023\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation'?\\nAnswer: 155\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation' have?\\nAnswer: 23\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation' have?\\nAnswer: 23\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation' have?\\nAnswer: 7\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation' open access?\\nAnswer: Yes\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation'?\\nAnswer: http://arxiv.org/pdf/2305.00955\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation'?\\nAnswer: Computer Science\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation'?\\nAnswer: ArXiv, volume: abs/2305.00955; ArXiv\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation'?\\nAnswer: Patrick Fernandes, Aman Madaan, Emmy Liu, Antonio Farinhas, Pedro Henrique Martins, Amanda Bertsch, Jose G. C. de Souza, Shuyan Zhou, Tongshuang Sherry Wu, Graham Neubig, Andre F. T. Martins\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation'?\\nAnswer: An overview of the recent research that has leveraged human feedback to improve natural language generation and the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention is provided.\\nNotes: ##Author: Graham Neubig, ##Title: Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation\", metadata={'source': '../data/papers_metadata_csv/74b05bba46db21e589a2cc0f916f81069b0368ef.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation', metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Shinji Watanabe?\\nAnswer: 1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation', metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Shinji Watanabe?\\nAnswer: 67\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation', metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation', metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: https://www.semanticscholar.org/author/1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation', metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Shinji Watanabe?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation', metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation?\\nAnswer: ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation', metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation?\\nAnswer: {'DBLP': 'journals/corr/abs-2305-07455', 'ArXiv': '2305.07455', 'DOI': '10.48550/arXiv.2305.07455', 'CorpusId': 258676617}\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation?\\nAnswer: https://www.semanticscholar.org/paper/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation', metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation'?\\nAnswer: Most of the speech translation models heavily rely on parallel data, which is hard to collect especially for low-resource languages. To tackle this issue, we propose to build a cascaded speech translation system without leveraging any kind of paired data. We use fully unpaired data to train our unsupervised systems and evaluate our results on CoVoST 2 and CVSS. The results show that our work is comparable with some other early supervised methods in some language pairs. While cascaded systems always suffer from severe error propagation problems, we proposed denoising back-translation (DBT), a novel approach to building robust unsupervised neural machine translation (UNMT). DBT successfully increases the BLEU score by 0.7--0.9 in all three translation directions. Moreover, we simplified the pipeline of our cascaded system to reduce inference latency and conducted a comprehensive analysis of every part of our work. We also demonstrate our unsupervised speech translation results on the established website.\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation' published?\\nAnswer: arXiv.org\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation' published?\\nAnswer: 2023\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation'?\\nAnswer: 44\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation' have?\\nAnswer: 3\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation' have?\\nAnswer: 3\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation' have?\\nAnswer: 1\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation' open access?\\nAnswer: Yes\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation'?\\nAnswer: http://arxiv.org/pdf/2305.07455\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation'?\\nAnswer: Computer Science, Engineering\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation'?\\nAnswer: ArXiv, volume: abs/2305.07455; ArXiv\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation'?\\nAnswer: Yu-Kuan Fu, Liang-Hsuan Tseng, Jiatong Shi, Chen-An Li, Tsung-Yuan Hsu, Shinji Watanabe, Hung-yi Lee\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation'?\\nAnswer: This work proposed denoising back-translation (DBT), a novel approach to building robust unsupervised neural machine translation (UNMT), which successfully increases the BLEU score by 0.7--0.9 in all three translation directions.\\nNotes: ##Author: Shinji Watanabe, ##Title: Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation\", metadata={'source': '../data/papers_metadata_csv/ef8b095292a8e38e9b8f56c54cbf3c67c3ed425d.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: GLIMMER: generalized late-interaction memory reranker\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker', metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of William Cohen?\\nAnswer: 50056360\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker', metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of William Cohen?\\nAnswer: 88\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker', metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of William Cohen?\\nAnswer: William W. Cohen\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker', metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of William Cohen?\\nAnswer: https://www.semanticscholar.org/author/50056360\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker', metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of William Cohen?\\nAnswer: Google\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker', metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper GLIMMER: generalized late-interaction memory reranker?\\nAnswer: c67099476f2b505dfd5a22c817707fad83de9994\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker', metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper GLIMMER: generalized late-interaction memory reranker?\\nAnswer: {'DBLP': 'journals/corr/abs-2306-10231', 'ArXiv': '2306.10231', 'DOI': '10.48550/arXiv.2306.10231', 'CorpusId': 259203489}\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper GLIMMER: generalized late-interaction memory reranker?\\nAnswer: https://www.semanticscholar.org/paper/c67099476f2b505dfd5a22c817707fad83de9994\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker', metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'GLIMMER: generalized late-interaction memory reranker'?\\nAnswer: Memory-augmentation is a powerful approach for efficiently incorporating external information into language models, but leads to reduced performance relative to retrieving text. Recent work introduced LUMEN, a memory-retrieval hybrid that partially pre-computes memory and updates memory representations on the fly with a smaller live encoder. We propose GLIMMER, which improves on this approach through 1) exploiting free access to the powerful memory representations by applying a shallow reranker on top of memory to drastically improve retrieval quality at low cost, and 2) incorporating multi-task training to learn a general and higher quality memory and live encoder. GLIMMER achieves strong gains in performance at faster speeds compared to LUMEN and FiD on the KILT benchmark of knowledge-intensive tasks.\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'GLIMMER: generalized late-interaction memory reranker' published?\\nAnswer: arXiv.org\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'GLIMMER: generalized late-interaction memory reranker' published?\\nAnswer: 2023\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'GLIMMER: generalized late-interaction memory reranker'?\\nAnswer: 53\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'GLIMMER: generalized late-interaction memory reranker' have?\\nAnswer: 3\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'GLIMMER: generalized late-interaction memory reranker' have?\\nAnswer: 3\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'GLIMMER: generalized late-interaction memory reranker' have?\\nAnswer: 0\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'GLIMMER: generalized late-interaction memory reranker' open access?\\nAnswer: Yes\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'GLIMMER: generalized late-interaction memory reranker'?\\nAnswer: http://arxiv.org/pdf/2306.10231\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'GLIMMER: generalized late-interaction memory reranker'?\\nAnswer: Computer Science\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'GLIMMER: generalized late-interaction memory reranker'?\\nAnswer: ArXiv, volume: abs/2306.10231; ArXiv\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'GLIMMER: generalized late-interaction memory reranker'?\\nAnswer: Michiel de Jong, Yury Zemlyanskiy, Nicholas FitzGerald, Sumit K. Sanghai, William W. Cohen, J. Ainslie\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'GLIMMER: generalized late-interaction memory reranker'?\\nAnswer: GLIMMER is proposed, which improves on LUMEN through exploiting free access to the powerful memory representations by applying a shallow reranker on top of memory to drastically improve retrieval quality at low cost and incorporating multi-task training to learn a general and higher quality memory and live encoder.\\nNotes: ##Author: William Cohen, ##Title: GLIMMER: generalized late-interaction memory reranker\", metadata={'source': '../data/papers_metadata_csv/c67099476f2b505dfd5a22c817707fad83de9994.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization', metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Matt Gormley?\\nAnswer: 1762110\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization', metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Matt Gormley?\\nAnswer: 17\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization', metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Matt Gormley?\\nAnswer: Matthew R. Gormley\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization', metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Matt Gormley?\\nAnswer: https://www.semanticscholar.org/author/1762110\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization', metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Matt Gormley?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization', metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization?\\nAnswer: ebb3d299213bae89b5d302cc3dfc36573ec83956\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization', metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization?\\nAnswer: {'ACL': '2023.clinicalnlp-1.51', 'DBLP': 'conf/acl-clinicalnlp/MathurRKPBG23', 'ArXiv': '2306.17384', 'DOI': '10.48550/arXiv.2306.17384', 'CorpusId': 259309155}\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization?\\nAnswer: https://www.semanticscholar.org/paper/ebb3d299213bae89b5d302cc3dfc36573ec83956\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization', metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization'?\\nAnswer: Medical dialogue summarization is challenging due to the unstructured nature of medical conversations, the use of medical terminologyin gold summaries, and the need to identify key information across multiple symptom sets. We present a novel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA 2023 Shared Task. Our approach for sectionwise summarization (Task A) is a two-stage process of selecting semantically similar dialogues and using the top-k similar dialogues as in-context examples for GPT-4. For full-note summarization (Task B), we use a similar solution with k=1. We achieved 3rd place in Task A (2nd among all teams), 4th place in Task B Division Wise Summarization (2nd among all teams), 15th place in Task A Section Header Classification (9th among all teams), and 8th place among all teams in Task B. Our results highlight the effectiveness of few-shot prompting for this task, though we also identify several weaknesses of prompting-based approaches. We compare GPT-4 performance with several finetuned baselines. We find that GPT-4 summaries are more abstractive and shorter. We make our code publicly available.\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization' published?\\nAnswer: Clinical Natural Language Processing Workshop\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization' published?\\nAnswer: 2023\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization'?\\nAnswer: 48\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization' have?\\nAnswer: 4\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization' have?\\nAnswer: 4\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization' have?\\nAnswer: 1\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization' open access?\\nAnswer: Yes\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization'?\\nAnswer: http://arxiv.org/pdf/2306.17384\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization'?\\nAnswer: Computer Science\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization'?\\nAnswer: Clinical Natural Language Processing Workshop, pages: 490-502; Clinical Natural Language Processing Workshop\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization'?\\nAnswer: Yash Mathur, Sanketh Rangreji, Raghav Kapoor, Medha Palavalli, Amanda Bertsch, Matthew R. Gormley\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization'?\\nAnswer: This work presents a novel system for the Dialogue2Note Medical Summarization tasks in the MEDIQA 2023 Shared Task, and highlights the effectiveness of few-shot prompting for this task, though it also identifies several weaknesses of prompting-based approaches.\\nNotes: ##Author: Matt Gormley, ##Title: SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\", metadata={'source': '../data/papers_metadata_csv/ebb3d299213bae89b5d302cc3dfc36573ec83956.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models', metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Yulia Tsvetkov?\\nAnswer: 2073587169\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models', metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Yulia Tsvetkov?\\nAnswer: 17\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models', metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yulia Tsvetkov?\\nAnswer: Yulia Tsvetkov\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models', metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yulia Tsvetkov?\\nAnswer: https://www.semanticscholar.org/author/2073587169\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models', metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Yulia Tsvetkov?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models', metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models?\\nAnswer: 17fbffb05fa14e21d1c506fd5f0f568b955fe983\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models', metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models?\\nAnswer: {'DBLP': 'conf/emnlp/Ahia0GKMST23', 'ArXiv': '2305.13707', 'DOI': '10.48550/arXiv.2305.13707', 'CorpusId': 258841465}\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models?\\nAnswer: https://www.semanticscholar.org/paper/17fbffb05fa14e21d1c506fd5f0f568b955fe983\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models', metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models'?\\nAnswer: Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models' published?\\nAnswer: Conference on Empirical Methods in Natural Language Processing\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models' published?\\nAnswer: 2023\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models'?\\nAnswer: 82\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models' have?\\nAnswer: 11\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models' have?\\nAnswer: 11\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models' have?\\nAnswer: 2\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models' open access?\\nAnswer: Yes\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models'?\\nAnswer: http://arxiv.org/pdf/2305.13707\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models'?\\nAnswer: Computer Science\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models'?\\nAnswer: ArXiv, volume: abs/2305.13707; ArXiv\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models'?\\nAnswer: Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models'?\\nAnswer: This work conducts a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages and shows evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results.\\nNotes: ##Author: Yulia Tsvetkov, ##Title: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\", metadata={'source': '../data/papers_metadata_csv/17fbffb05fa14e21d1c506fd5f0f568b955fe983.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient', metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Lori S Levin?\\nAnswer: 1686960\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient', metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Lori S Levin?\\nAnswer: 34\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient', metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Lori S Levin?\\nAnswer: Lori S. Levin\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient', metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Lori S Levin?\\nAnswer: https://www.semanticscholar.org/author/1686960\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient', metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Lori S Levin?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient', metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient?\\nAnswer: 2cdc646a6b70418e7cbd7fbdb8bb113176c4659f\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient', metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient?\\nAnswer: {'DOI': '10.1016/j.brainres.2023.148262', 'CorpusId': 256195620, 'PubMed': '36706858'}\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient?\\nAnswer: https://www.semanticscholar.org/paper/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient', metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient'?\\nAnswer: None\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient' published?\\nAnswer: Brain Research\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient' published?\\nAnswer: 2023\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient'?\\nAnswer: 38\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient' have?\\nAnswer: 0\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient' have?\\nAnswer: 0\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient' have?\\nAnswer: 0\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient' open access?\\nAnswer: Yes\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient'?\\nAnswer: openAccessPdf not available\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient'?\\nAnswer: Medicine\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient'?\\nAnswer: Brain Research, volume: 1804; Brain Research\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient'?\\nAnswer: W. Gaetz, C. Dockstader, P. Furlong, S. Amaral, A. Vossough, E. Schwartz, T. Roberts, Lori S. Levin\\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient'?\\nAnswer: \\nNotes: ##Author: Lori S Levin, ##Title: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient\", metadata={'source': '../data/papers_metadata_csv/2cdc646a6b70418e7cbd7fbdb8bb113176c4659f.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification', metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Louis-Philippe Morency?\\nAnswer: 49933077\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification', metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Louis-Philippe Morency?\\nAnswer: 79\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification', metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Louis-Philippe Morency?\\nAnswer: Louis-Philippe Morency\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification', metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Louis-Philippe Morency?\\nAnswer: https://www.semanticscholar.org/author/49933077\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification', metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Louis-Philippe Morency?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification', metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Multimodal Fusion Interactions: A Study of Human and Automatic Quantification?\\nAnswer: 90b09bdb1bd78875ee8d8d324a568a36955e4765\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification', metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Multimodal Fusion Interactions: A Study of Human and Automatic Quantification?\\nAnswer: {'DBLP': 'conf/icmi/LiangCSM23', 'ArXiv': '2306.04125', 'DOI': '10.1145/3577190.3614151', 'CorpusId': 259095686}\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Multimodal Fusion Interactions: A Study of Human and Automatic Quantification?\\nAnswer: https://www.semanticscholar.org/paper/90b09bdb1bd78875ee8d8d324a568a36955e4765\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification', metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification'?\\nAnswer: In order to perform multimodal fusion of heterogeneous signals, we need to understand their interactions: how each modality individually provides information useful for a task and how this information changes in the presence of other modalities. In this paper, we perform a comparative study of how humans annotate two categorizations of multimodal interactions: (1) partial labels, where different annotators annotate the label given the first, second, and both modalities, and (2) counterfactual labels, where the same annotator annotates the label given the first modality before asking them to explicitly reason about how their answer changes when given the second. We further propose an alternative taxonomy based on (3) information decomposition, where annotators annotate the degrees of redundancy: the extent to which modalities individually and together give the same predictions, uniqueness: the extent to which one modality enables a prediction that the other does not, and synergy: the extent to which both modalities enable one to make a prediction that one would not otherwise make using individual modalities. Through experiments and annotations, we highlight several opportunities and limitations of each approach and propose a method to automatically convert annotations of partial and counterfactual labels to information decomposition, yielding an accurate and efficient method for quantifying multimodal interactions.\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification' published?\\nAnswer: International Conference on Multimodal Interaction\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification' published?\\nAnswer: 2023\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification'?\\nAnswer: 79\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification' have?\\nAnswer: 0\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification' have?\\nAnswer: 0\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification' have?\\nAnswer: 0\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification' open access?\\nAnswer: Yes\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification'?\\nAnswer: https://dl.acm.org/doi/pdf/10.1145/3577190.3614151\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification'?\\nAnswer: Computer Science\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification'?\\nAnswer: Proceedings of the 25th International Conference on Multimodal Interaction\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification'?\\nAnswer: P. Liang, Yun Cheng, R. Salakhutdinov, Louis-Philippe Morency\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Multimodal Fusion Interactions: A Study of Human and Automatic Quantification'?\\nAnswer: A comparative study of how humans annotate two categorizations of multimodal interactions is performed and a method to automatically convert annotations of partial and counterfactual labels to information decomposition is proposed, yielding an accurate and efficient method for quantifying multimodals interactions.\\nNotes: ##Author: Louis-Philippe Morency, ##Title: Multimodal Fusion Interactions: A Study of Human and Automatic Quantification\", metadata={'source': '../data/papers_metadata_csv/90b09bdb1bd78875ee8d8d324a568a36955e4765.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: When to generate hedges in peer-tutoring interactions\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions', metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Justine Cassell?\\nAnswer: 145431806\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions', metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Justine Cassell?\\nAnswer: 62\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions', metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Justine Cassell?\\nAnswer: Justine Cassell\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions', metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Justine Cassell?\\nAnswer: https://www.semanticscholar.org/author/145431806\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions', metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Justine Cassell?\\nAnswer: Carnegie Mellon University, Inria Paris\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions', metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper When to generate hedges in peer-tutoring interactions?\\nAnswer: 24bff26f19051b1413d1e343322c1ae4bba05428\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions', metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper When to generate hedges in peer-tutoring interactions?\\nAnswer: {'ACL': '2023.sigdial-1.53', 'DBLP': 'conf/sigdial/AbulimitiCC23', 'ArXiv': '2307.15582', 'DOI': '10.48550/arXiv.2307.15582', 'CorpusId': 260315817}\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper When to generate hedges in peer-tutoring interactions?\\nAnswer: https://www.semanticscholar.org/paper/24bff26f19051b1413d1e343322c1ae4bba05428\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions', metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'When to generate hedges in peer-tutoring interactions'?\\nAnswer: This paper explores the application of machine learning techniques to predict where hedging occurs in peer-tutoring interactions. The study uses a naturalistic face-to-face dataset annotated for natural language turns, conversational strategies, tutoring strategies, and nonverbal behaviors. These elements are processed into a vector representation of the previous turns, which serves as input to several machine learning models, including MLP and LSTM. The results show that embedding layers, capturing the semantic information of the previous turns, significantly improves the model’s performance. Additionally, the study provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviors, in predicting hedges by using Shapley values for feature explanation. We discover that the eye gaze of both the tutor and the tutee has a significant impact on hedge prediction. We further validate this observation through a follow-up ablation study.\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'When to generate hedges in peer-tutoring interactions' published?\\nAnswer: SIGDIAL Conferences\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'When to generate hedges in peer-tutoring interactions' published?\\nAnswer: 2023\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'When to generate hedges in peer-tutoring interactions'?\\nAnswer: 43\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'When to generate hedges in peer-tutoring interactions' have?\\nAnswer: 0\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'When to generate hedges in peer-tutoring interactions' have?\\nAnswer: 0\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'When to generate hedges in peer-tutoring interactions' have?\\nAnswer: 0\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'When to generate hedges in peer-tutoring interactions' open access?\\nAnswer: Yes\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'When to generate hedges in peer-tutoring interactions'?\\nAnswer: https://arxiv.org/pdf/2307.15582\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'When to generate hedges in peer-tutoring interactions'?\\nAnswer: Computer Science\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'When to generate hedges in peer-tutoring interactions'?\\nAnswer: SIGDIAL Conferences, pages: 572-583; SIGDIAL Conferences\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'When to generate hedges in peer-tutoring interactions'?\\nAnswer: Alafate Abulimiti, C. Clavel, Justine Cassell\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'When to generate hedges in peer-tutoring interactions'?\\nAnswer: The results show that embedding layers, capturing the semantic information of the previous turns, significantly improves the model’s performance and provides insights into the importance of various features, such as interpersonal rapport and nonverbal behaviors, in predicting hedges by using Shapley values for feature explanation.\\nNotes: ##Author: Justine Cassell, ##Title: When to generate hedges in peer-tutoring interactions\", metadata={'source': '../data/papers_metadata_csv/24bff26f19051b1413d1e343322c1ae4bba05428.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores', metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Norman Sadeh?\\nAnswer: 2464164\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores', metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Norman Sadeh?\\nAnswer: 68\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores', metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Norman Sadeh?\\nAnswer: N. Sadeh\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores', metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Norman Sadeh?\\nAnswer: https://www.semanticscholar.org/author/2464164\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores', metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Norman Sadeh?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores', metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores?\\nAnswer: 88efd1663016c4170674c8f30067e7096b172598\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores', metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores?\\nAnswer: {'DBLP': 'conf/eurosp/RodriguezJAS23', 'DOI': '10.1109/EuroSPW59978.2023.00022', 'CorpusId': 260388094}\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores?\\nAnswer: https://www.semanticscholar.org/paper/88efd1663016c4170674c8f30067e7096b172598\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores', metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores'?\\nAnswer: Apple and Android introduced privacy labels in 2020 and 2022 respectively as a way of providing consumers with succinct summaries of mobile apps’ more salient data practices. A number of apps are published in both stores, offering us the opportunity to compare their privacy label disclosures in the two app stores. This paper compares the data practices privacy labels are intended to capture in each store. It then proceeds to analyze the disclosures of 822 apps published in both app stores, focusing on possible discrepancies. This analysis reveals that privacy label disclosures of what is ostensibly the same mobile app can be quite different. We discuss the different possible reasons behind these differences, including the possibility that these discrepancies might be indicative of potential privacy compliance issues. In particular, focusing on data collection disclosures of five different data types (location, contact info, sensitive info, identifiers, and health & fitness) we find discrepancies between iOS and Google Play privacy label disclosures in 66.5% of the mobile apps we analyze.\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores' published?\\nAnswer: 2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores' published?\\nAnswer: 2023\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores'?\\nAnswer: 26\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores' have?\\nAnswer: 2\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores' have?\\nAnswer: 2\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores' have?\\nAnswer: 0\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores' open access?\\nAnswer: Yes\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores'?\\nAnswer: https://oa.upm.es/75608/1/Final_Submission_Article.pdf\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores'?\\nAnswer: Computer Science\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores'?\\nAnswer: 2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW), pages: 150-157; 2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores'?\\nAnswer: David Rodriguez, Akshatha Jain, J. D. Alamo, N. Sadeh\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores'?\\nAnswer: It is revealed that privacy label disclosures of what is ostensibly the same mobile app can be quite different, including the possibility that these discrepancies might be indicative of potential privacy compliance issues.\\nNotes: ##Author: Norman Sadeh, ##Title: Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores\", metadata={'source': '../data/papers_metadata_csv/88efd1663016c4170674c8f30067e7096b172598.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling', metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Shinji Watanabe?\\nAnswer: 1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling', metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Shinji Watanabe?\\nAnswer: 67\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling', metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling', metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: https://www.semanticscholar.org/author/1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling', metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Shinji Watanabe?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling', metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling?\\nAnswer: 9cbd933c04218c9b642c15a49f8470d54524d9fb\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling', metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling?\\nAnswer: {'DBLP': 'conf/icassp/WangCCLKW23a', 'ArXiv': '2304.08707', 'DOI': '10.1109/ICASSP49357.2023.10095700', 'CorpusId': 258187590}\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling?\\nAnswer: https://www.semanticscholar.org/paper/9cbd933c04218c9b642c15a49f8470d54524d9fb\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling', metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling'?\\nAnswer: We propose FSB-LSTM, a novel long short-term memory (LSTM) based architecture that integrates full- and sub-band (FSB) modeling, for single- and multi-channel speech enhancement in the short-time Fourier transform (STFT) domain. The model maintains an information highway to flow an over-complete input representation through multiple FSB-LSTM modules. Each FSB-LSTM module consists of a full-band block to model spectro-temporal patterns at all frequencies and a sub-band block to model patterns within each sub-band, where each of the two blocks takes a down-sampled representation as input and returns an up-sampled discriminative representation to be added to the block input via a residual connection. The model is designed to have a low algorithmic complexity, a small run-time buffer and a very low algorithmic latency, at the same time producing a strong enhancement performance on a noisy-reverberant speech enhancement task even if the hop size is as low as 2 ms.\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling' published?\\nAnswer: IEEE International Conference on Acoustics, Speech, and Signal Processing\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling' published?\\nAnswer: 2023\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling'?\\nAnswer: 47\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling' have?\\nAnswer: 6\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling' have?\\nAnswer: 6\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling' open access?\\nAnswer: Yes\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling'?\\nAnswer: openAccessPdf not available\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling'?\\nAnswer: Engineering, Computer Science\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling'?\\nAnswer: ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages: 1-5; ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling'?\\nAnswer: Zhongqiu Wang, Samuele Cornell, Shukjae Choi, Younglo Lee, Byeonghak Kim, Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling'?\\nAnswer: The proposed FSB-LSTM model is designed to have a low algorithmic complexity, a small run-time buffer and a very lowgorithmic latency, at the same time producing a strong enhancement performance on a noisy-reverberant speech enhancement task even if the hop size is as low as 2 ms.\\nNotes: ##Author: Shinji Watanabe, ##Title: FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling\", metadata={'source': '../data/papers_metadata_csv/9cbd933c04218c9b642c15a49f8470d54524d9fb.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs', metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Yiming Yang?\\nAnswer: 35729970\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs', metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Yiming Yang?\\nAnswer: 64\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs', metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yiming Yang?\\nAnswer: Yiming Yang\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs', metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yiming Yang?\\nAnswer: https://www.semanticscholar.org/author/35729970\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs', metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Yiming Yang?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs', metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs?\\nAnswer: 6a42f6362afa3a1a0936f7a6a8927d04a2285cc5\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs', metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs?\\nAnswer: {'DBLP': 'journals/corr/abs-2307-12063', 'ArXiv': '2307.12063', 'DOI': '10.1109/IJCNN54540.2023.10190993', 'CorpusId': 260125530}\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs?\\nAnswer: https://www.semanticscholar.org/paper/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs', metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs'?\\nAnswer: Goal-Conditioned Hierarchical Reinforcement Learning (GCHRL) is a promising paradigm to address the exploration-exploitation dilemma in reinforcement learning. It decomposes the source task into sub goal conditional subtasks and conducts exploration and exploitation in the subgoal space. The effectiveness of GCHRL heavily relies on sub goal representation functions and sub goal selection strategy. However, existing works often overlook the temporal coherence in GCHRL when learning latent sub goal representations and lack an efficient sub goal selection strategy that balances exploration and exploitation. This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome these limitations. HILL learns latent subgoal representations that satisfy temporal coherence using a contrastive representation learning objective. Based on these representations, HILL dynamically builds latent landmark graphs and employs a novelty measure on nodes and a utility measure on edges. Finally, HILL develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures. Experimental results demonstrate that HILL outperforms state-of-the-art baselines on continuous control tasks with sparse rewards in sample efficiency and asymptotic performance. Our code is available at https://github.com/papercode2022/HILL.\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs' published?\\nAnswer: IEEE International Joint Conference on Neural Network\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs' published?\\nAnswer: 2023\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs'?\\nAnswer: 36\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs' have?\\nAnswer: 0\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs' have?\\nAnswer: 0\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs' have?\\nAnswer: 0\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs' open access?\\nAnswer: Yes\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs'?\\nAnswer: https://arxiv.org/pdf/2307.12063\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs'?\\nAnswer: Computer Science\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs'?\\nAnswer: 2023 International Joint Conference on Neural Networks (IJCNN), pages: 1-8; 2023 International Joint Conference on Neural Networks (IJCNN)\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs'?\\nAnswer: Qingyang Zhang, Yiming Yang, Jingqing Ruan, Xuantang Xiong, Dengpeng Xing, Bo Xu\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs'?\\nAnswer: This paper proposes HIerarchical reinforcement learning via dynamically building Latent Landmark graphs (HILL) to overcome limitations in GCHRL and develops a subgoal selection strategy that balances exploration and exploitation by jointly considering both measures.\\nNotes: ##Author: Yiming Yang, ##Title: Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs\", metadata={'source': '../data/papers_metadata_csv/6a42f6362afa3a1a0936f7a6a8927d04a2285cc5.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields', metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Eric P. Xing?\\nAnswer: 143977260\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields', metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Eric P. Xing?\\nAnswer: 103\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields', metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Eric P. Xing?\\nAnswer: E. Xing\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields', metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Eric P. Xing?\\nAnswer: https://www.semanticscholar.org/author/143977260\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields', metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Eric P. Xing?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields', metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields?\\nAnswer: 8cc1cd002bfc36a8cba8bcbe63d32eacc656097f\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields', metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields?\\nAnswer: {'DBLP': 'conf/cvpr/LiuZCZYELX23', 'ArXiv': '2303.10598', 'DOI': '10.1109/CVPR52729.2023.00806', 'CorpusId': 257632294}\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields?\\nAnswer: https://www.semanticscholar.org/paper/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields', metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields'?\\nAnswer: 3D style transfer aims to render stylized novel views of a 3D scene with multiview consistency. However, most existing work suffers from a three-way dilemma over accurate geometry reconstruction, high-quality stylization, and being generalizable to arbitrary new styles. We propose StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field. StyleRF employs an explicit grid of high-level features to represent 3D scenes, with which highfidelity geometry can be reliably restored via volume rendering. In addition, it transforms the grid features according to the reference style which directly leads to high-quality zero-shot style transfer. StyleRF consists of two innovative designs. The first is sampling-invariant content transformation that makes the transformation invariant to the holistic statistics of the sampled 3D points and accordingly ensures multi-view consistency. The second is deferred style transformation of 2D feature maps which is equivalent to the transformation of 3D points but greatly reduces memory footprint without degrading multi-view consistency. Extensive experiments show that StyleRF achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner. Project website: https://kunhao-liu.github.io/StyleRF/\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields' published?\\nAnswer: Computer Vision and Pattern Recognition\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields' published?\\nAnswer: 2023\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields'?\\nAnswer: 74\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields' have?\\nAnswer: 12\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields' have?\\nAnswer: 12\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields' have?\\nAnswer: 3\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields' open access?\\nAnswer: Yes\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields'?\\nAnswer: https://arxiv.org/pdf/2303.10598\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields'?\\nAnswer: Computer Science\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields'?\\nAnswer: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages: 8338-8348; 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields'?\\nAnswer: Kunhao Liu, Fangneng Zhan, Yiwen Chen, Jiahui Zhang, Yingchen Yu, Abdulmotaleb El Saddik, Shijian Lu, E. Xing\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields'?\\nAnswer: StyleRF (Style Radiance Fields), an innovative 3D style transfer technique that resolves the three-way dilemma by performing style transformation within the feature space of a radiance field, achieves superior 3D stylization quality with precise geometry reconstruction and it can generalize to various new styles in a zero-shot manner.\\nNotes: ##Author: Eric P. Xing, ##Title: StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields\", metadata={'source': '../data/papers_metadata_csv/8cc1cd002bfc36a8cba8bcbe63d32eacc656097f.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Reasoning about the Unseen for Efficient Outdoor Object Navigation\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation', metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Yonatan Bisk?\\nAnswer: 3312309\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation', metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Yonatan Bisk?\\nAnswer: 33\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation', metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yonatan Bisk?\\nAnswer: Yonatan Bisk\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation', metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yonatan Bisk?\\nAnswer: https://www.semanticscholar.org/author/3312309\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation', metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Yonatan Bisk?\\nAnswer: Carnegie Mellon University\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation', metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Reasoning about the Unseen for Efficient Outdoor Object Navigation?\\nAnswer: 8035a247980cb18abf2bb7b9d96e7d4c63622ef2\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation', metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Reasoning about the Unseen for Efficient Outdoor Object Navigation?\\nAnswer: {'ArXiv': '2309.10103', 'DBLP': 'journals/corr/abs-2309-10103', 'DOI': '10.48550/arXiv.2309.10103', 'CorpusId': 261945162}\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Reasoning about the Unseen for Efficient Outdoor Object Navigation?\\nAnswer: https://www.semanticscholar.org/paper/8035a247980cb18abf2bb7b9d96e7d4c63622ef2\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation', metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Reasoning about the Unseen for Efficient Outdoor Object Navigation'?\\nAnswer: Robots should exist anywhere humans do: indoors, outdoors, and even unmapped environments. In contrast, the focus of recent advancements in Object Goal Navigation(OGN) has targeted navigating in indoor environments by leveraging spatial and semantic cues that do not generalize outdoors. While these contributions provide valuable insights into indoor scenarios, the broader spectrum of real-world robotic applications often extends to outdoor settings. As we transition to the vast and complex terrains of outdoor environments, new challenges emerge. Unlike the structured layouts found indoors, outdoor environments lack clear spatial delineations and are riddled with inherent semantic ambiguities. Despite this, humans navigate with ease because we can reason about the unseen. We introduce a new task OUTDOOR, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain. Additionally, we show impressive results on both a simulated drone and physical quadruped in outdoor environments. Our agent has no premapping and our formalism outperforms naive LLM-based approaches\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Reasoning about the Unseen for Efficient Outdoor Object Navigation' published?\\nAnswer: arXiv.org\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Reasoning about the Unseen for Efficient Outdoor Object Navigation' published?\\nAnswer: 2023\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Reasoning about the Unseen for Efficient Outdoor Object Navigation'?\\nAnswer: 36\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Reasoning about the Unseen for Efficient Outdoor Object Navigation' have?\\nAnswer: 1\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Reasoning about the Unseen for Efficient Outdoor Object Navigation' have?\\nAnswer: 1\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Reasoning about the Unseen for Efficient Outdoor Object Navigation' have?\\nAnswer: 1\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Reasoning about the Unseen for Efficient Outdoor Object Navigation' open access?\\nAnswer: Yes\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Reasoning about the Unseen for Efficient Outdoor Object Navigation'?\\nAnswer: https://arxiv.org/pdf/2309.10103\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Reasoning about the Unseen for Efficient Outdoor Object Navigation'?\\nAnswer: Computer Science\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Reasoning about the Unseen for Efficient Outdoor Object Navigation'?\\nAnswer: ArXiv, volume: abs/2309.10103; ArXiv\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Reasoning about the Unseen for Efficient Outdoor Object Navigation'?\\nAnswer: Quanting Xie, Tianyi Zhang, Kedi Xu, M. Johnson-Roberson, Yonatan Bisk\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Reasoning about the Unseen for Efficient Outdoor Object Navigation'?\\nAnswer: A new task OUTDOOR is introduced, a new mechanism for Large Language Models (LLMs) to accurately hallucinate possible futures, and a new computationally aware success metric for pushing research forward in this more complex domain are introduced.\\nNotes: ##Author: Yonatan Bisk, ##Title: Reasoning about the Unseen for Efficient Outdoor Object Navigation\", metadata={'source': '../data/papers_metadata_csv/8035a247980cb18abf2bb7b9d96e7d4c63622ef2.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval', metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Shinji Watanabe?\\nAnswer: 1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval', metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Shinji Watanabe?\\nAnswer: 67\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval', metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval', metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: https://www.semanticscholar.org/author/1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval', metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Shinji Watanabe?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval', metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval?\\nAnswer: 00542e510058b11d1faf612de9b45fa0d4d3f4e5\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval', metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval?\\nAnswer: {'PubMedCentral': '10114312', 'DOI': '10.1016/j.isci.2023.106694', 'CorpusId': 258239706, 'PubMed': '37124417'}\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval?\\nAnswer: https://www.semanticscholar.org/paper/00542e510058b11d1faf612de9b45fa0d4d3f4e5\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval', metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval'?\\nAnswer: None\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval' published?\\nAnswer: iScience\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval' published?\\nAnswer: 2023\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval'?\\nAnswer: 39\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval' have?\\nAnswer: 1\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval' have?\\nAnswer: 1\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval' open access?\\nAnswer: Yes\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval'?\\nAnswer: http://www.cell.com/article/S258900422300771X/pdf\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval'?\\nAnswer: Medicine\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval'?\\nAnswer: iScience, volume: 26; iScience\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval'?\\nAnswer: Sho Miyamoto, Y. Kuroda, T. Kanno, A. Ueno, N. Shiwa-Sudo, N. Iwata-Yoshikawa, Yusuke Sakai, N. Nagata, T. Arashiro, A. Ainai, Saya Moriyama, N. Kishida, Shinji Watanabe, K. Nojima, Y. Seki, T. Mizukami, H. Hasegawa, H. Ebihara, S. Fukushi, Yoshimasa Takahashi, Maeda Ken, Tadaki Suzuki\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval'?\\nAnswer: The results highlight the importance of vaccine dosage intervals of 4 months or longer, regardless of the antigenicity of the exposed antigen, to maximize the breadth of serum cross-neutralization covering SARS-CoV-2 Omicron lineages.\\nNotes: ##Author: Shinji Watanabe, ##Title: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval\", metadata={'source': '../data/papers_metadata_csv/00542e510058b11d1faf612de9b45fa0d4d3f4e5.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Shinji Watanabe?\\nAnswer: 1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Shinji Watanabe?\\nAnswer: 67\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: https://www.semanticscholar.org/author/1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Shinji Watanabe?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation?\\nAnswer: 6c33625c7b0ffc37955921a145531d9d4eaee713\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation?\\nAnswer: {'DBLP': 'journals/corr/abs-2307-12231', 'ArXiv': '2307.12231', 'DOI': '10.1109/WASPAA58266.2023.10248096', 'CorpusId': 260125023}\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation?\\nAnswer: https://www.semanticscholar.org/paper/6c33625c7b0ffc37955921a145531d9d4eaee713\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation', metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation'?\\nAnswer: Neural speech separation has made remarkable progress and its integration with automatic speech recognition (ASR) is an important direction towards realizing multi-speaker ASR. This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end. In detail, we explore multi-channel separation methods, mask-based beamforming and complex spectral mapping, as well as the best features to use in the ASR back-end model. We employ the recent self-supervised learning representation (SSLR) as a feature and improve the recognition performance from the case with filterbank features. To further improve multi-speaker recognition performance, we present a carefully designed training strategy for integrating speech separation and recognition with SSLR. The proposed integration using TF-GridNet-based complex spectral mapping and WavLM-based SSLR achieves a 2.5% word error rate in reverberant WHAMR! test set, significantly outperforming an existing mask-based MVDR beamforming and filterbank integration (28.9%).\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation' published?\\nAnswer: IEEE Workshop on Applications of Signal Processing to Audio and Acoustics\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation' published?\\nAnswer: 2023\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation'?\\nAnswer: 46\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation' have?\\nAnswer: 1\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation' have?\\nAnswer: 1\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation' open access?\\nAnswer: Yes\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation'?\\nAnswer: https://arxiv.org/pdf/2307.12231\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation'?\\nAnswer: Computer Science, Engineering\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation'?\\nAnswer: 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), pages: 1-5; 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation'?\\nAnswer: Yoshiki Masuyama, Xuankai Chang, Wangyou Zhang, Samuele Cornell, Zhongqiu Wang, Nobutaka Ono, Y. Qian, Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation'?\\nAnswer: This work provides an insightful investigation of speech separation in reverberant and noisy-reverberant scenarios as an ASR front-end, and employs the recent self-supervised learning representation (SSLR) as a feature and improves the recognition performance from the case with filterbank features.\\nNotes: ##Author: Shinji Watanabe, ##Title: Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation\", metadata={'source': '../data/papers_metadata_csv/6c33625c7b0ffc37955921a145531d9d4eaee713.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: KIT’s Multilingual Speech Translation System for IWSLT 2023\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023', metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Alexander Waibel?\\nAnswer: 1724972\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023', metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Alexander Waibel?\\nAnswer: 80\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023', metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Alexander Waibel?\\nAnswer: A. Waibel\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023', metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Alexander Waibel?\\nAnswer: https://www.semanticscholar.org/author/1724972\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023', metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Alexander Waibel?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023', metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper KIT’s Multilingual Speech Translation System for IWSLT 2023?\\nAnswer: 610d9958390ab83515d0d81e19f8e5264faf8e9b\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023', metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper KIT’s Multilingual Speech Translation System for IWSLT 2023?\\nAnswer: {'ACL': '2023.iwslt-1.6', 'DBLP': 'conf/iwslt/LiuNKUPNDMWN23', 'ArXiv': '2306.05320', 'DOI': '10.48550/arXiv.2306.05320', 'CorpusId': 259108961}\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper KIT’s Multilingual Speech Translation System for IWSLT 2023?\\nAnswer: https://www.semanticscholar.org/paper/610d9958390ab83515d0d81e19f8e5264faf8e9b\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023', metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'KIT’s Multilingual Speech Translation System for IWSLT 2023'?\\nAnswer: Many existing speech translation benchmarks focus on native-English speech in high-quality recording conditions, which often do not match the conditions in real-life use-cases. In this paper, we describe our speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks. The test condition features accented input speech and terminology-dense contents. The tasks requires translation into 10 languages of varying amounts of resources. In absence of training data from the target domain, we use a retrieval-based approach (kNN-MT) for effective adaptation (+0.8 BLEU for speech translation). We also use adapters to easily integrate incremental training data from data augmentation, and show that it matches the performance of re-training. We observe that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules. Our cascaded speech system outperforms its end-to-end counterpart on scientific talk translation, although their performance remains similar on TED talks.\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'KIT’s Multilingual Speech Translation System for IWSLT 2023' published?\\nAnswer: International Workshop on Spoken Language Translation\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'KIT’s Multilingual Speech Translation System for IWSLT 2023' published?\\nAnswer: 2023\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'KIT’s Multilingual Speech Translation System for IWSLT 2023'?\\nAnswer: 41\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'KIT’s Multilingual Speech Translation System for IWSLT 2023' have?\\nAnswer: 2\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'KIT’s Multilingual Speech Translation System for IWSLT 2023' have?\\nAnswer: 2\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'KIT’s Multilingual Speech Translation System for IWSLT 2023' have?\\nAnswer: 0\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'KIT’s Multilingual Speech Translation System for IWSLT 2023' open access?\\nAnswer: Yes\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'KIT’s Multilingual Speech Translation System for IWSLT 2023'?\\nAnswer: https://arxiv.org/pdf/2306.05320\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'KIT’s Multilingual Speech Translation System for IWSLT 2023'?\\nAnswer: Computer Science\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'KIT’s Multilingual Speech Translation System for IWSLT 2023'?\\nAnswer: ArXiv, volume: abs/2306.05320; ArXiv\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'KIT’s Multilingual Speech Translation System for IWSLT 2023'?\\nAnswer: Danni Liu, T. Nguyen, Sai Koneru, Enes Yavuz Ugan, Ngoc-Quan Pham, Tuan-Nam Nguyen, Tu Anh Dinh, Carlos Mullov, A. Waibel, J. Niehues\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'KIT’s Multilingual Speech Translation System for IWSLT 2023'?\\nAnswer: This paper describes the speech translation system for the multilingual track of IWSLT 2023, which focuses on the translation of scientific conference talks, and observes that cascaded systems are more easily adaptable towards specific target domains, due to their separate modules.\\nNotes: ##Author: Alexander Waibel, ##Title: KIT’s Multilingual Speech Translation System for IWSLT 2023\", metadata={'source': '../data/papers_metadata_csv/610d9958390ab83515d0d81e19f8e5264faf8e9b.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms', metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Jamie Callan?\\nAnswer: 144987107\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms', metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Jamie Callan?\\nAnswer: 75\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms', metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Jamie Callan?\\nAnswer: Jamie Callan\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms', metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Jamie Callan?\\nAnswer: https://www.semanticscholar.org/author/144987107\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms', metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Jamie Callan?\\nAnswer: Carnegie Mellon University\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms', metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms?\\nAnswer: 6b7eefa15c0a461afeab4fa13cf862c5340fdc2a\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms', metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms?\\nAnswer: {'DBLP': 'conf/ictir/0003GC23', 'DOI': '10.1145/3578337.3605126', 'CorpusId': 260736470}\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms?\\nAnswer: https://www.semanticscholar.org/paper/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms', metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 8}),\n"," Document(page_content='Question: What is the abstract of the paper \\'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\\'?\\nAnswer: Lexical exact-match systems perform text retrieval efficiently with sparse matching signals and fast retrieval through inverted lists, but naturally suffer from the mismatch between lexical surface form and implicit term semantics. This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF). Each CSF pairs a lexical surface form with a context source, and is represented by a lexical form weight and a contextualized semantic vector representation. This framework is able to perform sparse lexicon-based retrieval by learning to represent each query and document as a \"bag-of-CSFs\", simultaneously addressing two key factors in sparse retrieval: vocabulary expansion of surface form and semantic representation of term meaning. At retrieval time, it efficiently matches CSFs through exact-match of learned surface forms, and effectively scores each CSF pair via contextual semantic representations, leading to joint improvement in both term match and term scoring. Multiple experiments show that this approach successfully resolves the main mismatch issues in lexical exact-match retrieval and outperforms state-of-the-art lexical exact-match systems, reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact-match-based system.\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms', metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms' published?\\nAnswer: International Conference on the Theory of Information Retrieval\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms' published?\\nAnswer: 2023\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms'?\\nAnswer: 52\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms' have?\\nAnswer: 0\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms' have?\\nAnswer: 0\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms' have?\\nAnswer: 0\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms' open access?\\nAnswer: Yes\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms'?\\nAnswer: https://dl.acm.org/doi/pdf/10.1145/3578337.3605126\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms'?\\nAnswer: Computer Science\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms'?\\nAnswer: Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms'?\\nAnswer: Zhen Fan, Luyu Gao, Jamie Callan\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms'?\\nAnswer: This paper proposes to directly bridge the surface form space and the term semantics space in lexical exact-match retrieval via contextualized surface forms (CSF), reaching comparable accuracy as lexical all-to-all soft match systems as an efficient exact- match-based system.\\nNotes: ##Author: Jamie Callan, ##Title: CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms\", metadata={'source': '../data/papers_metadata_csv/6b7eefa15c0a461afeab4fa13cf862c5340fdc2a.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4', metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Alexander Rudnicky?\\nAnswer: 1783635\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4', metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Alexander Rudnicky?\\nAnswer: 42\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4', metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Alexander Rudnicky?\\nAnswer: Alexander I. Rudnicky\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4', metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Alexander Rudnicky?\\nAnswer: https://www.semanticscholar.org/author/1783635\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4', metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Alexander Rudnicky?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4', metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4?\\nAnswer: 9799c17fd287bb9e8d231fe032c6dbf9c0c9d675\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4', metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4?\\nAnswer: {'ArXiv': '2306.12794', 'DBLP': 'journals/corr/abs-2306-12794', 'ACL': '2023.dstc-1.28', 'DOI': '10.48550/arXiv.2306.12794', 'CorpusId': 259224700}\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4?\\nAnswer: https://www.semanticscholar.org/paper/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4', metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4'?\\nAnswer: The advent and fast development of neural networks have revolutionized the research on dialogue systems and subsequently have triggered various challenges regarding their automatic evaluation. Automatic evaluation of open-domain dialogue systems as an open challenge has been the center of the attention of many researchers. Despite the consistent efforts to improve automatic metrics’ correlations with human evaluation, there have been very few attempts to assess their robustness over multiple domains and dimensions. Also, their focus is mainly on the English language. All of these challenges prompt the development of automatic evaluation metrics that are reliable in various domains, dimensions, and languages. This track in the 11th Dialogue System Technology Challenge (DSTC11) is part of the ongoing effort to promote robust and multilingual automatic evaluation metrics. This article describes the datasets and baselines provided to participants and discusses the submission and result details of the two proposed subtasks.\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4' published?\\nAnswer: DSTC\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4' published?\\nAnswer: 2023\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4'?\\nAnswer: 71\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4' have?\\nAnswer: 3\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4' have?\\nAnswer: 3\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4' have?\\nAnswer: 0\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4' open access?\\nAnswer: Yes\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4'?\\nAnswer: https://arxiv.org/pdf/2306.12794\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4'?\\nAnswer: Computer Science\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4'?\\nAnswer: ArXiv, volume: abs/2306.12794; ArXiv\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4'?\\nAnswer: Mario Rodr'iguez-Cantelar, Chen Zhang, Chengguang Tang, Ke Shi, Sarik Ghazarian, Joao Sedoc, L. F. D'Haro, Alexander I. Rudnicky\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4'?\\nAnswer: The datasets and baselines provided to participants are described and the submission and result details of the two proposed subtasks are discussed, which promote robust and multilingual automatic evaluation metrics.\\nNotes: ##Author: Alexander Rudnicky, ##Title: Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4\", metadata={'source': '../data/papers_metadata_csv/9799c17fd287bb9e8d231fe032c6dbf9c0c9d675.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: A corpus of Persian literary text\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text', metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Malihe Alikhani?\\nAnswer: 2715920\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text', metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Malihe Alikhani?\\nAnswer: 12\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text', metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Malihe Alikhani?\\nAnswer: Malihe Alikhani\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text', metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Malihe Alikhani?\\nAnswer: https://www.semanticscholar.org/author/2715920\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text', metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Malihe Alikhani?\\nAnswer: Rutgers University\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text', metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper A corpus of Persian literary text?\\nAnswer: ee866d0dd47351542e4924f14203a767dd03194a\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text', metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper A corpus of Persian literary text?\\nAnswer: {'DOI': '10.1007/s10579-023-09689-6', 'CorpusId': 265419296}\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper A corpus of Persian literary text?\\nAnswer: https://www.semanticscholar.org/paper/ee866d0dd47351542e4924f14203a767dd03194a\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text', metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'A corpus of Persian literary text'?\\nAnswer: None\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'A corpus of Persian literary text' published?\\nAnswer: Language Resources and Evaluation\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'A corpus of Persian literary text' published?\\nAnswer: 2023\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'A corpus of Persian literary text'?\\nAnswer: 35\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'A corpus of Persian literary text' have?\\nAnswer: 0\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'A corpus of Persian literary text' have?\\nAnswer: 0\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'A corpus of Persian literary text' have?\\nAnswer: 0\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'A corpus of Persian literary text' open access?\\nAnswer: Yes\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'A corpus of Persian literary text'?\\nAnswer: https://link.springer.com/content/pdf/10.1007/s10579-023-09689-6.pdf\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'A corpus of Persian literary text'?\\nAnswer: No fields of study available\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'A corpus of Persian literary text'?\\nAnswer: Language Resources and Evaluation, volume: , pages: 1-17; Language Resources and Evaluation\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'A corpus of Persian literary text'?\\nAnswer: Shahab Raji, Malihe Alikhani, Gerard de Melo, Matthew Stone\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'A corpus of Persian literary text'?\\nAnswer: The corpus, the tools, and experiments described in this paper can be used not only for digital humanities studies of Persian literature but also for processing Persian texts in general, as well as in other broader cross-linguistic applications.\\nNotes: ##Author: Malihe Alikhani, ##Title: A corpus of Persian literary text\", metadata={'source': '../data/papers_metadata_csv/ee866d0dd47351542e4924f14203a767dd03194a.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Conversational Search with Random Walks over Entity Graphs\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs', metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Jamie Callan?\\nAnswer: 144987107\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs', metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Jamie Callan?\\nAnswer: 75\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs', metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Jamie Callan?\\nAnswer: Jamie Callan\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs', metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Jamie Callan?\\nAnswer: https://www.semanticscholar.org/author/144987107\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs', metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Jamie Callan?\\nAnswer: Carnegie Mellon University\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs', metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Conversational Search with Random Walks over Entity Graphs?\\nAnswer: 197d5fbc3764ff18186275545d0764d5b1c7659b\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs', metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Conversational Search with Random Walks over Entity Graphs?\\nAnswer: {'DBLP': 'conf/ictir/GoncalvesMC23', 'DOI': '10.1145/3578337.3605125', 'CorpusId': 260736623}\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Conversational Search with Random Walks over Entity Graphs?\\nAnswer: https://www.semanticscholar.org/paper/197d5fbc3764ff18186275545d0764d5b1c7659b\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs', metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Conversational Search with Random Walks over Entity Graphs'?\\nAnswer: The entities that emerge during a conversation can be used to model topics, but not all entities are equally useful for this task. Modeling the conversation with entity graphs and predicting each entity's centrality in the conversation provides additional information that improves the retrieval of answer passages for the current question. Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Conversational Search with Random Walks over Entity Graphs' published?\\nAnswer: International Conference on the Theory of Information Retrieval\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Conversational Search with Random Walks over Entity Graphs' published?\\nAnswer: 2023\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Conversational Search with Random Walks over Entity Graphs'?\\nAnswer: 49\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Conversational Search with Random Walks over Entity Graphs' have?\\nAnswer: 1\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Conversational Search with Random Walks over Entity Graphs' have?\\nAnswer: 1\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Conversational Search with Random Walks over Entity Graphs' have?\\nAnswer: 0\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Conversational Search with Random Walks over Entity Graphs' open access?\\nAnswer: Yes\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Conversational Search with Random Walks over Entity Graphs'?\\nAnswer: https://dl.acm.org/doi/pdf/10.1145/3578337.3605125\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Conversational Search with Random Walks over Entity Graphs'?\\nAnswer: Computer Science\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Conversational Search with Random Walks over Entity Graphs'?\\nAnswer: Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Conversational Search with Random Walks over Entity Graphs'?\\nAnswer: Gustavo Goncalves, Joao Magalhaes, Jamie Callan\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Conversational Search with Random Walks over Entity Graphs'?\\nAnswer: Experiments show that using random walks to estimate entity centrality on conversation entity graphs improves top precision answer passage ranking over competitive transformer-based baselines.\\nNotes: ##Author: Jamie Callan, ##Title: Conversational Search with Random Walks over Entity Graphs\", metadata={'source': '../data/papers_metadata_csv/197d5fbc3764ff18186275545d0764d5b1c7659b.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation', metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of David R Mortensen?\\nAnswer: 3407646\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation', metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of David R Mortensen?\\nAnswer: 15\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation', metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of David R Mortensen?\\nAnswer: David R. Mortensen\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation', metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of David R Mortensen?\\nAnswer: https://www.semanticscholar.org/author/3407646\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation', metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of David R Mortensen?\\nAnswer: Carnegie Mellon University\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation', metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation?\\nAnswer: bf42c0462d1415cdde877c90d58da11545407b8a\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation', metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation?\\nAnswer: {'ACL': '2023.sigmorphon-1.7', 'DBLP': 'conf/sigmorphon/MortensenGHRATL23', 'DOI': '10.18653/v1/2023.sigmorphon-1.7', 'CorpusId': 259833816}\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation?\\nAnswer: https://www.semanticscholar.org/paper/bf42c0462d1415cdde877c90d58da11545407b8a\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation', metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation'?\\nAnswer: Interlinear glossing provides a vital type of morphosyntactic annotation, both for linguists and language revitalists, and numerous conventions exist for representing it formally and computationally. Some of these formats are human readable; others are machine readable. Some are easy to edit with general-purpose tools. Few represent non-concatentative processes like infixation, reduplication, mutation, truncation, and tonal overwriting in a consistent and formally rigorous way (on par with affixation). We propose an annotation conventionâ€”Generalized Glossing Guidelines (GGG) that combines all of these positive properties using an Item-and-Process (IP) framework. We describe the format, demonstrate its linguistic adequacy, and compare it with two other interlinear glossed text annotation schemes.\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation' published?\\nAnswer: Special Interest Group on Computational Morphology and Phonology Workshop\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation' published?\\nAnswer: 2023\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation'?\\nAnswer: 19\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation' have?\\nAnswer: 0\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation' have?\\nAnswer: 0\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation' have?\\nAnswer: 0\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation' open access?\\nAnswer: Yes\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation'?\\nAnswer: https://aclanthology.org/2023.sigmorphon-1.7.pdf\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation'?\\nAnswer: Computer Science\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation'?\\nAnswer: Special Interest Group on Computational Morphology and Phonology Workshop, pages: 58-67; Special Interest Group on Computational Morphology and Phonology Workshop\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation'?\\nAnswer: David R. Mortensen, Ela Gulsen, Taiqi He, Nathaniel R. Robinson, Jonathan D. Amith, Lindia Tjuatja, L. Levin\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation'?\\nAnswer: An annotation convention is proposed that combines all of these positive properties using an Item-and-Process (IP) framework, and its linguistic adequacy is demonstrated, and it is compared with two other interlinear glossed text annotation schemes.\\nNotes: ##Author: David R Mortensen, ##Title: Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation\", metadata={'source': '../data/papers_metadata_csv/bf42c0462d1415cdde877c90d58da11545407b8a.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN', metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Shinji Watanabe?\\nAnswer: 1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN', metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Shinji Watanabe?\\nAnswer: 67\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN', metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN', metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: https://www.semanticscholar.org/author/1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN', metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Shinji Watanabe?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN', metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN?\\nAnswer: f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN', metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN?\\nAnswer: {'ACL': '2023.iwslt-1.1', 'DBLP': 'conf/iwslt/AgrawalABBBCCCC23', 'DOI': '10.18653/v1/2023.iwslt-1.1', 'CorpusId': 259376816}\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN?\\nAnswer: https://www.semanticscholar.org/paper/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN', metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN'?\\nAnswer: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN' published?\\nAnswer: International Workshop on Spoken Language Translation\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN' published?\\nAnswer: 2023\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN'?\\nAnswer: 156\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN' have?\\nAnswer: 22\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN' have?\\nAnswer: 22\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN' open access?\\nAnswer: Yes\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN'?\\nAnswer: https://aclanthology.org/2023.iwslt-1.1.pdf\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN'?\\nAnswer: Computer Science\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN'?\\nAnswer: International Workshop on Spoken Language Translation, pages: 1-61; International Workshop on Spoken Language Translation\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN'?\\nAnswer: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Esteve, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, David Javorsky, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Polak, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stuker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, M. Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos\\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN'?\\nAnswer: \\nNotes: ##Author: Shinji Watanabe, ##Title: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\", metadata={'source': '../data/papers_metadata_csv/f7e995c3cae465963ecaa8c9b4ce8b9b4323a71b.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals', metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Tom Mitchell?\\nAnswer: 40975594\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals', metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Tom Mitchell?\\nAnswer: 81\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals', metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Tom Mitchell?\\nAnswer: Tom Michael Mitchell\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals', metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Tom Mitchell?\\nAnswer: https://www.semanticscholar.org/author/40975594\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals', metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Tom Mitchell?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals', metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals?\\nAnswer: 61678a9f1d8291bb0f3d704a439ac8cd64fa6482\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals', metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals?\\nAnswer: {'ArXiv': '2302.04449', 'DBLP': 'journals/corr/abs-2302-04449', 'DOI': '10.48550/arXiv.2302.04449', 'CorpusId': 256697185}\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals?\\nAnswer: https://www.semanticscholar.org/paper/61678a9f1d8291bb0f3d704a439ac8cd64fa6482\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals', metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals'?\\nAnswer: High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary reward is then provided to a standard A2C RL agent, when interaction is detected. Experimentally, various RL algorithms obtain significant improvement in performance and training speed when assisted by our design.\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals' published?\\nAnswer: arXiv.org\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals' published?\\nAnswer: 2023\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals'?\\nAnswer: 72\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals' have?\\nAnswer: 15\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals' have?\\nAnswer: 15\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals' have?\\nAnswer: 3\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals' open access?\\nAnswer: Yes\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals'?\\nAnswer: http://arxiv.org/pdf/2302.04449\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals'?\\nAnswer: Computer Science\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals'?\\nAnswer: ArXiv, volume: abs/2302.04449; ArXiv\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals'?\\nAnswer: Yue Wu, Yewen Fan, P. Liang, A. Azaria, Yuan-Fang Li, Tom Michael Mitchell\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals'?\\nAnswer: It is hypothesized that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent in the Read and Reward framework.\\nNotes: ##Author: Tom Mitchell, ##Title: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\", metadata={'source': '../data/papers_metadata_csv/61678a9f1d8291bb0f3d704a439ac8cd64fa6482.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge', metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Shinji Watanabe?\\nAnswer: 1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge', metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Shinji Watanabe?\\nAnswer: 67\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge', metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge', metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: https://www.semanticscholar.org/author/1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge', metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Shinji Watanabe?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge', metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge?\\nAnswer: dd5d797b837005fac464bb19b9396bddba61c0d8\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge', metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge?\\nAnswer: {'DBLP': 'journals/corr/abs-2302-07928', 'ArXiv': '2302.07928', 'DOI': '10.48550/arXiv.2302.07928', 'CorpusId': 256900671}\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge?\\nAnswer: https://www.semanticscholar.org/paper/dd5d797b837005fac464bb19b9396bddba61c0d8\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge', metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge'?\\nAnswer: This paper describes our submission to the Second Clarity Enhancement Challenge (CEC2), which consists of target speech enhancement for hearing-aid (HA) devices in noisy-reverberant environments with multiple interferers such as music and competing speakers. Our approach builds upon the powerful iterative neural/beamforming enhancement (iNeuBe) framework introduced in our recent work, and this paper extends it for target speaker extraction. We therefore name the proposed approach as iNeuBe-X, where the X stands for extraction. To address the challenges encountered in the CEC2 setting, we introduce four major novelties: (1) we extend the state-of-the-art TF-GridNet model, originally designed for monaural speaker separation, for multi-channel, causal speech enhancement, and large improvements are observed by replacing the TCNDenseNet used in iNeuBe with this new architecture; (2) we leverage a recent dual window size approach with future-frame prediction to ensure that iNueBe-X satisfies the 5 ms constraint on algorithmic latency required by CEC2; (3) we introduce a novel speaker-conditioning branch for TF-GridNet to achieve target speaker extraction; (4) we propose a fine-tuning step, where we compute an additional loss with respect to the target speaker signal compensated with the listener audiogram. Without using external data, on the official development set our best model reaches a hearing-aid speech perception index (HASPI) score of 0.942 and a scale-invariant signal-to-distortion ratio improvement (SI-SDRi) of 18.8 dB. These results are promising given the fact that the CEC2 data is extremely challenging (e.g., on the development set the mixture SI-SDR is -12.3 dB). A demo of our submitted system is available at WAVLab CEC2 demo.\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge' published?\\nAnswer: arXiv.org\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge' published?\\nAnswer: 2023\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge'?\\nAnswer: 14\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge' have?\\nAnswer: 5\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge' have?\\nAnswer: 5\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge' open access?\\nAnswer: Yes\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge'?\\nAnswer: http://arxiv.org/pdf/2302.07928\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge'?\\nAnswer: Engineering, Computer Science\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge'?\\nAnswer: ArXiv, volume: abs/2302.07928; ArXiv\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge'?\\nAnswer: Samuele Cornell, Zhongqiu Wang, Yoshiki Masuyama, Shinji Watanabe, Manuel Pariente, Nobutaka Ono\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge'?\\nAnswer: The approach builds upon the powerful iterative neural/beamforming enhancement (iNeuBe) framework introduced in recent work, and this paper extends it for target speaker extraction, and is named as iNeu be-X, where the X stands for extraction.\\nNotes: ##Author: Shinji Watanabe, ##Title: Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge\", metadata={'source': '../data/papers_metadata_csv/dd5d797b837005fac464bb19b9396bddba61c0d8.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials', metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Madhavi Ganapathiraju?\\nAnswer: 32747279\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials', metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Madhavi Ganapathiraju?\\nAnswer: 20\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials', metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Madhavi Ganapathiraju?\\nAnswer: M. Ganapathiraju\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials', metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Madhavi Ganapathiraju?\\nAnswer: https://www.semanticscholar.org/author/32747279\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials', metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Madhavi Ganapathiraju?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials', metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials?\\nAnswer: de52a2f746c5cf145ee2af3a978ee1942eec1a57\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials', metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials?\\nAnswer: {'DOI': '10.3389/fsysb.2023.1293298', 'CorpusId': 263319943}\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials?\\nAnswer: https://www.semanticscholar.org/paper/de52a2f746c5cf145ee2af3a978ee1942eec1a57\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials', metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials'?\\nAnswer: None\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials' published?\\nAnswer: Frontiers in Systems Biology\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials' published?\\nAnswer: 2023\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials'?\\nAnswer: 6\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials' have?\\nAnswer: 0\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials' have?\\nAnswer: 0\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials' have?\\nAnswer: 0\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials' open access?\\nAnswer: Yes\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials'?\\nAnswer: https://www.frontiersin.org/articles/10.3389/fsysb.2023.1293298/pdf?isPublishedV2=False\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials'?\\nAnswer: No fields of study available\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials'?\\nAnswer: Frontiers in Systems Biology\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials'?\\nAnswer: Jane A. Leopold, M. Ganapathiraju, N. Yanamala\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials'?\\nAnswer: TLDR not found\\nNotes: ##Author: Madhavi Ganapathiraju, ##Title: Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\", metadata={'source': '../data/papers_metadata_csv/de52a2f746c5cf145ee2af3a978ee1942eec1a57.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Structured Dialogue Discourse Parsing\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing', metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Alexander Rudnicky?\\nAnswer: 1783635\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing', metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Alexander Rudnicky?\\nAnswer: 42\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing', metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Alexander Rudnicky?\\nAnswer: Alexander I. Rudnicky\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing', metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Alexander Rudnicky?\\nAnswer: https://www.semanticscholar.org/author/1783635\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing', metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Alexander Rudnicky?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing', metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Structured Dialogue Discourse Parsing?\\nAnswer: 2670612b5e11297cd9b98f4d7ff796725f77fe35\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing', metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Structured Dialogue Discourse Parsing?\\nAnswer: {'DBLP': 'conf/sigdial/ChiR22', 'ArXiv': '2306.15103', 'ACL': '2022.sigdial-1.32', 'DOI': '10.48550/arXiv.2306.15103', 'CorpusId': 252847494}\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Structured Dialogue Discourse Parsing?\\nAnswer: https://www.semanticscholar.org/paper/2670612b5e11297cd9b98f4d7ff796725f77fe35\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing', metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Structured Dialogue Discourse Parsing'?\\nAnswer: Dialogue discourse parsing aims to uncover the internal structure of a multi-participant conversation by finding all the discourse links and corresponding relations. Previous work either treats this task as a series of independent multiple-choice problems, in which the link existence and relations are decoded separately, or the encoding is restricted to only local interaction, ignoring the holistic structural information. In contrast, we propose a principled method that improves upon previous work from two perspectives: encoding and decoding. From the encoding side, we perform structured encoding on the adjacency matrix followed by the matrix-tree learning algorithm, where all discourse links and relations in the dialogue are jointly optimized based on latent tree-level distribution. From the decoding side, we perform structured inference using the modified Chiu-Liu-Edmonds algorithm, which explicitly generates the labeled multi-root non-projective spanning tree that best captures the discourse structure. In addition, unlike in previous work, we do not rely on hand-crafted features; this improves the model’s robustness. Experiments show that our method achieves new state-of-the-art, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Structured Dialogue Discourse Parsing' published?\\nAnswer: SIGDIAL Conferences\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Structured Dialogue Discourse Parsing' published?\\nAnswer: 2023\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Structured Dialogue Discourse Parsing'?\\nAnswer: 64\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Structured Dialogue Discourse Parsing' have?\\nAnswer: 4\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Structured Dialogue Discourse Parsing' have?\\nAnswer: 4\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Structured Dialogue Discourse Parsing' have?\\nAnswer: 1\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Structured Dialogue Discourse Parsing' open access?\\nAnswer: Yes\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Structured Dialogue Discourse Parsing'?\\nAnswer: http://arxiv.org/pdf/2306.15103\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Structured Dialogue Discourse Parsing'?\\nAnswer: Computer Science\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Structured Dialogue Discourse Parsing'?\\nAnswer: ArXiv, volume: abs/2306.15103; ArXiv\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Structured Dialogue Discourse Parsing'?\\nAnswer: Ta-Chung Chi, Alexander I. Rudnicky\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Structured Dialogue Discourse Parsing'?\\nAnswer: This work proposes a principled method that improves upon previous work from two perspectives: encoding and decoding and achieves new state-of-the-art results, surpassing the previous model by 2.3 on STAC and 1.5 on Molweni (F1 scores).\\nNotes: ##Author: Alexander Rudnicky, ##Title: Structured Dialogue Discourse Parsing\", metadata={'source': '../data/papers_metadata_csv/2670612b5e11297cd9b98f4d7ff796725f77fe35.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses', metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Brian MacWhinney?\\nAnswer: 2414040\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses', metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Brian MacWhinney?\\nAnswer: 75\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses', metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Brian MacWhinney?\\nAnswer: B. MacWhinney\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses', metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Brian MacWhinney?\\nAnswer: https://www.semanticscholar.org/author/2414040\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses', metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Brian MacWhinney?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses', metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses?\\nAnswer: f7254ae607056ba5522c10dbcf21b394967b6d42\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses', metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses?\\nAnswer: {'PubMedCentral': '10171844', 'DOI': '10.1044/2022_AJSLP-22-00281', 'CorpusId': 256899351, 'PubMed': '36791255'}\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses?\\nAnswer: https://www.semanticscholar.org/paper/f7254ae607056ba5522c10dbcf21b394967b6d42\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses', metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses'?\\nAnswer: Purpose: Dementia from Alzheimer's disease (AD) is characterized primarily by a significant decline in memory abilities; however, language abilities are also commonly affected and may precede the decline of other cognitive abilities. To study the progression of language, there is a need for open-access databases that can be used to build algorithms to produce translational models sensitive enough to detect early declines in language abilities. DementiaBank is an open-access repository of transcribed video/audio data from communicative interactions from people with dementia, mild cognitive impairment (MCI), and controls. The aims of this tutorial are to (a) describe the newly established standardized DementiaBank discourse protocol, (b) describe the Delaware corpus data, and (c) provide examples of automated linguistic analyses that can be conducted with the Delaware corpus data and describe additional DementiaBank resources. Method: The DementiaBank discourse protocol elicits four types of discourse: picture description, story narrative, procedural, and personal narrative. The Delaware corpus currently includes data from 20 neurotypical adults and 33 adults with MCI from possible AD who completed the DementiaBank discourse protocol and a cognitive–linguistic battery. Language samples were video- and audio-recorded, transcribed, coded, and uploaded to DementiaBank. The protocol materials and transcription programs can be accessed for free via the DementiaBank website. Results: Illustrative analyses show the potential of the Delaware corpus data to help understand discourse metrics at the individual and group levels. In addition, they highlight analyses that could be used across TalkBank's other clinical banks (e.g., AphasiaBank). Information is also included on manual and automatic speech recognition transcription methods. Conclusions: DementiaBank is a shared online database that can facilitate research efforts to address the gaps in knowledge about language changes associated with MCI and dementia from AD. Identifying early language markers could lead to improved assessment and treatment approaches for adults at risk for dementia.\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses' published?\\nAnswer: American Journal of Speech-Language Pathology\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses' published?\\nAnswer: 2023\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses'?\\nAnswer: 82\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses' have?\\nAnswer: 8\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses' have?\\nAnswer: 8\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses' have?\\nAnswer: 0\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses' open access?\\nAnswer: Yes\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses'?\\nAnswer: openAccessPdf not available\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses'?\\nAnswer: Medicine\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses'?\\nAnswer: American Journal of Speech-Language Pathology, volume: 32, pages: 426 - 438; American Journal of Speech-Language Pathology\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses'?\\nAnswer: Alyssa M. Lanzi, Anna K Saylor, Davida Fromm, Houjun Liu, B. MacWhinney, Matthew L. Cohen\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses'?\\nAnswer: DementiaBank is a shared online database that can facilitate research efforts to address the gaps in knowledge about language changes associated with MCI and dementia from AD and is shown to have potential to help understand discourse metrics at the individual and group levels.\\nNotes: ##Author: Brian MacWhinney, ##Title: DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\", metadata={'source': '../data/papers_metadata_csv/f7254ae607056ba5522c10dbcf21b394967b6d42.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases', metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Chenyan Xiong?\\nAnswer: 144628574\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases', metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Chenyan Xiong?\\nAnswer: 35\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases', metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Chenyan Xiong?\\nAnswer: Chenyan Xiong\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases', metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Chenyan Xiong?\\nAnswer: https://www.semanticscholar.org/author/144628574\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases', metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Chenyan Xiong?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases', metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Text Matching Improves Sequential Recommendation by Reducing Popularity Biases?\\nAnswer: 159100c8323fc558e4073a3a006f3f243aca3a60\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases', metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Text Matching Improves Sequential Recommendation by Reducing Popularity Biases?\\nAnswer: {'DBLP': 'journals/corr/abs-2308-14029', 'ArXiv': '2308.14029', 'DOI': '10.1145/3583780.3615077', 'CorpusId': 261243943}\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Text Matching Improves Sequential Recommendation by Reducing Popularity Biases?\\nAnswer: https://www.semanticscholar.org/paper/159100c8323fc558e4073a3a006f3f243aca3a60\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases', metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases'?\\nAnswer: This paper proposes Text mAtching based SequenTial rEcommenda-tion model (TASTE), which maps items and users in an embedding space and recommends items by matching their text representations. TASTE verbalizes items and user-item interactions using identifiers and attributes of items. To better characterize user behaviors, TASTE additionally proposes an attention sparsity method, which enables TASTE to model longer user-item interactions by reducing the self-attention computations during encoding. Our experiments show that TASTE outperforms the state-of-the-art methods on widely used sequential recommendation datasets. TASTE alleviates the cold start problem by representing long-tail items using full-text modeling and bringing the benefits of pretrained language models to recommendation systems. Our further analyses illustrate that TASTE significantly improves the recommendation accuracy by reducing the popularity bias of previous item id based recommendation models and returning more appropriate and text-relevant items to satisfy users. All codes are available at https://github.com/OpenMatch/TASTE.\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases' published?\\nAnswer: International Conference on Information and Knowledge Management\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases' published?\\nAnswer: 2023\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases'?\\nAnswer: 74\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases' have?\\nAnswer: 3\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases' have?\\nAnswer: 3\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases' have?\\nAnswer: 0\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases' open access?\\nAnswer: Yes\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases'?\\nAnswer: https://arxiv.org/pdf/2308.14029\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases'?\\nAnswer: Computer Science\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases'?\\nAnswer: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases'?\\nAnswer: Zhenghao Liu, Senkun Mei, Chenyan Xiong, Xiaohua Li, Shi Yu, Zhiyuan Liu, Yu Gu, Ge Yu\\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Text Matching Improves Sequential Recommendation by Reducing Popularity Biases'?\\nAnswer: \\nNotes: ##Author: Chenyan Xiong, ##Title: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases\", metadata={'source': '../data/papers_metadata_csv/159100c8323fc558e4073a3a006f3f243aca3a60.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition', metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Jeffrey Bigham?\\nAnswer: 1744846\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition', metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Jeffrey Bigham?\\nAnswer: 56\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition', metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Jeffrey Bigham?\\nAnswer: Jeffrey P. Bigham\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition', metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Jeffrey Bigham?\\nAnswer: https://www.semanticscholar.org/author/1744846\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition', metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Jeffrey Bigham?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition', metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition?\\nAnswer: f29922cbfaf825d5e1d4986dc01bda74b4d88e04\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition', metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition?\\nAnswer: {'DBLP': 'conf/chi/LeaHNTYTGBF23', 'ArXiv': '2302.09044', 'DOI': '10.1145/3544548.3581224', 'CorpusId': 257019977}\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition?\\nAnswer: https://www.semanticscholar.org/paper/f29922cbfaf825d5e1d4986dc01bda74b4d88e04\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition', metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition'?\\nAnswer: Consumer speech recognition systems do not work as well for many people with speech differences, such as stuttering, relative to the rest of the general population. However, what is not clear is the degree to which these systems do not work, how they can be improved, or how much people want to use them. In this paper, we first address these questions using results from a 61-person survey from people who stutter and find participants want to use speech recognition but are frequently cut off, misunderstood, or speech predictions do not represent intent. In a second study, where 91 people who stutter recorded voice assistant commands and dictation, we quantify how dysfluencies impede performance in a consumer-grade speech recognition system. Through three technical investigations, we demonstrate how many common errors can be prevented, resulting in a system that cuts utterances off 79.1% less often and improves word error rate from 25.4% to 9.9%.\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition' published?\\nAnswer: International Conference on Human Factors in Computing Systems\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition' published?\\nAnswer: 2023\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition'?\\nAnswer: 91\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition' have?\\nAnswer: 5\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition' have?\\nAnswer: 5\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition' have?\\nAnswer: 1\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition' open access?\\nAnswer: Yes\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition'?\\nAnswer: https://dl.acm.org/doi/pdf/10.1145/3544548.3581224\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition'?\\nAnswer: Computer Science\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition'?\\nAnswer: Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition'?\\nAnswer: Colin S. Lea, Zifang Huang, Jaya Narain, Lauren Tooley, Dianna Yee, Dung Tien Tran, P. Georgiou, Jeffrey P. Bigham, Leah Findlater\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition'?\\nAnswer: Through three technical investigations, it is demonstrated how many common errors can be prevented, resulting in a system that cuts utterances off 79.1% less often and improves word error rate from 25.4% to 9.9%.\\nNotes: ##Author: Jeffrey Bigham, ##Title: From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\", metadata={'source': '../data/papers_metadata_csv/f29922cbfaf825d5e1d4986dc01bda74b4d88e04.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents', metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Matthias Grabmair?\\nAnswer: 2869551\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents', metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Matthias Grabmair?\\nAnswer: 13\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents', metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Matthias Grabmair?\\nAnswer: Matthias Grabmair\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents', metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Matthias Grabmair?\\nAnswer: https://www.semanticscholar.org/author/2869551\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents', metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Matthias Grabmair?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents', metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents?\\nAnswer: 1109a51ff68d8c7a09d651d706028e9e380f2af8\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents', metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents?\\nAnswer: {'DBLP': 'journals/corr/abs-2302-06448', 'ArXiv': '2302.06448', 'DOI': '10.48550/arXiv.2302.06448', 'CorpusId': 256826724}\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents?\\nAnswer: https://www.semanticscholar.org/paper/1109a51ff68d8c7a09d651d706028e9e380f2af8\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents', metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents'?\\nAnswer: Segmentation and Rhetorical Role Labeling of legal judgements play a crucial role in retrieval and adjacent tasks, including case summarization, semantic search, argument mining etc. Previous approaches have formulated this task either as independent classification or sequence labeling of sentences. In this work, we reformulate the task at span level as identifying spans of multiple consecutive sentences that share the same rhetorical role label to be assigned via classification. We employ semi-Markov Conditional Random Fields (CRF) to jointly learn span segmentation and span label assignment. We further explore three data augmentation strategies to mitigate the data scarcity in the specialized domain of law where individual documents tend to be very long and annotation cost is high. Our experiments demonstrate improvement of span-level prediction metrics with a semi-Markov CRF model over a CRF baseline. This benefit is contingent on the presence of multi sentence spans in the document.\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents' published?\\nAnswer: European Conference on Information Retrieval\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents' published?\\nAnswer: 2023\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents'?\\nAnswer: 33\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents' have?\\nAnswer: 1\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents' have?\\nAnswer: 1\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents' have?\\nAnswer: 0\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents' open access?\\nAnswer: Yes\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents'?\\nAnswer: http://arxiv.org/pdf/2302.06448\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents'?\\nAnswer: Computer Science\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents'?\\nAnswer: European Conference on Information Retrieval, pages: 627-636; European Conference on Information Retrieval\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents'?\\nAnswer: Santosh T.Y.S.S, Philipp Bock, Matthias Grabmair\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents'?\\nAnswer: This work reformulates the task at span level as identifying spans of multiple consecutive sentences that share the same rhetorical role label to be assigned via classification, and employs semi-Markov Conditional Random Fields (CRF) to jointly learn span segmentation and span label assignment.\\nNotes: ##Author: Matthias Grabmair, ##Title: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents\", metadata={'source': '../data/papers_metadata_csv/1109a51ff68d8c7a09d651d706028e9e380f2af8.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor', metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Brian MacWhinney?\\nAnswer: 2414040\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor', metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Brian MacWhinney?\\nAnswer: 75\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor', metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Brian MacWhinney?\\nAnswer: B. MacWhinney\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor', metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Brian MacWhinney?\\nAnswer: https://www.semanticscholar.org/author/2414040\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor', metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Brian MacWhinney?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor', metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor?\\nAnswer: 3859f18277f0c5876a53411b07f80d65254c52e5\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor', metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor?\\nAnswer: {'DOI': '10.1186/s40468-023-00232-6', 'CorpusId': 259950664}\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor?\\nAnswer: https://www.semanticscholar.org/paper/3859f18277f0c5876a53411b07f80d65254c52e5\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor', metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor'?\\nAnswer: None\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor' published?\\nAnswer: Language Testing in Asia\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor' published?\\nAnswer: 2023\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor'?\\nAnswer: 68\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor' have?\\nAnswer: 0\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor' have?\\nAnswer: 0\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor' have?\\nAnswer: 0\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor' open access?\\nAnswer: Yes\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor'?\\nAnswer: https://languagetestingasia.springeropen.com/counter/pdf/10.1186/s40468-023-00232-6\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor'?\\nAnswer: No fields of study available\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor'?\\nAnswer: Language Testing in Asia, volume: 13, pages: 1-22; Language Testing in Asia\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor'?\\nAnswer: Yanhui Zhang, B. MacWhinney\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor'?\\nAnswer: The findings demonstrated that the repeated feedback-embedded training with the Pinyin Tutor significantly boosted the learners’ proficiency in all aspects of PinyIn knowledge for second language (L2) learners of Chinese whose first language backgrounds were varied and whose initial proficiencies in Chinese were elementary.\\nNotes: ##Author: Brian MacWhinney, ##Title: Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor\", metadata={'source': '../data/papers_metadata_csv/3859f18277f0c5876a53411b07f80d65254c52e5.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation', metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Yiming Yang?\\nAnswer: 46286308\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation', metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Yiming Yang?\\nAnswer: 17\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation', metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yiming Yang?\\nAnswer: Yiming Yang\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation', metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yiming Yang?\\nAnswer: https://www.semanticscholar.org/author/46286308\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation', metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Yiming Yang?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation', metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation?\\nAnswer: 846f60ef3b98590c7ad1d84727c66a08cc2258c8\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation', metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation?\\nAnswer: {'DBLP': 'journals/corr/abs-2308-03725', 'ArXiv': '2308.03725', 'DOI': '10.48550/arXiv.2308.03725', 'CorpusId': 260681733}\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation?\\nAnswer: https://www.semanticscholar.org/paper/846f60ef3b98590c7ad1d84727c66a08cc2258c8\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation', metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation'?\\nAnswer: Temporal Sentence Grounding in Videos (TSGV) aims to detect the event timestamps described by the natural language query from untrimmed videos. This paper discusses the challenge of achieving efficient computation in TSGV models while maintaining high performance. Most existing approaches exquisitely design complex architectures to improve accuracy with extra layers and loss, suffering from inefficiency and heaviness. Although some works have noticed that, they only make an issue of feature fusion layers, which can hardly enjoy the highspeed merit in the whole clunky network. To tackle this problem, we propose a novel efficient multi-teacher model (EMTM) based on knowledge distillation to transfer diverse knowledge from both heterogeneous and isomorphic networks. Specifically, We first unify different outputs of the heterogeneous models into one single form. Next, a Knowledge Aggregation Unit (KAU) is built to acquire high-quality integrated soft labels from multiple teachers. After that, the KAU module leverages the multi-scale video and global query information to adaptively determine the weights of different teachers. A Shared Encoder strategy is then proposed to solve the problem that the student shallow layers hardly benefit from teachers, in which an isomorphic teacher is collaboratively trained with the student to align their hidden states. Extensive experimental results on three popular TSGV benchmarks demonstrate that our method is both effective and efficient without bells and whistles.\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation' published?\\nAnswer: arXiv.org\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation' published?\\nAnswer: 2023\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation'?\\nAnswer: 31\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation' have?\\nAnswer: 7\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation' have?\\nAnswer: 7\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation' have?\\nAnswer: 0\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation' open access?\\nAnswer: Yes\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation'?\\nAnswer: https://arxiv.org/pdf/2308.03725\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation'?\\nAnswer: Computer Science\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation'?\\nAnswer: ArXiv, volume: abs/2308.03725; ArXiv\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation'?\\nAnswer: Renjie Liang, Yiming Yang, Hui Lu, Li Li\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation'?\\nAnswer: A novel efficient multi-teacher model (EMTM) based on knowledge distillation to transfer diverse knowledge from both heterogeneous and isomorphic networks is proposed, which is both effective and efficient without bells and whistles.\\nNotes: ##Author: Yiming Yang, ##Title: Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation\", metadata={'source': '../data/papers_metadata_csv/846f60ef3b98590c7ad1d84727c66a08cc2258c8.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Challenges of Corporate Alliance CLOMA toward Plastic Litter\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter', metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Shinji Watanabe?\\nAnswer: 1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter', metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Shinji Watanabe?\\nAnswer: 67\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter', metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter', metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Shinji Watanabe?\\nAnswer: https://www.semanticscholar.org/author/1746678\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter', metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Shinji Watanabe?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter', metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Challenges of Corporate Alliance CLOMA toward Plastic Litter?\\nAnswer: c6f5da5eb57457457a49256f1434bf1db23d1898\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter', metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Challenges of Corporate Alliance CLOMA toward Plastic Litter?\\nAnswer: {'DOI': '10.5650/oleoscience.23.29', 'CorpusId': 255715394}\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Challenges of Corporate Alliance CLOMA toward Plastic Litter?\\nAnswer: https://www.semanticscholar.org/paper/c6f5da5eb57457457a49256f1434bf1db23d1898\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter', metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Challenges of Corporate Alliance CLOMA toward Plastic Litter'?\\nAnswer: None\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Challenges of Corporate Alliance CLOMA toward Plastic Litter' published?\\nAnswer: Oleoscience\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Challenges of Corporate Alliance CLOMA toward Plastic Litter' published?\\nAnswer: 2023\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Challenges of Corporate Alliance CLOMA toward Plastic Litter'?\\nAnswer: 1\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Challenges of Corporate Alliance CLOMA toward Plastic Litter' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Challenges of Corporate Alliance CLOMA toward Plastic Litter' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Challenges of Corporate Alliance CLOMA toward Plastic Litter' have?\\nAnswer: 0\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Challenges of Corporate Alliance CLOMA toward Plastic Litter' open access?\\nAnswer: Yes\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Challenges of Corporate Alliance CLOMA toward Plastic Litter'?\\nAnswer: https://www.jstage.jst.go.jp/article/oleoscience/23/1/23_29/_pdf\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Challenges of Corporate Alliance CLOMA toward Plastic Litter'?\\nAnswer: No fields of study available\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Challenges of Corporate Alliance CLOMA toward Plastic Litter'?\\nAnswer: Oleoscience\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Challenges of Corporate Alliance CLOMA toward Plastic Litter'?\\nAnswer: Shinji Watanabe\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Challenges of Corporate Alliance CLOMA toward Plastic Litter'?\\nAnswer: TLDR not found\\nNotes: ##Author: Shinji Watanabe, ##Title: Challenges of Corporate Alliance CLOMA toward Plastic Litter\", metadata={'source': '../data/papers_metadata_csv/c6f5da5eb57457457a49256f1434bf1db23d1898.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Large Language Models Enable Few-Shot Clustering\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering', metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Graham Neubig?\\nAnswer: 1700325\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering', metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Graham Neubig?\\nAnswer: 75\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering', metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Graham Neubig?\\nAnswer: Graham Neubig\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering', metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Graham Neubig?\\nAnswer: https://www.semanticscholar.org/author/1700325\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering', metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Graham Neubig?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering', metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Large Language Models Enable Few-Shot Clustering?\\nAnswer: 8e8a1489bf4d782d2435cdeb93f7d1f165747c63\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering', metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Large Language Models Enable Few-Shot Clustering?\\nAnswer: {'ArXiv': '2307.00524', 'DBLP': 'journals/corr/abs-2307-00524', 'DOI': '10.48550/arXiv.2307.00524', 'CorpusId': 259317075}\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Large Language Models Enable Few-Shot Clustering?\\nAnswer: https://www.semanticscholar.org/paper/8e8a1489bf4d782d2435cdeb93f7d1f165747c63\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering', metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Large Language Models Enable Few-Shot Clustering'?\\nAnswer: Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Large Language Models Enable Few-Shot Clustering' published?\\nAnswer: arXiv.org\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Large Language Models Enable Few-Shot Clustering' published?\\nAnswer: 2023\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Large Language Models Enable Few-Shot Clustering'?\\nAnswer: 38\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Large Language Models Enable Few-Shot Clustering' have?\\nAnswer: 6\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Large Language Models Enable Few-Shot Clustering' have?\\nAnswer: 6\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Large Language Models Enable Few-Shot Clustering' have?\\nAnswer: 0\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Large Language Models Enable Few-Shot Clustering' open access?\\nAnswer: Yes\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Large Language Models Enable Few-Shot Clustering'?\\nAnswer: http://arxiv.org/pdf/2307.00524\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Large Language Models Enable Few-Shot Clustering'?\\nAnswer: Computer Science\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Large Language Models Enable Few-Shot Clustering'?\\nAnswer: ArXiv, volume: abs/2307.00524; ArXiv\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Large Language Models Enable Few-Shot Clustering'?\\nAnswer: Vijay Viswanathan, Kiril Gashteovski, Carolin (Haas) Lawrence, Tongshuang Sherry Wu, Graham Neubig\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Large Language Models Enable Few-Shot Clustering'?\\nAnswer: It is found that incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters.\\nNotes: ##Author: Graham Neubig, ##Title: Large Language Models Enable Few-Shot Clustering\", metadata={'source': '../data/papers_metadata_csv/8e8a1489bf4d782d2435cdeb93f7d1f165747c63.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China', metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Yiming Yang?\\nAnswer: 35729970\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China', metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Yiming Yang?\\nAnswer: 64\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China', metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yiming Yang?\\nAnswer: Yiming Yang\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China', metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yiming Yang?\\nAnswer: https://www.semanticscholar.org/author/35729970\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China', metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Yiming Yang?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China', metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Impact of local governments’ construction land allocation strategies on innovation-driven development of China?\\nAnswer: ff4bd0966db6a5f30fe41c8479765e9d9702a8c0\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China', metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Impact of local governments’ construction land allocation strategies on innovation-driven development of China?\\nAnswer: {'DOI': '10.18402/resci.2023.06.05', 'CorpusId': 260944290}\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Impact of local governments’ construction land allocation strategies on innovation-driven development of China?\\nAnswer: https://www.semanticscholar.org/paper/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China', metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China'?\\nAnswer: None\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China' published?\\nAnswer: 资源科学\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China' published?\\nAnswer: 2023\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China'?\\nAnswer: 0\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China' have?\\nAnswer: 0\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China' have?\\nAnswer: 0\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China' have?\\nAnswer: 0\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China' open access?\\nAnswer: Yes\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China'?\\nAnswer: openAccessPdf not available\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China'?\\nAnswer: No fields of study available\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China'?\\nAnswer: 资源科学\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China'?\\nAnswer: Jian Wang, Shangui Peng, Yuhao Feng, Yiming Yang, Qun Wu\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Impact of local governments’ construction land allocation strategies on innovation-driven development of China'?\\nAnswer: TLDR not found\\nNotes: ##Author: Yiming Yang, ##Title: Impact of local governments’ construction land allocation strategies on innovation-driven development of China\", metadata={'source': '../data/papers_metadata_csv/ff4bd0966db6a5f30fe41c8479765e9d9702a8c0.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification', metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Yiming Yang?\\nAnswer: 46286308\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification', metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Yiming Yang?\\nAnswer: 17\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification', metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yiming Yang?\\nAnswer: Yiming Yang\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification', metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Yiming Yang?\\nAnswer: https://www.semanticscholar.org/author/46286308\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification', metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Yiming Yang?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification', metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification?\\nAnswer: 02ce4d3f93902a94ec2b57630b77696b7f18c84a\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification', metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification?\\nAnswer: {'ACL': '2023.acl-long.832', 'ArXiv': '2305.14963', 'DBLP': 'journals/corr/abs-2305-14963', 'DOI': '10.48550/arXiv.2305.14963', 'CorpusId': 258865225}\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification?\\nAnswer: https://www.semanticscholar.org/paper/02ce4d3f93902a94ec2b57630b77696b7f18c84a\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification', metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification'?\\nAnswer: We present PESCO, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification. We formulate text classification as a neural text retrieval problem where each document is treated as a query, and the system learns the mapping from each query to the relevant class labels by (1) adding prompts to enhance label retrieval, and (2) using retrieved labels to enrich the training set in a self-training loop of contrastive learning. PESCO achieves state-of-the-art performance on four benchmark text classification datasets. On DBpedia, we achieve 98.5% accuracy without any labeled data, which is close to the fully-supervised result. Extensive experiments and analyses show all the components of PESCO are necessary for improving the performance of zero-shot text classification.\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification' published?\\nAnswer: Annual Meeting of the Association for Computational Linguistics\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification' published?\\nAnswer: 2023\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification'?\\nAnswer: 48\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification' have?\\nAnswer: 2\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification' have?\\nAnswer: 2\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification' have?\\nAnswer: 0\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification' open access?\\nAnswer: Yes\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification'?\\nAnswer: http://arxiv.org/pdf/2305.14963\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification'?\\nAnswer: Computer Science\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification'?\\nAnswer: Annual Meeting of the Association for Computational Linguistics, pages: 14897-14911; Annual Meeting of the Association for Computational Linguistics\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification'?\\nAnswer: Yau-Shian Wang, Ta-Chung Chi, Ruohong Zhang, Yiming Yang\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification'?\\nAnswer: PESCO is presented, a novel contrastive learning framework that substantially improves the performance of zero-shot text classification and achieves state-of-the-art performance on four benchmark text classification datasets.\\nNotes: ##Author: Yiming Yang, ##Title: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification\", metadata={'source': '../data/papers_metadata_csv/02ce4d3f93902a94ec2b57630b77696b7f18c84a.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Unlimiformer: Long-Range Transformers with Unlimited Length Input\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input', metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Matt Gormley?\\nAnswer: 1762110\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input', metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Matt Gormley?\\nAnswer: 17\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input', metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Matt Gormley?\\nAnswer: Matthew R. Gormley\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input', metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Matt Gormley?\\nAnswer: https://www.semanticscholar.org/author/1762110\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input', metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Matt Gormley?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input', metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Unlimiformer: Long-Range Transformers with Unlimited Length Input?\\nAnswer: dbc368bc8b49347dd27679894524fa62f88492c9\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input', metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Unlimiformer: Long-Range Transformers with Unlimited Length Input?\\nAnswer: {'ArXiv': '2305.01625', 'DBLP': 'journals/corr/abs-2305-01625', 'DOI': '10.48550/arXiv.2305.01625', 'CorpusId': 258436892}\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Unlimiformer: Long-Range Transformers with Unlimited Length Input?\\nAnswer: https://www.semanticscholar.org/paper/dbc368bc8b49347dd27679894524fa62f88492c9\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input', metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?\\nAnswer: Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' published?\\nAnswer: arXiv.org\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' published?\\nAnswer: 2023\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?\\nAnswer: 56\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' have?\\nAnswer: 38\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' have?\\nAnswer: 38\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' have?\\nAnswer: 3\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input' open access?\\nAnswer: Yes\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?\\nAnswer: http://arxiv.org/pdf/2305.01625\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?\\nAnswer: Computer Science\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?\\nAnswer: ArXiv, volume: abs/2305.01625; ArXiv\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?\\nAnswer: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Unlimiformer: Long-Range Transformers with Unlimited Length Input'?\\nAnswer: This work proposes Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores.\\nNotes: ##Author: Matt Gormley, ##Title: Unlimiformer: Long-Range Transformers with Unlimited Length Input\", metadata={'source': '../data/papers_metadata_csv/dbc368bc8b49347dd27679894524fa62f88492c9.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters', metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Richard Stern?\\nAnswer: 1697819\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters', metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Richard Stern?\\nAnswer: 45\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters', metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Richard Stern?\\nAnswer: R. Stern\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters', metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Richard Stern?\\nAnswer: https://www.semanticscholar.org/author/1697819\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters', metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Richard Stern?\\nAnswer: LTI (CMU), No other affiliations on Semantic Scholar\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters', metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters?\\nAnswer: 8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters', metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters?\\nAnswer: {'DBLP': 'conf/icassp/LindseyVS23', 'DOI': '10.1109/ICASSP49357.2023.10097194', 'CorpusId': 258541612}\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters?\\nAnswer: https://www.semanticscholar.org/paper/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters', metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 8}),\n"," Document(page_content='Question: What is the abstract of the paper \\'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\\'?\\nAnswer: Voice type discrimination (VTD) is the task of automatically detecting speech produced in the same room as a recording device (\"live speech\") among other speech and non-speech noises, such as traffic noises or radio broadcasts (\"distractor audio\"). Existing work has described methods for performing the VTD task. This paper presents a method for adapting the output of these existing methods in an unsupervised manner via x-vector clustering and correlation. This adaptation method can be applied to the output of any VTD algorithm, requires no additional training data, and has been shown to yield a relative decrease in decision cost function (DCF) score of up to 47% on a standardized database collected for the task.\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters', metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters' published?\\nAnswer: IEEE International Conference on Acoustics, Speech, and Signal Processing\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters' published?\\nAnswer: 2023\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters'?\\nAnswer: 22\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters' have?\\nAnswer: 0\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters' have?\\nAnswer: 0\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters' have?\\nAnswer: 0\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters' open access?\\nAnswer: Yes\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters'?\\nAnswer: openAccessPdf data format unexpected\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters'?\\nAnswer: Computer Science\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters'?\\nAnswer: ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), pages: 1-5; ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters'?\\nAnswer: Mark Lindsey, Tyler Vuong, R. Stern\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters'?\\nAnswer: This adaptation method can be applied to the output of any VTD algorithm, requires no additional training data, and has been shown to yield a relative decrease in decision cost function (DCF) score of up to 47% on a standardized database collected for the task.\\nNotes: ##Author: Richard Stern, ##Title: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\", metadata={'source': '../data/papers_metadata_csv/8b49ebfc2b436c8b064ecf5b1eb3c5a12fc8d4b8.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Learning to Generate Equitable Text in Dialogue from Biased Training Data\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data', metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Malihe Alikhani?\\nAnswer: 2715920\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data', metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Malihe Alikhani?\\nAnswer: 12\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data', metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Malihe Alikhani?\\nAnswer: Malihe Alikhani\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data', metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Malihe Alikhani?\\nAnswer: https://www.semanticscholar.org/author/2715920\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data', metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Malihe Alikhani?\\nAnswer: Rutgers University\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data', metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Learning to Generate Equitable Text in Dialogue from Biased Training Data?\\nAnswer: 2386c6a7c40b5129960f2eb3c6be27db65b04c1f\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data', metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Learning to Generate Equitable Text in Dialogue from Biased Training Data?\\nAnswer: {'ACL': '2023.acl-long.163', 'ArXiv': '2307.04303', 'DBLP': 'journals/corr/abs-2307-04303', 'DOI': '10.48550/arXiv.2307.04303', 'CorpusId': 259370533}\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Learning to Generate Equitable Text in Dialogue from Biased Training Data?\\nAnswer: https://www.semanticscholar.org/paper/2386c6a7c40b5129960f2eb3c6be27db65b04c1f\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data', metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Learning to Generate Equitable Text in Dialogue from Biased Training Data'?\\nAnswer: The ingrained principles of fairness in a dialogue system’s decision-making process and generated responses are crucial for user engagement, satisfaction, and task achievement. Absence of equitable and inclusive principles can hinder the formation of common ground, which in turn negatively impacts the overall performance of the system. For example, misusing pronouns in a user interaction may cause ambiguity about the intended subject. Yet, there is no comprehensive study of equitable text generation in dialogue. Aptly, in this work, we use theories of computational learning to study this problem. We provide formal definitions of equity in text generation, and further, prove formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms for improving human-likeness (on augmented data). With this insight, we also formulate reasonable conditions under which text generation algorithms can learn to generate equitable text without any modifications to the biased training data on which they learn. To exemplify our theory in practice, we look at a group of algorithms for the GuessWhat?! visual dialogue game and, using this example, test our theory empirically. Our theory accurately predicts relative-performance of multiple algorithms in generating equitable text as measured by both human and automated evaluation.\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 9}),\n"," Document(page_content=\"Question: In which venue was the paper 'Learning to Generate Equitable Text in Dialogue from Biased Training Data' published?\\nAnswer: Annual Meeting of the Association for Computational Linguistics\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 10}),\n"," Document(page_content=\"Question: In what year was the paper 'Learning to Generate Equitable Text in Dialogue from Biased Training Data' published?\\nAnswer: 2023\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 11}),\n"," Document(page_content=\"Question: How many references are in the paper 'Learning to Generate Equitable Text in Dialogue from Biased Training Data'?\\nAnswer: 40\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 12}),\n"," Document(page_content=\"Question: How many citations does the paper 'Learning to Generate Equitable Text in Dialogue from Biased Training Data' have?\\nAnswer: 3\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 13}),\n"," Document(page_content=\"Question: What is the citation count of 'Learning to Generate Equitable Text in Dialogue from Biased Training Data' have?\\nAnswer: 3\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 14}),\n"," Document(page_content=\"Question: How many influential citations does the paper 'Learning to Generate Equitable Text in Dialogue from Biased Training Data' have?\\nAnswer: 0\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 15}),\n"," Document(page_content=\"Question: Is the paper 'Learning to Generate Equitable Text in Dialogue from Biased Training Data' open access?\\nAnswer: Yes\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 16}),\n"," Document(page_content=\"Question: What is the open access PDF URL of the paper titled 'Learning to Generate Equitable Text in Dialogue from Biased Training Data'?\\nAnswer: https://arxiv.org/pdf/2307.04303\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 17}),\n"," Document(page_content=\"Question: What are the fields of study for the paper titled 'Learning to Generate Equitable Text in Dialogue from Biased Training Data'?\\nAnswer: Computer Science\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 18}),\n"," Document(page_content=\"Question: What is the journal name for the paper titled 'Learning to Generate Equitable Text in Dialogue from Biased Training Data'?\\nAnswer: Annual Meeting of the Association for Computational Linguistics, pages: 2898-2917; Annual Meeting of the Association for Computational Linguistics\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 19}),\n"," Document(page_content=\"Question: Who are the authors of the paper 'Learning to Generate Equitable Text in Dialogue from Biased Training Data'?\\nAnswer: Anthony Sicilia, Malihe Alikhani\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 20}),\n"," Document(page_content=\"Question: What is the TLDR summary of the paper 'Learning to Generate Equitable Text in Dialogue from Biased Training Data'?\\nAnswer: This work provides formal definitions of equity in text generation, and proves formal connections between learning human-likeness and learning equity: algorithms for improving equity ultimately reduce to algorithms forimproving human- likeness (on augmented data).\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning to Generate Equitable Text in Dialogue from Biased Training Data\", metadata={'source': '../data/papers_metadata_csv/2386c6a7c40b5129960f2eb3c6be27db65b04c1f.csv', 'row': 21}),\n"," Document(page_content='Question: What is the name of this paper?\\nAnswer: Learning Multimodal Cues of Children’s Uncertainty\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning Multimodal Cues of Children’s Uncertainty', metadata={'source': '../data/papers_metadata_csv/2605bc92935ca67829e9542030be0e44ae81c22e.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Malihe Alikhani?\\nAnswer: 2715920\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning Multimodal Cues of Children’s Uncertainty', metadata={'source': '../data/papers_metadata_csv/2605bc92935ca67829e9542030be0e44ae81c22e.csv', 'row': 1}),\n"," Document(page_content='Question: What is the H-index of Malihe Alikhani?\\nAnswer: 12\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning Multimodal Cues of Children’s Uncertainty', metadata={'source': '../data/papers_metadata_csv/2605bc92935ca67829e9542030be0e44ae81c22e.csv', 'row': 2}),\n"," Document(page_content='Question: What is the semantic scholar author name of Malihe Alikhani?\\nAnswer: Malihe Alikhani\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning Multimodal Cues of Children’s Uncertainty', metadata={'source': '../data/papers_metadata_csv/2605bc92935ca67829e9542030be0e44ae81c22e.csv', 'row': 3}),\n"," Document(page_content='Question: What is the semantic scholar author name of Malihe Alikhani?\\nAnswer: https://www.semanticscholar.org/author/2715920\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning Multimodal Cues of Children’s Uncertainty', metadata={'source': '../data/papers_metadata_csv/2605bc92935ca67829e9542030be0e44ae81c22e.csv', 'row': 4}),\n"," Document(page_content='Question: What is the affiliation of Malihe Alikhani?\\nAnswer: Rutgers University\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning Multimodal Cues of Children’s Uncertainty', metadata={'source': '../data/papers_metadata_csv/2605bc92935ca67829e9542030be0e44ae81c22e.csv', 'row': 5}),\n"," Document(page_content='Question: What is the paper ID of the paper Learning Multimodal Cues of Children’s Uncertainty?\\nAnswer: 2605bc92935ca67829e9542030be0e44ae81c22e\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning Multimodal Cues of Children’s Uncertainty', metadata={'source': '../data/papers_metadata_csv/2605bc92935ca67829e9542030be0e44ae81c22e.csv', 'row': 6}),\n"," Document(page_content=\"Question: What are the external IDs of the paper Learning Multimodal Cues of Children’s Uncertainty?\\nAnswer: {'DBLP': 'conf/sigdial/ChengIMGCSPWA23', 'ACL': '2023.sigdial-1.41', 'DOI': '10.18653/v1/2023.sigdial-1.41', 'CorpusId': 263609496}\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning Multimodal Cues of Children’s Uncertainty\", metadata={'source': '../data/papers_metadata_csv/2605bc92935ca67829e9542030be0e44ae81c22e.csv', 'row': 7}),\n"," Document(page_content='Question: What is the URL of the paper Learning Multimodal Cues of Children’s Uncertainty?\\nAnswer: https://www.semanticscholar.org/paper/2605bc92935ca67829e9542030be0e44ae81c22e\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning Multimodal Cues of Children’s Uncertainty', metadata={'source': '../data/papers_metadata_csv/2605bc92935ca67829e9542030be0e44ae81c22e.csv', 'row': 8}),\n"," Document(page_content=\"Question: What is the abstract of the paper 'Learning Multimodal Cues of Children’s Uncertainty'?\\nAnswer: Understanding uncertainty plays a critical role in achieving common ground (Clark et al., 1983). This is especially important for multimodal AI systems that collaborate with users to solve a problem or guide the user through a challenging concept. In this work, for the first time, we present a dataset annotated in collaboration with developmental and cognitive psychologists for the purpose of studying nonverbal cues of uncertainty. We then present an analysis of the data, studying different roles of uncertainty and its relationship with task difficulty and performance. Lastly, we present a multimodal machine learning model that can predict uncertainty given a real-time video clip of a participant, which we find improves upon a baseline multimodal transformer model. This work informs research on cognitive coordination between human-human and human-AI and has broad implications for gesture understanding and generation. The anonymized version of our data and code will be publicly available upon the completion of the required consent forms and data sheets.\\nNotes: ##Author: Malihe Alikhani, ##Title: Learning Multimodal Cues of Children’s Uncertainty\", metadata={'source': '../data/papers_metadata_csv/2605bc92935ca67829e9542030be0e44ae81c22e.csv', 'row': 9}),\n"," ...]"]},"metadata":{},"execution_count":126}]},{"cell_type":"code","source":["db_csv = create_db(docs_csv)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"K4oUlc-hY5w5","executionInfo":{"status":"ok","timestamp":1710024362625,"user_tz":300,"elapsed":67726,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}},"outputId":"2dbfc7bf-955e-4b19-caa5-48fe0bd1544f"},"execution_count":127,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (577 > 512). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","source":["db_csv.save_local(\"../faiss_index_csv\")"],"metadata":{"id":"Ul9499jlHqHx","executionInfo":{"status":"ok","timestamp":1710024417826,"user_tz":300,"elapsed":388,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}}},"execution_count":128,"outputs":[]},{"cell_type":"code","source":["# author csv"],"metadata":{"id":"Ym2axSe-dMqH","executionInfo":{"status":"ok","timestamp":1710025440732,"user_tz":300,"elapsed":2,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}}},"execution_count":129,"outputs":[]},{"cell_type":"code","source":["csv_args={\n","          \"delimiter\": \",\",\n","          \"quotechar\": '\"',\n","          'fieldnames': ['Question', 'Answer', 'Document', 'Notes']\n","      }\n","\n","# author_csv = []\n","# for file_path in csv_files:\n","author_csv = '../data/paper_logs/author.csv'\n","loader = CSVLoader(\n","    file_path=author_csv,\n","    csv_args=csv_args,\n",")\n","author_csv = loader.load()\n","author_csv"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gf88wduOdNIz","executionInfo":{"status":"ok","timestamp":1710025926017,"user_tz":300,"elapsed":1038,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}},"outputId":"3ce89896-2fae-4a25-8d4f-c6f6b0e5abd1"},"execution_count":130,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Document(page_content='Question: Question\\nAnswer: Answer\\nDocument: Document\\nNotes: Notes', metadata={'source': '../data/paper_logs/author.csv', 'row': 0}),\n"," Document(page_content='Question: What is the author ID of Alexander Hauptmann?\\nAnswer: 145788702\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 1}),\n"," Document(page_content='Question: What are the papers written by Alexander Hauptmann?\\nAnswer: Zero-Shot and Few-Shot Stance Detection on Varied Topics via Conditional Generation, SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs, STMT: A Spatial-Temporal Mesh Transformer for MoCap-Based Action Recognition, DocumentNet: Bridging the Data Gap in Document Pre-training\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 2}),\n"," Document(page_content='Question: What is the H-index of Alexander Hauptmann?\\nAnswer: 27\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 3}),\n"," Document(page_content='Question: What is the author citation count of Alexander Hauptmann?\\nAnswer: 2295\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 4}),\n"," Document(page_content='Question: What is the author paper count of Alexander Hauptmann?\\nAnswer: 59\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 5}),\n"," Document(page_content='Question: What journals has Alexander Hauptmann published in?\\nAnswer: ArXiv, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 6}),\n"," Document(page_content='Question: What are the journals and how many papers has Alexander Hauptmann published in each?\\nAnswer: 1 in ArXiv, 1 in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 7}),\n"," Document(page_content='Question: What are the fields of study of Alexander Hauptmann?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 8}),\n"," Document(page_content='Question: How many papers has Alexander Hauptmann published in open access journals?\\nAnswer: 4\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 9}),\n"," Document(page_content='Question: What venues has Alexander Hauptmann published in?\\nAnswer: Annual Meeting of the Association for Computational Linguistics, arXiv.org, Computer Vision and Pattern Recognition, Conference on Empirical Methods in Natural Language Processing\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 10}),\n"," Document(page_content='Question: What is the most cited paper from Alexander Hauptmann?\\nAnswer: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 11}),\n"," Document(page_content='Question: What is the url of the most cited paper from Alexander Hauptmann?\\nAnswer: http://arxiv.org/pdf/2306.17842\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 12}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Alexander Hauptmann?\\nAnswer: Lijun Yu, Yong Cheng, Zhiruo Wang, Vivek Kumar, Wolfgang Macherey, Yanping Huang, David A. Ross, Irfan Essa, Yonatan Bisk, Ming Yang, K. Murphy, A. Hauptmann, Lu Jiang\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 13}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Alexander Hauptmann?\\nAnswer: This method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 14}),\n"," Document(page_content=\"Question: Abstract of the most cited paper from Alexander Hauptmann?\\nAnswer: In this work, we introduce Semantic Pyramid AutoEncoder (SPAE) for enabling frozen LLMs to perform both understanding and generation tasks involving non-linguistic modalities such as images or videos. SPAE converts between raw pixels and interpretable lexical tokens (or words) extracted from the LLM's vocabulary. The resulting tokens capture both the semantic meaning and the fine-grained details needed for visual reconstruction, effectively translating the visual content into a language comprehensible to the LLM, and empowering it to perform a wide array of multimodal tasks. Our approach is validated through in-context learning experiments with frozen PaLM 2 and GPT 3.5 on a diverse set of image understanding and generation tasks. Our method marks the first successful attempt to enable a frozen LLM to generate image content while surpassing state-of-the-art performance in image understanding tasks, under the same setting, by over 25%.\\nDocument: ../data/paper_jsons/A. Hauptmann_145788702.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 15}),\n"," Document(page_content='Question: What is the author ID of Alon Lavie?\\nAnswer: 1784914\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 16}),\n"," Document(page_content='Question: What are the papers written by Alon Lavie?\\nAnswer: The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics, Towards Multilingual Automatic Dialogue Evaluation, Results of WMT23 Metrics Shared Task: Metrics Might Be Guilty but References Are Not Innocent, Appropriateness is all you need!, Towards Multilingual Automatic Open-Domain Dialogue Evaluation, Simple LLM Prompting is State-of-the-Art for Robust and Multilingual Dialogue Evaluation\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 17}),\n"," Document(page_content='Question: What is the H-index of Alon Lavie?\\nAnswer: 47\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 18}),\n"," Document(page_content='Question: What is the author citation count of Alon Lavie?\\nAnswer: 14491\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 19}),\n"," Document(page_content='Question: What is the author paper count of Alon Lavie?\\nAnswer: 213\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 20}),\n"," Document(page_content='Question: What journals has Alon Lavie published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 21}),\n"," Document(page_content='Question: What are the journals and how many papers has Alon Lavie published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 22}),\n"," Document(page_content='Question: What are the fields of study of Alon Lavie?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 23}),\n"," Document(page_content='Question: How many papers has Alon Lavie published in open access journals?\\nAnswer: 6\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 24}),\n"," Document(page_content='Question: What venues has Alon Lavie published in?\\nAnswer: Annual Meeting of the Association for Computational Linguistics, arXiv.org, Conference on Machine Translation, SIGDIAL Conferences, DSTC\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 25}),\n"," Document(page_content='Question: What is the most cited paper from Alon Lavie?\\nAnswer: The Inside Story: Towards Better Understanding of Machine Translation Neural Evaluation Metrics\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 26}),\n"," Document(page_content='Question: What is the url of the most cited paper from Alon Lavie?\\nAnswer: http://arxiv.org/pdf/2305.11806\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 27}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Alon Lavie?\\nAnswer: Ricardo Rei, Nuno M. Guerreiro, Marcos Vinicius Treviso, Luisa Coheur, A. Lavie, Andre Martins\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 28}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Alon Lavie?\\nAnswer: This study reveals that neural explainability metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token- level neural saliency maps with Multidimensional Quality Metrics annotations and with synthetically-generated critical translation errors.\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 29}),\n"," Document(page_content='Question: Abstract of the most cited paper from Alon Lavie?\\nAnswer: Neural metrics for machine translation evaluation, such as COMET, exhibit significant improvements in their correlation with human judgments, as compared to traditional metrics based on lexical overlap, such as BLEU. Yet, neural metrics are, to a great extent, “black boxes” returning a single sentence-level score without transparency about the decision-making process. In this work, we develop and compare several neural explainability methods and demonstrate their effectiveness for interpreting state-of-the-art fine-tuned neural metrics. Our study reveals that these metrics leverage token-level information that can be directly attributed to translation errors, as assessed through comparison of token-level neural saliency maps with Multidimensional Quality Metrics (MQM) annotations and with synthetically-generated critical translation errors. To ease future research, we release our code at: https://github.com/Unbabel/COMET/tree/explainable-metrics\\nDocument: ../data/paper_jsons/A. Lavie_1784914.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 30}),\n"," Document(page_content='Question: What is the author ID of Alexander Rudnicky?\\nAnswer: 3156164\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 31}),\n"," Document(page_content='Question: What are the papers written by Alexander Rudnicky?\\nAnswer: Latent Positional Information is in the Self-Attention Variance of Transformer Language Models Without Positional Embeddings, Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 32}),\n"," Document(page_content='Question: What is the H-index of Alexander Rudnicky?\\nAnswer: 8\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 33}),\n"," Document(page_content='Question: What is the author citation count of Alexander Rudnicky?\\nAnswer: 309\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 34}),\n"," Document(page_content='Question: What is the author paper count of Alexander Rudnicky?\\nAnswer: 16\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 35}),\n"," Document(page_content='Question: What journals has Alexander Rudnicky published in?\\nAnswer: \\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 36}),\n"," Document(page_content='Question: What are the journals and how many papers has Alexander Rudnicky published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 37}),\n"," Document(page_content='Question: What are the fields of study of Alexander Rudnicky?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 38}),\n"," Document(page_content='Question: How many papers has Alexander Rudnicky published in open access journals?\\nAnswer: 2\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 39}),\n"," Document(page_content='Question: What venues has Alexander Rudnicky published in?\\nAnswer: Annual Meeting of the Association for Computational Linguistics, Conference on Empirical Methods in Natural Language Processing\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 40}),\n"," Document(page_content='Question: What is the most cited paper from Alexander Rudnicky?\\nAnswer: Transformer Working Memory Enables Regular Language Reasoning and Natural Language Length Extrapolation\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 41}),\n"," Document(page_content='Question: What is the url of the most cited paper from Alexander Rudnicky?\\nAnswer: http://arxiv.org/pdf/2305.03796\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 42}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Alexander Rudnicky?\\nAnswer: Ta-Chung Chi, Ting-Han Fan, A. Rudnicky, P. Ramadge\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 43}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Alexander Rudnicky?\\nAnswer: Inspired by the notion of working memory, a new Transformer variant named RegularGPT is proposed, which constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY.\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 44}),\n"," Document(page_content='Question: Abstract of the most cited paper from Alexander Rudnicky?\\nAnswer: Unlike recurrent models, conventional wisdom has it that Transformers cannot perfectly model regular languages. Inspired by the notion of working memory, we propose a new Transformer variant named RegularGPT. With its novel combination of Weight-Sharing, Adaptive-Depth, and Sliding-Dilated-Attention, RegularGPT constructs working memory along the depth dimension, thereby enabling efficient and successful modeling of regular languages such as PARITY. We further test RegularGPT on the task of natural language length extrapolation and surprisingly find that it rediscovers the local windowed attention effect deemed necessary in prior work for length extrapolation.\\nDocument: ../data/paper_jsons/A. Rudnicky_3156164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 45}),\n"," Document(page_content='Question: What is the author ID of Alexander Waibel?\\nAnswer: 1724972\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 46}),\n"," Document(page_content='Question: What are the papers written by Alexander Waibel?\\nAnswer: SYNTACC : Synthesizing Multi-Accent Speech By Weight Factorization, KIT’s Multilingual Speech Translation System for IWSLT 2023, Convoifilter: A case study of doing cocktail party speech recognition, Incremental Blockwise Beam Search for Simultaneous Speech Translation with Controllable Quality-Latency Tradeoff, End-to-End Evaluation for Low-Latency Simultaneous Speech Translation, FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 47}),\n"," Document(page_content='Question: What is the H-index of Alexander Waibel?\\nAnswer: 80\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 48}),\n"," Document(page_content='Question: What is the author citation count of Alexander Waibel?\\nAnswer: 27573\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 49}),\n"," Document(page_content='Question: What is the author paper count of Alexander Waibel?\\nAnswer: 719\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 50}),\n"," Document(page_content='Question: What journals has Alexander Waibel published in?\\nAnswer: ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), ArXiv\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 51}),\n"," Document(page_content='Question: What are the journals and how many papers has Alexander Waibel published in each?\\nAnswer: 3 in ArXiv, 1 in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 52}),\n"," Document(page_content='Question: What are the fields of study of Alexander Waibel?\\nAnswer: Computer Science, Engineering\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 53}),\n"," Document(page_content='Question: How many papers has Alexander Waibel published in open access journals?\\nAnswer: 6\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 54}),\n"," Document(page_content='Question: What venues has Alexander Waibel published in?\\nAnswer: IEEE International Conference on Acoustics, Speech, and Signal Processing, International Workshop on Spoken Language Translation, arXiv.org, Interspeech, Conference on Empirical Methods in Natural Language Processing\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 55}),\n"," Document(page_content='Question: What is the most cited paper from Alexander Waibel?\\nAnswer: FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 56}),\n"," Document(page_content='Question: What is the url of the most cited paper from Alexander Waibel?\\nAnswer: https://aclanthology.org/2023.iwslt-1.1.pdf\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 57}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Alexander Waibel?\\nAnswer: Sweta Agrawal, Antonios Anastasopoulos, L. Bentivogli, Ondrej Bojar, Claudia Borg, Marine Carpuat, Roldano Cattoni, Mauro Cettolo, Mingda Chen, William Chen, K. Choukri, Alexandra Chronopoulou, Anna Currey, T. Declerck, Qianqian Dong, Kevin Duh, Y. Esteve, Marcello Federico, Souhir Gahbiche, B. Haddow, B. Hsu, Phu Mon Htut, H. Inaguma, David Javorsky, J. Judge, Yasumasa Kano, Tom Ko, Rishu Kumar, Peng Li, Xutai Ma, Prashant Mathur, E. Matusov, Paul McNamee, John P. McCrae, Kenton Murray, Maria Nadejde, Satoshi Nakamura, Matteo Negri, H. Nguyen, J. Niehues, Xing Niu, Atul Kr. Ojha, John E. Ortega, Proyag Pal, J. Pino, Lonneke van der Plas, Peter Polak, Elijah Matthew Rippeth, Elizabeth Salesky, Jiatong Shi, Matthias Sperber, Sebastian Stuker, Katsuhito Sudoh, Yun Tang, Brian Thompson, Ke M. Tran, M. Turchi, A. Waibel, Mingxuan Wang, Shinji Watanabe, Rodolfo Zevallos\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 58}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Alexander Waibel?\\nAnswer: \\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 59}),\n"," Document(page_content='Question: Abstract of the most cited paper from Alexander Waibel?\\nAnswer: This paper reports on the shared tasks organized by the 20th IWSLT Conference. The shared tasks address 9 scientific challenges in spoken language translation: simultaneous and offline translation, automatic subtitling and dubbing, speech-to-speech translation, multilingual, dialect and low-resource speech translation, and formality control. The shared tasks attracted a total of 38 submissions by 31 teams. The growing interest towards spoken language translation is also witnessed by the constantly increasing number of shared task organizers and contributors to the overview paper, almost evenly distributed across industry and academia.\\nDocument: ../data/paper_jsons/A. Waibel_1724972.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 60}),\n"," Document(page_content='Question: What is the author ID of Alexander Waibel?\\nAnswer: 2064429921\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 61}),\n"," Document(page_content='Question: What are the papers written by Alexander Waibel?\\nAnswer: AdapITN: A Fast, Reliable, and Dynamic Adaptive Inverse Text Normalization, Train Global, Tailor Local: Minimalist Multilingual Translation into Endangered Languages, Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 62}),\n"," Document(page_content='Question: What is the H-index of Alexander Waibel?\\nAnswer: 6\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 63}),\n"," Document(page_content='Question: What is the author citation count of Alexander Waibel?\\nAnswer: 69\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 64}),\n"," Document(page_content='Question: What is the author paper count of Alexander Waibel?\\nAnswer: 18\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 65}),\n"," Document(page_content='Question: What journals has Alexander Waibel published in?\\nAnswer: ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), ArXiv\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 66}),\n"," Document(page_content='Question: What are the journals and how many papers has Alexander Waibel published in each?\\nAnswer: 1 in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1 in ArXiv\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 67}),\n"," Document(page_content='Question: What are the fields of study of Alexander Waibel?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 68}),\n"," Document(page_content='Question: How many papers has Alexander Waibel published in open access journals?\\nAnswer: 3\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 69}),\n"," Document(page_content='Question: What venues has Alexander Waibel published in?\\nAnswer: IEEE International Conference on Acoustics, Speech, and Signal Processing, LORESMT, International Workshop on Spoken Language Translation\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 70}),\n"," Document(page_content='Question: What is the most cited paper from Alexander Waibel?\\nAnswer: Towards Efficient Simultaneous Speech Translation: CUNI-KIT System for Simultaneous Track at IWSLT 2023\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 71}),\n"," Document(page_content='Question: What is the url of the most cited paper from Alexander Waibel?\\nAnswer: https://aclanthology.org/2023.iwslt-1.37.pdf\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 72}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Alexander Waibel?\\nAnswer: Peter Polak, Danni Liu, Ngoc-Quan Pham, J. Niehues, A. Waibel, Ondrej Bojar\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 73}),\n"," Document(page_content=\"Question: TLDR/Summary of the most cited paper from Alexander Waibel?\\nAnswer: This year's submission to the Simultaneous Track at IWSLT 2023 is described, and a novel online policy for attentional encoder-decoder models is proposed that prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer.\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 74}),\n"," Document(page_content='Question: Abstract of the most cited paper from Alexander Waibel?\\nAnswer: In this paper, we describe our submission to the Simultaneous Track at IWSLT 2023. This year, we continue with the successful setup from the last year, however, we adopt the latest methods that further improve the translation quality. Additionally, we propose a novel online policy for attentional encoder-decoder models. The policy prevents the model to generate translation beyond the current speech input by using an auxiliary CTC output layer. We show that the proposed simultaneous policy can be applied to both streaming blockwise models and offline encoder-decoder models. We observe significant improvements in quality (up to 1.1 BLEU) and the computational footprint (up to 45% relative RTF).\\nDocument: ../data/paper_jsons/A. Waibel_2064429921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 75}),\n"," Document(page_content='Question: What is the author ID of Alexander Hauptmann?\\nAnswer: 7661726\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 76}),\n"," Document(page_content='Question: What are the papers written by Alexander Hauptmann?\\nAnswer: Towards Open-Domain Twitter User Profile Inference\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 77}),\n"," Document(page_content='Question: What is the H-index of Alexander Hauptmann?\\nAnswer: 81\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 78}),\n"," Document(page_content='Question: What is the author citation count of Alexander Hauptmann?\\nAnswer: 25325\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 79}),\n"," Document(page_content='Question: What is the author paper count of Alexander Hauptmann?\\nAnswer: 543\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 80}),\n"," Document(page_content='Question: What journals has Alexander Hauptmann published in?\\nAnswer: \\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 81}),\n"," Document(page_content='Question: What are the journals and how many papers has Alexander Hauptmann published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 82}),\n"," Document(page_content='Question: What are the fields of study of Alexander Hauptmann?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 83}),\n"," Document(page_content='Question: How many papers has Alexander Hauptmann published in open access journals?\\nAnswer: 1\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 84}),\n"," Document(page_content='Question: What venues has Alexander Hauptmann published in?\\nAnswer: Annual Meeting of the Association for Computational Linguistics\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 85}),\n"," Document(page_content='Question: What is the most cited paper from Alexander Hauptmann?\\nAnswer: Towards Open-Domain Twitter User Profile Inference\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 86}),\n"," Document(page_content='Question: What is the url of the most cited paper from Alexander Hauptmann?\\nAnswer: https://aclanthology.org/2023.findings-acl.198.pdf\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 87}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Alexander Hauptmann?\\nAnswer: Haoyang Wen, Zhenxin Xiao, E. Hovy, Alexander Hauptmann\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 88}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Alexander Hauptmann?\\nAnswer: \\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 89}),\n"," Document(page_content='Question: Abstract of the most cited paper from Alexander Hauptmann?\\nAnswer: ,\\nDocument: ../data/paper_jsons/Alexander Hauptmann_7661726.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 90}),\n"," Document(page_content='Question: What is the author ID of Alexander Rudnicky?\\nAnswer: 1783635\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 91}),\n"," Document(page_content='Question: What are the papers written by Alexander Rudnicky?\\nAnswer: Advancing Regular Language Reasoning in Linear Recurrent Neural Networks, Structured Dialogue Discourse Parsing, A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech, Overview of Robust and Multilingual Automatic Evaluation Metrics\\n\\nfor Open-Domain Dialogue Systems at DSTC 11 Track 4, Learning to Ask Questions for Zero-shot Dialogue State Tracking\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 92}),\n"," Document(page_content='Question: What is the H-index of Alexander Rudnicky?\\nAnswer: 42\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 93}),\n"," Document(page_content='Question: What is the author citation count of Alexander Rudnicky?\\nAnswer: 7639\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 94}),\n"," Document(page_content='Question: What is the author paper count of Alexander Rudnicky?\\nAnswer: 264\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 95}),\n"," Document(page_content='Question: What journals has Alexander Rudnicky published in?\\nAnswer: ArXiv, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 96}),\n"," Document(page_content='Question: What are the journals and how many papers has Alexander Rudnicky published in each?\\nAnswer: 4 in ArXiv, 1 in Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 97}),\n"," Document(page_content='Question: What are the fields of study of Alexander Rudnicky?\\nAnswer: Computer Science, Engineering\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 98}),\n"," Document(page_content='Question: How many papers has Alexander Rudnicky published in open access journals?\\nAnswer: 5\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 99}),\n"," Document(page_content='Question: What venues has Alexander Rudnicky published in?\\nAnswer: arXiv.org, SIGDIAL Conferences, AAAI Conference on Artificial Intelligence, DSTC, Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 100}),\n"," Document(page_content='Question: What is the most cited paper from Alexander Rudnicky?\\nAnswer: A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 101}),\n"," Document(page_content='Question: What is the url of the most cited paper from Alexander Rudnicky?\\nAnswer: http://arxiv.org/pdf/2302.04215\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 102}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Alexander Rudnicky?\\nAnswer: Li-Wei Chen, Shinji Watanabe, Alexander I. Rudnicky\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 103}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Alexander Rudnicky?\\nAnswer: This work introduces the MQTTS system, a Text-to-Speech system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality, and shows that MqTTS outperforms existing TTS systems in several objective and subjective measures.\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 104}),\n"," Document(page_content='Question: Abstract of the most cited paper from Alexander Rudnicky?\\nAnswer: Recent Text-to-Speech (TTS) systems trained on reading or acted corpora have achieved near human-level naturalness. The diversity of human speech, however, often goes beyond the coverage of these corpora. We believe the ability to handle such diversity is crucial for AI systems to achieve human-level communication. Our work explores the use of more abundant real-world data for building speech synthesizers. We train TTS systems using real-world speech from YouTube and podcasts. We observe the mismatch between training and inference alignments in mel-spectrogram based autoregressive models, leading to unintelligible synthesis, and demonstrate that learned discrete codes within multiple code groups effectively resolves this issue. We introduce our MQTTS system whose architecture is designed for multiple code generation and monotonic alignment, along with the use of a clean silence prompt to improve synthesis quality. We conduct ablation analyses to identify the efficacy of our methods. We show that MQTTS outperforms existing TTS systems in several objective and subjective measures.\\nDocument: ../data/paper_jsons/Alexander I. Rudnicky_1783635.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 105}),\n"," Document(page_content='Question: What is the author ID of Brian MacWhinney?\\nAnswer: 2414040\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 106}),\n"," Document(page_content='Question: What are the papers written by Brian MacWhinney?\\nAnswer: Assessment and Therapy Goal Planning Using Free Computerized Language Analysis Software., Automation of Language Sample Analysis, The role of novelty stimuli in second language acquisition: evidence from the optimized training by the Pinyin Tutor at TalkBank, Using diagnostic feedback to enhance the development of phonetic knowledge of an L2: a CALL design based on the unified competition model and the implementation with the Pinyin Tutor, A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning, Collaborative Commentary for Understanding Communication Disorders., Establishing the DementiaBank Delaware Corpus: An Online Multimedia Database for the Study of Language and Cognition in Dementia, Evaluating Picture Description Speech for Dementia Detection using Image-text Alignment, Multilingual Alzheimer’s Dementia Recognition through Spontaneous Speech: A Signal Processing Grand Challenge, DementiaBank: Theoretical Rationale, Protocol, and Illustrative Analyses\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 107}),\n"," Document(page_content='Question: What is the H-index of Brian MacWhinney?\\nAnswer: 75\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 108}),\n"," Document(page_content='Question: What is the author citation count of Brian MacWhinney?\\nAnswer: 27192\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 109}),\n"," Document(page_content='Question: What is the author paper count of Brian MacWhinney?\\nAnswer: 432\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 110}),\n"," Document(page_content=\"Question: What journals has Brian MacWhinney published in?\\nAnswer: Perspectives of the ASHA special interest groups, Journal of Speech, Language, and Hearing Research : JSLHR, Smart Learning Environments, Language Testing in Asia, ArXiv, American journal of speech-language pathology, Alzheimer's & Dementia, ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), American Journal of Speech-Language Pathology\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 111}),\n"," Document(page_content=\"Question: What are the journals and how many papers has Brian MacWhinney published in each?\\nAnswer: 2 in ArXiv, 1 in Perspectives of the ASHA special interest groups, 1 in Journal of Speech, Language, and Hearing Research : JSLHR, 1 in Smart Learning Environments, 1 in Language Testing in Asia, 1 in American journal of speech-language pathology, 1 in Alzheimer's & Dementia, 1 in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1 in American Journal of Speech-Language Pathology\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 112}),\n"," Document(page_content='Question: What are the fields of study of Brian MacWhinney?\\nAnswer: Medicine, Computer Science, Engineering\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 113}),\n"," Document(page_content='Question: How many papers has Brian MacWhinney published in open access journals?\\nAnswer: 10\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 114}),\n"," Document(page_content=\"Question: What venues has Brian MacWhinney published in?\\nAnswer: Perspectives of the ASHA Special Interest Groups, Journal of Speech, Language and Hearing Research, Smart Learning Environments, Language Testing in Asia, Interspeech, American Journal of Speech-Language Pathology, Alzheimer's &amp; Dementia, arXiv.org, IEEE International Conference on Acoustics, Speech, and Signal Processing\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 115}),\n"," Document(page_content='Question: What is the most cited paper from Brian MacWhinney?\\nAnswer: Multilingual Alzheimer’s Dementia Recognition through Spontaneous Speech: A Signal Processing Grand Challenge\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 116}),\n"," Document(page_content='Question: What is the url of the most cited paper from Brian MacWhinney?\\nAnswer: https://arxiv.org/pdf/2301.05562\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 117}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Brian MacWhinney?\\nAnswer: S. Luz, F. Haider, Davida Fromm, Ioulietta Lazarou, Y. Kompatsiaris, B. MacWhinney\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 118}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Brian MacWhinney?\\nAnswer: This Signal Processing Grand Challenge (SPGC) targets a difficult automatic prediction problem of societal and medical relevance, namely, the detection of Alzheimer’s Dementia, and aims to assess the extent to which predictive models built based on speech in one language generalise to another language.\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 119}),\n"," Document(page_content='Question: Abstract of the most cited paper from Brian MacWhinney?\\nAnswer: This Signal Processing Grand Challenge (SPGC) targets a difficult automatic prediction problem of societal and medical relevance, namely, the detection of Alzheimer’s Dementia (AD). Participants were invited to employ signal processing and machine learning methods to create predictive models based on spontaneous speech data. The Challenge has been designed to assess the extent to which predictive models built based on speech in one language (English) generalise to another language (Greek). To the best of our knowledge no work has investigated acoustic features of the speech signal in multilingual AD detection. Our baseline system used conventional machine learning algorithms with Active Data Representation of acoustic features, achieving accuracy of 73.91% on AD detection, and 4.95 root mean squared error on cognitive score prediction.\\nDocument: ../data/paper_jsons/B. MacWhinney_2414040.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 120}),\n"," Document(page_content='Question: What is the author ID of Bhiksha Raj?\\nAnswer: 1681921\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 121}),\n"," Document(page_content='Question: What are the papers written by Bhiksha Raj?\\nAnswer: FREDOM: Fairness Domain Adaptation Approach to Semantic Scene Understanding, UTOPIA: Unconstrained Tracking Objects without Preliminary Examination via Cross-Domain Adaptation, SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning, Fixed Inter-Neuron Covariability Induces Adversarial Robustness, Understanding political polarization using language models: A dataset and method, Prolonged school closure during the pandemic time in successive waves of COVID-19- vulnerability of children to sexual abuses – A case study in Tamil Nadu, India, BASS: Block-wise Adaptation for Speech Summarization, Understanding Political Polarisation using Language Models: A dataset and method, An Approach to Ontological Learning from Weak Labels, Rethinking Voice-Face Correlation: A Geometry View, Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement, Improving Perceptual Quality, Intelligibility, and Acoustics on VoIP Platforms, Approach to Learning Generalized Audio Representation Through Batch Embedding Covariance Regularization and Constant-Q Transforms, Training on Foveated Images Improves Robustness to Adversarial Attacks, Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations, The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features, Fairness Continual Learning Approach to Semantic Scene Understanding in Open-World Environments, PaintSeg: Training-free Segmentation via Painting, TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement, Synergy between human and machine approaches to sound/scene recognition and processing: An overview of ICASSP special session\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 122}),\n"," Document(page_content='Question: What is the H-index of Bhiksha Raj?\\nAnswer: 54\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 123}),\n"," Document(page_content='Question: What is the author citation count of Bhiksha Raj?\\nAnswer: 14485\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 124}),\n"," Document(page_content='Question: What is the author paper count of Bhiksha Raj?\\nAnswer: 404\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 125}),\n"," Document(page_content='Question: What journals has Bhiksha Raj published in?\\nAnswer: 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), ArXiv, AI Mag., Heliyon, ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Proceedings of the 31st ACM International Conference on Multimedia\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 126}),\n"," Document(page_content='Question: What are the journals and how many papers has Bhiksha Raj published in each?\\nAnswer: 13 in ArXiv, 3 in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 1 in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 1 in AI Mag., 1 in Heliyon, 1 in Proceedings of the 31st ACM International Conference on Multimedia\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 127}),\n"," Document(page_content='Question: What are the fields of study of Bhiksha Raj?\\nAnswer: Computer Science, Medicine, Engineering\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 128}),\n"," Document(page_content='Question: How many papers has Bhiksha Raj published in open access journals?\\nAnswer: 20\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 129}),\n"," Document(page_content='Question: What venues has Bhiksha Raj published in?\\nAnswer: Computer Vision and Pattern Recognition, arXiv.org, International Conference on Learning Representations, The AI Magazine, Heliyon, Interspeech, IEEE International Conference on Acoustics, Speech, and Signal Processing, ACM Multimedia\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 130}),\n"," Document(page_content='Question: What is the most cited paper from Bhiksha Raj?\\nAnswer: SoftMatch: Addressing the Quantity-Quality Trade-off in Semi-supervised Learning\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 131}),\n"," Document(page_content='Question: What is the url of the most cited paper from Bhiksha Raj?\\nAnswer: http://arxiv.org/pdf/2301.10921\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 132}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Bhiksha Raj?\\nAnswer: Hao Chen, R. Tao, Yue Fan, Yidong Wang, Jindong Wang, B. Schiele, Xingxu Xie, B. Raj, M. Savvides\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 133}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Bhiksha Raj?\\nAnswer: This paper revisits the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrates the inherent quantity-quality trade-off problem of pseudo-labels with thresholding, which may prohibit learning.\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 134}),\n"," Document(page_content=\"Question: Abstract of the most cited paper from Bhiksha Raj?\\nAnswer: The critical challenge of Semi-Supervised Learning (SSL) is how to effectively leverage the limited labeled data and massive unlabeled data to improve the model's generalization performance. In this paper, we first revisit the popular pseudo-labeling methods via a unified sample weighting formulation and demonstrate the inherent quantity-quality trade-off problem of pseudo-labeling with thresholding, which may prohibit learning. To this end, we propose SoftMatch to overcome the trade-off by maintaining both high quantity and high quality of pseudo-labels during training, effectively exploiting the unlabeled data. We derive a truncated Gaussian function to weight samples based on their confidence, which can be viewed as a soft version of the confidence threshold. We further enhance the utilization of weakly-learned classes by proposing a uniform alignment approach. In experiments, SoftMatch shows substantial improvements across a wide variety of benchmarks, including image, text, and imbalanced classification.\\nDocument: ../data/paper_jsons/B. Raj_1681921.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 135}),\n"," Document(page_content='Question: What is the author ID of Carolyn Rose?\\nAnswer: 35959897\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 136}),\n"," Document(page_content='Question: What are the papers written by Carolyn Rose?\\nAnswer: High school students’ data modeling practices and processes: from modeling unstructured data to evaluating automated decisions, Linguistic representations for fewer-shot relation extraction across domains, Using counterfactual contrast to improve compositional generalization for multi-step quantitative reasoning, Towards Extracting and Understanding the Implicit Rubrics of Transformer Based Automatic Essay Scoring Models, Exploring Artificial Intelligence in English Language Arts with StoryQ\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 137}),\n"," Document(page_content='Question: What is the H-index of Carolyn Rose?\\nAnswer: 54\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 138}),\n"," Document(page_content='Question: What is the author citation count of Carolyn Rose?\\nAnswer: 11882\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 139}),\n"," Document(page_content='Question: What is the author paper count of Carolyn Rose?\\nAnswer: 441\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 140}),\n"," Document(page_content='Question: What journals has Carolyn Rose published in?\\nAnswer: Learning, Media and Technology, ArXiv\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 141}),\n"," Document(page_content='Question: What are the journals and how many papers has Carolyn Rose published in each?\\nAnswer: 1 in Learning, Media and Technology, 1 in ArXiv\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 142}),\n"," Document(page_content='Question: What are the fields of study of Carolyn Rose?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 143}),\n"," Document(page_content='Question: How many papers has Carolyn Rose published in open access journals?\\nAnswer: 5\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 144}),\n"," Document(page_content='Question: What venues has Carolyn Rose published in?\\nAnswer: Journal of Educational Media, Annual Meeting of the Association for Computational Linguistics, Workshop on Innovative Use of NLP for Building Educational Applications, AAAI Conference on Artificial Intelligence\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 145}),\n"," Document(page_content='Question: What is the most cited paper from Carolyn Rose?\\nAnswer: High school students’ data modeling practices and processes: from modeling unstructured data to evaluating automated decisions\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 146}),\n"," Document(page_content='Question: What is the url of the most cited paper from Carolyn Rose?\\nAnswer: https://www.tandfonline.com/doi/pdf/10.1080/17439884.2023.2189735?needAccess=true&role=button\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 147}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Carolyn Rose?\\nAnswer: Shiyan Jiang, Hengtao Tang, Can Tatar, C. Rose, J. Chao\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 148}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Carolyn Rose?\\nAnswer: It’s critical to foster artificial intelligence literacy for high school students to understand working mechanism of data-driven AI technologies and critically evaluate automated decisions from predictive models.\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 149}),\n"," Document(page_content='Question: Abstract of the most cited paper from Carolyn Rose?\\nAnswer: ABSTRACT It’s critical to foster artificial intelligence (AI) literacy for high school students, the first generation to grow up surrounded by AI, to understand working mechanism of data-driven AI technologies and critically evaluate automated decisions from predictive models. While efforts have been made to engage youth in understanding AI through developing machine learning models, few provided in-depth insights into the nuanced learning processes. In this study, we examined high school students’ data modeling practices and processes. Twenty-eight students developed machine learning models with text data for classifying negative and positive reviews of ice cream stores. We identified nine data modeling practices that describe students’ processes of model exploration, development, and testing and two themes about evaluating automated decisions from data technologies. The results provide implications for designing accessible data modeling experiences for students to understand data justice as well as the role and responsibility of data modelers in creating AI technologies.\\nDocument: ../data/paper_jsons/C. Rose_35959897.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 150}),\n"," Document(page_content='Question: What is the author ID of Chenyan Xiong?\\nAnswer: 144628574\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 151}),\n"," Document(page_content='Question: What are the papers written by Chenyan Xiong?\\nAnswer: Text Matching Improves Sequential Recommendation by Reducing Popularity Biases, Fusion-in-T5: Unifying Document Ranking Signals for Improved Information Retrieval, OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 152}),\n"," Document(page_content='Question: What is the H-index of Chenyan Xiong?\\nAnswer: 35\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 153}),\n"," Document(page_content='Question: What is the author citation count of Chenyan Xiong?\\nAnswer: 5048\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 154}),\n"," Document(page_content='Question: What is the author paper count of Chenyan Xiong?\\nAnswer: 94\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 155}),\n"," Document(page_content='Question: What journals has Chenyan Xiong published in?\\nAnswer: Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, ArXiv, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 156}),\n"," Document(page_content='Question: What are the journals and how many papers has Chenyan Xiong published in each?\\nAnswer: 1 in Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, 1 in ArXiv, 1 in Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 157}),\n"," Document(page_content='Question: What are the fields of study of Chenyan Xiong?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 158}),\n"," Document(page_content='Question: How many papers has Chenyan Xiong published in open access journals?\\nAnswer: 3\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 159}),\n"," Document(page_content='Question: What venues has Chenyan Xiong published in?\\nAnswer: International Conference on Information and Knowledge Management, arXiv.org, Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 160}),\n"," Document(page_content='Question: What is the most cited paper from Chenyan Xiong?\\nAnswer: OpenMatch-v2: An All-in-one Multi-Modality PLM-based Information Retrieval Toolkit\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 161}),\n"," Document(page_content='Question: What is the url of the most cited paper from Chenyan Xiong?\\nAnswer: https://dl.acm.org/doi/pdf/10.1145/3539618.3591813\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 162}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Chenyan Xiong?\\nAnswer: Shi Yu, Zhenghao Liu, Chenyan Xiong, Zhiyuan Liu\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 163}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Chenyan Xiong?\\nAnswer: As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure.\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 164}),\n"," Document(page_content='Question: Abstract of the most cited paper from Chenyan Xiong?\\nAnswer: Pre-trained language models (PLMs) have emerged as the foundation of the most advanced Information Retrieval (IR) models. Powered by PLMs, the latest IR research has proposed novel models, new domain adaptation algorithms as well as enlarged datasets. In this paper, we present a Python-based IR toolkit OpenMatch-v2. As a full upgrade of OpenMatch proposed in 2021, OpenMatch-v2 incorporates the most recent advancements of PLM-based IR research, providing support for new, cross-modality models and enhanced domain adaptation techniques with a streamlined, optimized infrastructure. The code of OpenMatch is publicly available at https://github.com/OpenMatch/OpenMatch.\\nDocument: ../data/paper_jsons/Chenyan Xiong_144628574.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 165}),\n"," Document(page_content='Question: What is the author ID of Chenyan Xiong?\\nAnswer: 2139787803\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 166}),\n"," Document(page_content='Question: What are the papers written by Chenyan Xiong?\\nAnswer: Improving Multitask Retrieval by Promoting Task Specialization, Unsupervised Dense Retrieval Training with Web Anchors, Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In, Model-Generated Pretraining Signals Improves Zero-Shot Generalization of Text-to-Text Transformers, Structure-Aware Language Model Pretraining Improves Dense Retrieval on Structured Data, Toolink: Linking Toolkit Creation and Using through Chain-of-Solving on Open-Source Model, Augmenting Zero-Shot Dense Retrievers with Plug-in Mixture-of-Memories\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 167}),\n"," Document(page_content='Question: What is the H-index of Chenyan Xiong?\\nAnswer: 7\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 168}),\n"," Document(page_content='Question: What is the author citation count of Chenyan Xiong?\\nAnswer: 222\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 169}),\n"," Document(page_content='Question: What is the author paper count of Chenyan Xiong?\\nAnswer: 22\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 170}),\n"," Document(page_content='Question: What journals has Chenyan Xiong published in?\\nAnswer: Transactions of the Association for Computational Linguistics, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval, ArXiv\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 171}),\n"," Document(page_content='Question: What are the journals and how many papers has Chenyan Xiong published in each?\\nAnswer: 3 in ArXiv, 1 in Transactions of the Association for Computational Linguistics, 1 in Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 172}),\n"," Document(page_content='Question: What are the fields of study of Chenyan Xiong?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 173}),\n"," Document(page_content='Question: How many papers has Chenyan Xiong published in open access journals?\\nAnswer: 7\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 174}),\n"," Document(page_content='Question: What venues has Chenyan Xiong published in?\\nAnswer: Transactions of the Association for Computational Linguistics, Annual International ACM SIGIR Conference on Research and Development in Information Retrieval, Annual Meeting of the Association for Computational Linguistics, arXiv.org, Conference on Empirical Methods in Natural Language Processing\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 175}),\n"," Document(page_content='Question: What is the most cited paper from Chenyan Xiong?\\nAnswer: Augmentation-Adapted Retriever Improves Generalization of Language Models as Generic Plug-In\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 176}),\n"," Document(page_content='Question: What is the url of the most cited paper from Chenyan Xiong?\\nAnswer: http://arxiv.org/pdf/2305.17331\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 177}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Chenyan Xiong?\\nAnswer: Zichun Yu, Chenyan Xiong, S. Yu, Zhiyuan Liu\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 178}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Chenyan Xiong?\\nAnswer: This paper proposes augmentation-adapted retriever (AAR), which learns LM’s preferences obtained from a known source LM to assist target LMs that may not be known beforehand or are unable to be fine-tuned together in a generic retrieval plug-in.\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 179}),\n"," Document(page_content='Question: Abstract of the most cited paper from Chenyan Xiong?\\nAnswer: Retrieval augmentation can aid language models (LMs) in knowledge-intensive tasks by supplying them with external information. Prior works on retrieval augmentation usually jointly fine-tune the retriever and the LM, making them closely coupled. In this paper, we explore the scheme of generic retrieval plug-in: the retriever is to assist target LMs that may not be known beforehand or are unable to be fine-tuned together. To retrieve useful documents for unseen target LMs, we propose augmentation-adapted retriever (AAR), which learns LM’s preferences obtained from a known source LM. Experiments on the MMLU and PopQA datasets demonstrate that our AAR trained with a small source LM is able to significantly improve the zero-shot generalization of larger target LMs ranging from 250M Flan-T5 to 175B InstructGPT. Further analysis indicates that the preferences of different LMs overlap, enabling AAR trained with a single source LM to serve as a generic plug-in for various target LMs. Our code is open-sourced at https://github.com/OpenMatch/Augmentation-Adapted-Retriever.\\nDocument: ../data/paper_jsons/Chenyan Xiong_2139787803.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 180}),\n"," Document(page_content='Question: What is the author ID of Daniel Fried?\\nAnswer: 47070750\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 181}),\n"," Document(page_content=\"Question: What are the papers written by Daniel Fried?\\nAnswer: Pragmatic Inference with a CLIP Listener for Contrastive Captioning, SantaCoder: don't reach for the stars!, Grounding Language Models to Images for Multimodal Generation, StarCoder: may the source be with you!, Generating Images with Multimodal Language Models, WebArena: A Realistic Web Environment for Building Autonomous Agents\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 182}),\n"," Document(page_content='Question: What is the H-index of Daniel Fried?\\nAnswer: 26\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 183}),\n"," Document(page_content='Question: What is the author citation count of Daniel Fried?\\nAnswer: 2668\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 184}),\n"," Document(page_content='Question: What is the author paper count of Daniel Fried?\\nAnswer: 50\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 185}),\n"," Document(page_content='Question: What journals has Daniel Fried published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 186}),\n"," Document(page_content='Question: What are the journals and how many papers has Daniel Fried published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 187}),\n"," Document(page_content='Question: What are the fields of study of Daniel Fried?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 188}),\n"," Document(page_content='Question: How many papers has Daniel Fried published in open access journals?\\nAnswer: 6\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 189}),\n"," Document(page_content='Question: What venues has Daniel Fried published in?\\nAnswer: Annual Meeting of the Association for Computational Linguistics, arXiv.org\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 190}),\n"," Document(page_content='Question: What is the most cited paper from Daniel Fried?\\nAnswer: StarCoder: may the source be with you!\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 191}),\n"," Document(page_content='Question: What is the url of the most cited paper from Daniel Fried?\\nAnswer: http://arxiv.org/pdf/2305.06161\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 192}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Daniel Fried?\\nAnswer: Raymond Li, Loubna Ben Allal, Yangtian Zi, Niklas Muennighoff, Denis Kocetkov, Chenghao Mou, Marc Marone, Christopher Akiki, Jia Li, Jenny Chim, Qian Liu, Evgenii Zheltonozhskii, Terry Yue Zhuo, Thomas Wang, Olivier Dehaene, Mishig Davaadorj, J. Lamy-Poirier, Joao Monteiro, Oleh Shliazhko, Nicolas Gontier, Nicholas Meade, Armel Zebaze, Ming-Ho Yee, Logesh Kumar Umapathi, Jian Zhu, Benjamin Lipkin, Muhtasham Oblokulov, Zhiruo Wang, Rudra Murthy, J. Stillerman, S. Patel, Dmitry Abulkhanov, Marco Zocca, Manan Dey, Zhihan Zhang, N. Fahmy, Urvashi Bhattacharyya, W. Yu, Swayam Singh, Sasha Luccioni, Paulo Villegas, M. Kunakov, Fedor Zhdanov, Manuel Romero, Tony Lee, Nadav Timor, Jennifer Ding, Claire Schlesinger, Hailey Schoelkopf, Jana Ebert, Tri Dao, Mayank Mishra, A. Gu, Jennifer Robinson, Carolyn Jane Anderson, Brendan Dolan-Gavitt, Danish Contractor, Siva Reddy, Daniel Fried, Dzmitry Bahdanau, Yacine Jernite, Carlos Munoz Ferrandis, Sean M. Hughes, Thomas Wolf, Arjun Guha, Leandro von Werra, Harm de Vries\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 193}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Daniel Fried?\\nAnswer: This work performs the most comprehensive evaluation of Code LLMs to date and shows that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model.\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 194}),\n"," Document(page_content='Question: Abstract of the most cited paper from Daniel Fried?\\nAnswer: The BigCode community, an open-scientific collaboration working on the responsible development of Large Language Models for Code (Code LLMs), introduces StarCoder and StarCoderBase: 15.5B parameter models with 8K context length, infilling capabilities and fast large-batch inference enabled by multi-query attention. StarCoderBase is trained on 1 trillion tokens sourced from The Stack, a large collection of permissively licensed GitHub repositories with inspection tools and an opt-out process. We fine-tuned StarCoderBase on 35B Python tokens, resulting in the creation of StarCoder. We perform the most comprehensive evaluation of Code LLMs to date and show that StarCoderBase outperforms every open Code LLM that supports multiple programming languages and matches or outperforms the OpenAI code-cushman-001 model. Furthermore, StarCoder outperforms every model that is fine-tuned on Python, can be prompted to achieve 40\\\\% pass@1 on HumanEval, and still retains its performance on other programming languages. We take several important steps towards a safe open-access model release, including an improved PII redaction pipeline and a novel attribution tracing tool, and make the StarCoder models publicly available under a more commercially viable version of the Open Responsible AI Model license.\\nDocument: ../data/paper_jsons/Daniel Fried_47070750.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 195}),\n"," Document(page_content='Question: What is the author ID of Daphne Ippolito?\\nAnswer: 7975935\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 196}),\n"," Document(page_content=\"Question: What are the papers written by Daphne Ippolito?\\nAnswer: Reverse-Engineering Decoding Strategies Given Blackbox Access to a Language Generation System, A Pretrainer's Guide to Training Data: Measuring the Effects of Data Age, Domain Coverage, Quality, & Toxicity, Extracting Training Data from Diffusion Models, Are aligned neural networks adversarially aligned?\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 197}),\n"," Document(page_content='Question: What is the H-index of Daphne Ippolito?\\nAnswer: 25\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 198}),\n"," Document(page_content='Question: What is the author citation count of Daphne Ippolito?\\nAnswer: 6460\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 199}),\n"," Document(page_content='Question: What is the author paper count of Daphne Ippolito?\\nAnswer: 44\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 200}),\n"," Document(page_content='Question: What journals has Daphne Ippolito published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 201}),\n"," Document(page_content='Question: What are the journals and how many papers has Daphne Ippolito published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 202}),\n"," Document(page_content='Question: What are the fields of study of Daphne Ippolito?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 203}),\n"," Document(page_content='Question: How many papers has Daphne Ippolito published in open access journals?\\nAnswer: 4\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 204}),\n"," Document(page_content='Question: What venues has Daphne Ippolito published in?\\nAnswer: International Conference on Natural Language Generation, arXiv.org, USENIX Security Symposium\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 205}),\n"," Document(page_content='Question: What is the most cited paper from Daphne Ippolito?\\nAnswer: Extracting Training Data from Diffusion Models\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 206}),\n"," Document(page_content='Question: What is the url of the most cited paper from Daphne Ippolito?\\nAnswer: http://arxiv.org/pdf/2301.13188\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 207}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Daphne Ippolito?\\nAnswer: Nicholas Carlini, Jamie Hayes, Milad Nasr, Matthew Jagielski, Vikash Sehwag, Florian Tramer, B. Balle, Daphne Ippolito, Eric Wallace\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 208}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Daphne Ippolito?\\nAnswer: The results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 209}),\n"," Document(page_content='Question: Abstract of the most cited paper from Daphne Ippolito?\\nAnswer: Image diffusion models such as DALL-E 2, Imagen, and Stable Diffusion have attracted significant attention due to their ability to generate high-quality synthetic images. In this work, we show that diffusion models memorize individual images from their training data and emit them at generation time. With a generate-and-filter pipeline, we extract over a thousand training examples from state-of-the-art models, ranging from photographs of individual people to trademarked company logos. We also train hundreds of diffusion models in various settings to analyze how different modeling and data decisions affect privacy. Overall, our results show that diffusion models are much less private than prior generative models such as GANs, and that mitigating these vulnerabilities may require new advances in privacy-preserving training.\\nDocument: ../data/paper_jsons/Daphne Ippolito_7975935.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 210}),\n"," Document(page_content='Question: What is the author ID of David R Mortensen?\\nAnswer: 3407646\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 211}),\n"," Document(page_content='Question: What are the papers written by David R Mortensen?\\nAnswer: ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages, Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models, SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing, Construction Grammar Provides Unique Insight into Neural Language Models, Generalized Glossing Guidelines: An Explicit, Human- and Machine-Readable, Item-and-Process Convention for Morphological Annotation, Transformed Protoform Reconstruction, PWESuite: Phonetic Word Embeddings and Tasks They Facilitate\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 212}),\n"," Document(page_content='Question: What is the H-index of David R Mortensen?\\nAnswer: 15\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 213}),\n"," Document(page_content='Question: What is the author citation count of David R Mortensen?\\nAnswer: 1049\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 214}),\n"," Document(page_content='Question: What is the author paper count of David R Mortensen?\\nAnswer: 75\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 215}),\n"," Document(page_content='Question: What journals has David R Mortensen published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 216}),\n"," Document(page_content='Question: What are the journals and how many papers has David R Mortensen published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 217}),\n"," Document(page_content='Question: What are the fields of study of David R Mortensen?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 218}),\n"," Document(page_content='Question: How many papers has David R Mortensen published in open access journals?\\nAnswer: 7\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 219}),\n"," Document(page_content='Question: What venues has David R Mortensen published in?\\nAnswer: Conference on Machine Translation, Conference on Empirical Methods in Natural Language Processing, Special Interest Group on Computational Morphology and Phonology Workshop, CXGSNLP, Annual Meeting of the Association for Computational Linguistics, arXiv.org\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 220}),\n"," Document(page_content='Question: What is the most cited paper from David R Mortensen?\\nAnswer: Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 221}),\n"," Document(page_content='Question: What is the url of the most cited paper from David R Mortensen?\\nAnswer: http://arxiv.org/pdf/2305.13707\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 222}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from David R Mortensen?\\nAnswer: Orevaoghene Ahia, Sachin Kumar, Hila Gonen, Jungo Kasai, David R. Mortensen, Noah A. Smith, Yulia Tsvetkov\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 223}),\n"," Document(page_content=\"Question: TLDR/Summary of the most cited paper from David R Mortensen?\\nAnswer: This work conducts a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages and shows evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results.\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 224}),\n"," Document(page_content=\"Question: Abstract of the most cited paper from David R Mortensen?\\nAnswer: Language models have graduated from being research prototypes to commercialized products offered as web APIs, and recent works have highlighted the multilingual capabilities of these products. The API vendors charge their users based on usage, more specifically on the number of ``tokens'' processed or generated by the underlying language models. What constitutes a token, however, is training data and model dependent with a large variance in the number of tokens required to convey the same information in different languages. In this work, we analyze the effect of this non-uniformity on the fairness of an API's pricing policy across languages. We conduct a systematic analysis of the cost and utility of OpenAI's language model API on multilingual benchmarks in 22 typologically diverse languages. We show evidence that speakers of a large number of the supported languages are overcharged while obtaining poorer results. These speakers tend to also come from regions where the APIs are less affordable to begin with. Through these analyses, we aim to increase transparency around language model APIs' pricing policies and encourage the vendors to make them more equitable.\\nDocument: ../data/paper_jsons/David R. Mortensen_3407646.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 225}),\n"," Document(page_content='Question: What is the author ID of Eric P. Xing?\\nAnswer: 143977260\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 226}),\n"," Document(page_content='Question: What are the papers written by Eric P. Xing?\\nAnswer: Identification of Nonlinear Latent Hierarchical Models, StyleRF: Zero-Shot 3D Style Transfer of Neural Radiance Fields, Judging LLM-as-a-judge with MT-Bench and Chatbot Arena, Understanding Masked Autoencoders via Hierarchical Latent Variable Models\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 227}),\n"," Document(page_content='Question: What is the H-index of Eric P. Xing?\\nAnswer: 103\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 228}),\n"," Document(page_content='Question: What is the author citation count of Eric P. Xing?\\nAnswer: 46108\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 229}),\n"," Document(page_content='Question: What is the author paper count of Eric P. Xing?\\nAnswer: 631\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 230}),\n"," Document(page_content='Question: What journals has Eric P. Xing published in?\\nAnswer: ArXiv, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 231}),\n"," Document(page_content='Question: What are the journals and how many papers has Eric P. Xing published in each?\\nAnswer: 2 in ArXiv, 2 in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 232}),\n"," Document(page_content='Question: What are the fields of study of Eric P. Xing?\\nAnswer: Computer Science, Mathematics\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 233}),\n"," Document(page_content='Question: How many papers has Eric P. Xing published in open access journals?\\nAnswer: 4\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 234}),\n"," Document(page_content='Question: What venues has Eric P. Xing published in?\\nAnswer: arXiv.org, Computer Vision and Pattern Recognition\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 235}),\n"," Document(page_content='Question: What is the most cited paper from Eric P. Xing?\\nAnswer: Judging LLM-as-a-judge with MT-Bench and Chatbot Arena\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 236}),\n"," Document(page_content='Question: What is the url of the most cited paper from Eric P. Xing?\\nAnswer: https://arxiv.org/pdf/2306.05685\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 237}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Eric P. Xing?\\nAnswer: Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li, Dacheng Li, E. Xing, Haotong Zhang, Joseph Gonzalez, I. Stoica\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 238}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Eric P. Xing?\\nAnswer: The results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans, and LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain.\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 239}),\n"," Document(page_content='Question: Abstract of the most cited paper from Eric P. Xing?\\nAnswer: Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.\\nDocument: ../data/paper_jsons/E. Xing_143977260.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 240}),\n"," Document(page_content='Question: What is the author ID of Emma Strubell?\\nAnswer: 2268272\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 241}),\n"," Document(page_content='Question: What are the papers written by Emma Strubell?\\nAnswer: To Build Our Future, We Must Know Our Past: Contextualizing Paradigm Shifts in Natural Language Processing, Understanding the Effect of Model Compression on Social Bias in Large Language Models, Surveying (Dis)Parities and Concerns of Compute Hungry NLP Research, On the Interactions of Structural Constraints and Data Resources for Structured Prediction, Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation, Queer People are People First: Deconstructing Sexual Identity Stereotypes in Large Language Models, Making Scalable Meta Learning Practical, Regularizing Self-training for Unsupervised Domain Adaptation via Structural Constraints, The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment, Data-efficient Active Learning for Structured Prediction with Partial Annotation and Self-Training\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 242}),\n"," Document(page_content='Question: What is the H-index of Emma Strubell?\\nAnswer: 17\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 243}),\n"," Document(page_content='Question: What is the author citation count of Emma Strubell?\\nAnswer: 4143\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 244}),\n"," Document(page_content='Question: What is the author paper count of Emma Strubell?\\nAnswer: 62\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 245}),\n"," Document(page_content='Question: What journals has Emma Strubell published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 246}),\n"," Document(page_content='Question: What are the journals and how many papers has Emma Strubell published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 247}),\n"," Document(page_content='Question: What are the fields of study of Emma Strubell?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 248}),\n"," Document(page_content='Question: How many papers has Emma Strubell published in open access journals?\\nAnswer: 10\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 249}),\n"," Document(page_content='Question: What venues has Emma Strubell published in?\\nAnswer: Conference on Empirical Methods in Natural Language Processing, arXiv.org, SUSTAINLP\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 250}),\n"," Document(page_content='Question: What is the most cited paper from Emma Strubell?\\nAnswer: Efficiency Pentathlon: A Standardized Arena for Efficiency Evaluation\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 251}),\n"," Document(page_content='Question: What is the url of the most cited paper from Emma Strubell?\\nAnswer: https://arxiv.org/pdf/2307.09701\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 252}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Emma Strubell?\\nAnswer: Hao Peng, Qingqing Cao, Jesse Dodge, Matthew E. Peters, Jared Fernandez, Tom Sherborne, Kyle Lo, Sam Skjonsberg, Emma Strubell, Darrell Plessas, Iz Beltagy, Pete Walsh, Noah A. Smith, Hannaneh Hajishirzi\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 253}),\n"," Document(page_content=\"Question: TLDR/Summary of the most cited paper from Emma Strubell?\\nAnswer: Pentathlon is a benchmark for holistic and realistic evaluation of model efficiency, which focuses on inference, which accounts for a majority of the compute in a model's lifecycle, and is designed to mirror real-world applications scenarios.\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 254}),\n"," Document(page_content=\"Question: Abstract of the most cited paper from Emma Strubell?\\nAnswer: Rising computational demands of modern natural language processing (NLP) systems have increased the barrier to entry for cutting-edge research while posing serious environmental concerns. Yet, progress on model efficiency has been impeded by practical challenges in model evaluation and comparison. For example, hardware is challenging to control due to disparate levels of accessibility across different institutions. Moreover, improvements in metrics such as FLOPs often fail to translate to progress in real-world applications. In response, we introduce Pentathlon, a benchmark for holistic and realistic evaluation of model efficiency. Pentathlon focuses on inference, which accounts for a majority of the compute in a model's lifecycle. It offers a strictly-controlled hardware platform, and is designed to mirror real-world applications scenarios. It incorporates a suite of metrics that target different aspects of efficiency, including latency, throughput, memory overhead, and energy consumption. Pentathlon also comes with a software library that can be seamlessly integrated into any codebase and enable evaluation. As a standardized and centralized evaluation platform, Pentathlon can drastically reduce the workload to make fair and reproducible efficiency comparisons. While initially focused on natural language processing (NLP) models, Pentathlon is designed to allow flexible extension to other fields. We envision Pentathlon will stimulate algorithmic innovations in building efficient models, and foster an increased awareness of the social and environmental implications in the development of future-generation NLP models.\\nDocument: ../data/paper_jsons/Emma Strubell_2268272.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 255}),\n"," Document(page_content='Question: What is the author ID of Eric Nyberg?\\nAnswer: 144287919\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 256}),\n"," Document(page_content='Question: What are the papers written by Eric Nyberg?\\nAnswer: GameQA: Gamified Mobile App Platform for Building Multiple-Domain Question-Answering Datasets, InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers, Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA, Chain-of-Skills: A Configurable Model for Open-Domain Question Answering\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 257}),\n"," Document(page_content='Question: What is the H-index of Eric Nyberg?\\nAnswer: 38\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 258}),\n"," Document(page_content='Question: What is the author citation count of Eric Nyberg?\\nAnswer: 6031\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 259}),\n"," Document(page_content='Question: What is the author paper count of Eric Nyberg?\\nAnswer: 216\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 260}),\n"," Document(page_content='Question: What journals has Eric Nyberg published in?\\nAnswer: ArXiv, Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 261}),\n"," Document(page_content='Question: What are the journals and how many papers has Eric Nyberg published in each?\\nAnswer: 1 in ArXiv, 1 in Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 262}),\n"," Document(page_content='Question: What are the fields of study of Eric Nyberg?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 263}),\n"," Document(page_content='Question: How many papers has Eric Nyberg published in open access journals?\\nAnswer: 4\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 264}),\n"," Document(page_content='Question: What venues has Eric Nyberg published in?\\nAnswer: Conference of the European Chapter of the Association for Computational Linguistics, arXiv.org, Workshop on Document-grounded Dialogue and Conversational Question Answering, Annual Meeting of the Association for Computational Linguistics\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 265}),\n"," Document(page_content='Question: What is the most cited paper from Eric Nyberg?\\nAnswer: InPars-Light: Cost-Effective Unsupervised Training of Efficient Rankers\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 266}),\n"," Document(page_content='Question: What is the url of the most cited paper from Eric Nyberg?\\nAnswer: http://arxiv.org/pdf/2301.02998\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 267}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Eric Nyberg?\\nAnswer: Leonid Boytsov, Preksha Patel, Vivek Sourabh, Riddhi Nisar, Sayan Kundu, R. Ramanathan, Eric Nyberg\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 268}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Eric Nyberg?\\nAnswer: InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25.\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 269}),\n"," Document(page_content='Question: Abstract of the most cited paper from Eric Nyberg?\\nAnswer: We carried out a reproducibility study of InPars, which is a method for unsupervised training of neural rankers (Bonifacio et al., 2022). As a by-product, we developed InPars-light, which is a simple-yet-effective modification of InPars. Unlike InPars, InPars-light uses 7x-100x smaller ranking models and only a freely available language model BLOOM, which -- as we found out -- produced more accurate rankers compared to a proprietary GPT-3 model. On all five English retrieval collections (used in the original InPars study) we obtained substantial (7%-30%) and statistically significant improvements over BM25 (in nDCG and MRR) using only a 30M parameter six-layer MiniLM-30M ranker and a single three-shot prompt. In contrast, in the InPars study only a 100x larger monoT5-3B model consistently outperformed BM25, whereas their smaller monoT5-220M model (which is still 7x larger than our MiniLM ranker) outperformed BM25 only on MS MARCO and TREC DL 2020. In the same three-shot prompting scenario, our 435M parameter DeBERTA v3 ranker was at par with the 7x larger monoT5-3B (average gain over BM25 of 1.3 vs 1.32): In fact, on three out of five datasets, DeBERTA slightly outperformed monoT5-3B. Finally, these good results were achieved by re-ranking only 100 candidate documents compared to 1000 used by Bonifacio et al. (2022). We believe that InPars-light is the first truly cost-effective prompt-based unsupervised recipe to train and deploy neural ranking models that outperform BM25. Our code and data is publicly available. https://github.com/searchivarius/inpars_light/\\nDocument: ../data/paper_jsons/Eric Nyberg_144287919.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 270}),\n"," Document(page_content='Question: What is the author ID of Fernando-Diaz?\\nAnswer: 145472333\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 271}),\n"," Document(page_content='Question: What are the papers written by Fernando-Diaz?\\nAnswer: Best-Case Retrieval Evaluation: Improving the Sensitivity of Reciprocal Rank with Lexicographic Precision, Fairness Through Domain Awareness: Mitigating Popularity Bias For Music Discovery, Overview of the TREC 2021 Fair Ranking Track\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 272}),\n"," Document(page_content='Question: What is the H-index of Fernando-Diaz?\\nAnswer: 45\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 273}),\n"," Document(page_content='Question: What is the author citation count of Fernando-Diaz?\\nAnswer: 7996\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 274}),\n"," Document(page_content='Question: What is the author paper count of Fernando-Diaz?\\nAnswer: 142\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 275}),\n"," Document(page_content='Question: What journals has Fernando-Diaz published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 276}),\n"," Document(page_content='Question: What are the journals and how many papers has Fernando-Diaz published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 277}),\n"," Document(page_content='Question: What are the fields of study of Fernando-Diaz?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 278}),\n"," Document(page_content='Question: How many papers has Fernando-Diaz published in open access journals?\\nAnswer: 3\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 279}),\n"," Document(page_content='Question: What venues has Fernando-Diaz published in?\\nAnswer: arXiv.org, Text Retrieval Conference\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 280}),\n"," Document(page_content='Question: What is the most cited paper from Fernando-Diaz?\\nAnswer: Overview of the TREC 2021 Fair Ranking Track\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 281}),\n"," Document(page_content='Question: What is the url of the most cited paper from Fernando-Diaz?\\nAnswer: http://arxiv.org/pdf/2302.10856\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 282}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Fernando-Diaz?\\nAnswer: Asia J. Biega, Fernando Diaz, Michael D. Ekstrand, Sebastian Kohlmeier\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 283}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Fernando-Diaz?\\nAnswer: The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia.\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 284}),\n"," Document(page_content=\"Question: Abstract of the most cited paper from Fernando-Diaz?\\nAnswer: The TREC Fair Ranking Track aims to provide a platform for participants to develop and evaluate novel retrieval algorithms that can provide a fair exposure to a mixture of demographics or attributes, such as ethnicity, that are represented by relevant documents in response to a search query. For example, particular demographics or attributes can be represented by the documents' topical content or authors. The 2021 Fair Ranking Track adopted a resource allocation task. The task focused on supporting Wikipedia editors who are looking to improve the encyclopedia's coverage of topics under the purview of a WikiProject. WikiProject coordinators and/or Wikipedia editors search for Wikipedia documents that are in need of editing to improve the quality of the article. The 2021 Fair Ranking track aimed to ensure that documents that are about, or somehow represent, certain protected characteristics receive a fair exposure to the Wikipedia editors, so that the documents have an fair opportunity of being improved and, therefore, be well-represented in Wikipedia. The under-representation of particular protected characteristics in Wikipedia can result in systematic biases that can have a negative human, social, and economic impact, particularly for disadvantaged or protected societal groups.\\nDocument: ../data/paper_jsons/Fernando Diaz_145472333.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 285}),\n"," Document(page_content='Question: What is the author ID of Graham Neubig?\\nAnswer: 1700325\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 286}),\n"," Document(page_content='Question: What are the papers written by Graham Neubig?\\nAnswer: Cross-Modal Fine-Tuning: Align then Refine, ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages, GlobalBench: A Benchmark for Global Progress in Natural Language Processing, Learning Performance-Improving Code Edits, CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code, Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction, User-Centric Evaluation of OCR Systems for Kwak’wala, Multi-Dimensional Evaluation of Text Summarization with In-Context Learning, A Gold Standard Dataset for the Reviewer Assignment Problem, SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing, Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation, FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios, Active Retrieval Augmented Generation, Large Language Models Enable Few-Shot Clustering, Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach, Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting, Why do Nearest Neighbor Language Models Work?, Syntax and Semantics Meet in the “Middle”: Probing the Syntax-Semantics Interface of LMs Through Agentivity, DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions, Multi-lingual and Multi-cultural Figurative Language Understanding, It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk, Unlimiformer: Long-Range Transformers with Unlimited Length Input, WebArena: A Realistic Web Environment for Building Autonomous Agents, Prompt2Model: Generating Deployable Models from Natural Language Instructions, Computational Language Acquisition with Theory of Mind, Improving Factuality of Abstractive Summarization via Contrastive Reward Learning, The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 287}),\n"," Document(page_content='Question: What is the H-index of Graham Neubig?\\nAnswer: 75\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 288}),\n"," Document(page_content='Question: What is the author citation count of Graham Neubig?\\nAnswer: 24256\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 289}),\n"," Document(page_content='Question: What is the author paper count of Graham Neubig?\\nAnswer: 600\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 290}),\n"," Document(page_content='Question: What journals has Graham Neubig published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 291}),\n"," Document(page_content='Question: What are the journals and how many papers has Graham Neubig published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 292}),\n"," Document(page_content='Question: What are the fields of study of Graham Neubig?\\nAnswer: Computer Science, Mathematics\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 293}),\n"," Document(page_content='Question: How many papers has Graham Neubig published in open access journals?\\nAnswer: 27\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 294}),\n"," Document(page_content='Question: What venues has Graham Neubig published in?\\nAnswer: International Conference on Machine Learning, Conference on Machine Translation, Conference on Empirical Methods in Natural Language Processing, arXiv.org, AMERICASNLP, COMPUTEL, Annual Meeting of the Association for Computational Linguistics, Special Interest Group on Computational Morphology and Phonology Workshop, STARSEM, BIGPICTURE, International Conference on Learning Representations, TRUSTNLP\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 295}),\n"," Document(page_content='Question: What is the most cited paper from Graham Neubig?\\nAnswer: WebArena: A Realistic Web Environment for Building Autonomous Agents\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 296}),\n"," Document(page_content='Question: What is the url of the most cited paper from Graham Neubig?\\nAnswer: https://arxiv.org/pdf/2307.13854\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 297}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Graham Neubig?\\nAnswer: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 298}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Graham Neubig?\\nAnswer: This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 299}),\n"," Document(page_content='Question: Abstract of the most cited paper from Graham Neubig?\\nAnswer: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.\\nDocument: ../data/paper_jsons/Graham Neubig_1700325.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 300}),\n"," Document(page_content='Question: What is the author ID of Jamie Callan?\\nAnswer: 144987107\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 301}),\n"," Document(page_content='Question: What are the papers written by Jamie Callan?\\nAnswer: Conversational Search with Random Walks over Entity Graphs, KALE: Using a K-Sparse Projector for Lexical Expansion, CSurF: Sparse Lexical Retrieval through Contextualized Surface Forms, Active Retrieval Augmented Generation, Multi-Objective Improvement of Android Applications\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 302}),\n"," Document(page_content='Question: What is the H-index of Jamie Callan?\\nAnswer: 75\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 303}),\n"," Document(page_content='Question: What is the author citation count of Jamie Callan?\\nAnswer: 19329\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 304}),\n"," Document(page_content='Question: What is the author paper count of Jamie Callan?\\nAnswer: 331\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 305}),\n"," Document(page_content='Question: What journals has Jamie Callan published in?\\nAnswer: Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval, ArXiv\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 306}),\n"," Document(page_content='Question: What are the journals and how many papers has Jamie Callan published in each?\\nAnswer: 3 in Proceedings of the 2023 ACM SIGIR International Conference on Theory of Information Retrieval, 2 in ArXiv\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 307}),\n"," Document(page_content='Question: What are the fields of study of Jamie Callan?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 308}),\n"," Document(page_content='Question: How many papers has Jamie Callan published in open access journals?\\nAnswer: 5\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 309}),\n"," Document(page_content='Question: What venues has Jamie Callan published in?\\nAnswer: International Conference on the Theory of Information Retrieval, Conference on Empirical Methods in Natural Language Processing, arXiv.org\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 310}),\n"," Document(page_content='Question: What is the most cited paper from Jamie Callan?\\nAnswer: Active Retrieval Augmented Generation\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 311}),\n"," Document(page_content='Question: What is the url of the most cited paper from Jamie Callan?\\nAnswer: http://arxiv.org/pdf/2305.06983\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 312}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Jamie Callan?\\nAnswer: Zhengbao Jiang, Frank F. Xu, Luyu Gao, Zhiqing Sun, Qian Liu, Jane Dwivedi-Yu, Yiming Yang, Jamie Callan, Graham Neubig\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 313}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Jamie Callan?\\nAnswer: This work proposes Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens.\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 314}),\n"," Document(page_content='Question: Abstract of the most cited paper from Jamie Callan?\\nAnswer: Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.\\nDocument: ../data/paper_jsons/Jamie Callan_144987107.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 315}),\n"," Document(page_content='Question: What is the author ID of Jeffrey Bigham?\\nAnswer: 1744846\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 316}),\n"," Document(page_content='Question: What are the papers written by Jeffrey Bigham?\\nAnswer: Screen Correspondence: Mapping Interchangeable Elements between UIs, Exploring Stigmergic Collaboration and Task Modularity Through an Expert Crowdsourcing Annotation System: The Case of Storm Phenomena in the Euro-Atlantic Region, Nonverbal Communication through Expressive Objects, USB: A Unified Summarization Benchmark Across Tasks and Domains, WebUI: A Dataset for Enhancing Visual UI Understanding with Web Semantics, Never-ending Learning of User Interfaces, Latent Phrase Matching for Dysarthric Speech, From User Perceptions to Technical Improvement: Enabling People Who Stutter to Better Use Speech Recognition\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 317}),\n"," Document(page_content='Question: What is the H-index of Jeffrey Bigham?\\nAnswer: 56\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 318}),\n"," Document(page_content='Question: What is the author citation count of Jeffrey Bigham?\\nAnswer: 10897\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 319}),\n"," Document(page_content='Question: What is the author paper count of Jeffrey Bigham?\\nAnswer: 288\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 320}),\n"," Document(page_content='Question: What journals has Jeffrey Bigham published in?\\nAnswer: ArXiv, IEEE Access, Communications of the ACM, Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 321}),\n"," Document(page_content='Question: What are the journals and how many papers has Jeffrey Bigham published in each?\\nAnswer: 2 in ArXiv, 2 in Proceedings of the 2023 CHI Conference on Human Factors in Computing Systems, 1 in IEEE Access, 1 in Communications of the ACM, 1 in Proceedings of the 36th Annual ACM Symposium on User Interface Software and Technology\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 322}),\n"," Document(page_content='Question: What are the fields of study of Jeffrey Bigham?\\nAnswer: Computer Science, Engineering\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 323}),\n"," Document(page_content='Question: How many papers has Jeffrey Bigham published in open access journals?\\nAnswer: 8\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 324}),\n"," Document(page_content='Question: What venues has Jeffrey Bigham published in?\\nAnswer: arXiv.org, IEEE Access, Communications of the ACM, Conference on Empirical Methods in Natural Language Processing, International Conference on Human Factors in Computing Systems, ACM Symposium on User Interface Software and Technology, Interspeech\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 325}),\n"," Document(page_content='Question: What is the most cited paper from Jeffrey Bigham?\\nAnswer: WebUI: A Dataset for Enhancing Visual UI Understanding with Web Semantics\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 326}),\n"," Document(page_content='Question: What is the url of the most cited paper from Jeffrey Bigham?\\nAnswer: https://dl.acm.org/doi/pdf/10.1145/3544548.3581158\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 327}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Jeffrey Bigham?\\nAnswer: Jason Wu, Siyan Wang, Siman Shen, Yi-Hao Peng, Jeffrey Nichols, Jeffrey P. Bigham\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 328}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Jeffrey Bigham?\\nAnswer: This work crawled the web to construct WebUI, a large dataset of 400,000 rendered web pages associated with automatically extracted metadata, and analyzed the composition of WebUI to show that while automatically extracted data is noisy, most examples meet basic criteria for visual UI modeling.\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 329}),\n"," Document(page_content='Question: Abstract of the most cited paper from Jeffrey Bigham?\\nAnswer: Modeling user interfaces (UIs) from visual information allows systems to make inferences about the functionality and semantics needed to support use cases in accessibility, app automation, and testing. Current datasets for training machine learning models are limited in size due to the costly and time-consuming process of manually collecting and annotating UIs. We crawled the web to construct WebUI, a large dataset of 400,000 rendered web pages associated with automatically extracted metadata. We analyze the composition of WebUI and show that while automatically extracted data is noisy, most examples meet basic criteria for visual UI modeling. We applied several strategies for incorporating semantics found in web pages to increase the performance of visual UI understanding models in the mobile domain, where less labeled data is available: (i) element detection, (ii) screen classification and (iii) screen similarity.\\nDocument: ../data/paper_jsons/Jeffrey P. Bigham_1744846.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 330}),\n"," Document(page_content='Question: What is the author ID of Justine Cassell?\\nAnswer: 145431806\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 331}),\n"," Document(page_content='Question: What are the papers written by Justine Cassell?\\nAnswer: When to generate hedges in peer-tutoring interactions, How About Kind of Generating Hedges using End-to-End Neural Models?, \"You might think about slightly revising the title”: Identifying Hedges in Peer-tutoring Interactions\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 332}),\n"," Document(page_content='Question: What is the H-index of Justine Cassell?\\nAnswer: 62\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 333}),\n"," Document(page_content='Question: What is the author citation count of Justine Cassell?\\nAnswer: 16004\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 334}),\n"," Document(page_content='Question: What is the author paper count of Justine Cassell?\\nAnswer: 218\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 335}),\n"," Document(page_content='Question: What journals has Justine Cassell published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 336}),\n"," Document(page_content='Question: What are the journals and how many papers has Justine Cassell published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 337}),\n"," Document(page_content='Question: What are the fields of study of Justine Cassell?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 338}),\n"," Document(page_content='Question: How many papers has Justine Cassell published in open access journals?\\nAnswer: 3\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 339}),\n"," Document(page_content='Question: What venues has Justine Cassell published in?\\nAnswer: SIGDIAL Conferences, Annual Meeting of the Association for Computational Linguistics\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 340}),\n"," Document(page_content='Question: What is the most cited paper from Justine Cassell?\\nAnswer: \"You might think about slightly revising the title”: Identifying Hedges in Peer-tutoring Interactions\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 341}),\n"," Document(page_content='Question: What is the url of the most cited paper from Justine Cassell?\\nAnswer: https://aclanthology.org/2022.acl-long.153.pdf\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 342}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Justine Cassell?\\nAnswer: Yann Raphalen, C. Clavel, Justine Cassell\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 343}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Justine Cassell?\\nAnswer: A model explainability tool is employed to explore the features that characterize hedges in peer-tutoring conversations, and some novel features, and the benefits of a such a hybrid model approach are identified.\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 344}),\n"," Document(page_content='Question: Abstract of the most cited paper from Justine Cassell?\\nAnswer: Hedges have an important role in the management of rapport. In peer-tutoring, they are notably used by tutors in dyads experiencing low rapport to tone down the impact of instructions and negative feedback.Pursuing the objective of building a tutoring agent that manages rapport with teenagers in order to improve learning, we used a multimodal peer-tutoring dataset to construct a computational framework for identifying hedges. We compared approaches relying on pre-trained resources with others that integrate insights from the social science literature. Our best performance involved a hybrid approach that outperforms the existing baseline while being easier to interpret. We employ a model explainability tool to explore the features that characterize hedges in peer-tutoring conversations, and we identify some novel features, and the benefits of a such a hybrid model approach.\\nDocument: ../data/paper_jsons/Justine Cassell_145431806.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 345}),\n"," Document(page_content='Question: What is the author ID of Lei Li?\\nAnswer: 143900005\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 346}),\n"," Document(page_content='Question: What are the papers written by Lei Li?\\nAnswer: Co-benefits of carbon neutrality in enhancing and stabilizing solar and wind energy, A Reverse-Biased Voltage Controlling Method for Mitigating Arm Overcurrent and Submodule Overvoltage in Hybrid MMCs During DC Faults, Impacts of Aerosol Chemical Composition on Cloud Condensation Nuclei (CCN) Activity during Wintertime in Beijing, China, Quantitative Analysis of Natural Gas Diffusion Characteristics in Tight Oil Reservoirs Based on Experimental and Numerical Simulation Methods, Quantitative Evaluation of Dust and Black Carbon Column Concentration in the MERRA-2 Reanalysis Dataset Using Satellite-Based Component Retrievals, Seasonal and Diurnal Characteristics of the Vertical Profile of Aerosol Optical Properties in Urban Beijing, 2017-2021, PlayGround Low Resource Machine Translation System for the 2023 AmericasNLP Shared Task, Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 347}),\n"," Document(page_content='Question: What is the H-index of Lei Li?\\nAnswer: 47\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 348}),\n"," Document(page_content='Question: What is the author citation count of Lei Li?\\nAnswer: 9898\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 349}),\n"," Document(page_content='Question: What is the author paper count of Lei Li?\\nAnswer: 181\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 350}),\n"," Document(page_content='Question: What journals has Lei Li published in?\\nAnswer: Nature Climate Change, IEEE Transactions on Power Electronics, Remote. Sens., ACS Omega, Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), ArXiv\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 351}),\n"," Document(page_content='Question: What are the journals and how many papers has Lei Li published in each?\\nAnswer: 3 in Remote. Sens., 1 in Nature Climate Change, 1 in IEEE Transactions on Power Electronics, 1 in ACS Omega, 1 in Proceedings of the Workshop on Natural Language Processing for Indigenous Languages of the Americas (AmericasNLP), 1 in ArXiv\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 352}),\n"," Document(page_content='Question: What are the fields of study of Lei Li?\\nAnswer: Computer Science, Medicine\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 353}),\n"," Document(page_content='Question: How many papers has Lei Li published in open access journals?\\nAnswer: 8\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 354}),\n"," Document(page_content='Question: What venues has Lei Li published in?\\nAnswer: Nature Climate Change, IEEE transactions on power electronics, Remote Sensing, ACS Omega, AMERICASNLP, arXiv.org\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 355}),\n"," Document(page_content='Question: What is the most cited paper from Lei Li?\\nAnswer: Multilingual Machine Translation with Large Language Models: Empirical Results and Analysis\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 356}),\n"," Document(page_content='Question: What is the url of the most cited paper from Lei Li?\\nAnswer: http://arxiv.org/pdf/2304.04675\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 357}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Lei Li?\\nAnswer: Wenhao Zhu, Hongyi Liu, Qingxiu Dong, Jingjing Xu, Lingpeng Kong, Jiajun Chen, Lei Li, Shujian Huang\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 358}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Lei Li?\\nAnswer: It is discovered that LLMs exhibit new working patterns when used for MMT and cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs.\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 359}),\n"," Document(page_content=\"Question: Abstract of the most cited paper from Lei Li?\\nAnswer: Large language models (LLMs) have demonstrated remarkable potential in handling multilingual machine translation (MMT). In this paper, we systematically investigate the advantages and challenges of LLMs for MMT by answering two questions: 1) How well do LLMs perform in translating massive languages? 2) Which factors affect LLMs' performance in translation? We thoroughly evaluate eight popular LLMs, including ChatGPT and GPT-4. Our empirical results show that translation capabilities of LLMs are continually improving. GPT-4 has beat the strong supervised baseline NLLB in 40.91% of translation directions but still faces a large gap towards the commercial translation system, especially on low-resource languages. Through further analysis, we discover that LLMs exhibit new working patterns when used for MMT. First, instruction semantics can surprisingly be ignored when given in-context exemplars. Second, cross-lingual exemplars can provide better task guidance for low-resource translation than exemplars in the same language pairs. Third, LLM can acquire translation ability in a resource-efficient way and generate moderate translation even on zero-resource languages.\\nDocument: ../data/paper_jsons/Lei Li_143900005.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 360}),\n"," Document(page_content='Question: What is the author ID of Lori S Levin?\\nAnswer: 1686960\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 361}),\n"," Document(page_content='Question: What are the papers written by Lori S Levin?\\nAnswer: Somatosensory and motor representations following bilateral transplants of the hands: A 6-year longitudinal case report on the first pediatric bilateral hand transplant patient, Identifying Health-Related Quality of Life Domains after Upper Extremity Transplantation., Assessment of quality of life after upper extremity transplantation: Framework for patient-reported outcome scale domains\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 362}),\n"," Document(page_content='Question: What is the H-index of Lori S Levin?\\nAnswer: 34\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 363}),\n"," Document(page_content='Question: What is the author citation count of Lori S Levin?\\nAnswer: 3290\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 364}),\n"," Document(page_content='Question: What is the author paper count of Lori S Levin?\\nAnswer: 174\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 365}),\n"," Document(page_content='Question: What journals has Lori S Levin published in?\\nAnswer: Brain Research, Archives of physical medicine and rehabilitation, Frontiers in Psychology\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 366}),\n"," Document(page_content='Question: What are the journals and how many papers has Lori S Levin published in each?\\nAnswer: 1 in Brain Research, 1 in Archives of physical medicine and rehabilitation, 1 in Frontiers in Psychology\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 367}),\n"," Document(page_content='Question: What are the fields of study of Lori S Levin?\\nAnswer: Medicine\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 368}),\n"," Document(page_content='Question: How many papers has Lori S Levin published in open access journals?\\nAnswer: 3\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 369}),\n"," Document(page_content='Question: What venues has Lori S Levin published in?\\nAnswer: Brain Research, Archives of Physical Medicine and Rehabilitation, Frontiers in Psychology\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 370}),\n"," Document(page_content='Question: What is the most cited paper from Lori S Levin?\\nAnswer: Identifying Health-Related Quality of Life Domains after Upper Extremity Transplantation.\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 371}),\n"," Document(page_content='Question: What is the url of the most cited paper from Lori S Levin?\\nAnswer: openAccessPdf not available\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 372}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Lori S Levin?\\nAnswer: D. Tulsky, Pamela A. Kisala, Callie E Tyner, J. Slotkin, C. Kaufman, C. Dearth, A. Horan, S. Talbot, J. Shores, K. Azari, C. Cetrulo, G. Brandacher, C. Cooney, David E Victorson, M. Dooley, Lori S. Levin, Cdr Scott M Tintle\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 373}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Lori S Levin?\\nAnswer: This study identified key constructs for use in evaluation of the potentially substantial physical, medical, social, and emotional effects of UET, including physical functioning and medical complications, positive and negative emotional functioning, and social participation, relationships, and independence.\\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 374}),\n"," Document(page_content='Question: Abstract of the most cited paper from Lori S Levin?\\nAnswer: \\nDocument: ../data/paper_jsons/Lori S. Levin_1686960.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 375}),\n"," Document(page_content='Question: What is the author ID of Louis-Philippe Morency?\\nAnswer: 49933077\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 376}),\n"," Document(page_content=\"Question: What are the papers written by Louis-Philippe Morency?\\nAnswer: Quantifying & Modeling Feature Interactions: An Information Decomposition Framework, Reconstructing the neurodynamics of face perception during real world vision in humans using intracranial EEG recordings, MultiZoo & MultiBench: A Standardized Toolkit for Multimodal Deep Learning, SenteCon: Leveraging Lexicons to Learn Human-Interpretable Language Representations, Neural Mixed Effects for Nonlinear Personalized Predictions, SHAP-based Prediction of Mother's History of Depression to Understand the Influence on Child Behavior, Counterfactual Augmentation for Multimodal Learning Under Presentation Bias, Difference-Masking: Choosing What to Mask in Continued Pretraining, Expanding the Role of Affective Phenomena in Multimodal Interaction Research, Multimodal Fusion Interactions: A Study of Human and Automatic Quantification, Multimodal Learning Without Labeled Multimodal Data: Guarantees and Applications, Representation Learning for Interpersonal and Multimodal Behavior Dynamics: A Multiview Extension of Latent Change Score Models, MultiViz: Towards User-Centric Visualizations and Interpretations of Multimodal Models, Understanding Masked Autoencoders via Hierarchical Latent Variable Models, Factorized Contrastive Learning: Going Beyond Multi-view Redundancy, Language Models Get a Gender Makeover: Mitigating Gender Bias with Few-Shot Data Interventions\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 377}),\n"," Document(page_content='Question: What is the H-index of Louis-Philippe Morency?\\nAnswer: 79\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 378}),\n"," Document(page_content='Question: What is the author citation count of Louis-Philippe Morency?\\nAnswer: 28711\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 379}),\n"," Document(page_content='Question: What is the author paper count of Louis-Philippe Morency?\\nAnswer: 443\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 380}),\n"," Document(page_content='Question: What journals has Louis-Philippe Morency published in?\\nAnswer: ArXiv, Journal of Vision, Proceedings of the 25th International Conference on Multimodal Interaction, Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems, 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 381}),\n"," Document(page_content='Question: What are the journals and how many papers has Louis-Philippe Morency published in each?\\nAnswer: 5 in ArXiv, 5 in Proceedings of the 25th International Conference on Multimodal Interaction, 1 in Journal of Vision, 1 in Extended Abstracts of the 2023 CHI Conference on Human Factors in Computing Systems, 1 in 2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 382}),\n"," Document(page_content='Question: What are the fields of study of Louis-Philippe Morency?\\nAnswer: Computer Science, Mathematics\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 383}),\n"," Document(page_content='Question: How many papers has Louis-Philippe Morency published in open access journals?\\nAnswer: 16\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 384}),\n"," Document(page_content='Question: What venues has Louis-Philippe Morency published in?\\nAnswer: arXiv.org, Journal of Vision, Annual Meeting of the Association for Computational Linguistics, International Conference on Multimodal Interaction, Conference on Empirical Methods in Natural Language Processing, CHI Extended Abstracts, Computer Vision and Pattern Recognition\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 385}),\n"," Document(page_content='Question: What is the most cited paper from Louis-Philippe Morency?\\nAnswer: Quantifying & Modeling Feature Interactions: An Information Decomposition Framework\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 386}),\n"," Document(page_content='Question: What is the url of the most cited paper from Louis-Philippe Morency?\\nAnswer: https://arxiv.org/pdf/2302.12247\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 387}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Louis-Philippe Morency?\\nAnswer: P. Liang, Yun Cheng, Xiang Fan, Chun Kai Ling, Suzanne Nie, Richard J. Chen, Zihao Deng, Faisal Mahmood, R. Salakhutdinov, Louis-Philippe Morency\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 388}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Louis-Philippe Morency?\\nAnswer: This work proposes an information-theoretic approach to quantify the degree of redundancy, uniqueness, and synergy across input features, which is term the PID statistics of a multimodal distribution.\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 389}),\n"," Document(page_content='Question: Abstract of the most cited paper from Louis-Philippe Morency?\\nAnswer: The recent explosion of interest in multimodal applications has resulted in a wide selection of datasets and methods for representing and inte-grating information from different signals. Despite these empirical advances, there remain fundamental research questions: how can we quantify the nature of interactions that exist among input features? Subsequently, how can we capture these interactions using suitable data-driven methods? To answer this question, we propose an information-theoretic approach to quantify the degree of redundancy , uniqueness , and synergy across input features, which we term the PID statistics of a multimodal distribution. Using 2 newly proposed estimators that scale to high-dimensional distributions, we demonstrate their usefulness in quantifying the interactions within multimodal datasets, the nature of interactions captured by multimodal models, and principled approaches for model selection. We conduct extensive experiments on both synthetic datasets where the PID statistics are known and on large-scale multimodal benchmarks where PID estimation was previously impossible. Finally, to demonstrate the real-world applicability of our approach, we present three case studies in pathology, mood prediction, and robotic perception where our framework accurately recommends strong multimodal models for each application.\\nDocument: ../data/paper_jsons/Louis-Philippe Morency_49933077.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 390}),\n"," Document(page_content='Question: What is the author ID of Lu Jiang?\\nAnswer: 39978626\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 391}),\n"," Document(page_content='Question: What are the papers written by Lu Jiang?\\nAnswer: Muse: Text-To-Image Generation via Masked Generative Transformers, SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs, StyleDrop: Text-to-Image Generation in Any Style, Learning Disentangled Prompts for Compositional Image Synthesis, Language Model Beats Diffusion - Tokenizer is Key to Visual Generation, VideoGLUE: Video General Understanding Evaluation of Foundation Models\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 392}),\n"," Document(page_content='Question: What is the H-index of Lu Jiang?\\nAnswer: 41\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 393}),\n"," Document(page_content='Question: What is the author citation count of Lu Jiang?\\nAnswer: 7875\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 394}),\n"," Document(page_content='Question: What is the author paper count of Lu Jiang?\\nAnswer: 79\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 395}),\n"," Document(page_content='Question: What journals has Lu Jiang published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 396}),\n"," Document(page_content='Question: What are the journals and how many papers has Lu Jiang published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 397}),\n"," Document(page_content='Question: What are the fields of study of Lu Jiang?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 398}),\n"," Document(page_content='Question: How many papers has Lu Jiang published in open access journals?\\nAnswer: 6\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 399}),\n"," Document(page_content='Question: What venues has Lu Jiang published in?\\nAnswer: International Conference on Machine Learning, arXiv.org\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 400}),\n"," Document(page_content='Question: What is the most cited paper from Lu Jiang?\\nAnswer: Muse: Text-To-Image Generation via Masked Generative Transformers\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 401}),\n"," Document(page_content='Question: What is the url of the most cited paper from Lu Jiang?\\nAnswer: http://arxiv.org/pdf/2301.00704\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 402}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Lu Jiang?\\nAnswer: Huiwen Chang, Han Zhang, Jarred Barber, AJ Maschinot, Jose Lezama, Lu Jiang, Ming Yang, K. Murphy, W. Freeman, Michael Rubinstein, Yuanzhen Li, Dilip Krishnan\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 403}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Lu Jiang?\\nAnswer: \\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 404}),\n"," Document(page_content='Question: Abstract of the most cited paper from Lu Jiang?\\nAnswer: We present Muse, a text-to-image Transformer model that achieves state-of-the-art image generation performance while being significantly more efficient than diffusion or autoregressive models. Muse is trained on a masked modeling task in discrete token space: given the text embedding extracted from a pre-trained large language model (LLM), Muse is trained to predict randomly masked image tokens. Compared to pixel-space diffusion models, such as Imagen and DALL-E 2, Muse is significantly more efficient due to the use of discrete tokens and requiring fewer sampling iterations; compared to autoregressive models, such as Parti, Muse is more efficient due to the use of parallel decoding. The use of a pre-trained LLM enables fine-grained language understanding, translating to high-fidelity image generation and the understanding of visual concepts such as objects, their spatial relationships, pose, cardinality etc. Our 900M parameter model achieves a new SOTA on CC3M, with an FID score of 6.06. The Muse 3B parameter model achieves an FID of 7.88 on zero-shot COCO evaluation, along with a CLIP score of 0.32. Muse also directly enables a number of image editing applications without the need to fine-tune or invert the model: inpainting, outpainting, and mask-free editing. More results are available at https://muse-model.github.io\\nDocument: ../data/paper_jsons/Lu Jiang_39978626.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 405}),\n"," Document(page_content='Question: What is the author ID of Madhavi Ganapathiraju?\\nAnswer: 32747279\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 406}),\n"," Document(page_content='Question: What are the papers written by Madhavi Ganapathiraju?\\nAnswer: Pathogenic mechanisms underlying adverse neurodevelopmental outcome in congenital heart disease, Editorial: Systems biology, women in science 2021/22: translational systems biology and in silico trials\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 407}),\n"," Document(page_content='Question: What is the H-index of Madhavi Ganapathiraju?\\nAnswer: 20\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 408}),\n"," Document(page_content='Question: What is the author citation count of Madhavi Ganapathiraju?\\nAnswer: 2040\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 409}),\n"," Document(page_content='Question: What is the author paper count of Madhavi Ganapathiraju?\\nAnswer: 94\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 410}),\n"," Document(page_content='Question: What journals has Madhavi Ganapathiraju published in?\\nAnswer: bioRxiv, Frontiers in Systems Biology\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 411}),\n"," Document(page_content='Question: What are the journals and how many papers has Madhavi Ganapathiraju published in each?\\nAnswer: 1 in bioRxiv, 1 in Frontiers in Systems Biology\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 412}),\n"," Document(page_content='Question: What are the fields of study of Madhavi Ganapathiraju?\\nAnswer: Biology\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 413}),\n"," Document(page_content='Question: How many papers has Madhavi Ganapathiraju published in open access journals?\\nAnswer: 2\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 414}),\n"," Document(page_content='Question: What venues has Madhavi Ganapathiraju published in?\\nAnswer: bioRxiv, Frontiers in Systems Biology\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 415}),\n"," Document(page_content='Question: What is the most cited paper from Madhavi Ganapathiraju?\\nAnswer: Pathogenic mechanisms underlying adverse neurodevelopmental outcome in congenital heart disease\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 416}),\n"," Document(page_content='Question: What is the url of the most cited paper from Madhavi Ganapathiraju?\\nAnswer: https://www.biorxiv.org/content/biorxiv/early/2023/11/06/2023.11.05.565716.full.pdf\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 417}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Madhavi Ganapathiraju?\\nAnswer: George C Gabriel, Hisato Yagi, Tuantuan Tan, A. Bais, Benjamin J. Glennon, Margaret C. Stapleton, Lihua Huang, William T Reynolds, Marla G. Shaffer, Xinxiu Xu, M. Ganapathiraju, Dennis Simon, Ashok Panigrahy, Yijen L. Wu, Cecilia W Lo\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 418}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Madhavi Ganapathiraju?\\nAnswer: The observations indicate the intrinsic factors contributing to the adverse neurodevelopmental outcome associated with HLHS involve spindle defects causing impaired corticoneurogenesis, and brain and behavioral deficits associated with perturbed epigenetic regulation of neuro developmental pathways.\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 419}),\n"," Document(page_content='Question: Abstract of the most cited paper from Madhavi Ganapathiraju?\\nAnswer: Background Hypoplastic left heart syndrome (HLHS), a severe congenital heart disease, is associated with poor neurodevelopmental outcomes, microcephaly, reduced cortical brain volume, brain dysmaturation, and neurobehavioral disorders such as autism. The involvement of patient intrinsic factors was indicated, but the mechanism is largely unknown. Methods Ohia mice with HLHS causing mutations in chromatin modifier Sin3A-associated protein 130 (Sap130) and cell adhesion protein ProtocadherinA9 (Pcdha9) were investigated for brain abnormalities by histology, immunomicroscopy, and molecular profiling by RNAseq, Sap130 ChIPseq, and genome-wide methylome analysis. Additionally, adult viable Pcdha9m/m and Emx1-cre:Sap130f/− mice with forebrain deletion of Sap130 were examined by brain MRI and behavioral assessments. Results Ohia mice have brain abnormalities comprising forebrain hypoplasia and microcephaly in conjunction with a cortical neurogenesis defect. This is associated with loss of intermediate progenitors due to mitotic arrest and apoptosis from multipolar spindle formation, a mechanism also observed in primary microcephaly. Brain RNAseq showed perturbation of REST transcriptional regulation of neurogenesis, disruption of CREB signaling regulating synaptic plasticity and memory, and defects in neurovascular coupling indicating perturbation of brain-sparing cerebral autoregulation. Disease pathways recovered included autism, intellectual disability, and other neurobehavioral/neurological deficits. These same pathways were observed upon intersection of genes that are differentially expressed with those that are differentially methylated and also are ChIPseq targets of Sap130, suggesting the transcriptional changes are epigenetically regulated. Adult viable mice harboring either the Pcdha9 mutation or forebrain-specific Sap130 deletion showed similar learning/memory deficits and autism-like behavior, suggesting they act on convergent pathways. Conclusions Our observations indicate the intrinsic factors contributing to the adverse neurodevelopmental outcome associated with HLHS involve spindle defects causing impaired corticoneurogenesis, and brain and behavioral deficits associated with perturbed epigenetic regulation of neurodevelopmental pathways. Novelty and Significance What is known? Hypoplastic left heart syndrome (HLHS), a severe congenital heart disease, is associated with adverse neurodevelopmental outcome attributable to patient intrinsic factors. Cortical neurogenesis defect with reduced brain volume and microcephaly are observed beginning in utero, suggesting a developmental etiology. Learning impairment and autism spectrum disorder are commonly observed in HLHS. What new information does this article contribute? The Ohia HLHS mouse model exhibits neurodevelopmental deficits comprising microcephaly and cortical neurogenesis defects with loss of neural progenitors from multipolar spindle formation, as well as impaired neurovascular coupling. Molecular profiling showed disturbance of REST, transcriptional regulator of neural stem cells, and CREB signaling regulating synaptic plasticity, with neurobehavioral assessments of the mutant mice showing learning/memory and autism-like behavioral deficits. Intersection of transcriptome and DNA methylation analyses uncovered an epigenetic basis for the neurodevelopmental/neurobehavioral abnormalities, Analysis of an HLHS mouse model indicated patient intrinsic factors causing adverse neurodevelopment in HLHS are genetic and epigenetic in etiology. This may include a mitotic spindle defect that would not be rescued by in utero aortic valvuloplasty, and a defect in neurovascular coupling that is likely to reduce the efficacy of maternal hyperoxygenation. However, epigenetic therapy may provide a new avenue for treatment that should be explored.\\nDocument: ../data/paper_jsons/M. Ganapathiraju_32747279.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 420}),\n"," Document(page_content='Question: What is the author ID of Maarten Sap?\\nAnswer: 2729164\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 421}),\n"," Document(page_content='Question: What are the papers written by Maarten Sap?\\nAnswer: Modeling Empathic Similarity in Personal Narratives, COBRA Frames: Contextual Reasoning about Effects and Harms of Offensive Statements, Riveter: Measuring Power and Social Dynamics Between Entities, Don\\'t Take This Out of Context!: On the Need for Contextual Models and Evaluations for Stylistic Rewriting, BiasX: \"Thinking Slow\" in Toxic Content Moderation with Explanations of Implied Social Biases, Improving Language Models with Advantage-based Offline Policy Gradients, From Dogwhistles to Bullhorns: Unveiling Coded Rhetoric with Language Models, NLPositionality: Characterizing Design Biases of Datasets and Models, Don\\'t Take This Out of Context! On the Need for Contextual Models and Evaluations for Stylistic Rewriting, Queer In AI: A Case Study in Community-Led Participatory AI, Value Kaleidoscope: Engaging AI with Pluralistic Human Values, Rights, and Duties, Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models, Towards Countering Essentialism through Social Bias Reasoning\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 422}),\n"," Document(page_content='Question: What is the H-index of Maarten Sap?\\nAnswer: 36\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 423}),\n"," Document(page_content='Question: What is the author citation count of Maarten Sap?\\nAnswer: 7517\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 424}),\n"," Document(page_content='Question: What is the author paper count of Maarten Sap?\\nAnswer: 75\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 425}),\n"," Document(page_content='Question: What journals has Maarten Sap published in?\\nAnswer: ArXiv, Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 426}),\n"," Document(page_content='Question: What are the journals and how many papers has Maarten Sap published in each?\\nAnswer: 10 in ArXiv, 1 in Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 427}),\n"," Document(page_content='Question: What are the fields of study of Maarten Sap?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 428}),\n"," Document(page_content='Question: How many papers has Maarten Sap published in open access journals?\\nAnswer: 13\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 429}),\n"," Document(page_content='Question: What venues has Maarten Sap published in?\\nAnswer: Conference on Empirical Methods in Natural Language Processing, Annual Meeting of the Association for Computational Linguistics, arXiv.org, Conference on Fairness, Accountability and Transparency\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 430}),\n"," Document(page_content='Question: What is the most cited paper from Maarten Sap?\\nAnswer: Clever Hans or Neural Theory of Mind? Stress Testing Social Reasoning in Large Language Models\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 431}),\n"," Document(page_content='Question: What is the url of the most cited paper from Maarten Sap?\\nAnswer: http://arxiv.org/pdf/2305.14763\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 432}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Maarten Sap?\\nAnswer: Natalie Shapira, Mosh Levy, S. Alavi, Xuhui Zhou, Yejin Choi, Yoav Goldberg, Maarten Sap, Vered Shwartz\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 433}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Maarten Sap?\\nAnswer: It is found that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust, indicating reliance on shallow heuristics rather than robust ToM abilities.\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 434}),\n"," Document(page_content='Question: Abstract of the most cited paper from Maarten Sap?\\nAnswer: The escalating debate on AI\\'s capabilities warrants developing reliable metrics to assess machine\"intelligence\". Recently, many anecdotal examples were used to suggest that newer large language models (LLMs) like ChatGPT and GPT-4 exhibit Neural Theory-of-Mind (N-ToM); however, prior work reached conflicting conclusions regarding those abilities. We investigate the extent of LLMs\\' N-ToM through an extensive evaluation on 6 tasks and find that while LLMs exhibit certain N-ToM abilities, this behavior is far from being robust. We further examine the factors impacting performance on N-ToM tasks and discover that LLMs struggle with adversarial examples, indicating reliance on shallow heuristics rather than robust ToM abilities. We caution against drawing conclusions from anecdotal examples, limited benchmark testing, and using human-designed psychological tests to evaluate models.\\nDocument: ../data/paper_jsons/Maarten Sap_2729164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 435}),\n"," Document(page_content='Question: What is the author ID of Malihe Alikhani?\\nAnswer: 2715920\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 436}),\n"," Document(page_content='Question: What are the papers written by Malihe Alikhani?\\nAnswer: Learning to Generate Equitable Text in Dialogue from Biased Training Data, Learning Multimodal Cues of Children’s Uncertainty, Findings of the Second WMT Shared Task on Sign Language Translation (WMT-SLT23), Multilingual Content Moderation: A Case Study on Reddit, Image–text coherence and its implications for multimodal AI, A corpus of Persian literary text, D-CALM: A Dynamic Clustering-based Active Learning Approach for Mitigating Bias\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 437}),\n"," Document(page_content='Question: What is the H-index of Malihe Alikhani?\\nAnswer: 12\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 438}),\n"," Document(page_content='Question: What is the author citation count of Malihe Alikhani?\\nAnswer: 520\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 439}),\n"," Document(page_content='Question: What is the author paper count of Malihe Alikhani?\\nAnswer: 58\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 440}),\n"," Document(page_content='Question: What journals has Malihe Alikhani published in?\\nAnswer: ArXiv, Frontiers in Artificial Intelligence, Language Resources and Evaluation\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 441}),\n"," Document(page_content='Question: What are the journals and how many papers has Malihe Alikhani published in each?\\nAnswer: 1 in ArXiv, 1 in Frontiers in Artificial Intelligence, 1 in Language Resources and Evaluation\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 442}),\n"," Document(page_content='Question: What are the fields of study of Malihe Alikhani?\\nAnswer: Computer Science, Medicine\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 443}),\n"," Document(page_content='Question: How many papers has Malihe Alikhani published in open access journals?\\nAnswer: 7\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 444}),\n"," Document(page_content='Question: What venues has Malihe Alikhani published in?\\nAnswer: Annual Meeting of the Association for Computational Linguistics, SIGDIAL Conferences, Conference on Machine Translation, Conference of the European Chapter of the Association for Computational Linguistics, Frontiers in Artificial Intelligence, Language Resources and Evaluation\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 445}),\n"," Document(page_content='Question: What is the most cited paper from Malihe Alikhani?\\nAnswer: Findings of the Second WMT Shared Task on Sign Language Translation (WMT-SLT23)\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 446}),\n"," Document(page_content='Question: What is the url of the most cited paper from Malihe Alikhani?\\nAnswer: https://aclanthology.org/2023.wmt-1.4.pdf\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 447}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Malihe Alikhani?\\nAnswer: Mathias Muller, Malihe Alikhani, Eleftherios Avramidis, Richard Bowden, Annelies Braffort, Necati Cihan Camgoz, Sarah Ebling, Cristina Espana-Bonet, A. Gohring, Roman Grundkiewicz, Mert Inan, Zifan Jiang, Oscar Koller, Amit Moryossef, Annette Rios, D. Shterionov, Sandra Sidler-Miserez, Katja Tissi, Davy Van Landuyt\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 448}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Malihe Alikhani?\\nAnswer: This paper presents the results of the Second WMT Shared Task on Sign Language Translation, concerned with automatic translation between signed and spoken languages, which resulted in publicly available sets of system outputs and more human evaluation scores for sign language translation.\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 449}),\n"," Document(page_content='Question: Abstract of the most cited paper from Malihe Alikhani?\\nAnswer: This paper presents the results of the Second WMT Shared Task on Sign Language Translation (WMT-SLT23; https://www.wmt-slt.com/). This shared task is concerned with automatic translation between signed and spoken languages. The task is unusual in the sense that it requires processing visual information (such as video frames or human pose estimation) beyond the well-known paradigm of text-to-text machine translation (MT). The task offers four tracks involving the following languages: Swiss German Sign Language (DSGS), French Sign Language of Switzerland (LSF-CH), Italian Sign Language of Switzerland (LIS-CH), German, French and Italian. Four teams (including one working on a baseline submission) participated in this second edition of the task, all submitting to the DSGS-to-German track. Besides a system ranking and system papers describing state-of-the-art techniques, this shared task makes the following scientific contributions: novel corpora and reproducible baseline systems. Finally, the task also resulted in publicly available sets of system outputs and more human evaluation scores for sign language translation.\\nDocument: ../data/paper_jsons/Malihe Alikhani_2715920.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 450}),\n"," Document(page_content='Question: What is the author ID of Matt Gormley?\\nAnswer: 1762110\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 451}),\n"," Document(page_content='Question: What are the papers written by Matt Gormley?\\nAnswer: It’s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk, Unlimiformer: Long-Range Transformers with Unlimited Length Input, MDACE: MIMIC Documents Annotated with Code Evidence, SummQA at MEDIQA-Chat 2023: In-Context Learning with GPT-4 for Medical Summarization\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 452}),\n"," Document(page_content='Question: What is the H-index of Matt Gormley?\\nAnswer: 17\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 453}),\n"," Document(page_content='Question: What is the author citation count of Matt Gormley?\\nAnswer: 1117\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 454}),\n"," Document(page_content='Question: What is the author paper count of Matt Gormley?\\nAnswer: 42\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 455}),\n"," Document(page_content='Question: What journals has Matt Gormley published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 456}),\n"," Document(page_content='Question: What are the journals and how many papers has Matt Gormley published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 457}),\n"," Document(page_content='Question: What are the fields of study of Matt Gormley?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 458}),\n"," Document(page_content='Question: How many papers has Matt Gormley published in open access journals?\\nAnswer: 4\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 459}),\n"," Document(page_content='Question: What venues has Matt Gormley published in?\\nAnswer: BIGPICTURE, arXiv.org, Annual Meeting of the Association for Computational Linguistics, Clinical Natural Language Processing Workshop\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 460}),\n"," Document(page_content='Question: What is the most cited paper from Matt Gormley?\\nAnswer: Unlimiformer: Long-Range Transformers with Unlimited Length Input\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 461}),\n"," Document(page_content='Question: What is the url of the most cited paper from Matt Gormley?\\nAnswer: http://arxiv.org/pdf/2305.01625\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 462}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Matt Gormley?\\nAnswer: Amanda Bertsch, Uri Alon, Graham Neubig, Matthew R. Gormley\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 463}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Matt Gormley?\\nAnswer: This work proposes Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores.\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 464}),\n"," Document(page_content='Question: Abstract of the most cited paper from Matt Gormley?\\nAnswer: Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .\\nDocument: ../data/paper_jsons/Matthew R. Gormley_1762110.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 465}),\n"," Document(page_content='Question: What is the author ID of Matthias Grabmair?\\nAnswer: 2869551\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 466}),\n"," Document(page_content='Question: What are the papers written by Matthias Grabmair?\\nAnswer: Joint Span Segmentation and Rhetorical Role Labeling with Data Augmentation for Legal Documents, Zero-shot Transfer of Article-aware Legal Outcome Classification for European Court of Human Rights Cases, Leveraging Task Dependency and Contrastive Learning for Case Outcome Classification on European Court of Human Rights Cases\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 467}),\n"," Document(page_content='Question: What is the H-index of Matthias Grabmair?\\nAnswer: 13\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 468}),\n"," Document(page_content='Question: What is the author citation count of Matthias Grabmair?\\nAnswer: 464\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 469}),\n"," Document(page_content='Question: What is the author paper count of Matthias Grabmair?\\nAnswer: 38\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 470}),\n"," Document(page_content='Question: What journals has Matthias Grabmair published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 471}),\n"," Document(page_content='Question: What are the journals and how many papers has Matthias Grabmair published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 472}),\n"," Document(page_content='Question: What are the fields of study of Matthias Grabmair?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 473}),\n"," Document(page_content='Question: How many papers has Matthias Grabmair published in open access journals?\\nAnswer: 3\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 474}),\n"," Document(page_content='Question: What venues has Matthias Grabmair published in?\\nAnswer: European Conference on Information Retrieval, Findings, Conference of the European Chapter of the Association for Computational Linguistics\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 475}),\n"," Document(page_content='Question: What is the most cited paper from Matthias Grabmair?\\nAnswer: Zero-shot Transfer of Article-aware Legal Outcome Classification for European Court of Human Rights Cases\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 476}),\n"," Document(page_content='Question: What is the url of the most cited paper from Matthias Grabmair?\\nAnswer: http://arxiv.org/pdf/2302.00609\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 477}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Matthias Grabmair?\\nAnswer: Santosh T.Y.S.S, O. Ichim, Matthias Grabmair\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 478}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Matthias Grabmair?\\nAnswer: This paper casts Legal Judgment Prediction on European Court of Human Rights cases into an article-aware classification task, where the case outcome is classified from a combined input of case facts and convention articles, and finds that domain adaptation methods improve zero-shot transfer performance.\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 479}),\n"," Document(page_content='Question: Abstract of the most cited paper from Matthias Grabmair?\\nAnswer: In this paper, we cast Legal Judgment Prediction on European Court of Human Rights cases into an article-aware classification task, where the case outcome is classified from a combined input of case facts and convention articles. This configuration facilitates the model learning some legal reasoning ability in mapping article text to specific case fact text. It also provides an opportunity to evaluate the model’s ability to generalize to zero-shot settings when asked to classify the case outcome with respect to articles not seen during training. We devise zero-shot experiments and apply domain adaptation methods based on domain discrimination and Wasserstein distance. Our results demonstrate that the article-aware architecture outperforms straightforward fact classification. We also find that domain adaptation methods improve zero-shot transfer performance, with article relatedness and encoder pre-training influencing the effect.\\nDocument: ../data/paper_jsons/Matthias Grabmair_2869551.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 480}),\n"," Document(page_content='Question: What is the author ID of Mona Diab?\\nAnswer: 1700007\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 481}),\n"," Document(page_content='Question: What are the papers written by Mona Diab?\\nAnswer: Author Correction: Arabic natural language processing for Qur’anic research: a systematic review, Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 482}),\n"," Document(page_content='Question: What is the H-index of Mona Diab?\\nAnswer: 50\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 483}),\n"," Document(page_content='Question: What is the author citation count of Mona Diab?\\nAnswer: 12703\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 484}),\n"," Document(page_content='Question: What is the author paper count of Mona Diab?\\nAnswer: 228\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 485}),\n"," Document(page_content='Question: What journals has Mona Diab published in?\\nAnswer: Artificial Intelligence Review\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 486}),\n"," Document(page_content='Question: What are the journals and how many papers has Mona Diab published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 487}),\n"," Document(page_content='Question: What are the fields of study of Mona Diab?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 488}),\n"," Document(page_content='Question: How many papers has Mona Diab published in open access journals?\\nAnswer: 2\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 489}),\n"," Document(page_content='Question: What venues has Mona Diab published in?\\nAnswer: Artificial Intelligence Review, International Workshop on Spoken Language Translation\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 490}),\n"," Document(page_content='Question: What is the most cited paper from Mona Diab?\\nAnswer: Evaluating Multilingual Speech Translation under Realistic Conditions with Resegmentation and Terminology\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 491}),\n"," Document(page_content='Question: What is the url of the most cited paper from Mona Diab?\\nAnswer: https://aclanthology.org/2023.iwslt-1.2.pdf\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 492}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Mona Diab?\\nAnswer: Elizabeth Salesky, Kareem Darwish, Mohamed Al-Badrashiny, Mona T. Diab, J. Niehues\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 493}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Mona Diab?\\nAnswer: This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 494}),\n"," Document(page_content='Question: Abstract of the most cited paper from Mona Diab?\\nAnswer: We present the ACL 60/60 evaluation sets for multilingual translation of ACL 2022 technical presentations into 10 target languages. This dataset enables further research into multilingual speech translation under realistic recording conditions with unsegmented audio and domain-specific terminology, applying NLP tools to text and speech in the technical domain, and evaluating and improving model robustness to diverse speaker demographics.\\nDocument: ../data/paper_jsons/Mona T. Diab_1700007.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 495}),\n"," Document(page_content='Question: What is the author ID of Mona Diab?\\nAnswer: 2138579860\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 496}),\n"," Document(page_content='Question: What are the papers written by Mona Diab?\\nAnswer: Can Large Language Models Infer Causation from Correlation?, ALERT: Adapt Language Models to Reasoning Tasks, Care4Lang at MEDIQA-Chat 2023: Fine-tuning Language Models for Classifying and Summarizing Clinical Dialogues, The Troubling Emergence of Hallucination in Large Language Models - An Extensive Definition, Quantification, and Prescriptive Remediations, OPT-R: Exploring the Role of Explanations in Finetuning and Prompting for Reasoning Skills of Large Language Models, Methods for Measuring, Updating, and Visualizing Factual Beliefs in Language Models\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 497}),\n"," Document(page_content='Question: What is the H-index of Mona Diab?\\nAnswer: 10\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 498}),\n"," Document(page_content='Question: What is the author citation count of Mona Diab?\\nAnswer: 2128\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 499}),\n"," Document(page_content='Question: What is the author paper count of Mona Diab?\\nAnswer: 23\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 500}),\n"," Document(page_content='Question: What journals has Mona Diab published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 501}),\n"," Document(page_content='Question: What are the journals and how many papers has Mona Diab published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 502}),\n"," Document(page_content='Question: What are the fields of study of Mona Diab?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 503}),\n"," Document(page_content='Question: How many papers has Mona Diab published in open access journals?\\nAnswer: 6\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 504}),\n"," Document(page_content='Question: What venues has Mona Diab published in?\\nAnswer: arXiv.org, Annual Meeting of the Association for Computational Linguistics, Clinical Natural Language Processing Workshop, Conference on Empirical Methods in Natural Language Processing, NLRSE, Conference of the European Chapter of the Association for Computational Linguistics\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 505}),\n"," Document(page_content='Question: What is the most cited paper from Mona Diab?\\nAnswer: Can Large Language Models Infer Causation from Correlation?\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 506}),\n"," Document(page_content='Question: What is the url of the most cited paper from Mona Diab?\\nAnswer: http://arxiv.org/pdf/2306.05836\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 507}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Mona Diab?\\nAnswer: Zhijing Jin, Jiarui Liu, Zhiheng Lyu, Spencer Poff, Mrinmaya Sachan, Rada Mihalcea, Mona T. Diab, B. Scholkopf\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 508}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Mona Diab?\\nAnswer: This work proposes the first benchmark dataset to test the pure causal inference skills of large language models (LLMs), and formulates a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables.\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 509}),\n"," Document(page_content=\"Question: Abstract of the most cited paper from Mona Diab?\\nAnswer: Causal inference is one of the hallmarks of human intelligence. While the field of CausalNLP has attracted much interest in the recent years, existing causal inference datasets in NLP primarily rely on discovering causality from empirical knowledge (e.g., commonsense knowledge). In this work, we propose the first benchmark dataset to test the pure causal inference skills of large language models (LLMs). Specifically, we formulate a novel task Corr2Cause, which takes a set of correlational statements and determines the causal relationship between the variables. We curate a large-scale dataset of more than 200K samples, on which we evaluate seventeen existing LLMs. Through our experiments, we identify a key shortcoming of LLMs in terms of their causal inference skills, and show that these models achieve almost close to random performance on the task. This shortcoming is somewhat mitigated when we try to re-purpose LLMs for this skill via finetuning, but we find that these models still fail to generalize -- they can only perform causal inference in in-distribution settings when variable names and textual expressions used in the queries are similar to those in the training set, but fail in out-of-distribution settings generated by perturbing these queries. Corr2Cause is a challenging task for LLMs, and would be helpful in guiding future research on improving LLMs' pure reasoning skills and generalizability. Our data is at https://huggingface.co/datasets/causalnlp/corr2cause. Our code is at https://github.com/causalNLP/corr2cause.\\nDocument: ../data/paper_jsons/Mona T. Diab_2138579860.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 510}),\n"," Document(page_content='Question: What is the author ID of Norman Sadeh?\\nAnswer: 2464164\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 511}),\n"," Document(page_content=\"Question: What are the papers written by Norman Sadeh?\\nAnswer: Do Privacy Labels Answer Users' Privacy Questions?, Exploring Smart Commercial Building Occupants’ Perceptions and Notification Preferences of Internet of Things Data Collection in the United States, Comparing Privacy Label Disclosures of Apps Published in both the App Store and Google Play Stores, ATLAS: Automatically Detecting Discrepancies Between Privacy Policies and Privacy Labels\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 512}),\n"," Document(page_content='Question: What is the H-index of Norman Sadeh?\\nAnswer: 68\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 513}),\n"," Document(page_content='Question: What is the author citation count of Norman Sadeh?\\nAnswer: 17165\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 514}),\n"," Document(page_content='Question: What is the author paper count of Norman Sadeh?\\nAnswer: 262\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 515}),\n"," Document(page_content='Question: What journals has Norman Sadeh published in?\\nAnswer: Proceedings 2023 Symposium on Usable Security, 2023 IEEE 8th European Symposium on Security and Privacy (EuroS&P), 2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 516}),\n"," Document(page_content='Question: What are the journals and how many papers has Norman Sadeh published in each?\\nAnswer: 2 in 2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW), 1 in Proceedings 2023 Symposium on Usable Security, 1 in 2023 IEEE 8th European Symposium on Security and Privacy (EuroS&P)\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 517}),\n"," Document(page_content='Question: What are the fields of study of Norman Sadeh?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 518}),\n"," Document(page_content='Question: How many papers has Norman Sadeh published in open access journals?\\nAnswer: 4\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 519}),\n"," Document(page_content='Question: What venues has Norman Sadeh published in?\\nAnswer: Proceedings 2023 Symposium on Usable Security, European Symposium on Security and Privacy, 2023 IEEE European Symposium on Security and Privacy Workshops (EuroS&PW)\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 520}),\n"," Document(page_content=\"Question: What is the most cited paper from Norman Sadeh?\\nAnswer: Do Privacy Labels Answer Users' Privacy Questions?\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 521}),\n"," Document(page_content='Question: What is the url of the most cited paper from Norman Sadeh?\\nAnswer: openAccessPdf not available\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 522}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Norman Sadeh?\\nAnswer: Shikun Zhang, N. Sadeh\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 523}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Norman Sadeh?\\nAnswer: A crowd-sourced corpus of privacy questions collected from mobile app users is analyzed to determine to what extent these mobile app labels actually address users’ privacy concerns and questions.\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 524}),\n"," Document(page_content='Question: Abstract of the most cited paper from Norman Sadeh?\\nAnswer: —Inspired by earlier academic research, iOS app privacy labels and the recent Google Play data safety labels have been introduced as a way to systematically present users with concise summaries of an app’s data practices. Yet, little research has been conducted to deter- mine how well today’s mobile app privacy labels address people’s actual privacy concerns or questions. We analyze a crowd-sourced corpus of privacy questions collected from mobile app users to determine to what extent these mobile app labels actually address users’ privacy concerns and questions. While there are differences between iOS labels and Google Play labels, our results indicate that an important percentage of people’s privacy questions are not answered or only partially addressed in today’s labels. Findings from this work not only shed light on the additional fields that would need to be included in mobile app privacy labels but can also help inform refinements to existing labels to better address users’ typical privacy questions\\nDocument: ../data/paper_jsons/N. Sadeh_2464164.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 525}),\n"," Document(page_content='Question: What is the author ID of Richard Stern?\\nAnswer: 1697819\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 526}),\n"," Document(page_content='Question: What are the papers written by Richard Stern?\\nAnswer: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 527}),\n"," Document(page_content='Question: What is the H-index of Richard Stern?\\nAnswer: 45\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 528}),\n"," Document(page_content='Question: What is the author citation count of Richard Stern?\\nAnswer: 8518\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 529}),\n"," Document(page_content='Question: What is the author paper count of Richard Stern?\\nAnswer: 294\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 530}),\n"," Document(page_content='Question: What journals has Richard Stern published in?\\nAnswer: ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 531}),\n"," Document(page_content='Question: What are the journals and how many papers has Richard Stern published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 532}),\n"," Document(page_content='Question: What are the fields of study of Richard Stern?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 533}),\n"," Document(page_content='Question: How many papers has Richard Stern published in open access journals?\\nAnswer: 1\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 534}),\n"," Document(page_content='Question: What venues has Richard Stern published in?\\nAnswer: IEEE International Conference on Acoustics, Speech, and Signal Processing\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 535}),\n"," Document(page_content='Question: What is the most cited paper from Richard Stern?\\nAnswer: Unsupervised Voice Type Discrimination Score Adaptation Using X-Vector Clusters\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 536}),\n"," Document(page_content='Question: What is the url of the most cited paper from Richard Stern?\\nAnswer: openAccessPdf not available\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 537}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Richard Stern?\\nAnswer: Mark Lindsey, Tyler Vuong, R. Stern\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 538}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Richard Stern?\\nAnswer: This adaptation method can be applied to the output of any VTD algorithm, requires no additional training data, and has been shown to yield a relative decrease in decision cost function (DCF) score of up to 47% on a standardized database collected for the task.\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 539}),\n"," Document(page_content='Question: Abstract of the most cited paper from Richard Stern?\\nAnswer: Voice type discrimination (VTD) is the task of automatically detecting speech produced in the same room as a recording device (\"live speech\") among other speech and non-speech noises, such as traffic noises or radio broadcasts (\"distractor audio\"). Existing work has described methods for performing the VTD task. This paper presents a method for adapting the output of these existing methods in an unsupervised manner via x-vector clustering and correlation. This adaptation method can be applied to the output of any VTD algorithm, requires no additional training data, and has been shown to yield a relative decrease in decision cost function (DCF) score of up to 47% on a standardized database collected for the task.\\nDocument: ../data/paper_jsons/R. Stern_1697819.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 540}),\n"," Document(page_content='Question: What is the author ID of Rita Singh?\\nAnswer: 153915824\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 541}),\n"," Document(page_content='Question: What are the papers written by Rita Singh?\\nAnswer: BASS: Block-wise Adaptation for Speech Summarization, Rethinking Voice-Face Correlation: A Geometry View, A Gene-Based Algorithm for Identifying Factors That May Affect a Speaker’s Voice, Deriving Vocal Fold Oscillation Information from Recorded Voice Signals Using Models of Phonation, Imprecise Label Learning: A Unified Framework for Learning with Various Imprecise Label Configurations, The Hidden Dance of Phonemes and Visage: Unveiling the Enigmatic Link between Phonemes and Facial Features, Pengi: An Audio Language Model for Audio Tasks\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 542}),\n"," Document(page_content='Question: What is the H-index of Rita Singh?\\nAnswer: 25\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 543}),\n"," Document(page_content='Question: What is the author citation count of Rita Singh?\\nAnswer: 2781\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 544}),\n"," Document(page_content='Question: What is the author paper count of Rita Singh?\\nAnswer: 146\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 545}),\n"," Document(page_content='Question: What journals has Rita Singh published in?\\nAnswer: ArXiv, Proceedings of the 31st ACM International Conference on Multimedia, Entropy\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 546}),\n"," Document(page_content='Question: What are the journals and how many papers has Rita Singh published in each?\\nAnswer: 4 in ArXiv, 2 in Entropy, 1 in Proceedings of the 31st ACM International Conference on Multimedia\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 547}),\n"," Document(page_content='Question: What are the fields of study of Rita Singh?\\nAnswer: Computer Science, Engineering, Medicine\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 548}),\n"," Document(page_content='Question: How many papers has Rita Singh published in open access journals?\\nAnswer: 7\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 549}),\n"," Document(page_content='Question: What venues has Rita Singh published in?\\nAnswer: Interspeech, ACM Multimedia, Entropy, arXiv.org\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 550}),\n"," Document(page_content='Question: What is the most cited paper from Rita Singh?\\nAnswer: Pengi: An Audio Language Model for Audio Tasks\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 551}),\n"," Document(page_content='Question: What is the url of the most cited paper from Rita Singh?\\nAnswer: http://arxiv.org/pdf/2305.11834\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 552}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Rita Singh?\\nAnswer: Soham Deshmukh, Benjamin Elizalde, Rita Singh, Huaming Wang\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 553}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Rita Singh?\\nAnswer: Pengi is introduced, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks, and shows that connecting language models with audio models is a major step towards general-purpose audio understanding.\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 554}),\n"," Document(page_content='Question: Abstract of the most cited paper from Rita Singh?\\nAnswer: In the domain of audio processing, Transfer Learning has facilitated the rise of Self-Supervised Learning and Zero-Shot Learning techniques. These approaches have led to the development of versatile models capable of tackling a wide array of tasks, while delivering state-of-the-art performance. However, current models inherently lack the capacity to produce the requisite language for open-ended tasks, such as Audio Captioning or Audio Question&Answering. We introduce Pengi, a novel Audio Language Model that leverages Transfer Learning by framing all audio tasks as text-generation tasks. It takes as input, an audio recording, and text, and generates free-form text as output. The input audio is represented as a sequence of continuous embeddings by an audio encoder. A text encoder does the same for the corresponding text input. Both sequences are combined as a prefix to prompt a pre-trained frozen language model. The unified architecture of Pengi enables open-ended tasks and close-ended tasks without any additional fine-tuning or task-specific extensions. When evaluated on 22 downstream tasks, our approach yields state-of-the-art performance in several of them. Our results show that connecting language models with audio models is a major step towards general-purpose audio understanding\\nDocument: ../data/paper_jsons/Rita Singh_153915824.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 555}),\n"," Document(page_content='Question: What is the author ID of Roni Rosenfeld?\\nAnswer: 88507334\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 556}),\n"," Document(page_content='Question: What are the papers written by Roni Rosenfeld?\\nAnswer: Evaluation of FluSight influenza forecasting in the 2021–22 and 2022–23 seasons with a new target laboratory-confirmed influenza hospitalizations, Computationally Assisted Quality Control for Public Health Data Streams, Correcting for heterogeneity in real-time epidemiological indicators\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 557}),\n"," Document(page_content='Question: What is the H-index of Roni Rosenfeld?\\nAnswer: 24\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 558}),\n"," Document(page_content='Question: What is the author citation count of Roni Rosenfeld?\\nAnswer: 2904\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 559}),\n"," Document(page_content='Question: What is the author paper count of Roni Rosenfeld?\\nAnswer: 66\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 560}),\n"," Document(page_content='Question: What journals has Roni Rosenfeld published in?\\nAnswer: medRxiv, ArXiv\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 561}),\n"," Document(page_content='Question: What are the journals and how many papers has Roni Rosenfeld published in each?\\nAnswer: 1 in medRxiv, 1 in ArXiv\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 562}),\n"," Document(page_content='Question: What are the fields of study of Roni Rosenfeld?\\nAnswer: Medicine, Computer Science\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 563}),\n"," Document(page_content='Question: How many papers has Roni Rosenfeld published in open access journals?\\nAnswer: 3\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 564}),\n"," Document(page_content='Question: What venues has Roni Rosenfeld published in?\\nAnswer: medRxiv, International Joint Conference on Artificial Intelligence, arXiv.org\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 565}),\n"," Document(page_content='Question: What is the most cited paper from Roni Rosenfeld?\\nAnswer: Computationally Assisted Quality Control for Public Health Data Streams\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 566}),\n"," Document(page_content='Question: What is the url of the most cited paper from Roni Rosenfeld?\\nAnswer: http://arxiv.org/pdf/2306.16914\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 567}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Roni Rosenfeld?\\nAnswer: Ananya Joshi, Kathryn Mazaitis, Roni Rosenfeld, Bryan Wilder\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 568}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Roni Rosenfeld?\\nAnswer: FlaSH (Flagging Streams in public Health), a practical outlier detection framework for public health data users that uses simple, scalable models to capture these statistical properties explicitly, has been deployed on data streams used by public health stakeholders.\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 569}),\n"," Document(page_content='Question: Abstract of the most cited paper from Roni Rosenfeld?\\nAnswer: Irregularities in public health data streams (like COVID-19 Cases) hamper data-driven decision-making for public health stakeholders. A real-time, computer-generated list of the most important, outlying data points from thousands of public health data streams could assist an expert reviewer in identifying these irregularities. However, existing outlier detection frameworks perform poorly on this task because they do not account for the data volume or for the statistical properties of public health streams. Accordingly, we developed FlaSH (Flagging Streams in public Health), a practical outlier detection framework for public health data users that uses simple, scalable models to capture these statistical properties explicitly. In an experiment where human experts evaluate FlaSH and existing methods (including deep learning approaches), FlaSH scales to the data volume of this task, matches or exceeds these other methods in mean accuracy, and identifies the outlier points that users empirically rate as more helpful. Based on these results, FlaSH has been deployed on data streams used by public health stakeholders.\\nDocument: ../data/paper_jsons/Roni Rosenfeld_88507334.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 570}),\n"," Document(page_content='Question: What is the author ID of Scott Fahlman?\\nAnswer: 1758714\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 571}),\n"," Document(page_content='Question: What are the papers written by Scott Fahlman?\\nAnswer: Score: A Rule Engine for the Scone Knowledge Base System\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 572}),\n"," Document(page_content='Question: What is the H-index of Scott Fahlman?\\nAnswer: 23\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 573}),\n"," Document(page_content='Question: What is the author citation count of Scott Fahlman?\\nAnswer: 6875\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 574}),\n"," Document(page_content='Question: What is the author paper count of Scott Fahlman?\\nAnswer: 123\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 575}),\n"," Document(page_content='Question: What journals has Scott Fahlman published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 576}),\n"," Document(page_content='Question: What are the journals and how many papers has Scott Fahlman published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 577}),\n"," Document(page_content='Question: What are the fields of study of Scott Fahlman?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 578}),\n"," Document(page_content='Question: How many papers has Scott Fahlman published in open access journals?\\nAnswer: 1\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 579}),\n"," Document(page_content='Question: What venues has Scott Fahlman published in?\\nAnswer: arXiv.org\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 580}),\n"," Document(page_content='Question: What is the most cited paper from Scott Fahlman?\\nAnswer: Score: A Rule Engine for the Scone Knowledge Base System\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 581}),\n"," Document(page_content='Question: What is the url of the most cited paper from Scott Fahlman?\\nAnswer: http://arxiv.org/pdf/2305.04154\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 582}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Scott Fahlman?\\nAnswer: Jeffrey Chen, S. Fahlman\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 583}),\n"," Document(page_content=\"Question: TLDR/Summary of the most cited paper from Scott Fahlman?\\nAnswer: The Scone system is augmented with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone's knowledge base, potentially improving the capabilities of any planning systems built on top of Scone.\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 584}),\n"," Document(page_content='Question: Abstract of the most cited paper from Scott Fahlman?\\nAnswer: We present Score, a rule engine designed and implemented for the Scone knowledge base system. Scone is a knowledge base system designed for storing and manipulating rich representations of general knowledge in symbolic form. It represents knowledge in the form of nodes and links in a network structure, and it can perform basic inference about the relationships between different elements efficiently. On its own, Scone acts as a sort of\"smart memory\"that can interface with other software systems. One area of improvement for Scone is how useful it can be in supplying knowledge to an intelligent agent that can use the knowledge to perform actions and update the knowledge base with its observations. We augment the Scone system with a production rule engine that automatically performs simple inference based on existing and newly-added structures in Scone\\'s knowledge base, potentially improving the capabilities of any planning systems built on top of Scone. Production rule systems consist of\"if-then\"production rules that try to match their predicates to existing knowledge and fire their actions when their predicates are satisfied. We propose two kinds of production rules, if-added and if-needed rules, that differ in how they are checked and fired to cover multiple use cases. We then implement methods to efficiently check and fire these rules in a large knowledge base. The new rule engine is not meant to be a complex stand-alone planner, so we discuss how it fits into the context of Scone and future work on planning systems.\\nDocument: ../data/paper_jsons/S. Fahlman_1758714.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 585}),\n"," Document(page_content='Question: What is the author ID of Sean Welleck?\\nAnswer: 2129663\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 586}),\n"," Document(page_content='Question: What are the papers written by Sean Welleck?\\nAnswer: Self-Refine: Iterative Refinement with Self-Feedback, Inference-Time Policy Adapters (IPA): Tailoring Extreme-Scale LMs without Fine-tuning\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 587}),\n"," Document(page_content='Question: What is the H-index of Sean Welleck?\\nAnswer: 23\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 588}),\n"," Document(page_content='Question: What is the author citation count of Sean Welleck?\\nAnswer: 2845\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 589}),\n"," Document(page_content='Question: What is the author paper count of Sean Welleck?\\nAnswer: 42\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 590}),\n"," Document(page_content='Question: What journals has Sean Welleck published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 591}),\n"," Document(page_content='Question: What are the journals and how many papers has Sean Welleck published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 592}),\n"," Document(page_content='Question: What are the fields of study of Sean Welleck?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 593}),\n"," Document(page_content='Question: How many papers has Sean Welleck published in open access journals?\\nAnswer: 2\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 594}),\n"," Document(page_content='Question: What venues has Sean Welleck published in?\\nAnswer: arXiv.org, Conference on Empirical Methods in Natural Language Processing\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 595}),\n"," Document(page_content='Question: What is the most cited paper from Sean Welleck?\\nAnswer: Self-Refine: Iterative Refinement with Self-Feedback\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 596}),\n"," Document(page_content='Question: What is the url of the most cited paper from Sean Welleck?\\nAnswer: http://arxiv.org/pdf/2303.17651\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 597}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Sean Welleck?\\nAnswer: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, Peter Clark\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 598}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Sean Welleck?\\nAnswer: Self-Refine is introduced, an approach for improving initial outputs from LLMs through iterative feedback and refinement that demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using this simple, standalone approach.\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 599}),\n"," Document(page_content='Question: Abstract of the most cited paper from Sean Welleck?\\nAnswer: Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.\\nDocument: ../data/paper_jsons/S. Welleck_2129663.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 600}),\n"," Document(page_content='Question: What is the author ID of Shinji Watanabe?\\nAnswer: 1746678\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 601}),\n"," Document(page_content='Question: What are the papers written by Shinji Watanabe?\\nAnswer: Saturation time of exposure interval for cross-neutralization response to SARS-CoV-2: Implications for vaccine dose interval, Joint Prediction and Denoising for Large-Scale Multilingual Self-Supervised Learning, Tensor decomposition for minimization of E2E SLU model toward on-device processing, Decoder-only Architecture for Speech Recognition with CTC Prompts and Text Data Augmentation, ML-SUPERB: Multilingual Speech Universal PERformance Benchmark, Structured Pruning of Self-Supervised Pre-Trained Models for Speech Recognition and Understanding, Prompting the Hidden Talent of Web-Scale Speech Models for Zero-Shot Task Generalization, A Comparative Study on E-Branchformer vs Conformer in Speech Recognition, Translation, and Understanding Tasks, Efficient Sequence Transduction by Jointly Predicting Tokens and Durations, Speech Summarization of Long Spoken Document: Improving Memory Efficiency of Speech/Text Encoders, UNSSOR: Unsupervised Neural Speech Separation by Leveraging Over-determined Training Mixtures, Segment-Level Vectorized Beam Search Based on Partially Autoregressive Inference, Unsupervised Data Selection for TTS: Using Arabic Broadcast News as a Case Study, BASS: Block-wise Adaptation for Speech Summarization, I3D: Transformer Architectures with Input-Dependent Dynamic Depth for Speech Recognition, A Vector Quantized Approach for Text to Speech Synthesis on Real-World Spontaneous Speech, A New Benchmark of Aphasia Speech Recognition and Detection Based on E-Branchformer and Multi-task Learning, Antiviral Susceptibilities of Distinct Lineages of Influenza C and D Viruses, Paaploss: A Phonetic-Aligned Acoustic Parameter Loss for Speech Enhancement, SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing, Exploring the Integration of Speech Separation and Recognition with Self-Supervised Learning Representation, Dynamic-SUPERB: Towards A Dynamic, Collaborative, and Comprehensive Instruction-Tuning Benchmark for Speech, Speaker-Independent Acoustic-to-Articulatory Speech Inversion, Towards Practical and Efficient Image-to-Speech Captioning with Vision-Language Pre-training and Multi-modal Tokens, Enhancing Speech-To-Speech Translation with Multiple TTS Targets, AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head, Improving Audio Captioning Models with Fine-grained Audio Features, Text Embedding Supervision, and LLM Mix-up Augmentation, Voxtlm: unified decoder-only models for consolidating speech recognition/synthesis and speech/text continuation tasks, Learning to Speak from Text: Zero-Shot Multilingual Text-to-Speech with Unsupervised Text Pretraining, FNeural Speech Enhancement with Very Low Algorithmic Latency and Complexity via Integrated full- and sub-band Modeling, Improving Massively Multilingual ASR with Auxiliary CTC Objectives, Toward Universal Speech Enhancement For Diverse Input Conditions, The Pipeline System of ASR and NLU with MLM-based data Augmentation Toward Stop Low-Resource Challenge, A Study on the Integration of Pipeline and E2E SLU Systems for Spoken Semantic Parsing Toward Stop Quality Challenge, A community cluster of influenza A(H3N2) virus infection with reduced susceptibility to baloxavir due to a PA E199G substitution in Japan, February to March 2023, Exploring Speech Recognition, Translation, and Understanding with Discrete Speech Units: A Comparative Study, Joint Modelling of Spoken Language Understanding Tasks with Integrated Dialog History, Enhancing End-to-End Conversational Speech Translation Through Target Language Context Utilization, Challenges of Corporate Alliance CLOMA toward Plastic Litter, The Multimodal Information Based Speech Processing (MISP) 2023 Challenge: Audio-Visual Target Speaker Extraction, Reproducing Whisper-Style Training Using An Open-Source Toolkit And Publicly Available Data, The CHiME-7 DASR Challenge: Distant Meeting Transcription with Multiple Devices in Diverse Scenarios, Cross-Modal Multi-Tasking for Speech-to-Text Translation via Hard Parameter Sharing, ESPnet-ST-v2: Multipurpose Spoken Language Translation Toolkit, Multi-Channel Target Speaker Extraction with Refinement: The WavLab Submission to the Second Clarity Enhancement Challenge, Fully Unsupervised Topic Clustering of Unlabelled Spoken Audio Using Self-Supervised Representation Learning and Topic Model, TAPLoss: A Temporal Acoustic Parameter Loss for Speech Enhancement, End-to-End Speech Recognition: A Survey, The Multimodal Information Based Speech Processing (Misp) 2022 Challenge: Audio-Visual Diarization And Recognition, DPHuBERT: Joint Distillation and Pruning of Self-Supervised Speech Models, An external quality assessment feasibility study; cross laboratory comparison of haemagglutination inhibition assay and microneutralization assay performance for seasonal influenza serology testing: A FLUCOP study, Multi-Channel Speaker Extraction with Adversarial Training: The Wavlab Submission to The Clarity ICASSP 2023 Grand Challenge, AV-SUPERB: A Multi-Task Evaluation Benchmark for Audio-Visual Representation Models, Improving Cascaded Unsupervised Speech Translation with Denoising Back-translation, Deep Speech Synthesis from MRI-Based Articulatory Representations, FINDINGS OF THE IWSLT 2023 EVALUATION CAMPAIGN, Speech collage: code-switched audio generation by collaging monolingual corpora, Exploration on HuBERT with Multiple Resolutions\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 602}),\n"," Document(page_content='Question: What is the H-index of Shinji Watanabe?\\nAnswer: 67\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 603}),\n"," Document(page_content='Question: What is the author citation count of Shinji Watanabe?\\nAnswer: 22222\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 604}),\n"," Document(page_content='Question: What is the author paper count of Shinji Watanabe?\\nAnswer: 597\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 605}),\n"," Document(page_content='Question: What journals has Shinji Watanabe published in?\\nAnswer: iScience, 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), INTERSPEECH 2023, ArXiv, ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Viruses, 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), Eurosurveillance, Oleoscience, IEEE/ACM Transactions on Audio, Speech, and Language Processing, Frontiers in Immunology\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 606}),\n"," Document(page_content='Question: What are the journals and how many papers has Shinji Watanabe published in each?\\nAnswer: 27 in ArXiv, 15 in ICASSP 2023 - 2023 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), 4 in 2023 IEEE Automatic Speech Recognition and Understanding Workshop (ASRU), 2 in INTERSPEECH 2023, 1 in iScience, 1 in Viruses, 1 in 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA), 1 in Eurosurveillance, 1 in Oleoscience, 1 in IEEE/ACM Transactions on Audio, Speech, and Language Processing, 1 in Frontiers in Immunology\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 607}),\n"," Document(page_content='Question: What are the fields of study of Shinji Watanabe?\\nAnswer: Medicine, Computer Science, Engineering\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 608}),\n"," Document(page_content='Question: How many papers has Shinji Watanabe published in open access journals?\\nAnswer: 58\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 609}),\n"," Document(page_content='Question: What venues has Shinji Watanabe published in?\\nAnswer: iScience, Automatic Speech Recognition & Understanding, Interspeech, arXiv.org, IEEE International Conference on Acoustics, Speech, and Signal Processing, International Conference on Machine Learning, AAAI Conference on Artificial Intelligence, Viruses, Special Interest Group on Computational Morphology and Phonology Workshop, IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, International Joint Conference on Artificial Intelligence, Euro surveillance : bulletin Europeen sur les maladies transmissibles = European communicable disease bulletin, Oleoscience, 7th International Workshop on Speech Processing in Everyday Environments (CHiME 2023), Annual Meeting of the Association for Computational Linguistics, IEEE/ACM Transactions on Audio Speech and Language Processing, Frontiers in Immunology, International Workshop on Spoken Language Translation\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 610}),\n"," Document(page_content='Question: What is the most cited paper from Shinji Watanabe?\\nAnswer: AudioGPT: Understanding and Generating Speech, Music, Sound, and Talking Head\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 611}),\n"," Document(page_content='Question: What is the url of the most cited paper from Shinji Watanabe?\\nAnswer: http://arxiv.org/pdf/2304.12995\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 612}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Shinji Watanabe?\\nAnswer: Rongjie Huang, Mingze Li, Dongchao Yang, Jiatong Shi, Xuankai Chang, Zhenhui Ye, Yuning Wu, Zhiqing Hong, Jia-Bin Huang, Jinglin Liu, Yixiang Ren, Zhou Zhao, Shinji Watanabe\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 613}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Shinji Watanabe?\\nAnswer: A multi-modal AI system named AudioGPT is proposed, which complements LLMs with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue.\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 614}),\n"," Document(page_content='Question: Abstract of the most cited paper from Shinji Watanabe?\\nAnswer: Large language models (LLMs) have exhibited remarkable capabilities across a variety of domains and tasks, challenging our understanding of learning and cognition. Despite the recent success, current LLMs are not capable of processing complex audio information or conducting spoken conversations (like Siri or Alexa). In this work, we propose a multi-modal AI system named AudioGPT, which complements LLMs (i.e., ChatGPT) with 1) foundation models to process complex audio information and solve numerous understanding and generation tasks; and 2) the input/output interface (ASR, TTS) to support spoken dialogue. With an increasing demand to evaluate multi-modal LLMs of human intention understanding and cooperation with foundation models, we outline the principles and processes and test AudioGPT in terms of consistency, capability, and robustness. Experimental results demonstrate the capabilities of AudioGPT in solving AI tasks with speech, music, sound, and talking head understanding and generation in multi-round dialogues, which empower humans to create rich and diverse audio content with unprecedented ease. Our system is publicly available at \\\\url{https://github.com/AIGC-Audio/AudioGPT}.\\nDocument: ../data/paper_jsons/Shinji Watanabe_1746678.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 615}),\n"," Document(page_content='Question: What is the author ID of Teruko Mitamura?\\nAnswer: 1706595\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 616}),\n"," Document(page_content='Question: What are the papers written by Teruko Mitamura?\\nAnswer: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA, Hierarchical Event Grounding\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 617}),\n"," Document(page_content='Question: What is the H-index of Teruko Mitamura?\\nAnswer: 36\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 618}),\n"," Document(page_content='Question: What is the author citation count of Teruko Mitamura?\\nAnswer: 4681\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 619}),\n"," Document(page_content='Question: What is the author paper count of Teruko Mitamura?\\nAnswer: 198\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 620}),\n"," Document(page_content='Question: What journals has Teruko Mitamura published in?\\nAnswer: Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, ArXiv\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 621}),\n"," Document(page_content='Question: What are the journals and how many papers has Teruko Mitamura published in each?\\nAnswer: 1 in Proceedings of the Third DialDoc Workshop on Document-grounded Dialogue and Conversational Question Answering, 1 in ArXiv\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 622}),\n"," Document(page_content='Question: What are the fields of study of Teruko Mitamura?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 623}),\n"," Document(page_content='Question: How many papers has Teruko Mitamura published in open access journals?\\nAnswer: 2\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 624}),\n"," Document(page_content='Question: What venues has Teruko Mitamura published in?\\nAnswer: Workshop on Document-grounded Dialogue and Conversational Question Answering, AAAI Conference on Artificial Intelligence\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 625}),\n"," Document(page_content='Question: What is the most cited paper from Teruko Mitamura?\\nAnswer: Language-Agnostic Transformers and Assessing ChatGPT-Based Query Rewriting for Multilingual Document-Grounded QA\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 626}),\n"," Document(page_content='Question: What is the url of the most cited paper from Teruko Mitamura?\\nAnswer: https://aclanthology.org/2023.dialdoc-1.11.pdf\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 627}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Teruko Mitamura?\\nAnswer: Srinivas Gowriraj, Soham Dinesh Tiwari, Mitali Potnis, Srijan Bansal, T. Mitamura, Eric Nyberg\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 628}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Teruko Mitamura?\\nAnswer: This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-gnostic approach is superior.\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 629}),\n"," Document(page_content='Question: Abstract of the most cited paper from Teruko Mitamura?\\nAnswer: The DialDoc 2023 shared task has expanded the document-grounded dialogue task to encompass multiple languages, despite having limited annotated data. This paper assesses the effectiveness of both language-agnostic and language-aware paradigms for multilingual pre-trained transformer models in a bi-encoder-based dense passage retriever (DPR), concluding that the language-agnostic approach is superior. Additionally, the study investigates the impact of query rewriting techniques using large language models, such as ChatGPT, on multilingual, document-grounded question-answering systems. The experiments conducted demonstrate that, for the examples examined, query rewriting does not enhance performance compared to the original queries. This failure is due to topic switching in final dialogue turns and irrelevant topics being considered for query rewriting.\\nDocument: ../data/paper_jsons/T. Mitamura_1706595.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 630}),\n"," Document(page_content='Question: What is the author ID of Taylor Berg-Kirkpatrick?\\nAnswer: 1400419309\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 631}),\n"," Document(page_content='Question: What are the papers written by Taylor Berg-Kirkpatrick?\\nAnswer: Smaller Language Models are Better Black-box Machine-Generated Text Detectors, Towards Improving Harmonic Sensitivity and Prediction Stability for Singing Melody Extraction, Jointly modeling products and resource pages for task-oriented recommendation, MusicLDM: Enhancing Novelty in Text-to-Music Generation Using Beat-Synchronous Mixup Strategies, Universal Source Separation with Weakly Labelled Data, ClimaBench: A Benchmark Dataset For Climate Change Text Understanding in English, CLIPSonic: Text-to-Audio Synthesis with Unlabeled Videos and Pretrained Language-Vision Models, Membership Inference Attacks against Language Models via Neighbourhood Comparison, Contrastive Attention Networks for Attribution of Early Modern Print\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 632}),\n"," Document(page_content='Question: What is the H-index of Taylor Berg-Kirkpatrick?\\nAnswer: 36\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 633}),\n"," Document(page_content='Question: What is the author citation count of Taylor Berg-Kirkpatrick?\\nAnswer: 5655\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 634}),\n"," Document(page_content='Question: What is the author paper count of Taylor Berg-Kirkpatrick?\\nAnswer: 112\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 635}),\n"," Document(page_content='Question: What journals has Taylor Berg-Kirkpatrick published in?\\nAnswer: ArXiv, Companion Proceedings of the ACM Web Conference 2023, 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 636}),\n"," Document(page_content='Question: What are the journals and how many papers has Taylor Berg-Kirkpatrick published in each?\\nAnswer: 5 in ArXiv, 1 in Companion Proceedings of the ACM Web Conference 2023, 1 in 2023 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA)\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 637}),\n"," Document(page_content='Question: What are the fields of study of Taylor Berg-Kirkpatrick?\\nAnswer: Computer Science, Engineering\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 638}),\n"," Document(page_content='Question: How many papers has Taylor Berg-Kirkpatrick published in open access journals?\\nAnswer: 9\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 639}),\n"," Document(page_content='Question: What venues has Taylor Berg-Kirkpatrick published in?\\nAnswer: arXiv.org, International Society for Music Information Retrieval Conference, The Web Conference, IEEE Workshop on Applications of Signal Processing to Audio and Acoustics, Annual Meeting of the Association for Computational Linguistics, AAAI Conference on Artificial Intelligence\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 640}),\n"," Document(page_content='Question: What is the most cited paper from Taylor Berg-Kirkpatrick?\\nAnswer: Membership Inference Attacks against Language Models via Neighbourhood Comparison\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 641}),\n"," Document(page_content='Question: What is the url of the most cited paper from Taylor Berg-Kirkpatrick?\\nAnswer: https://arxiv.org/pdf/2305.18462\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 642}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Taylor Berg-Kirkpatrick?\\nAnswer: Justus Mattern, Fatemehsadat Mireshghallah, Zhijing Jin, B. Scholkopf, Mrinmaya Sachan, Taylor Berg-Kirkpatrick\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 643}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Taylor Berg-Kirkpatrick?\\nAnswer: Neighbourhood attacks are proposed and evaluated, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution and clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge.\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 644}),\n"," Document(page_content='Question: Abstract of the most cited paper from Taylor Berg-Kirkpatrick?\\nAnswer: Membership Inference attacks (MIAs) aim to predict whether a data sample was present in the training data of a machine learning model or not, and are widely used for assessing the privacy risks of language models. Most existing attacks rely on the observation that models tend to assign higher probabilities to their training samples than non-training points. However, simple thresholding of the model score in isolation tends to lead to high false-positive rates as it does not account for the intrinsic complexity of a sample. Recent work has demonstrated that reference-based attacks which compare model scores to those obtained from a reference model trained on similar data can substantially improve the performance of MIAs. However, in order to train reference models, attacks of this kind make the strong and arguably unrealistic assumption that an adversary has access to samples closely resembling the original training data. Therefore, we investigate their performance in more realistic scenarios and find that they are highly fragile in relation to the data distribution used to train reference models. To investigate whether this fragility provides a layer of safety, we propose and evaluate neighbourhood attacks, which compare model scores for a given sample to scores of synthetically generated neighbour texts and therefore eliminate the need for access to the training data distribution. We show that, in addition to being competitive with reference-based attacks that have perfect knowledge about the training data distribution, our attack clearly outperforms existing reference-free attacks as well as reference-based attacks with imperfect knowledge, which demonstrates the need for a reevaluation of the threat model of adversarial attacks.\\nDocument: ../data/paper_jsons/Taylor Berg-Kirkpatrick_1400419309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 645}),\n"," Document(page_content='Question: What is the author ID of Tom Mitchell?\\nAnswer: 40975594\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 646}),\n"," Document(page_content='Question: What are the papers written by Tom Mitchell?\\nAnswer: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals, Learning to Give Useful Hints: Assistance Action Evaluation and Policy Improvements, Genitourinary Management and Follow-Up for Patients with Stevens-Johnson Syndrome/Toxic Epidermal Necrolysis\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 647}),\n"," Document(page_content='Question: What is the H-index of Tom Mitchell?\\nAnswer: 81\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 648}),\n"," Document(page_content='Question: What is the author citation count of Tom Mitchell?\\nAnswer: 34877\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 649}),\n"," Document(page_content='Question: What is the author paper count of Tom Mitchell?\\nAnswer: 353\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 650}),\n"," Document(page_content='Question: What journals has Tom Mitchell published in?\\nAnswer: ArXiv, Burns Open\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 651}),\n"," Document(page_content='Question: What are the journals and how many papers has Tom Mitchell published in each?\\nAnswer: 1 in ArXiv, 1 in Burns Open\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 652}),\n"," Document(page_content='Question: What are the fields of study of Tom Mitchell?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 653}),\n"," Document(page_content='Question: How many papers has Tom Mitchell published in open access journals?\\nAnswer: 3\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 654}),\n"," Document(page_content='Question: What venues has Tom Mitchell published in?\\nAnswer: arXiv.org, European Conference on Technology Enhanced Learning, Burns Open\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 655}),\n"," Document(page_content='Question: What is the most cited paper from Tom Mitchell?\\nAnswer: Read and Reap the Rewards: Learning to Play Atari with the Help of Instruction Manuals\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 656}),\n"," Document(page_content='Question: What is the url of the most cited paper from Tom Mitchell?\\nAnswer: http://arxiv.org/pdf/2302.04449\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 657}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Tom Mitchell?\\nAnswer: Yue Wu, Yewen Fan, P. Liang, A. Azaria, Yuan-Fang Li, Tom Michael Mitchell\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 658}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Tom Mitchell?\\nAnswer: It is hypothesized that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent in the Read and Reward framework.\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 659}),\n"," Document(page_content='Question: Abstract of the most cited paper from Tom Mitchell?\\nAnswer: High sample complexity has long been a challenge for RL. On the other hand, humans learn to perform tasks not only from interaction or demonstrations, but also by reading unstructured text documents, e.g., instruction manuals. Instruction manuals and wiki pages are among the most abundant data that could inform agents of valuable features and policies or task-specific environmental dynamics and reward structures. Therefore, we hypothesize that the ability to utilize human-written instruction manuals to assist learning policies for specific tasks should lead to a more efficient and better-performing agent. We propose the Read and Reward framework. Read and Reward speeds up RL algorithms on Atari games by reading manuals released by the Atari game developers. Our framework consists of a QA Extraction module that extracts and summarizes relevant information from the manual and a Reasoning module that evaluates object-agent interactions based on information from the manual. An auxiliary reward is then provided to a standard A2C RL agent, when interaction is detected. Experimentally, various RL algorithms obtain significant improvement in performance and training speed when assisted by our design.\\nDocument: ../data/paper_jsons/Tom Michael Mitchell_40975594.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 660}),\n"," Document(page_content='Question: What is the author ID of William Cohen?\\nAnswer: 50056360\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 661}),\n"," Document(page_content='Question: What are the papers written by William Cohen?\\nAnswer: Answering Ambiguous Questions with a Database of Questions, Answers, and Revisions, MEMORY-VQ: Compression for Tractable Internet-Scale Memory, Subject-driven Text-to-Image Generation via Apprenticeship Learning, GLIMMER: generalized late-interaction memory reranker\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 662}),\n"," Document(page_content='Question: What is the H-index of William Cohen?\\nAnswer: 88\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 663}),\n"," Document(page_content='Question: What is the author citation count of William Cohen?\\nAnswer: 41245\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 664}),\n"," Document(page_content='Question: What is the author paper count of William Cohen?\\nAnswer: 440\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 665}),\n"," Document(page_content='Question: What journals has William Cohen published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 666}),\n"," Document(page_content='Question: What are the journals and how many papers has William Cohen published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 667}),\n"," Document(page_content='Question: What are the fields of study of William Cohen?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 668}),\n"," Document(page_content='Question: How many papers has William Cohen published in open access journals?\\nAnswer: 4\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 669}),\n"," Document(page_content='Question: What venues has William Cohen published in?\\nAnswer: arXiv.org\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 670}),\n"," Document(page_content='Question: What is the most cited paper from William Cohen?\\nAnswer: Subject-driven Text-to-Image Generation via Apprenticeship Learning\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 671}),\n"," Document(page_content='Question: What is the url of the most cited paper from William Cohen?\\nAnswer: https://arxiv.org/pdf/2304.00186\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 672}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from William Cohen?\\nAnswer: Wenhu Chen, Hexiang Hu, Yandong Li, Nataniel Rui, Xuhui Jia, Ming-Wei Chang, William W. Cohen\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 673}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from William Cohen?\\nAnswer: Human evaluation shows that SuTI significantly outperforms existing models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen and DreamBooth, especially on the subject and text alignment aspects.\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 674}),\n"," Document(page_content=\"Question: Abstract of the most cited paper from William Cohen?\\nAnswer: Recent text-to-image generation models like DreamBooth have made remarkable progress in generating highly customized images of a target subject, by fine-tuning an ``expert model'' for a given subject from a few examples. However, this process is expensive, since a new expert model must be learned for each subject. In this paper, we present SuTI, a Subject-driven Text-to-Image generator that replaces subject-specific fine tuning with in-context learning. Given a few demonstrations of a new subject, SuTI can instantly generate novel renditions of the subject in different scenes, without any subject-specific optimization. SuTI is powered by apprenticeship learning, where a single apprentice model is learned from data generated by a massive number of subject-specific expert models. Specifically, we mine millions of image clusters from the Internet, each centered around a specific visual subject. We adopt these clusters to train a massive number of expert models, each specializing in a different subject. The apprentice model SuTI then learns to imitate the behavior of these fine-tuned experts. SuTI can generate high-quality and customized subject-specific images 20x faster than optimization-based SoTA methods. On the challenging DreamBench and DreamBench-v2, our human evaluation shows that SuTI significantly outperforms existing models like InstructPix2Pix, Textual Inversion, Imagic, Prompt2Prompt, Re-Imagen and DreamBooth, especially on the subject and text alignment aspects.\\nDocument: ../data/paper_jsons/William W. Cohen_50056360.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 675}),\n"," Document(page_content='Question: What is the author ID of Yiming Yang?\\nAnswer: 35729970\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 676}),\n"," Document(page_content='Question: What are the papers written by Yiming Yang?\\nAnswer: Accelerating Diffusion-based Combinatorial Optimization Solvers by Progressive Distillation, Automatic synchrotron tomographic alignment schemes based on genetic algorithms and human-in-the-loop software, High CD8+tumor-infiltrating lymphocytes indicate severe exhaustion and poor prognosis in angioimmunoblastic T-cell lymphoma, Numerical Investigation of Fatigue Crack Propagation Behaviour of 550E High-Performance Steel, Association between albumin-to-globulin ratio and the risk of overall survival in advanced non-small cell lung cancer patients with anlotinib treatment: a retrospective cohort study, Analysis of Volatile Components in Dried Fruits and Branch Exudates of Schisandra chinensis with Different Fruit Colors Using GC-IMS Technology, Secreted endogenous macrosomes reduce Aβ burden and ameliorate Alzheimer’s disease, DIFUSCO: Graph-based Diffusion Solvers for Combinatorial Optimization, Balancing Exploration and Exploitation in Hierarchical Reinforcement Learning via Latent Landmark Graphs, Aligning Large Multimodal Models with Factually Augmented RLHF, An Experimental Study on Secondary Transfer Performances of Prestress after Anchoring Failure of Steel Wire Strands, MRI Features for Predicting Microvascular Invasion and Postoperative Recurrence in Hepatocellular Carcinoma Without Peritumoral Hypointensity, Impact of local governments’ construction land allocation strategies on innovation-driven development of China\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 677}),\n"," Document(page_content='Question: What is the H-index of Yiming Yang?\\nAnswer: 64\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 678}),\n"," Document(page_content='Question: What is the author citation count of Yiming Yang?\\nAnswer: 47104\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 679}),\n"," Document(page_content='Question: What is the author paper count of Yiming Yang?\\nAnswer: 357\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 680}),\n"," Document(page_content='Question: What journals has Yiming Yang published in?\\nAnswer: ArXiv, Journal of Synchrotron Radiation, Frontiers in Immunology, Metals, BMC Pulmonary Medicine, Molecules, Science Advances, 2023 International Joint Conference on Neural Networks (IJCNN), Journal of Hepatocellular Carcinoma, 资源科学\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 681}),\n"," Document(page_content='Question: What are the journals and how many papers has Yiming Yang published in each?\\nAnswer: 3 in ArXiv, 2 in Metals, 1 in Journal of Synchrotron Radiation, 1 in Frontiers in Immunology, 1 in BMC Pulmonary Medicine, 1 in Molecules, 1 in Science Advances, 1 in 2023 International Joint Conference on Neural Networks (IJCNN), 1 in Journal of Hepatocellular Carcinoma, 1 in 资源科学\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 682}),\n"," Document(page_content='Question: What are the fields of study of Yiming Yang?\\nAnswer: Computer Science, Medicine\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 683}),\n"," Document(page_content='Question: How many papers has Yiming Yang published in open access journals?\\nAnswer: 13\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 684}),\n"," Document(page_content='Question: What venues has Yiming Yang published in?\\nAnswer: arXiv.org, Journal of Synchrotron Radiation, Frontiers in Immunology, Metals, BMC Pulmonary Medicine, Molecules, Science Advances, IEEE International Joint Conference on Neural Network, Journal of Hepatocellular Carcinoma, 资源科学\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 685}),\n"," Document(page_content='Question: What is the most cited paper from Yiming Yang?\\nAnswer: Aligning Large Multimodal Models with Factually Augmented RLHF\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 686}),\n"," Document(page_content='Question: What is the url of the most cited paper from Yiming Yang?\\nAnswer: https://arxiv.org/pdf/2309.14525\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 687}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Yiming Yang?\\nAnswer: Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liangyan Gui, Yu-Xiong Wang, Yiming Yang, K. Keutzer, Trevor Darrell\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 688}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Yiming Yang?\\nAnswer: A new alignment algorithm called Factually Augmented RLHF is proposed that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance.\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 689}),\n"," Document(page_content='Question: Abstract of the most cited paper from Yiming Yang?\\nAnswer: Large Multimodal Models (LMM) are built across modalities and the misalignment between two modalities can result in\"hallucination\", generating textual outputs that are not grounded by the multimodal information in context. To address the multimodal misalignment issue, we adapt the Reinforcement Learning from Human Feedback (RLHF) from the text domain to the task of vision-language alignment, where human annotators are asked to compare two responses and pinpoint the more hallucinated one, and the vision-language model is trained to maximize the simulated human rewards. We propose a new alignment algorithm called Factually Augmented RLHF that augments the reward model with additional factual information such as image captions and ground-truth multi-choice options, which alleviates the reward hacking phenomenon in RLHF and further improves the performance. We also enhance the GPT-4-generated training data (for vision instruction tuning) with previously available human-written image-text pairs to improve the general capabilities of our model. To evaluate the proposed approach in real-world scenarios, we develop a new evaluation benchmark MMHAL-BENCH with a special focus on penalizing hallucinations. As the first LMM trained with RLHF, our approach achieves remarkable improvement on the LLaVA-Bench dataset with the 94% performance level of the text-only GPT-4 (while previous best methods can only achieve the 87% level), and an improvement by 60% on MMHAL-BENCH over other baselines. We opensource our code, model, data at https://llava-rlhf.github.io.\\nDocument: ../data/paper_jsons/Yiming Yang_35729970.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 690}),\n"," Document(page_content='Question: What is the author ID of Yiming Yang?\\nAnswer: 46286308\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 691}),\n"," Document(page_content=\"Question: What are the papers written by Yiming Yang?\\nAnswer: PESCO: Prompt-enhanced Self Contrastive Learning for Zero-shot Text Classification, Learning Performance-Improving Code Edits, Generation-driven Contrastive Self-training for Zero-shot Text Classification with Instruction-tuned GPT, Self-Refine: Iterative Refinement with Self-Feedback, Long-tailed Extreme Multi-label Text Classification by the Retrieval of Generated Pseudo Label Descriptions, Learning a Fourier Transform for Linear Relative Positional Encodings in Transformers, Efficient Temporal Sentence Grounding in Videos with Multi-Teacher Knowledge Distillation, Active Retrieval Augmented Generation, Retrieval-Enhanced Generative Model for Large-Scale Knowledge Graph Completion, Let's Sample Step by Step: Adaptive-Consistency for Efficient Reasoning and Coding with LLMs, Principle-Driven Self-Alignment of Language Models from Scratch with Minimal Human Supervision, Policy Representation via Diffusion Probability Model for Reinforcement Learning\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: \", metadata={'source': '../data/paper_logs/author.csv', 'row': 692}),\n"," Document(page_content='Question: What is the H-index of Yiming Yang?\\nAnswer: 17\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 693}),\n"," Document(page_content='Question: What is the author citation count of Yiming Yang?\\nAnswer: 1532\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 694}),\n"," Document(page_content='Question: What is the author paper count of Yiming Yang?\\nAnswer: 43\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 695}),\n"," Document(page_content='Question: What journals has Yiming Yang published in?\\nAnswer: ArXiv, Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 696}),\n"," Document(page_content='Question: What are the journals and how many papers has Yiming Yang published in each?\\nAnswer: 8 in ArXiv, 1 in Proceedings of the 46th International ACM SIGIR Conference on Research and Development in Information Retrieval\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 697}),\n"," Document(page_content='Question: What are the fields of study of Yiming Yang?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 698}),\n"," Document(page_content='Question: How many papers has Yiming Yang published in open access journals?\\nAnswer: 12\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 699}),\n"," Document(page_content='Question: What venues has Yiming Yang published in?\\nAnswer: Annual Meeting of the Association for Computational Linguistics, arXiv.org, Findings, Conference on Empirical Methods in Natural Language Processing, Annual International ACM SIGIR Conference on Research and Development in Information Retrieval\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 700}),\n"," Document(page_content='Question: What is the most cited paper from Yiming Yang?\\nAnswer: Self-Refine: Iterative Refinement with Self-Feedback\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 701}),\n"," Document(page_content='Question: What is the url of the most cited paper from Yiming Yang?\\nAnswer: http://arxiv.org/pdf/2303.17651\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 702}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Yiming Yang?\\nAnswer: Aman Madaan, Niket Tandon, Prakhar Gupta, Skyler Hallinan, Luyu Gao, Sarah Wiegreffe, Uri Alon, Nouha Dziri, Shrimai Prabhumoye, Yiming Yang, S. Welleck, Bodhisattwa Prasad Majumder, Shashank Gupta, A. Yazdanbakhsh, Peter Clark\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 703}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Yiming Yang?\\nAnswer: Self-Refine is introduced, an approach for improving initial outputs from LLMs through iterative feedback and refinement that demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using this simple, standalone approach.\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 704}),\n"," Document(page_content='Question: Abstract of the most cited paper from Yiming Yang?\\nAnswer: Like humans, large language models (LLMs) do not always generate the best output on their first try. Motivated by how humans refine their written text, we introduce Self-Refine, an approach for improving initial outputs from LLMs through iterative feedback and refinement. The main idea is to generate an initial output using an LLMs; then, the same LLMs provides feedback for its output and uses it to refine itself, iteratively. Self-Refine does not require any supervised training data, additional training, or reinforcement learning, and instead uses a single LLM as the generator, refiner, and feedback provider. We evaluate Self-Refine across 7 diverse tasks, ranging from dialog response generation to mathematical reasoning, using state-of-the-art (GPT-3.5, ChatGPT, and GPT-4) LLMs. Across all evaluated tasks, outputs generated with Self-Refine are preferred by humans and automatic metrics over those generated with the same LLM using conventional one-step generation, improving by ~20% absolute on average in task performance. Our work demonstrates that even state-of-the-art LLMs like GPT-4 can be further improved at test time using our simple, standalone approach.\\nDocument: ../data/paper_jsons/Yiming Yang_46286308.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 705}),\n"," Document(page_content='Question: What is the author ID of Yonatan Bisk?\\nAnswer: 3312309\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 706}),\n"," Document(page_content='Question: What are the papers written by Yonatan Bisk?\\nAnswer: SPAE: Semantic Pyramid AutoEncoder for Multimodal Generation with Frozen LLMs, HomeRobot: Open-Vocabulary Mobile Manipulation, Plan, Eliminate, and Track - Language Models are Good Teachers for Embodied Agents, MOSAIC: Learning Unified Multi-Sensory Object Property Representations for Robot Perception, Reasoning about the Unseen for Efficient Outdoor Object Navigation, The Framework Tax: Disparities Between Inference Efficiency in Research and Deployment, WebArena: A Realistic Web Environment for Building Autonomous Agents, Computational Language Acquisition with Theory of Mind\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 707}),\n"," Document(page_content='Question: What is the H-index of Yonatan Bisk?\\nAnswer: 33\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 708}),\n"," Document(page_content='Question: What is the author citation count of Yonatan Bisk?\\nAnswer: 7048\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 709}),\n"," Document(page_content='Question: What is the author paper count of Yonatan Bisk?\\nAnswer: 98\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 710}),\n"," Document(page_content='Question: What journals has Yonatan Bisk published in?\\nAnswer: ArXiv\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 711}),\n"," Document(page_content='Question: What are the journals and how many papers has Yonatan Bisk published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 712}),\n"," Document(page_content='Question: What are the fields of study of Yonatan Bisk?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 713}),\n"," Document(page_content='Question: How many papers has Yonatan Bisk published in open access journals?\\nAnswer: 8\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 714}),\n"," Document(page_content='Question: What venues has Yonatan Bisk published in?\\nAnswer: arXiv.org, Conference on Robot Learning, Conference on Empirical Methods in Natural Language Processing, International Conference on Learning Representations\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 715}),\n"," Document(page_content='Question: What is the most cited paper from Yonatan Bisk?\\nAnswer: WebArena: A Realistic Web Environment for Building Autonomous Agents\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 716}),\n"," Document(page_content='Question: What is the url of the most cited paper from Yonatan Bisk?\\nAnswer: https://arxiv.org/pdf/2307.13854\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 717}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Yonatan Bisk?\\nAnswer: Shuyan Zhou, Frank F. Xu, Hao Zhu, Xuhui Zhou, Robert Lo, Abishek Sridhar, Xianyi Cheng, Yonatan Bisk, Daniel Fried, Uri Alon, Graham Neubig\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 718}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Yonatan Bisk?\\nAnswer: This paper builds an environment for language-guided agents that is highly realistic and reproducible, and creates an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management.\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 719}),\n"," Document(page_content='Question: Abstract of the most cited paper from Yonatan Bisk?\\nAnswer: With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.\\nDocument: ../data/paper_jsons/Yonatan Bisk_3312309.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 720}),\n"," Document(page_content='Question: What is the author ID of Yulia Tsvetkov?\\nAnswer: 145317727\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 721}),\n"," Document(page_content='Question: What are the papers written by Yulia Tsvetkov?\\nAnswer: Understanding Ethics in NLP Authoring and Reviewing\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 722}),\n"," Document(page_content='Question: What is the H-index of Yulia Tsvetkov?\\nAnswer: 33\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 723}),\n"," Document(page_content='Question: What is the author citation count of Yulia Tsvetkov?\\nAnswer: 4659\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 724}),\n"," Document(page_content='Question: What is the author paper count of Yulia Tsvetkov?\\nAnswer: 92\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 725}),\n"," Document(page_content='Question: What journals has Yulia Tsvetkov published in?\\nAnswer: Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics: Tutorial Abstracts\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 726}),\n"," Document(page_content='Question: What are the journals and how many papers has Yulia Tsvetkov published in each?\\nAnswer: No journal data available.\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 727}),\n"," Document(page_content='Question: What are the fields of study of Yulia Tsvetkov?\\nAnswer: \\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 728}),\n"," Document(page_content='Question: How many papers has Yulia Tsvetkov published in open access journals?\\nAnswer: 1\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 729}),\n"," Document(page_content='Question: What venues has Yulia Tsvetkov published in?\\nAnswer: Conference of the European Chapter of the Association for Computational Linguistics\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 730}),\n"," Document(page_content='Question: What is the most cited paper from Yulia Tsvetkov?\\nAnswer: Understanding Ethics in NLP Authoring and Reviewing\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 731}),\n"," Document(page_content='Question: What is the url of the most cited paper from Yulia Tsvetkov?\\nAnswer: https://aclanthology.org/2023.eacl-tutorials.4.pdf\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 732}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Yulia Tsvetkov?\\nAnswer: Luciana Benotti, Karen Fort, Min-Yen Kan, Yulia Tsvetkov\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 733}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Yulia Tsvetkov?\\nAnswer: This tutorial will equip participants with basic guidelines for thinking deeply about ethical issues and review common considerations that recur in NLP research.\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 734}),\n"," Document(page_content='Question: Abstract of the most cited paper from Yulia Tsvetkov?\\nAnswer: With NLP research now quickly being transferred into real-world applications, it is important to be aware of and think through the consequences of our scientific investigation. Such ethical considerations are important in both authoring and reviewing. This tutorial will equip participants with basic guidelines for thinking deeply about ethical issues and review common considerations that recur in NLP research. The methodology is interactive and participatory, including case studies and working in groups. Importantly, the participants will be co-building the tutorial outcomes and will be working to create further tutorial materials to share as public outcomes.\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_145317727.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 735}),\n"," Document(page_content='Question: What is the author ID of Yulia Tsvetkov?\\nAnswer: 2073587169\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 736}),\n"," Document(page_content='Question: What are the papers written by Yulia Tsvetkov?\\nAnswer: GlobalBench: A Benchmark for Global Progress in Natural Language Processing, Do All Languages Cost the Same? Tokenization in the Era of Commercial Language Models, BotPercent: Estimating Twitter Bot Populations from Groups to Crowds, Examining risks of racial biases in NLP tools for child protective services, Understanding In-Context Learning via Supportive Pretraining Data, Assessing Language Model Deployment with Risk Cards, From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models, FactKB: Generalizable Factuality Evaluation using Language Models Enhanced with Factual Knowledge, TalkUp: Paving the Way for Understanding Empowering Language, Trusting Your Evidence: Hallucinate Less with Context-aware Decoding, BUFFET: Benchmarking Large Language Models for Few-shot Cross-lingual Transfer, Mitigating Societal Harms in Large Language Models, Minding Language Models’ (Lack of) Theory of Mind: A Plug-and-Play Multi-Character Belief Tracker, Can Language Models Solve Graph Problems in Natural Language?, LEXPLAIN: Improving Model Explanations via Lexicon Supervision\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 737}),\n"," Document(page_content='Question: What is the H-index of Yulia Tsvetkov?\\nAnswer: 17\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 738}),\n"," Document(page_content='Question: What is the author citation count of Yulia Tsvetkov?\\nAnswer: 1382\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 739}),\n"," Document(page_content='Question: What is the author paper count of Yulia Tsvetkov?\\nAnswer: 48\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 740}),\n"," Document(page_content='Question: What journals has Yulia Tsvetkov published in?\\nAnswer: ArXiv, Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 741}),\n"," Document(page_content='Question: What are the journals and how many papers has Yulia Tsvetkov published in each?\\nAnswer: 8 in ArXiv, 1 in Proceedings of the 2023 ACM Conference on Fairness, Accountability, and Transparency, 1 in Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 742}),\n"," Document(page_content='Question: What are the fields of study of Yulia Tsvetkov?\\nAnswer: Computer Science\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 743}),\n"," Document(page_content='Question: How many papers has Yulia Tsvetkov published in open access journals?\\nAnswer: 15\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 744}),\n"," Document(page_content='Question: What venues has Yulia Tsvetkov published in?\\nAnswer: Conference on Empirical Methods in Natural Language Processing, arXiv.org, Conference on Fairness, Accountability and Transparency, Annual Meeting of the Association for Computational Linguistics, Proceedings of the 2023 Conference on Empirical Methods in Natural Language Processing: Tutorial Abstracts, STARSEM\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 745}),\n"," Document(page_content='Question: What is the most cited paper from Yulia Tsvetkov?\\nAnswer: From Pretraining Data to Language Models to Downstream Tasks: Tracking the Trails of Political Biases Leading to Unfair NLP Models\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 746}),\n"," Document(page_content='Question: What is the url of the most cited paper from Yulia Tsvetkov?\\nAnswer: https://arxiv.org/pdf/2305.08283\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 747}),\n"," Document(page_content='Question: Who are the authors of the most cited paper from Yulia Tsvetkov?\\nAnswer: Shangbin Feng, Chan Young Park, Yuhan Liu, Yulia Tsvetkov\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 748}),\n"," Document(page_content='Question: TLDR/Summary of the most cited paper from Yulia Tsvetkov?\\nAnswer: The findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors.\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 749}),\n"," Document(page_content='Question: Abstract of the most cited paper from Yulia Tsvetkov?\\nAnswer: Language models (LMs) are pretrained on diverse data sources—news, discussion forums, books, online encyclopedias. A significant portion of this data includes facts and opinions which, on one hand, celebrate democracy and diversity of ideas, and on the other hand are inherently socially biased. Our work develops new methods to (1) measure media biases in LMs trained on such corpora, along social and economic axes, and (2) measure the fairness of downstream NLP models trained on top of politically biased LMs. We focus on hate speech and misinformation detection, aiming to empirically quantify the effects of political (social, economic) biases in pretraining data on the fairness of high-stakes social-oriented tasks. Our findings reveal that pretrained LMs do have political leanings which reinforce the polarization present in pretraining corpora, propagating social biases into hate speech predictions and media biases into misinformation detectors. We discuss the implications of our findings for NLP research and propose future directions to mitigate unfairness.\\nDocument: ../data/paper_jsons/Yulia Tsvetkov_2073587169.json\\nNotes: ', metadata={'source': '../data/paper_logs/author.csv', 'row': 750})]"]},"metadata":{},"execution_count":130}]},{"cell_type":"code","source":["db_author_csv = create_db(author_csv)\n","db_author_csv.save_local(\"../faiss_index_authors\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RWRt7B5ydNYq","executionInfo":{"status":"ok","timestamp":1710026046793,"user_tz":300,"elapsed":17761,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}},"outputId":"639b7d36-f249-452b-9433-d6791d793cc5"},"execution_count":132,"outputs":[{"output_type":"stream","name":"stderr","text":["Token indices sequence length is longer than the specified maximum sequence length for this model (846 > 512). Running this sequence through the model will result in indexing errors\n"]}]},{"cell_type":"code","source":["db_combined = db_csv\n","db_combined.merge_from(db_author_csv)"],"metadata":{"id":"TceUlcK8dNfj","executionInfo":{"status":"ok","timestamp":1710026449805,"user_tz":300,"elapsed":142,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}}},"execution_count":134,"outputs":[]},{"cell_type":"code","source":["db_combined.save_local(\"faiss_index_author_papers_lang\")"],"metadata":{"id":"AIv5fw6ZdNmp","executionInfo":{"status":"ok","timestamp":1710026451245,"user_tz":300,"elapsed":301,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}}},"execution_count":135,"outputs":[]},{"cell_type":"code","source":["# json index saving"],"metadata":{"id":"0uDEPJfZh6UP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["db_combined = db_json\n","db_combined.merge_from(db_author_csv)\n","db_combined.save_local(\"faiss_index_author_papers_json\")"],"metadata":{"id":"5yEgOE0Yh6u2","executionInfo":{"status":"ok","timestamp":1710026663938,"user_tz":300,"elapsed":135,"user":{"displayName":"Vashisth Tiwari","userId":"14058204908965748495"}}},"execution_count":138,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gjHEEYuFh64Y"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"73381c9bc58b41febb95b6cf451ec0a5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_70ce8f12e7564712bc8dd21490addac7","IPY_MODEL_4405c5fcad3f4b56b1012602149f85b4","IPY_MODEL_049db641804e4662a86758ae1ab97c20"],"layout":"IPY_MODEL_2b7e1f7357524494a8708d80b2d0b03b"}},"70ce8f12e7564712bc8dd21490addac7":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_204311f4b93f4ca2adbc75e799dcc269","placeholder":"​","style":"IPY_MODEL_ab210c96464541d9a49ed1eecdd2a482","value":"modules.json: 100%"}},"4405c5fcad3f4b56b1012602149f85b4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_bb79d53db4914eabbec43ba4f6d393c2","max":385,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2fae61055efd40aaace631946908d9c0","value":385}},"049db641804e4662a86758ae1ab97c20":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5327b2ab06fc497882a192ff80572dd1","placeholder":"​","style":"IPY_MODEL_bfb6cf42ae2a4223a67d450d04fa58d6","value":" 385/385 [00:00&lt;00:00, 6.84kB/s]"}},"2b7e1f7357524494a8708d80b2d0b03b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"204311f4b93f4ca2adbc75e799dcc269":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ab210c96464541d9a49ed1eecdd2a482":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"bb79d53db4914eabbec43ba4f6d393c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2fae61055efd40aaace631946908d9c0":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"5327b2ab06fc497882a192ff80572dd1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"bfb6cf42ae2a4223a67d450d04fa58d6":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9275b2e658044cf8255babb574bbfaa":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d55b45a257d54d3eb379310d25ee65ed","IPY_MODEL_3aea1e0a6677492980b427cbc9e91ab3","IPY_MODEL_2babd835e39f4d79ae45e0706da0fd28"],"layout":"IPY_MODEL_971d978505954d98b3338bd7d64cf08c"}},"d55b45a257d54d3eb379310d25ee65ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6ac0f6cbf8e44f894f8f745a241acdf","placeholder":"​","style":"IPY_MODEL_c46177cdfb694c308523aa1ff9ebef50","value":"README.md: 100%"}},"3aea1e0a6677492980b427cbc9e91ab3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_0b79308aa8d2448897e850c21fc15154","max":68075,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e5b636ea02fe4f93a929a36ff77375ad","value":68075}},"2babd835e39f4d79ae45e0706da0fd28":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df4855de7e3449a8b600b6585c834c94","placeholder":"​","style":"IPY_MODEL_86233655364345cdbfc5bba2d0fea91e","value":" 68.1k/68.1k [00:00&lt;00:00, 1.19MB/s]"}},"971d978505954d98b3338bd7d64cf08c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6ac0f6cbf8e44f894f8f745a241acdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c46177cdfb694c308523aa1ff9ebef50":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"0b79308aa8d2448897e850c21fc15154":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e5b636ea02fe4f93a929a36ff77375ad":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"df4855de7e3449a8b600b6585c834c94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86233655364345cdbfc5bba2d0fea91e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"394a1323cb0943638187472da7cad0c0":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_eb59a54810cc4e33ad1f662accb181cd","IPY_MODEL_337c4ca8dac441e08a24aca235436524","IPY_MODEL_12bc787071f14b5aac2c943ec1dfeb53"],"layout":"IPY_MODEL_c5470d77d157400580dc79658b53d7f4"}},"eb59a54810cc4e33ad1f662accb181cd":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4bf6f09713e64a07ab676042cb921928","placeholder":"​","style":"IPY_MODEL_9c484f8cc5114ab88bc7816dc3d2a552","value":"sentence_bert_config.json: 100%"}},"337c4ca8dac441e08a24aca235436524":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_3eccb3e57ac14719abdb4778a78c73e9","max":57,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70efbe1306954d1788372cf5f3ef5091","value":57}},"12bc787071f14b5aac2c943ec1dfeb53":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3db41973588045c58f6759d7001fffbc","placeholder":"​","style":"IPY_MODEL_1d9f595c7cf0484baf373d9fb60e6c0c","value":" 57.0/57.0 [00:00&lt;00:00, 1.14kB/s]"}},"c5470d77d157400580dc79658b53d7f4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4bf6f09713e64a07ab676042cb921928":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9c484f8cc5114ab88bc7816dc3d2a552":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"3eccb3e57ac14719abdb4778a78c73e9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70efbe1306954d1788372cf5f3ef5091":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3db41973588045c58f6759d7001fffbc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1d9f595c7cf0484baf373d9fb60e6c0c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c1396d674b3b42f18924bc088c7eeb65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c8ef58c234744846ba38dbdceb6b77fa","IPY_MODEL_fb50d42f326b44dbb51680b07edf4403","IPY_MODEL_31f006c985384e7aa3c78ebe4f1ecdab"],"layout":"IPY_MODEL_23972042c8df4a7aba905e37ce4f6113"}},"c8ef58c234744846ba38dbdceb6b77fa":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ccc02de561354c6ab0daed49b95ca23e","placeholder":"​","style":"IPY_MODEL_fcdbb160558149508fa50fed1d385ae7","value":"config.json: 100%"}},"fb50d42f326b44dbb51680b07edf4403":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ef0cef9099f4ff585a18b37c020c3ed","max":618,"min":0,"orientation":"horizontal","style":"IPY_MODEL_90b1477470054fd68210622e22437ee7","value":618}},"31f006c985384e7aa3c78ebe4f1ecdab":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_89d13559cd554c8b8994154a440f3415","placeholder":"​","style":"IPY_MODEL_c14994d191e443999287a6456275c92a","value":" 618/618 [00:00&lt;00:00, 22.9kB/s]"}},"23972042c8df4a7aba905e37ce4f6113":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ccc02de561354c6ab0daed49b95ca23e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fcdbb160558149508fa50fed1d385ae7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ef0cef9099f4ff585a18b37c020c3ed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90b1477470054fd68210622e22437ee7":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"89d13559cd554c8b8994154a440f3415":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c14994d191e443999287a6456275c92a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b82501f97a664f919648c68528e98732":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_65f936e9b8bd4fadaac87005e0541c39","IPY_MODEL_9903794c798c498c80f96da246427448","IPY_MODEL_f23807ad890a40c485173939461d6ac5"],"layout":"IPY_MODEL_e78590a6925545e7ac37632853869b1c"}},"65f936e9b8bd4fadaac87005e0541c39":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c8626b94dc404c519826cc20a20f4adc","placeholder":"​","style":"IPY_MODEL_66f50fb62e9e4c559e905360f02caded","value":"model.safetensors: 100%"}},"9903794c798c498c80f96da246427448":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_b29ddfcdfe5547fea7b53c7ddd518082","max":218990904,"min":0,"orientation":"horizontal","style":"IPY_MODEL_99967e20e9d3441794323949472f025c","value":218990904}},"f23807ad890a40c485173939461d6ac5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4976707ee6644b5fa66e5ace7c3edfee","placeholder":"​","style":"IPY_MODEL_77c2a046ac3d48b5b625c0c4cfc2c10c","value":" 219M/219M [00:03&lt;00:00, 66.1MB/s]"}},"e78590a6925545e7ac37632853869b1c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c8626b94dc404c519826cc20a20f4adc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"66f50fb62e9e4c559e905360f02caded":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b29ddfcdfe5547fea7b53c7ddd518082":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"99967e20e9d3441794323949472f025c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"4976707ee6644b5fa66e5ace7c3edfee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"77c2a046ac3d48b5b625c0c4cfc2c10c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"aee5ea91e79147b3bff12ed9996e086a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_af1934d6b9df4ea1bae2c7a9b81e84f5","IPY_MODEL_3b4df1d228574aa78ee1c729c2463807","IPY_MODEL_a75e7d4dddaa4151ae035a04ee798c5b"],"layout":"IPY_MODEL_f7f49188d5da4a4caf8478fc69fd89a5"}},"af1934d6b9df4ea1bae2c7a9b81e84f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_23c7160fe5bd4b539f58d2d7d5dab63b","placeholder":"​","style":"IPY_MODEL_2b2afcf3d68b4d48b5255db1b857dcd3","value":"tokenizer_config.json: 100%"}},"3b4df1d228574aa78ee1c729c2463807":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9e85535974874e54814cbf9520eb54f0","max":314,"min":0,"orientation":"horizontal","style":"IPY_MODEL_c106f61ad3c44b9281b658ada1c4ea55","value":314}},"a75e7d4dddaa4151ae035a04ee798c5b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_661459366fe0423e919f913a4d83a31c","placeholder":"​","style":"IPY_MODEL_272ad2985f3d41ababb9644e34501301","value":" 314/314 [00:00&lt;00:00, 19.9kB/s]"}},"f7f49188d5da4a4caf8478fc69fd89a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"23c7160fe5bd4b539f58d2d7d5dab63b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2b2afcf3d68b4d48b5255db1b857dcd3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9e85535974874e54814cbf9520eb54f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c106f61ad3c44b9281b658ada1c4ea55":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"661459366fe0423e919f913a4d83a31c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"272ad2985f3d41ababb9644e34501301":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"6ba5845fabd64d0d82a63d524c861bde":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_06368b4be61243249c4c2d5551ad3434","IPY_MODEL_b7e7d4ea149b4a4a90bb9abd56bc8aa4","IPY_MODEL_17ad3c546c9c4d76b5446ee460e42631"],"layout":"IPY_MODEL_2d3e3cba9e9d4ab6916e8cf49b1b302d"}},"06368b4be61243249c4c2d5551ad3434":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_335cf7a69dd04ecdb0abcb03ccf68fc5","placeholder":"​","style":"IPY_MODEL_90e1eb2004a44e0480ccbf3aa44d7f12","value":"vocab.txt: 100%"}},"b7e7d4ea149b4a4a90bb9abd56bc8aa4":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_7e178f71bcb44093b4604efa62749e1d","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6e37f23bb8e4cef98fdf56cdb9219cc","value":231508}},"17ad3c546c9c4d76b5446ee460e42631":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd47c89371f7419a828b9552d3fc5de5","placeholder":"​","style":"IPY_MODEL_3353e8f58c2a48929f0b06d540396197","value":" 232k/232k [00:00&lt;00:00, 3.53MB/s]"}},"2d3e3cba9e9d4ab6916e8cf49b1b302d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"335cf7a69dd04ecdb0abcb03ccf68fc5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"90e1eb2004a44e0480ccbf3aa44d7f12":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"7e178f71bcb44093b4604efa62749e1d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6e37f23bb8e4cef98fdf56cdb9219cc":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd47c89371f7419a828b9552d3fc5de5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3353e8f58c2a48929f0b06d540396197":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8b64eda4f1254744a3249420ee24d80b":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_18726390fbb94e2ebfafdbf115e5b2f2","IPY_MODEL_f8c86673b1414202a4e461cd0895beaf","IPY_MODEL_ef4ee6174ba945c9b2bec5ae6566bfbb"],"layout":"IPY_MODEL_d1a454f2490b453fa5eda3cb2d55ce13"}},"18726390fbb94e2ebfafdbf115e5b2f2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_541e038f2839435fa5a47c020e004aed","placeholder":"​","style":"IPY_MODEL_7c16f94e804142cc8ca538d7608244aa","value":"tokenizer.json: 100%"}},"f8c86673b1414202a4e461cd0895beaf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c95ab0557ca4460fb74fefcf8da40066","max":711661,"min":0,"orientation":"horizontal","style":"IPY_MODEL_58e0f55eedd34c99baf104381909ccce","value":711661}},"ef4ee6174ba945c9b2bec5ae6566bfbb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c62abfa1fc264123a2a73995c7bebf6f","placeholder":"​","style":"IPY_MODEL_857a7e8b5bf645f0a729883d3bfe7371","value":" 712k/712k [00:00&lt;00:00, 8.54MB/s]"}},"d1a454f2490b453fa5eda3cb2d55ce13":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"541e038f2839435fa5a47c020e004aed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c16f94e804142cc8ca538d7608244aa":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c95ab0557ca4460fb74fefcf8da40066":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"58e0f55eedd34c99baf104381909ccce":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c62abfa1fc264123a2a73995c7bebf6f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"857a7e8b5bf645f0a729883d3bfe7371":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9712385ae82f4fbfa04ea909fd46258f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_f9094c8b4bcb4744a52cacb3c334b85d","IPY_MODEL_d3b546d4c3d94e0ebe90f874fd76e776","IPY_MODEL_7c49a27b11e94c07b80333f52fc826b5"],"layout":"IPY_MODEL_bf6508f46fe3454d9bc3af6ee31e8bdf"}},"f9094c8b4bcb4744a52cacb3c334b85d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d3757ac836ef4d9b93239f4455a29684","placeholder":"​","style":"IPY_MODEL_0307710609fd4f2b832f87cfa9be60f8","value":"special_tokens_map.json: 100%"}},"d3b546d4c3d94e0ebe90f874fd76e776":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_1b9803e22b754d52a1aacbb297c8948c","max":125,"min":0,"orientation":"horizontal","style":"IPY_MODEL_f87b7f4b74854c88bde127d01b9fbeb5","value":125}},"7c49a27b11e94c07b80333f52fc826b5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_93ef878d55b84249b2d51ae7434036b8","placeholder":"​","style":"IPY_MODEL_dda1a81e97ed46c58604cbe5ba335fac","value":" 125/125 [00:00&lt;00:00, 8.49kB/s]"}},"bf6508f46fe3454d9bc3af6ee31e8bdf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d3757ac836ef4d9b93239f4455a29684":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0307710609fd4f2b832f87cfa9be60f8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"1b9803e22b754d52a1aacbb297c8948c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f87b7f4b74854c88bde127d01b9fbeb5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"93ef878d55b84249b2d51ae7434036b8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"dda1a81e97ed46c58604cbe5ba335fac":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"153ffc6c9db347a98e0b6ae582769cc9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_fa6ffcd7180c45a588f9e7c5860828ed","IPY_MODEL_fa787bd9fedc4d7a89384f33e853ce64","IPY_MODEL_95aaaa54fc184ca6912737fed003347e"],"layout":"IPY_MODEL_7e1af9ed45b14ecf95435b9eff217397"}},"fa6ffcd7180c45a588f9e7c5860828ed":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_0c3a1d08e4f14df2a36fe132349e33bc","placeholder":"​","style":"IPY_MODEL_3d16a72cda2b4097a8742b21d2172275","value":"1_Pooling/config.json: 100%"}},"fa787bd9fedc4d7a89384f33e853ce64":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd944b63c4f944f5aae18c76118c803f","max":190,"min":0,"orientation":"horizontal","style":"IPY_MODEL_4af9843f83214d55902799b5fb6665df","value":190}},"95aaaa54fc184ca6912737fed003347e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6484c7ab71bd4b5e8286feed649b393e","placeholder":"​","style":"IPY_MODEL_532d9ecf84304085b0f1e092da85b1a1","value":" 190/190 [00:00&lt;00:00, 11.7kB/s]"}},"7e1af9ed45b14ecf95435b9eff217397":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"0c3a1d08e4f14df2a36fe132349e33bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d16a72cda2b4097a8742b21d2172275":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd944b63c4f944f5aae18c76118c803f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4af9843f83214d55902799b5fb6665df":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"6484c7ab71bd4b5e8286feed649b393e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"532d9ecf84304085b0f1e092da85b1a1":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}