{
  "authorId": "1700325",
  "url": "https://www.semanticscholar.org/author/1700325",
  "name": "Graham Neubig",
  "affiliations": [],
  "paperCount": 487,
  "citationCount": 17886,
  "hIndex": 70,
  "papers": [
    {
      "paperId": "028d75496e51943f52c7b2177344a3c089c18058",
      "externalIds": {
        "ArXiv": "2401.06855",
        "DBLP": "journals/corr/abs-2401-06855",
        "DOI": "10.48550/arXiv.2401.06855",
        "CorpusId": 266999558
      },
      "url": "https://www.semanticscholar.org/paper/028d75496e51943f52c7b2177344a3c089c18058",
      "title": "Fine-grained Hallucination Detection and Editing for Language Models",
      "abstract": "Large language models (LMs) are prone to generate diverse factually incorrect statements, which are widely called hallucinations. Current approaches predominantly focus on coarse-grained automatic hallucination detection or editing, overlooking nuanced error levels. In this paper, we propose a novel task -- automatic fine-grained hallucination detection -- and present a comprehensive taxonomy encompassing six hierarchically defined types of hallucination. To facilitate evaluation, we introduce a new benchmark that includes fine-grained human judgments on two LM outputs across various domains. Our analysis reveals that ChatGPT and Llama 2-Chat exhibit hallucinations in 60% and 75% of their outputs, respectively, and a majority of these hallucinations fall into categories that have been underexplored. As an initial step to address this, we train FAVA, a retrieval-augmented LM by carefully designing synthetic data generations to detect and correct fine-grained hallucinations. On our benchmark, our automatic and human evaluations show that FAVA significantly outperforms ChatGPT on fine-grained hallucination detection by a large margin though a large room for future improvement still exists. FAVA's suggested edits also improve the factuality of LM-generated text, resulting in 5-10% FActScore improvements.",
      "year": 2024,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2401.06855",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2279677197",
          "name": "Abhika Mishra"
        },
        {
          "authorId": "35584853",
          "name": "Akari Asai"
        },
        {
          "authorId": "143820870",
          "name": "Vidhisha Balachandran"
        },
        {
          "authorId": "2253856151",
          "name": "Yizhong Wang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2257032956",
          "name": "Yulia Tsvetkov"
        },
        {
          "authorId": "2548384",
          "name": "Hannaneh Hajishirzi"
        }
      ]
    },
    {
      "paperId": "79116e20e9a3083867c142ef1e88f799f6cfb9b8",
      "externalIds": {
        "DBLP": "journals/corr/abs-2401-16788",
        "ArXiv": "2401.16788",
        "DOI": "10.48550/arXiv.2401.16788",
        "CorpusId": 267320303
      },
      "url": "https://www.semanticscholar.org/paper/79116e20e9a3083867c142ef1e88f799f6cfb9b8",
      "title": "Can Large Language Models be Trusted for Evaluation? Scalable Meta-Evaluation of LLMs as Evaluators via Agent Debate",
      "abstract": "Despite the utility of Large Language Models (LLMs) across a wide range of tasks and scenarios, developing a method for reliably evaluating LLMs across varied contexts continues to be challenging. Modern evaluation approaches often use LLMs to assess responses generated by LLMs. However, the meta-evaluation conducted to assess the effectiveness of these LLMs as evaluators is typically constrained by the coverage of existing benchmarks or requires extensive human annotation. This underscores the urgency of methods for scalable meta-evaluation that can effectively, reliably, and efficiently evaluate the performance of LLMs as evaluators across diverse tasks and scenarios, particularly in potentially new, user-defined scenarios. To fill this gap, we propose ScaleEval, an agent-debate-assisted meta-evaluation framework that leverages the capabilities of multiple communicative LLM agents. This framework supports multi-round discussions to assist human annotators in discerning the most capable LLMs as evaluators, which significantly eases their workload in cases that used to require large-scale annotations during meta-evaluation. We release the code for our framework, which is publicly available at: \\url{https://github.com/GAIR-NLP/scaleeval}.",
      "year": 2024,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2401.16788",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2224851117",
          "name": "Steffi Chern"
        },
        {
          "authorId": "2273658317",
          "name": "Ethan Chern"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2282138571",
          "name": "Pengfei Liu"
        }
      ]
    },
    {
      "paperId": "7e6c1bb54bb2e36cc1092b080e9928942f7f8a68",
      "externalIds": {
        "ArXiv": "2401.12869",
        "DBLP": "journals/corr/abs-2401-12869",
        "DOI": "10.48550/arXiv.2401.12869",
        "CorpusId": 267095061
      },
      "url": "https://www.semanticscholar.org/paper/7e6c1bb54bb2e36cc1092b080e9928942f7f8a68",
      "title": "TroVE: Inducing Verifiable and Efficient Toolboxes for Solving Programmatic Tasks",
      "abstract": "Language models (LMs) can solve tasks such as answering questions about tables or images by writing programs. However, using primitive functions often leads to verbose and error-prone programs, and higher-level functions require expert design. To enable better solutions without human labor, we ask code LMs to curate reusable high-level functions, and use them to write solutions. We present TROVE, a training-free method of inducing a verifiable and efficient toolbox of functions, by generating via using, growing, and periodically trimming the toolbox. On 11 datasets from math, table question answering, and image reasoning tasks, TROVE consistently yields simpler solutions with higher accuracy than baselines using CODELLAMA and previous methods using GPT, while using 79-98% smaller toolboxes. TROVE further enables 31% faster and 13% more accurate human verification than baselines. With the same pipeline, it creates diverse functions for varied tasks and datasets, providing insights into their individual characteristics.",
      "year": 2024,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2401.12869",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2261355095",
          "name": "Zhiruo Wang"
        },
        {
          "authorId": "2261278355",
          "name": "Daniel Fried"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "f554b22d2ccf786a6d61d5858f43024ba9115e15",
      "externalIds": {
        "ArXiv": "2401.13649",
        "DBLP": "journals/corr/abs-2401-13649",
        "DOI": "10.48550/arXiv.2401.13649",
        "CorpusId": 267199749
      },
      "url": "https://www.semanticscholar.org/paper/f554b22d2ccf786a6d61d5858f43024ba9115e15",
      "title": "VisualWebArena: Evaluating Multimodal Agents on Realistic Visual Web Tasks",
      "abstract": "Autonomous agents capable of planning, reasoning, and executing actions on the web offer a promising avenue for automating computer tasks. However, the majority of existing benchmarks primarily focus on text-based agents, neglecting many natural tasks that require visual information to effectively solve. Given that most computer interfaces cater to human perception, visual information often augments textual data in ways that text-only models struggle to harness effectively. To bridge this gap, we introduce VisualWebArena, a benchmark designed to assess the performance of multimodal web agents on realistic \\textit{visually grounded tasks}. VisualWebArena comprises of a set of diverse and complex web-based tasks that evaluate various capabilities of autonomous multimodal agents. To perform on this benchmark, agents need to accurately process image-text inputs, interpret natural language instructions, and execute actions on websites to accomplish user-defined objectives. We conduct an extensive evaluation of state-of-the-art LLM-based autonomous agents, including several multimodal models. Through extensive quantitative and qualitative analysis, we identify several limitations of text-only LLM agents, and reveal gaps in the capabilities of state-of-the-art multimodal language agents. VisualWebArena provides a framework for evaluating multimodal autonomous language agents, and offers insights towards building stronger autonomous agents for the web. Our code, baseline models, and data is publicly available at https://jykoh.com/vwa.",
      "year": 2024,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2401.13649",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "23978705",
          "name": "Jing Yu Koh"
        },
        {
          "authorId": "145250604",
          "name": "Robert Lo"
        },
        {
          "authorId": "2280901917",
          "name": "Lawrence Jang"
        },
        {
          "authorId": "2223753659",
          "name": "Vikram Duvvur"
        },
        {
          "authorId": "2280879326",
          "name": "Ming Chong Lim"
        },
        {
          "authorId": "2281408098",
          "name": "Po-Yu Huang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "2280902225",
          "name": "Ruslan Salakhutdinov"
        },
        {
          "authorId": "2280902252",
          "name": "Daniel Fried"
        }
      ]
    },
    {
      "paperId": "0004c4e27e54f13577951416d8ed476e93f00ef9",
      "externalIds": {
        "DBLP": "conf/iclr/ReidHN23",
        "CorpusId": 259298665
      },
      "url": "https://www.semanticscholar.org/paper/0004c4e27e54f13577951416d8ed476e93f00ef9",
      "title": "DiffusER: Diffusion via Edit-based Reconstruction",
      "abstract": null,
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": null,
      "authors": [
        {
          "authorId": "1557386977",
          "name": "Machel Reid"
        },
        {
          "authorId": "22747364",
          "name": "V. Hellendoorn"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3",
      "externalIds": {
        "DBLP": "journals/corr/abs-2302-05738",
        "ArXiv": "2302.05738",
        "DOI": "10.48550/arXiv.2302.05738",
        "CorpusId": 256827706
      },
      "url": "https://www.semanticscholar.org/paper/03c7f61b0c6bc4c480ed6da4e9d7dda104ddfec3",
      "title": "Cross-Modal Fine-Tuning: Align then Refine",
      "abstract": "Fine-tuning large-scale pretrained models has led to tremendous progress in well-studied modalities such as vision and NLP. However, similar gains have not been observed in many other modalities due to a lack of relevant pretrained models. In this work, we propose ORCA, a general cross-modal fine-tuning framework that extends the applicability of a single large-scale pretrained model to diverse modalities. ORCA adapts to a target task via an align-then-refine workflow: given the target input, ORCA first learns an embedding network that aligns the embedded feature distribution with the pretraining modality. The pretrained model is then fine-tuned on the embedded data to exploit the knowledge shared across modalities. Through extensive experiments, we show that ORCA obtains state-of-the-art results on 3 benchmarks containing over 60 datasets from 12 modalities, outperforming a wide range of hand-designed, AutoML, general-purpose, and task-specific methods. We highlight the importance of data alignment via a series of ablation studies and demonstrate ORCA's utility in data-limited regimes.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2302.05738",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "31030-31056"
      },
      "authors": [
        {
          "authorId": "2143669058",
          "name": "Junhong Shen"
        },
        {
          "authorId": "51517360",
          "name": "Liam Li"
        },
        {
          "authorId": "32273391",
          "name": "L. Dery"
        },
        {
          "authorId": "2081410501",
          "name": "Corey Staten"
        },
        {
          "authorId": "10398264",
          "name": "M. Khodak"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145532827",
          "name": "Ameet Talwalkar"
        }
      ]
    },
    {
      "paperId": "11a571eaab42a6ffb1d938635a093315e392756d",
      "externalIds": {
        "DBLP": "journals/corr/abs-2309-07423",
        "ArXiv": "2309.07423",
        "ACL": "2023.wmt-1.40",
        "DOI": "10.48550/arXiv.2309.07423",
        "CorpusId": 261824661
      },
      "url": "https://www.semanticscholar.org/paper/11a571eaab42a6ffb1d938635a093315e392756d",
      "title": "ChatGPT MT: Competitive for High- (but Not Low-) Resource Languages",
      "abstract": "Large language models (LLMs) implicitly learn to perform a range of language tasks, including machine translation (MT). Previous studies explore aspects of LLMs\u2019 MT capabilities. However, there exist a wide variety of languages for which recent LLM MT performance has never before been evaluated. Without published experimental evidence on the matter, it is difficult for speakers of the world\u2019s diverse languages to know how and whether they can use LLMs for their languages. We present the first experimental evidence for an expansive set of 204 languages, along with MT cost analysis, using the FLORES-200 benchmark. Trends reveal that GPT models approach or exceed traditional MT model performance for some high-resource languages (HRLs) but consistently lag for low-resource languages (LRLs), under-performing traditional MT for 84.1% of languages we covered. Our analysis reveals that a language\u2019s resource level is the most important feature in determining ChatGPT\u2019s relative ability to translate it, and suggests that ChatGPT is especially disadvantaged for LRLs and African languages.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2309.07423",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2309.07423",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2240569372",
          "name": "Nathaniel R. Robinson"
        },
        {
          "authorId": "1988654955",
          "name": "Perez Ogayo"
        },
        {
          "authorId": "3407646",
          "name": "David R. Mortensen"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "17605c43ca3eb982c99642052ddc21a93d116594",
      "externalIds": {
        "DBLP": "conf/emnlp/SongK0FOWACTAN23",
        "ArXiv": "2305.14716",
        "DOI": "10.48550/arXiv.2305.14716",
        "CorpusId": 258866051
      },
      "url": "https://www.semanticscholar.org/paper/17605c43ca3eb982c99642052ddc21a93d116594",
      "title": "GlobalBench: A Benchmark for Global Progress in Natural Language Processing",
      "abstract": "Despite the major advances in NLP, significant disparities in NLP system performance across languages still exist. Arguably, these are due to uneven resource allocation and sub-optimal incentives to work on less resourced languages. To track and further incentivize the global development of equitable language technology, we introduce GlobalBench. Prior multilingual benchmarks are static and have focused on a limited number of tasks and languages. In contrast, GlobalBench is an ever-expanding collection that aims to dynamically track progress on all NLP datasets in all languages. Rather than solely measuring accuracy, GlobalBench also tracks the estimated per-speaker utility and equity of technology across all languages, providing a multi-faceted view of how language technology is serving people of the world. Furthermore, GlobalBench is designed to identify the most under-served languages, and rewards research efforts directed towards those languages. At present, the most under-served languages are the ones with a relatively high population, but nonetheless overlooked by composite multilingual benchmarks (like Punjabi, Portuguese, and Wu Chinese). Currently, GlobalBench covers 966 datasets in 190 languages, and has 1,128 system submissions spanning 62 languages.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2305.14716",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "14157-14171"
      },
      "authors": [
        {
          "authorId": "148310739",
          "name": "Yueqi Song"
        },
        {
          "authorId": "2218206121",
          "name": "Catherine Cui"
        },
        {
          "authorId": "1452678825",
          "name": "Simran Khanuja"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "48556979",
          "name": "FAHIM FAISAL"
        },
        {
          "authorId": "1475670743",
          "name": "Alissa Ostapenko"
        },
        {
          "authorId": "9162688",
          "name": "Genta Indra Winata"
        },
        {
          "authorId": "8129718",
          "name": "Alham Fikri Aji"
        },
        {
          "authorId": "66986482",
          "name": "Samuel Cahyawijaya"
        },
        {
          "authorId": "2073587169",
          "name": "Yulia Tsvetkov"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "1786a2f9140ed7211b21302977de64e948b92308",
      "externalIds": {
        "ArXiv": "2302.07867",
        "DBLP": "journals/corr/abs-2302-07867",
        "DOI": "10.48550/arXiv.2302.07867",
        "CorpusId": 256868633
      },
      "url": "https://www.semanticscholar.org/paper/1786a2f9140ed7211b21302977de64e948b92308",
      "title": "Learning Performance-Improving Code Edits",
      "abstract": "The waning of Moore's Law has shifted the focus of the tech industry towards alternative methods for continued performance gains. While optimizing compilers are a standard tool to help increase program efficiency, programmers continue to shoulder much responsibility in crafting and refactoring code with better performance characteristics. In this paper, we investigate the ability of large language models (LLMs) to suggest functionally correct, performance improving code edits. We hypothesize that language models can suggest such edits in ways that would be impractical for static analysis alone. We investigate these questions by curating a large-scale dataset of Performance-Improving Edits, PIE. PIE contains trajectories of programs, where a programmer begins with an initial, slower version and iteratively makes changes to improve the program's performance. We use PIE to evaluate and improve the capacity of large language models. Specifically, use examples from PIE to fine-tune multiple variants of CODEGEN, a billion-scale Transformer-decoder model. Additionally, we use examples from PIE to prompt OpenAI's CODEX using a few-shot prompting. By leveraging PIE, we find that both CODEX and CODEGEN can generate performance-improving edits, with speedups of more than 2.5x for over 25% of the programs, for C++ and Python, even after the C++ programs were compiled using the O3 optimization level. Crucially, we show that PIE allows CODEGEN, an open-sourced and 10x smaller model than CODEX, to match the performance of CODEX on this challenging task. Overall, this work opens new doors for creating systems and methods that can help programmers write efficient code.",
      "year": 2023,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2302.07867",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2302.07867",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "21626987",
          "name": "Aman Madaan"
        },
        {
          "authorId": "2129995371",
          "name": "Alex Shypula"
        },
        {
          "authorId": "47051926",
          "name": "Uri Alon"
        },
        {
          "authorId": "33798741",
          "name": "Milad Hashemi"
        },
        {
          "authorId": "1770926",
          "name": "Parthasarathy Ranganathan"
        },
        {
          "authorId": "46286308",
          "name": "Yiming Yang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2112229",
          "name": "A. Yazdanbakhsh"
        }
      ]
    },
    {
      "paperId": "31366ff634fc905affd78dbd8ddc9a872c006a87",
      "externalIds": {
        "DBLP": "journals/corr/abs-2302-05527",
        "ArXiv": "2302.05527",
        "DOI": "10.48550/arXiv.2302.05527",
        "CorpusId": 256827797
      },
      "url": "https://www.semanticscholar.org/paper/31366ff634fc905affd78dbd8ddc9a872c006a87",
      "title": "CodeBERTScore: Evaluating Code Generation with Pretrained Models of Code",
      "abstract": "Since the rise of neural natural-language-to-code models (NL->Code) that can generate long expressions and statements rather than a single next-token, one of the major problems has been reliably evaluating their generated output. In this paper, we propose CodeBERTScore: an evaluation metric for code generation, which builds on BERTScore (Zhang et al., 2020). Instead of encoding only the generated tokens as in BERTScore, CodeBERTScore also encodes the natural language input preceding the generated code, thus modeling the consistency between the generated code and its given natural language context as well. We perform an extensive evaluation of CodeBERTScore across four programming languages. We find that CodeBERTScore achieves a higher correlation with human preference and with functional correctness than all existing metrics. That is, generated code that receives a higher score by CodeBERTScore is more likely to be preferred by humans, as well as to function correctly when executed. We release five language-specific pretrained models to use with our publicly available code. Our language-specific models have been downloaded more than 1,000,000 times from the Huggingface Hub. Our code and data are available at https://github.com/neulab/code-bert-score",
      "year": 2023,
      "influentialCitationCount": 5,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2302.05527",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "13921-13937"
      },
      "authors": [
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "47051926",
          "name": "Uri Alon"
        },
        {
          "authorId": "2114357424",
          "name": "Sumit Agarwal"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "3194a28bcab3c7dcd556fc5f6895b9c38311308d",
      "externalIds": {
        "ArXiv": "2311.09308",
        "DBLP": "journals/corr/abs-2311-09308",
        "DOI": "10.48550/arXiv.2311.09308",
        "CorpusId": 265221310
      },
      "url": "https://www.semanticscholar.org/paper/3194a28bcab3c7dcd556fc5f6895b9c38311308d",
      "title": "Divergences between Language Models and Human Brains",
      "abstract": "Do machines and humans process language in similar ways? Recent research has hinted in the affirmative, finding that brain signals can be effectively predicted using the internal representations of language models (LMs). Although such results are thought to reflect shared computational principles between LMs and human brains, there are also clear differences in how LMs and humans represent and use language. In this work, we systematically explore the divergences between human and machine language processing by examining the differences between LM representations and human brain responses to language as measured by Magnetoencephalography (MEG) across two datasets in which subjects read and listened to narrative stories. Using a data-driven approach, we identify two domains that are not captured well by LMs: social/emotional intelligence and physical commonsense. We then validate these domains with human behavioral experiments and show that fine-tuning LMs on these domains can improve their alignment with human brain responses.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Biology"
      ],
      "journal": {
        "volume": "abs/2311.09308",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2267004051",
          "name": "Yuchen Zhou"
        },
        {
          "authorId": "2266945896",
          "name": "Emmy Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2266841495",
          "name": "Leila Wehbe"
        }
      ]
    },
    {
      "paperId": "405f1a5602867c66e015491c26d2be5504eed458",
      "externalIds": {
        "ArXiv": "2306.06804",
        "DBLP": "journals/corr/abs-2306-06804",
        "ACL": "2023.americasnlp-1.13",
        "DOI": "10.48550/arXiv.2306.06804",
        "CorpusId": 259138596
      },
      "url": "https://www.semanticscholar.org/paper/405f1a5602867c66e015491c26d2be5504eed458",
      "title": "Neural Machine Translation for the Indigenous Languages of the Americas: An Introduction",
      "abstract": "Neural models have drastically advanced state of the art for machine translation (MT) between high-resource languages. Traditionally, these models rely on large amounts of training data, but many language pairs lack these resources. However, an important part of the languages in the world do not have this amount of data. Most languages from the Americas are among them, having a limited amount of parallel and monolingual data, if any. Here, we present an introduction to the interested reader to the basic challenges, concepts, and techniques that involve the creation of MT systems for these languages. Finally, we discuss the recent advances and findings and open questions, product of an increased interest of the NLP community in these languages.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2306.06804",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "volume": "abs/2306.06804",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "153151470",
          "name": "Manuel Mager"
        },
        {
          "authorId": "2061975276",
          "name": "R. Bhatnagar"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "4160376",
          "name": "Ngoc Thang Vu"
        },
        {
          "authorId": "3422953",
          "name": "Katharina Kann"
        }
      ]
    },
    {
      "paperId": "43c0f77f116f986b53eb04f5c9b33f10132ded55",
      "externalIds": {
        "ACL": "2023.computel-1.4",
        "DBLP": "journals/corr/abs-2302-13410",
        "ArXiv": "2302.13410",
        "DOI": "10.48550/arXiv.2302.13410",
        "CorpusId": 257219604
      },
      "url": "https://www.semanticscholar.org/paper/43c0f77f116f986b53eb04f5c9b33f10132ded55",
      "title": "User-Centric Evaluation of OCR Systems for Kwak\u2019wala",
      "abstract": "There has been recent interest in improving optical character recognition (OCR) for endangered languages, particularly because a large number of documents and books in these languages are not in machine-readable formats. The performance of OCR systems is typically evaluated using automatic metrics such as character and word error rates. While error rates are useful for the comparison of different models and systems, they do not measure whether and how the transcriptions produced from OCR tools are useful to downstream users. In this paper, we present a human-centric evaluation of OCR systems, focusing on the Kwak'wala language as a case study. With a user study, we show that utilizing OCR reduces the time spent in the manual transcription of culturally valuable documents -- a task that is often undertaken by endangered language community members and researchers -- by over 50%. Our results demonstrate the potential benefits that OCR tools can have on downstream language documentation and revitalization efforts.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2302.13410",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2302.13410",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "102758386",
          "name": "Daisy Rosenblum"
        },
        {
          "authorId": "2209989543",
          "name": "Michayla King"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "4c711000915f8f45c54fdcb70c9db7880e3373ea",
      "externalIds": {
        "DBLP": "conf/cvpr/ZhuKMHLGNBKW23",
        "DOI": "10.1109/CVPR52729.2023.01434",
        "CorpusId": 259343886
      },
      "url": "https://www.semanticscholar.org/paper/4c711000915f8f45c54fdcb70c9db7880e3373ea",
      "title": "EXCALIBUR: Encouraging and Evaluating Embodied Exploration",
      "abstract": "Experience precedes understanding. Humans constantly explore and learn about their environment out of curiosity, gather information, and update their models of the world. On the other hand, machines are either trained to learn passively from static and fixed datasets, or taught to complete specific goal-conditioned tasks. To encourage the development of exploratory interactive agents, we present the EXCALIBUR benchmark. EXCALIBUR allows agents to explore their environment for long durations and then query their understanding of the physical world via inquiries like: \u201cis the small heavy red bowl made from glass?\u201d or \u201cis there a silver spoon heavier than the egg?\u201d. This design encourages agents to perform free-form home exploration without myopia induced by goal conditioning. Once the agents have answered a series of questions, they can renter the scene to refine their knowledge, update their beliefs, and improve their performance on the questions. Our experiments demonstrate the challenges posed by this dataset for the present-day state-of-the-art embodied systems and the headroom afforded to develop new innovative methods. Finally, we present a virtual reality interface that enables humans to seamlessly interact within the simulated world and use it to gather human performance measures. EXCALIBUR affords unique challenges in comparison to presentday benchmarks and represents the next frontier for embodied AI research.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "14931-14942",
        "name": "2023 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
      },
      "authors": [
        {
          "authorId": "144459859",
          "name": "Hao Zhu"
        },
        {
          "authorId": "32536265",
          "name": "Raghav Kapoor"
        },
        {
          "authorId": "2008204295",
          "name": "So Yeon Min"
        },
        {
          "authorId": "1443358534",
          "name": "Winson Han"
        },
        {
          "authorId": "2221150064",
          "name": "Jiatai Li"
        },
        {
          "authorId": "2221124809",
          "name": "Kaiwen Geng"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3312309",
          "name": "Yonatan Bisk"
        },
        {
          "authorId": "2684226",
          "name": "Aniruddha Kembhavi"
        },
        {
          "authorId": "20745881",
          "name": "Luca Weihs"
        }
      ]
    },
    {
      "paperId": "4d74a5048b884e8bb3842240abf98915c619c8f8",
      "externalIds": {
        "ArXiv": "2306.01200",
        "DBLP": "conf/acl/JainKSF0NZ23",
        "DOI": "10.48550/arXiv.2306.01200",
        "CorpusId": 259064002
      },
      "url": "https://www.semanticscholar.org/paper/4d74a5048b884e8bb3842240abf98915c619c8f8",
      "title": "Multi-Dimensional Evaluation of Text Summarization with In-Context Learning",
      "abstract": "Evaluation of natural language generation (NLG) is complex and multi-dimensional. Generated text can be evaluated for fluency, coherence, factuality, or any other dimensions of interest. Most frameworks that perform such multi-dimensional evaluation require training on large manually or synthetically generated datasets. In this paper, we study the efficacy of large language models as multi-dimensional evaluators using in-context learning, obviating the need for large training datasets. Our experiments show that in-context learning-based evaluators are competitive with learned evaluation frameworks for the task of text summarization, establishing state-of-the-art on dimensions such as relevance and factual consistency. We then analyze the effects of factors such as the selection and number of in-context examples on performance. Finally, we study the efficacy of in-context learning based evaluators in evaluating zero-shot summaries written by large language models such as GPT-3.",
      "year": 2023,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2306.01200",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2306.01200",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2116998524",
          "name": "Sameer Jain"
        },
        {
          "authorId": "17320214",
          "name": "Vaishakh Keshava"
        },
        {
          "authorId": "1644192946",
          "name": "Swarnashree Mysore Sathyendra"
        },
        {
          "authorId": "2058640028",
          "name": "Patrick Fernandes"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2384711",
          "name": "Chunting Zhou"
        }
      ]
    },
    {
      "paperId": "5ea8eedcb31859c5730dd1da3804e1be529ffabb",
      "externalIds": {
        "ArXiv": "2303.16750",
        "DBLP": "journals/corr/abs-2303-16750",
        "DOI": "10.48550/arXiv.2303.16750",
        "CorpusId": 257805004
      },
      "url": "https://www.semanticscholar.org/paper/5ea8eedcb31859c5730dd1da3804e1be529ffabb",
      "title": "A Gold Standard Dataset for the Reviewer Assignment Problem",
      "abstract": "Many peer-review venues are either using or looking to use algorithms to assign submissions to reviewers. The crux of such automated approaches is the notion of the\"similarity score\"--a numerical estimate of the expertise of a reviewer in reviewing a paper--and many algorithms have been proposed to compute these scores. However, these algorithms have not been subjected to a principled comparison, making it difficult for stakeholders to choose the algorithm in an evidence-based manner. The key challenge in comparing existing algorithms and developing better algorithms is the lack of the publicly available gold-standard data that would be needed to perform reproducible research. We address this challenge by collecting a novel dataset of similarity scores that we release to the research community. Our dataset consists of 477 self-reported expertise scores provided by 58 researchers who evaluated their expertise in reviewing papers they have read previously. We use this data to compare several popular algorithms employed in computer science conferences and come up with recommendations for stakeholders. Our main findings are as follows. First, all algorithms make a non-trivial amount of error. For the task of ordering two papers in terms of their relevance for a reviewer, the error rates range from 12%-30% in easy cases to 36%-43% in hard cases, highlighting the vital need for more research on the similarity-computation problem. Second, most existing algorithms are designed to work with titles and abstracts of papers, and in this regime the Specter+MFR algorithm performs best. Third, to improve performance, it may be important to develop modern deep-learning based algorithms that can make use of the full texts of papers: the classical TD-IDF algorithm enhanced with full texts of papers is on par with the deep-learning based Specter+MFR that cannot make use of this information.",
      "year": 2023,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2303.16750",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2303.16750",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "50825200",
          "name": "Ivan Stelmakh"
        },
        {
          "authorId": "1771118",
          "name": "J. Wieting"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1737249",
          "name": "Nihar B. Shah"
        }
      ]
    },
    {
      "paperId": "659be1ff350634f50cc066d258ee6a45e697e552",
      "externalIds": {
        "DBLP": "conf/sigmorphon/HeTR0MNL23",
        "ACL": "2023.sigmorphon-1.22",
        "DOI": "10.18653/v1/2023.sigmorphon-1.22",
        "CorpusId": 259833803
      },
      "url": "https://www.semanticscholar.org/paper/659be1ff350634f50cc066d258ee6a45e697e552",
      "title": "SigMoreFun Submission to the SIGMORPHON Shared Task on Interlinear Glossing",
      "abstract": "In our submission to the SIGMORPHON 2023 Shared Task on interlinear glossing (IGT), we explore approaches to data augmentation and modeling across seven low-resource languages. For data augmentation, we explore two approaches: creating artificial data from the provided training data and utilizing existing IGT resources in other languages. On the modeling side, we test an enhanced version of the provided token classification baseline as well as a pretrained multilingual seq2seq model. Additionally, we apply post-correction using a dictionary for Gitksan, the language with the smallest amount of data. We find that our token classification models are the best performing, with the highest word-level accuracy for Arapaho and highest morpheme-level accuracy for Gitksan out of all submissions. We also show that data augmentation is an effective strategy, though applying artificial data pretraining has very different effects across both models tested.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2023.sigmorphon-1.22.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "209-216"
      },
      "authors": [
        {
          "authorId": "2107034039",
          "name": "Taiqi He"
        },
        {
          "authorId": "2219036626",
          "name": "Lindia Tjuatja"
        },
        {
          "authorId": "2067645226",
          "name": "Nathaniel R. Robinson"
        },
        {
          "authorId": "1746678",
          "name": "Shinji Watanabe"
        },
        {
          "authorId": "3407646",
          "name": "David R. Mortensen"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145585627",
          "name": "L. Levin"
        }
      ]
    },
    {
      "paperId": "6cfbbf7604adda1df65932e3c4d157770a2df000",
      "externalIds": {
        "DBLP": "journals/corr/abs-2312-07000",
        "ArXiv": "2312.07000",
        "DOI": "10.48550/arXiv.2312.07000",
        "CorpusId": 266174420
      },
      "url": "https://www.semanticscholar.org/paper/6cfbbf7604adda1df65932e3c4d157770a2df000",
      "title": "Alignment for Honesty",
      "abstract": "Recent research has made significant strides in applying alignment techniques to enhance the helpfulness and harmlessness of large language models (LLMs) in accordance with human intentions. In this paper, we argue for the importance of alignment for honesty, ensuring that LLMs proactively refuse to answer questions when they lack knowledge, while still not being overly conservative. However, a pivotal aspect of alignment for honesty involves discerning the limits of an LLM's knowledge, which is far from straightforward. This challenge demands comprehensive solutions in terms of metric development, benchmark creation, and training methodologies. In this paper, we address these challenges by first establishing a precise problem definition and defining ``honesty'' inspired by the Analects of Confucius. This serves as a cornerstone for developing metrics that effectively measure an LLM's honesty by quantifying its progress post-alignment. Furthermore, we introduce a flexible training framework which is further instantiated by several efficient fine-tuning techniques that emphasize honesty without sacrificing performance on other tasks. Our extensive experiments reveal that these aligned models show a marked increase in honesty, as indicated by our proposed metrics. We open-source a wealth of resources to facilitate future research at https://github.com/GAIR-NLP/alignment-for-honesty, including honesty-aligned models, training and evaluation datasets for honesty alignment, concept glossary, as well as all relevant source code.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2312.07000",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2145435513",
          "name": "Yuqing Yang"
        },
        {
          "authorId": "2273658317",
          "name": "Ethan Chern"
        },
        {
          "authorId": "2273725403",
          "name": "Xipeng Qiu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        }
      ]
    },
    {
      "paperId": "74b05bba46db21e589a2cc0f916f81069b0368ef",
      "externalIds": {
        "DBLP": "journals/corr/abs-2305-00955",
        "ArXiv": "2305.00955",
        "DOI": "10.48550/arXiv.2305.00955",
        "CorpusId": 258426970
      },
      "url": "https://www.semanticscholar.org/paper/74b05bba46db21e589a2cc0f916f81069b0368ef",
      "title": "Bridging the Gap: A Survey on Integrating (Human) Feedback for Natural Language Generation",
      "abstract": "Many recent advances in natural language generation have been fueled by training large language models on internet-scale data. However, this paradigm can lead to models that generate toxic, inaccurate, and unhelpful content, and automatic evaluation metrics often fail to identify these behaviors. As models become more capable, human feedback is an invaluable signal for evaluating and improving models. This survey aims to provide an overview of the recent research that has leveraged human feedback to improve natural language generation. First, we introduce an encompassing formalization of feedback, and identify and organize existing research into a taxonomy following this formalization. Next, we discuss how feedback can be described by its format and objective, and cover the two approaches proposed to use feedback (either for training or decoding): directly using the feedback or training feedback models. We also discuss existing datasets for human-feedback data collection, and concerns surrounding feedback collection. Finally, we provide an overview of the nascent field of AI feedback, which exploits large language models to make judgments based on a set of principles and minimize the need for human intervention.",
      "year": 2023,
      "influentialCitationCount": 7,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2305.00955",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2305.00955",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2058640028",
          "name": "Patrick Fernandes"
        },
        {
          "authorId": "21626987",
          "name": "Aman Madaan"
        },
        {
          "authorId": "1381444447",
          "name": "Emmy Liu"
        },
        {
          "authorId": "1748971692",
          "name": "Ant\u00f3nio Farinhas"
        },
        {
          "authorId": "144869806",
          "name": "Pedro Henrique Martins"
        },
        {
          "authorId": "2138301112",
          "name": "Amanda Bertsch"
        },
        {
          "authorId": "34876539",
          "name": "Jos\u00e9 G. C. de Souza"
        },
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "35232494",
          "name": "Tongshuang Sherry Wu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400227478",
          "name": "Andr\u00e9 F. T. Martins"
        }
      ]
    },
    {
      "paperId": "76f6142d2b62972de89cb8651ea036d0dd6be68b",
      "externalIds": {
        "ArXiv": "2311.09553",
        "DBLP": "journals/corr/abs-2311-09553",
        "DOI": "10.48550/arXiv.2311.09553",
        "CorpusId": 265220704
      },
      "url": "https://www.semanticscholar.org/paper/76f6142d2b62972de89cb8651ea036d0dd6be68b",
      "title": "Program-Aided Reasoners (better) Know What They Know",
      "abstract": "Prior work shows that program-aided reasoning, in which large language models (LLMs) are combined with programs written in programming languages such as Python, can significantly improve accuracy on various reasoning tasks. However, while accuracy is essential, it is also important for such reasoners to\"know what they know\", which can be quantified through the calibration of the model. In this paper, we compare the calibration of Program Aided Language Models (PAL) and text-based Chain-of-thought (COT) prompting techniques over 5 datasets and 2 model types: LLaMA models and OpenAI models. Our results indicate that PAL leads to improved calibration in 75% of the instances. Our analysis uncovers that prompting styles that produce lesser diversity in generations also have more calibrated results, and thus we also experiment with inducing lower generation diversity using temperature scaling and find that for certain temperatures, PAL is not only more accurate but is also more calibrated than COT. Overall, we demonstrate that, in the majority of cases, program-aided reasoners better know what they know than text-based counterparts.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2311.09553",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1735001746",
          "name": "Anubha Kabra"
        },
        {
          "authorId": "2031481495",
          "name": "Sanketh Rangreji"
        },
        {
          "authorId": "2275195526",
          "name": "Yash Mathur"
        },
        {
          "authorId": "21626987",
          "name": "Aman Madaan"
        },
        {
          "authorId": "2266945896",
          "name": "Emmy Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "7848d4b4e6ba0897a85cebb6467e94eb0b60d583",
      "externalIds": {
        "DBLP": "journals/corr/abs-2311-08377",
        "ArXiv": "2311.08377",
        "DOI": "10.48550/arXiv.2311.08377",
        "CorpusId": 265157538
      },
      "url": "https://www.semanticscholar.org/paper/7848d4b4e6ba0897a85cebb6467e94eb0b60d583",
      "title": "Learning to Filter Context for Retrieval-Augmented Generation",
      "abstract": "On-the-fly retrieval of relevant knowledge has proven an essential element of reliable systems for tasks such as open-domain question answering and fact verification. However, because retrieval systems are not perfect, generation models are required to generate outputs given partially or entirely irrelevant passages. This can cause over- or under-reliance on context, and result in problems in the generated output such as hallucinations. To alleviate these problems, we propose FILCO, a method that improves the quality of the context provided to the generator by (1) identifying useful context based on lexical and information-theoretic approaches, and (2) training context filtering models that can filter retrieved contexts at test time. We experiment on six knowledge-intensive tasks with FLAN-T5 and LLaMa2, and demonstrate that our method outperforms existing approaches on extractive question answering (QA), complex multi-hop and long-form QA, fact verification, and dialog generation tasks. FILCO effectively improves the quality of context, whether or not it supports the canonical output.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2311.08377",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1390877035",
          "name": "Zhiruo Wang"
        },
        {
          "authorId": "2266466286",
          "name": "Jun Araki"
        },
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "3405393",
          "name": "Md. Rizwan Parvez"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "7a5b44ea10a51708e18786595c8d70b18950da11",
      "externalIds": {
        "DBLP": "journals/corr/abs-2307-13528",
        "ArXiv": "2307.13528",
        "DOI": "10.48550/arXiv.2307.13528",
        "CorpusId": 260154834
      },
      "url": "https://www.semanticscholar.org/paper/7a5b44ea10a51708e18786595c8d70b18950da11",
      "title": "FacTool: Factuality Detection in Generative AI - A Tool Augmented Framework for Multi-Task and Multi-Domain Scenarios",
      "abstract": "The emergence of generative pre-trained models has facilitated the synthesis of high-quality text, but it has also posed challenges in identifying factual errors in the generated text. In particular: (1) A wider range of tasks now face an increasing risk of containing factual errors when handled by generative models. (2) Generated texts tend to be lengthy and lack a clearly defined granularity for individual facts. (3) There is a scarcity of explicit evidence available during the process of fact checking. With the above challenges in mind, in this paper, we propose FacTool, a task and domain agnostic framework for detecting factual errors of texts generated by large language models (e.g., ChatGPT). Experiments on four different tasks (knowledge-based QA, code generation, mathematical reasoning, and scientific literature review) show the efficacy of the proposed method. We release the code of FacTool associated with ChatGPT plugin interface at https://github.com/GAIR-NLP/factool .",
      "year": 2023,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2307.13528",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2307.13528",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2047713083",
          "name": "Ethan Chern"
        },
        {
          "authorId": "2224851117",
          "name": "Steffi Chern"
        },
        {
          "authorId": "2108956946",
          "name": "Shiqi Chen"
        },
        {
          "authorId": "30300197",
          "name": "Weizhe Yuan"
        },
        {
          "authorId": "2224772135",
          "name": "Kehua Feng"
        },
        {
          "authorId": "2110714400",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        }
      ]
    },
    {
      "paperId": "88884b8806262a4095036041e3567d450dba39f7",
      "externalIds": {
        "DBLP": "journals/corr/abs-2305-06983",
        "ArXiv": "2305.06983",
        "DOI": "10.48550/arXiv.2305.06983",
        "CorpusId": 258615731
      },
      "url": "https://www.semanticscholar.org/paper/88884b8806262a4095036041e3567d450dba39f7",
      "title": "Active Retrieval Augmented Generation",
      "abstract": "Despite the remarkable ability of large language models (LMs) to comprehend and generate language, they have a tendency to hallucinate and create factually inaccurate output. Augmenting LMs by retrieving information from external knowledge resources is one promising solution. Most existing retrieval augmented LMs employ a retrieve-and-generate setup that only retrieves information once based on the input. This is limiting, however, in more general scenarios involving generation of long texts, where continually gathering information throughout generation is essential. In this work, we provide a generalized view of active retrieval augmented generation, methods that actively decide when and what to retrieve across the course of the generation. We propose Forward-Looking Active REtrieval augmented generation (FLARE), a generic method which iteratively uses a prediction of the upcoming sentence to anticipate future content, which is then utilized as a query to retrieve relevant documents to regenerate the sentence if it contains low-confidence tokens. We test FLARE along with baselines comprehensively over 4 long-form knowledge-intensive generation tasks/datasets. FLARE achieves superior or competitive performance on all tasks, demonstrating the effectiveness of our method. Code and datasets are available at https://github.com/jzbjyb/FLARE.",
      "year": 2023,
      "influentialCitationCount": 5,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2305.06983",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2305.06983",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "49715441",
          "name": "Luyu Gao"
        },
        {
          "authorId": "48064856",
          "name": "Zhiqing Sun"
        },
        {
          "authorId": "1409707585",
          "name": "Qian Liu"
        },
        {
          "authorId": "2173509991",
          "name": "Jane Dwivedi-Yu"
        },
        {
          "authorId": "46286308",
          "name": "Yiming Yang"
        },
        {
          "authorId": "144987107",
          "name": "Jamie Callan"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "8e8a1489bf4d782d2435cdeb93f7d1f165747c63",
      "externalIds": {
        "ArXiv": "2307.00524",
        "DBLP": "journals/corr/abs-2307-00524",
        "DOI": "10.48550/arXiv.2307.00524",
        "CorpusId": 259317075
      },
      "url": "https://www.semanticscholar.org/paper/8e8a1489bf4d782d2435cdeb93f7d1f165747c63",
      "title": "Large Language Models Enable Few-Shot Clustering",
      "abstract": "Unlike traditional unsupervised clustering, semi-supervised clustering allows users to provide meaningful structure to the data, which helps the clustering algorithm to match the user's intent. Existing approaches to semi-supervised clustering require a significant amount of feedback from an expert to improve the clusters. In this paper, we ask whether a large language model can amplify an expert's guidance to enable query-efficient, few-shot semi-supervised text clustering. We show that LLMs are surprisingly effective at improving clustering. We explore three stages where LLMs can be incorporated into clustering: before clustering (improving input features), during clustering (by providing constraints to the clusterer), and after clustering (using LLMs post-correction). We find incorporating LLMs in the first two stages can routinely provide significant improvements in cluster quality, and that LLMs enable a user to make trade-offs between cost and accuracy to produce desired clusters. We release our code and LLM prompts for the public to use.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2307.00524",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2307.00524",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2061499362",
          "name": "Vijay Viswanathan"
        },
        {
          "authorId": "24868638",
          "name": "Kiril Gashteovski"
        },
        {
          "authorId": "19752252",
          "name": "Carolin (Haas) Lawrence"
        },
        {
          "authorId": "35232494",
          "name": "Tongshuang Sherry Wu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "9bce3661f01825ad56dc9d2b3d254fd9e3792360",
      "externalIds": {
        "DBLP": "journals/corr/abs-2305-11789",
        "ArXiv": "2305.11789",
        "DOI": "10.48550/arXiv.2305.11789",
        "CorpusId": 258823453
      },
      "url": "https://www.semanticscholar.org/paper/9bce3661f01825ad56dc9d2b3d254fd9e3792360",
      "title": "Solving NLP Problems through Human-System Collaboration: A Discussion-based Approach",
      "abstract": "Humans work together to solve common problems by having discussions, explaining, and agreeing or disagreeing with each other. Similarly, if a system can have discussions with humans when solving tasks, it can improve the system's performance and reliability. In previous research on explainability, it has only been possible for the system to make predictions and for humans to ask questions about them rather than having a mutual exchange of opinions. This research aims to create a dataset and computational framework for systems that discuss and refine their predictions through dialogue. Through experiments, we show that the proposed system can have beneficial discussions with humans improving the accuracy by up to 25 points in the natural language inference task.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2305.11789",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2305.11789",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "143655216",
          "name": "Masahiro Kaneko"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1764004",
          "name": "Naoaki Okazaki"
        }
      ]
    },
    {
      "paperId": "b4987da792dd45a84232cfb06d71b1c2ec488f38",
      "externalIds": {
        "DBLP": "conf/emnlp/LiuCN23",
        "ArXiv": "2310.07081",
        "DOI": "10.48550/arXiv.2310.07081",
        "CorpusId": 265907218
      },
      "url": "https://www.semanticscholar.org/paper/b4987da792dd45a84232cfb06d71b1c2ec488f38",
      "title": "Crossing the Threshold: Idiomatic Machine Translation through Retrieval Augmentation and Loss Weighting",
      "abstract": "Idioms are common in everyday language, but often pose a challenge to translators because their meanings do not follow from the meanings of their parts. Despite significant advances, machine translation systems still struggle to translate idiomatic expressions. We provide a simple characterization of idiomatic translation and related issues. This allows us to conduct a synthetic experiment revealing a tipping point at which transformer-based machine translation models correctly default to idiomatic translations. To expand multilingual resources, we compile a dataset of ~4k natural sentences containing idiomatic expressions in French, Finnish, and Japanese. To improve translation of natural idioms, we introduce two straightforward yet effective techniques: the strategic upweighting of training loss on potentially idiomatic sentences, and using retrieval-augmented models. This not only improves the accuracy of a strong pretrained MT model on idiomatic sentences by up to 13% in absolute accuracy, but also holds potential benefits for non-idiomatic sentences.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2310.07081",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "15095-15111"
      },
      "authors": [
        {
          "authorId": "2266945896",
          "name": "Emmy Liu"
        },
        {
          "authorId": "2266465306",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "c432aff446d55e72a28394a1508e760cc9a25c08",
      "externalIds": {
        "DBLP": "conf/icml/Xu0N23",
        "ArXiv": "2301.02828",
        "DOI": "10.48550/arXiv.2301.02828",
        "CorpusId": 255546631
      },
      "url": "https://www.semanticscholar.org/paper/c432aff446d55e72a28394a1508e760cc9a25c08",
      "title": "Why do Nearest Neighbor Language Models Work?",
      "abstract": "Language models (LMs) compute the probability of a text by sequentially computing a representation of an already-seen context and using this representation to predict the next word. Currently, most LMs calculate these representations through a neural network consuming the immediate previous context. However recently, retrieval-augmented LMs have shown to improve over standard neural LMs, by accessing information retrieved from a large datastore, in addition to their standard, parametric, next-word prediction. In this paper, we set out to understand why retrieval-augmented language models, and specifically why k-nearest neighbor language models (kNN-LMs) perform better than standard parametric LMs, even when the k-nearest neighbor component retrieves examples from the same training set that the LM was originally trained on. To this end, we perform a careful analysis of the various dimensions over which kNN-LM diverges from standard LMs, and investigate these dimensions one by one. Empirically, we identify three main reasons why kNN-LM performs better than standard LMs: using a different input representation for predicting the next tokens, approximate kNN search, and the importance of softmax temperature for the kNN distribution. Further, we incorporate these insights into the model architecture or the training procedure of the standard parametric LM, improving its results without the need for an explicit retrieval component. The code is available at https://github.com/frankxu2004/knnlm-why.",
      "year": 2023,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2301.02828",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2301.02828",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "47051926",
          "name": "Uri Alon"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "c5207241406586f4263b235667e004b71ea68953",
      "externalIds": {
        "ArXiv": "2305.18185",
        "ACL": "2023.starsem-1.14",
        "DBLP": "conf/starsem/TjuatjaLLN23",
        "DOI": "10.48550/arXiv.2305.18185",
        "CorpusId": 258959069
      },
      "url": "https://www.semanticscholar.org/paper/c5207241406586f4263b235667e004b71ea68953",
      "title": "Syntax and Semantics Meet in the \u201cMiddle\u201d: Probing the Syntax-Semantics Interface of LMs Through Agentivity",
      "abstract": "Recent advances in large language models have prompted researchers to examine their abilities across a variety of linguistic tasks, but little has been done to investigate how models handle the interactions in meaning across words and larger syntactic forms\u2014i.e. phenomena at the intersection of syntax and semantics. We present the semantic notion of agentivity as a case study for probing such interactions. We created a novel evaluation dataset by utilitizing the unique linguistic properties of a subset of optionally transitive English verbs. This dataset was used to prompt varying sizes of three model classes to see if they are sensitive to agentivity at the lexical level, and if they can appropriately employ these word-level priors given a specific syntactic context. Overall, GPT-3 text-davinci-003 performs extremely well across all experiments, outperforming all other models tested by far. In fact, the results are even better correlated with human judgements than both syntactic and semantic corpus statistics. This suggests that LMs may potentially serve as more useful tools for linguistic annotation, theory testing, and discovery than select corpora for certain tasks.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2305.18185",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "149-164"
      },
      "authors": [
        {
          "authorId": "2219036626",
          "name": "Lindia Tjuatja"
        },
        {
          "authorId": "1381444447",
          "name": "Emmy Liu"
        },
        {
          "authorId": "145585627",
          "name": "L. Levin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "cc1705fe421c70d85254b557634bd4669fdd49b0",
      "externalIds": {
        "ArXiv": "2305.16636",
        "ACL": "2023.acl-long.573",
        "DBLP": "journals/corr/abs-2305-16636",
        "DOI": "10.48550/arXiv.2305.16636",
        "CorpusId": 258947254
      },
      "url": "https://www.semanticscholar.org/paper/cc1705fe421c70d85254b557634bd4669fdd49b0",
      "title": "DataFinder: Scientific Dataset Recommendation from Natural Language Descriptions",
      "abstract": "Modern machine learning relies on datasets to develop and validate research ideas. Given the growth of publicly available data, finding the right dataset to use is increasingly difficult. Any research question imposes explicit and implicit constraints on how well a given dataset will enable researchers to answer this question, such as dataset size, modality, and domain. We operationalize the task of recommending datasets given a short natural language description of a research idea, to help people find relevant datasets for their needs. Dataset recommendation poses unique challenges as an information retrieval problem; datasets are hard to directly index for search and there are no corpora readily available for this task. To facilitate this task, we build the DataFinder Dataset which consists of a larger automatically-constructed training set (17.5K queries) and a smaller expert-annotated evaluation set (392 queries). Using this data, we compare various information retrieval algorithms on our test set and present a superior bi-encoder retriever for text-based dataset recommendation. This system, trained on the DataFinder Dataset, finds more relevant search results than existing third-party dataset search engines. To encourage progress on dataset recommendation, we release our dataset and models to the public.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2305.16636",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2305.16636",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2061499362",
          "name": "Vijay Viswanathan"
        },
        {
          "authorId": "49715441",
          "name": "Luyu Gao"
        },
        {
          "authorId": "35232494",
          "name": "Tongshuang Sherry Wu"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "d5dd7230cccace7e77095d3b5fd8394850f59170",
      "externalIds": {
        "DBLP": "conf/acl/KabraLKAWCAON23",
        "ArXiv": "2305.16171",
        "DOI": "10.48550/arXiv.2305.16171",
        "CorpusId": 258887835
      },
      "url": "https://www.semanticscholar.org/paper/d5dd7230cccace7e77095d3b5fd8394850f59170",
      "title": "Multi-lingual and Multi-cultural Figurative Language Understanding",
      "abstract": "Figurative language permeates human communication, but at the same time is relatively understudied in NLP. Datasets have been created in English to accelerate progress towards measuring and improving figurative language processing in language models (LMs). However, the use of figurative language is an expression of our cultural and societal experiences, making it difficult for these phrases to be universally applicable. In this work, we create a figurative language inference dataset, \\datasetname, for seven diverse languages associated with a variety of cultures: Hindi, Indonesian, Javanese, Kannada, Sundanese, Swahili and Yoruba. Our dataset reveals that each language relies on cultural and regional concepts for figurative expressions, with the highest overlap between languages originating from the same region. We assess multilingual LMs' abilities to interpret figurative language in zero-shot and few-shot settings. All languages exhibit a significant deficiency compared to English, with variations in performance reflecting the availability of pre-training and fine-tuning data, emphasizing the need for LMs to be exposed to a broader range of linguistic and cultural variation during training.",
      "year": 2023,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2305.16171",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "8269-8284"
      },
      "authors": [
        {
          "authorId": "1735001746",
          "name": "Anubha Kabra"
        },
        {
          "authorId": "1381444447",
          "name": "Emmy Liu"
        },
        {
          "authorId": "1452678825",
          "name": "Simran Khanuja"
        },
        {
          "authorId": "8129718",
          "name": "Alham Fikri Aji"
        },
        {
          "authorId": "9162688",
          "name": "Genta Indra Winata"
        },
        {
          "authorId": "2220548276",
          "name": "Samuel Cahyawijaya"
        },
        {
          "authorId": "2056773747",
          "name": "Anuoluwapo Aremu"
        },
        {
          "authorId": "1988654955",
          "name": "Perez Ogayo"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "d6ae4c0679bdceb029f652efd2a854ac5ade772f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2310-01387",
        "ACL": "2023.bigpicture-1.9",
        "ArXiv": "2310.01387",
        "DOI": "10.48550/arXiv.2310.01387",
        "CorpusId": 263605610
      },
      "url": "https://www.semanticscholar.org/paper/d6ae4c0679bdceb029f652efd2a854ac5ade772f",
      "title": "It\u2019s MBR All the Way Down: Modern Generation Techniques Through the Lens of Minimum Bayes Risk",
      "abstract": "Minimum Bayes Risk (MBR) decoding is a method for choosing the outputs of a machine learning system based not on the output with the highest probability, but the output with the lowest risk (expected error) among multiple candidates. It is a simple but powerful method: for an additional cost at inference time, MBR provides reliable several-point improvements across metrics for a wide variety of tasks without any additional data or training. Despite this, MBR is not frequently applied in NLP works, and knowledge of the method itself is limited. We first provide an introduction to the method and the recent literature. We show that several recent methods that do not reference MBR can be written as special cases of MBR; this reformulation provides additional theoretical justification for the performance of these methods, explaining some results that were previously only empirical. We provide theoretical and empirical results about the effectiveness of various MBR variants and make concrete recommendations for the application of MBR in NLP models, including future directions in this area.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2310.01387",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2310.01387",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2138301112",
          "name": "Amanda Bertsch"
        },
        {
          "authorId": "2253395527",
          "name": "Alex Xie"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1762110",
          "name": "Matthew R. Gormley"
        }
      ]
    },
    {
      "paperId": "dbc368bc8b49347dd27679894524fa62f88492c9",
      "externalIds": {
        "ArXiv": "2305.01625",
        "DBLP": "journals/corr/abs-2305-01625",
        "DOI": "10.48550/arXiv.2305.01625",
        "CorpusId": 258436892
      },
      "url": "https://www.semanticscholar.org/paper/dbc368bc8b49347dd27679894524fa62f88492c9",
      "title": "Unlimiformer: Long-Range Transformers with Unlimited Length Input",
      "abstract": "Since the proposal of transformers, these models have been limited to bounded input lengths, because of their need to attend to every token in the input. In this work, we propose Unlimiformer: a general approach that wraps any existing pretrained encoder-decoder transformer, and offloads the cross-attention computation to a single k-nearest-neighbor (kNN) index, while the returned kNN distances are the attention dot-product scores. This kNN index can be kept on either the GPU or CPU memory and queried in sub-linear time; this way, we can index practically unlimited input sequences, while every attention head in every decoder layer retrieves its top-k keys, instead of attending to every key. We evaluate Unlimiformer on several long-document and book-summarization benchmarks, showing that it can process even 500k token-long inputs from the BookSum dataset, without any input truncation at test time. We demonstrate that Unlimiformer improves pretrained models such as BART and Longformer by extending them to unlimited inputs without additional learned weights and without modifying their code. We make our code and models publicly available at https://github.com/abertsch72/unlimiformer .",
      "year": 2023,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2305.01625",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2305.01625",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2138301112",
          "name": "Amanda Bertsch"
        },
        {
          "authorId": "47051926",
          "name": "Uri Alon"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1762110",
          "name": "Matthew R. Gormley"
        }
      ]
    },
    {
      "paperId": "e41482f4ee984f17382f6cdd900df094d928be06",
      "externalIds": {
        "ArXiv": "2307.13854",
        "DBLP": "journals/corr/abs-2307-13854",
        "DOI": "10.48550/arXiv.2307.13854",
        "CorpusId": 260164780
      },
      "url": "https://www.semanticscholar.org/paper/e41482f4ee984f17382f6cdd900df094d928be06",
      "title": "WebArena: A Realistic Web Environment for Building Autonomous Agents",
      "abstract": "With advances in generative AI, there is now potential for autonomous agents to manage daily tasks via natural language commands. However, current agents are primarily created and tested in simplified synthetic environments, leading to a disconnect with real-world scenarios. In this paper, we build an environment for language-guided agents that is highly realistic and reproducible. Specifically, we focus on agents that perform tasks on the web, and create an environment with fully functional websites from four common domains: e-commerce, social forum discussions, collaborative software development, and content management. Our environment is enriched with tools (e.g., a map) and external knowledge bases (e.g., user manuals) to encourage human-like task-solving. Building upon our environment, we release a set of benchmark tasks focusing on evaluating the functional correctness of task completions. The tasks in our benchmark are diverse, long-horizon, and designed to emulate tasks that humans routinely perform on the internet. We experiment with several baseline agents, integrating recent techniques such as reasoning before acting. The results demonstrate that solving complex tasks is challenging: our best GPT-4-based agent only achieves an end-to-end task success rate of 14.41%, significantly lower than the human performance of 78.24%. These results highlight the need for further development of robust agents, that current state-of-the-art large language models are far from perfect performance in these real-life tasks, and that WebArena can be used to measure such progress.",
      "year": 2023,
      "influentialCitationCount": 7,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2307.13854",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2307.13854",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "2115313911",
          "name": "Hao Zhu"
        },
        {
          "authorId": "144101734",
          "name": "Xuhui Zhou"
        },
        {
          "authorId": "145250604",
          "name": "Robert Lo"
        },
        {
          "authorId": "66820957",
          "name": "Abishek Sridhar"
        },
        {
          "authorId": "144691454",
          "name": "Xianyi Cheng"
        },
        {
          "authorId": "3312309",
          "name": "Yonatan Bisk"
        },
        {
          "authorId": "47070750",
          "name": "Daniel Fried"
        },
        {
          "authorId": "47051926",
          "name": "Uri Alon"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43",
      "externalIds": {
        "DBLP": "conf/emnlp/ViswanathanZBWN23",
        "ArXiv": "2308.12261",
        "DOI": "10.48550/arXiv.2308.12261",
        "CorpusId": 261075905
      },
      "url": "https://www.semanticscholar.org/paper/e69684fb06a7b1fe621d7ef0c97fc2ca0e122c43",
      "title": "Prompt2Model: Generating Deployable Models from Natural Language Instructions",
      "abstract": "Large language models (LLMs) enable system builders today to create competent NLP systems through prompting, where they only need to describe the task in natural language and provide a few examples. However, in other ways, LLMs are a step backward from traditional special-purpose NLP models; they require extensive computational resources for deployment and can be gated behind APIs. In this paper, we propose Prompt2Model, a general-purpose method that takes a natural language task description like the prompts provided to LLMs, and uses it to train a special-purpose model that is conducive to deployment. This is done through a multi-step process of retrieval of existing datasets and pretrained models, dataset generation using LLMs, and supervised fine-tuning on these retrieved and generated datasets. Over three tasks, we demonstrate that given the same few-shot prompt as input, Prompt2Model trains models that outperform the results of a strong LLM, gpt-3.5-turbo, by an average of 20% while being up to 700 times smaller. We also show that this data can be used to obtain reliable performance estimates of model performance, enabling model developers to assess model reliability before deployment. Prompt2Model is available open-source at https://github.com/neulab/prompt2model.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2308.12261",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2308.12261",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2061499362",
          "name": "Vijay Viswanathan"
        },
        {
          "authorId": "2023526",
          "name": "Chenyang Zhao"
        },
        {
          "authorId": "2138301112",
          "name": "Amanda Bertsch"
        },
        {
          "authorId": "35232494",
          "name": "Tongshuang Sherry Wu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "e7b3b692b0816821aafc0d354749bc3802cbf6ac",
      "externalIds": {
        "DBLP": "journals/corr/abs-2303-01502",
        "ArXiv": "2303.01502",
        "DOI": "10.48550/arXiv.2303.01502",
        "CorpusId": 257280165
      },
      "url": "https://www.semanticscholar.org/paper/e7b3b692b0816821aafc0d354749bc3802cbf6ac",
      "title": "Computational Language Acquisition with Theory of Mind",
      "abstract": "Unlike current state-of-the-art language models, young children actively acquire language through interactions with their surrounding environment and caretakers. One mechanism that has been argued to be critical to language learning is the ability to infer the mental states of other agents in social environments, coined Theory of Mind (ToM) by Premack&Woodruff (1978). Drawing inspiration from the modern operationalized versions of ToM implemented in Rabinowitz et al. (2018) and Zhu et al. (2021), we build language-learning agents equipped with ToM, and measure its effects on the learning process. We model ToM by giving the speaker agent an internal listener model that is trained alongside the speaker and used to rerank potential utterances. We experiment with varying task difficulty, hypothesizing that models will acquire more complex language to adapt to stronger environmental pressures. We find that training speakers with a highly weighted ToM listener component leads to performance gains in our image referential game setting. We also find some evidence that increasing task difficulty in the training process results in more fluent and precise utterances in evaluation. This suggests the potential utility of further incorporating ToM, as well as other insights from child language acquisition, into computational models of language acquisition.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2303.01502",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2303.01502",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "46263614",
          "name": "Andy T. Liu"
        },
        {
          "authorId": "2115314674",
          "name": "Hao Zhu"
        },
        {
          "authorId": "1381444447",
          "name": "Emmy Liu"
        },
        {
          "authorId": "3312309",
          "name": "Yonatan Bisk"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "eeb6601a3c557e970817ed056265a6c62e5d09dc",
      "externalIds": {
        "ArXiv": "2311.06379",
        "DBLP": "journals/corr/abs-2311-06379",
        "DOI": "10.48550/arXiv.2311.06379",
        "CorpusId": 265149939
      },
      "url": "https://www.semanticscholar.org/paper/eeb6601a3c557e970817ed056265a6c62e5d09dc",
      "title": "DeMuX: Data-efficient Multilingual Learning",
      "abstract": "We consider the task of optimally fine-tuning pre-trained multilingual models, given small amounts of unlabelled target data and an annotation budget. In this paper, we introduce DEMUX, a framework that prescribes the exact data-points to label from vast amounts of unlabelled multilingual data, having unknown degrees of overlap with the target set. Unlike most prior works, our end-to-end framework is language-agnostic, accounts for model representations, and supports multilingual target configurations. Our active learning strategies rely upon distance and uncertainty measures to select task-specific neighbors that are most informative to label, given a model. DeMuX outperforms strong baselines in 84% of the test cases, in the zero-shot setting of disjoint source and target language sets (including multilingual target pools), across three models and four tasks. Notably, in low-budget settings (5-100 examples), we observe gains of up to 8-11 F1 points for token-level tasks, and 2-5 F1 for complex tasks. Our code is released here: https://github.com/simran-khanuja/demux.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2311.06379",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1452678825",
          "name": "Simran Khanuja"
        },
        {
          "authorId": "2220844123",
          "name": "Srinivas Gowriraj"
        },
        {
          "authorId": "32273391",
          "name": "L. Dery"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "f640e89fcede075b4bde3b2fa0dc78f591589ba3",
      "externalIds": {
        "ACL": "2023.trustnlp-1.6",
        "ArXiv": "2307.04507",
        "DBLP": "journals/corr/abs-2307-04507",
        "DOI": "10.48550/arXiv.2307.04507",
        "CorpusId": 259501636
      },
      "url": "https://www.semanticscholar.org/paper/f640e89fcede075b4bde3b2fa0dc78f591589ba3",
      "title": "Improving Factuality of Abstractive Summarization via Contrastive Reward Learning",
      "abstract": "Modern abstractive summarization models often generate summaries that contain hallucinated or contradictory information. In this paper, we propose a simple but effective contrastive learning framework that incorporates recent developments in reward learning and factuality metrics. Empirical studies demonstrate that the proposed framework enables summarization models to learn from feedback of factuality metrics using contrastive reward learning, leading to more factual summaries by human evaluations. This suggests that further advances in learning and evaluation algorithms can feed directly into providing more factual summaries. Code and human evaluation results will be publicly available at \\url{https://github.com/EthanC111/factuality_summarization}.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2307.04507",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2307.04507",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2047713083",
          "name": "Ethan Chern"
        },
        {
          "authorId": "1390877035",
          "name": "Zhiruo Wang"
        },
        {
          "authorId": "2221736765",
          "name": "Sanjan Das"
        },
        {
          "authorId": "2118766697",
          "name": "Bhavuk Sharma"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "f6e893b3e2ee7a62c2fe8a3b0e33920c3e596969",
      "externalIds": {
        "DBLP": "journals/corr/abs-2310-11667",
        "ArXiv": "2310.11667",
        "DOI": "10.48550/arXiv.2310.11667",
        "CorpusId": 264289186
      },
      "url": "https://www.semanticscholar.org/paper/f6e893b3e2ee7a62c2fe8a3b0e33920c3e596969",
      "title": "SOTOPIA: Interactive Evaluation for Social Intelligence in Language Agents",
      "abstract": "Humans are social beings; we pursue social goals in our daily interactions, which is a crucial aspect of social intelligence. Yet, AI systems' abilities in this realm remain elusive. We present SOTOPIA, an open-ended environment to simulate complex social interactions between artificial agents and evaluate their social intelligence. In our environment, agents role-play and interact under a wide variety of scenarios; they coordinate, collaborate, exchange, and compete with each other to achieve complex social goals. We simulate the role-play interaction between LLM-based agents and humans within this task space and evaluate their performance with a holistic evaluation framework called SOTOPIA-Eval. With SOTOPIA, we find significant differences between these models in terms of their social intelligence, and we identify a subset of SOTOPIA scenarios, SOTOPIA-hard, that is generally challenging for all models. We find that on this subset, GPT-4 achieves a significantly lower goal completion rate than humans and struggles to exhibit social commonsense reasoning and strategic communication skills. These findings demonstrate SOTOPIA's promise as a general platform for research on evaluating and improving social intelligence in artificial agents.",
      "year": 2023,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2310.11667",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2260306432",
          "name": "Xuhui Zhou"
        },
        {
          "authorId": "2260859845",
          "name": "Hao Zhu"
        },
        {
          "authorId": "2259929826",
          "name": "Leena Mathur"
        },
        {
          "authorId": "2259932919",
          "name": "Ruohong Zhang"
        },
        {
          "authorId": "2260283233",
          "name": "Haofei Yu"
        },
        {
          "authorId": "2259939615",
          "name": "Zhengyang Qi"
        },
        {
          "authorId": "49933077",
          "name": "Louis-Philippe Morency"
        },
        {
          "authorId": "3312309",
          "name": "Yonatan Bisk"
        },
        {
          "authorId": "2259931814",
          "name": "Daniel Fried"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2729164",
          "name": "Maarten Sap"
        }
      ]
    },
    {
      "paperId": "fd80f7f3673fc6ca02f192d5d73426f11a4be659",
      "externalIds": {
        "DBLP": "journals/corr/abs-2308-07286",
        "ArXiv": "2308.07286",
        "ACL": "2023.wmt-1.100",
        "DOI": "10.48550/arXiv.2308.07286",
        "CorpusId": 260886800
      },
      "url": "https://www.semanticscholar.org/paper/fd80f7f3673fc6ca02f192d5d73426f11a4be659",
      "title": "The Devil Is in the Errors: Leveraging Large Language Models for Fine-grained Machine Translation Evaluation",
      "abstract": "Automatic evaluation of machine translation (MT) is a critical tool driving the rapid iterative development of MT systems. While considerable progress has been made on estimating a single scalar quality score, current metrics lack the informativeness of more detailed schemes that annotate individual errors, such as Multidimensional Quality Metrics (MQM). In this paper, we help fill this gap by proposing AutoMQM, a prompting technique which leverages the reasoning and in-context learning capabilities of large language models (LLMs) and asks them to identify and categorize errors in translations. We start by evaluating recent LLMs, such as PaLM and PaLM-2, through simple score prediction prompting, and we study the impact of labeled data through in-context learning and finetuning. We then evaluate AutoMQM with PaLM-2 models, and we find that it improves performance compared to just prompting for scores (with particularly large gains for larger models) while providing interpretability through error spans that align with human annotations.",
      "year": 2023,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2308.07286",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1066-1083"
      },
      "authors": [
        {
          "authorId": "2058640028",
          "name": "Patrick Fernandes"
        },
        {
          "authorId": "145346875",
          "name": "Daniel Deutsch"
        },
        {
          "authorId": "2056981575",
          "name": "M. Finkelstein"
        },
        {
          "authorId": "47718053",
          "name": "Parker Riley"
        },
        {
          "authorId": "145644643",
          "name": "Andr\u00e9 F. T. Martins"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2499663",
          "name": "Ankush Garg"
        },
        {
          "authorId": "144797264",
          "name": "J. Clark"
        },
        {
          "authorId": "35307070",
          "name": "Markus Freitag"
        },
        {
          "authorId": "2345617",
          "name": "Orhan Firat"
        }
      ]
    },
    {
      "paperId": "03c19ddaa26068f23e27ba94b10f08160e87f668",
      "externalIds": {
        "ArXiv": "2212.09648",
        "DBLP": "journals/corr/abs-2212-09648",
        "DOI": "10.48550/arXiv.2212.09648",
        "CorpusId": 254853901
      },
      "url": "https://www.semanticscholar.org/paper/03c19ddaa26068f23e27ba94b10f08160e87f668",
      "title": "NusaCrowd: Open Source Initiative for Indonesian NLP Resources",
      "abstract": "We present NusaCrowd, a collaborative initiative to collect and unify existing resources for Indonesian languages, including opening access to previously non-public resources. Through this initiative, we have brought together 137 datasets and 118 standardized data loaders. The quality of the datasets has been assessed manually and automatically, and their value is demonstrated through multiple experiments. NusaCrowd's data collection enables the creation of the first zero-shot benchmarks for natural language understanding and generation in Indonesian and the local languages of Indonesia. Furthermore, NusaCrowd brings the creation of the first multilingual automatic speech recognition benchmark in Indonesian and the local languages of Indonesia. Our work strives to advance natural language processing (NLP) research for languages that are under-represented despite being widely spoken.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2212.09648",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "13745-13818"
      },
      "authors": [
        {
          "authorId": "66986482",
          "name": "Samuel Cahyawijaya"
        },
        {
          "authorId": "116344405",
          "name": "Holy Lovenia"
        },
        {
          "authorId": "8129718",
          "name": "Alham Fikri Aji"
        },
        {
          "authorId": "9162688",
          "name": "Genta Indra Winata"
        },
        {
          "authorId": "150048491",
          "name": "Bryan Wilie"
        },
        {
          "authorId": "1935324",
          "name": "Rahmad Mahendra"
        },
        {
          "authorId": "104768157",
          "name": "C. Wibisono"
        },
        {
          "authorId": "2279712392",
          "name": "Ade Romadhony"
        },
        {
          "authorId": "1939999507",
          "name": "Karissa Vincentio"
        },
        {
          "authorId": "2789148",
          "name": "Fajri Koto"
        },
        {
          "authorId": "117696399",
          "name": "Jennifer Santoso"
        },
        {
          "authorId": "35722593",
          "name": "David Moeljadi"
        },
        {
          "authorId": "2197090678",
          "name": "Cahya Wirawan"
        },
        {
          "authorId": "2197090652",
          "name": "Frederikus Hudi"
        },
        {
          "authorId": "134112343",
          "name": "Ivan Halim Parmonangan"
        },
        {
          "authorId": "2683858",
          "name": "Ika Alfina"
        },
        {
          "authorId": "2196919922",
          "name": "Muhammad Satrio Wicaksono"
        },
        {
          "authorId": "1943296899",
          "name": "Ilham Firdausi Putra"
        },
        {
          "authorId": "2197090705",
          "name": "Samsul Rahmadani"
        },
        {
          "authorId": "9128778",
          "name": "Yulianti Oenang"
        },
        {
          "authorId": "22171680",
          "name": "Ali Akbar Septiandri"
        },
        {
          "authorId": "2197071075",
          "name": "James Jaya"
        },
        {
          "authorId": "4834571",
          "name": "Kaustubh D. Dhole"
        },
        {
          "authorId": "9366773",
          "name": "Arie A. Suryani"
        },
        {
          "authorId": "9358635",
          "name": "Rifki Afina Putri"
        },
        {
          "authorId": "144610224",
          "name": "Dan Su"
        },
        {
          "authorId": "144077726",
          "name": "K. Stevens"
        },
        {
          "authorId": "66436856",
          "name": "Made Nindyatama Nityasya"
        },
        {
          "authorId": "2191731497",
          "name": "Muhammad Farid Adilazuarda"
        },
        {
          "authorId": "2197071063",
          "name": "Ryan Ignatius"
        },
        {
          "authorId": "2197070752",
          "name": "Ryandito Diandaru"
        },
        {
          "authorId": "1660855299",
          "name": "Tiezheng Yu"
        },
        {
          "authorId": "2197070698",
          "name": "Vito Ghifari"
        },
        {
          "authorId": "47653392",
          "name": "Wenliang Dai"
        },
        {
          "authorId": "98271906",
          "name": "Yan Xu"
        },
        {
          "authorId": "2197071047",
          "name": "Dyah Damapuspita"
        },
        {
          "authorId": "120064613",
          "name": "C. Tho"
        },
        {
          "authorId": "18159304",
          "name": "I. M. K. Karo"
        },
        {
          "authorId": "36045311",
          "name": "Tirana Noor Fatyanosa"
        },
        {
          "authorId": "3391272",
          "name": "Ziwei Ji"
        },
        {
          "authorId": "2057151752",
          "name": "Pascale Fung"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145465286",
          "name": "Timothy Baldwin"
        },
        {
          "authorId": "2124014463",
          "name": "Sebastian Ruder"
        },
        {
          "authorId": "2085419515",
          "name": "Herry Sujaini"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1962263",
          "name": "A. Purwarianti"
        }
      ]
    },
    {
      "paperId": "07c70ca55793984ffdf31582a05170ef3d62381a",
      "externalIds": {
        "DBLP": "journals/corr/abs-2205-00049",
        "ArXiv": "2205.00049",
        "DOI": "10.48550/arXiv.2205.00049",
        "CorpusId": 248496641
      },
      "url": "https://www.semanticscholar.org/paper/07c70ca55793984ffdf31582a05170ef3d62381a",
      "title": "Prompt Consistency for Zero-Shot Task Generalization",
      "abstract": "One of the most impressive results of recent NLP history is the ability of pre-trained language models to solve new tasks in a zero-shot setting. To achieve this, NLP tasks are framed as natural language prompts, generating a response indicating the predicted output. Nonetheless, the performance in such settings often lags far behind its supervised counterpart, suggesting a large space for potential improvement. In this paper, we explore methods to utilize unlabeled data to improve zero-shot performance. Specifically, we take advantage of the fact that multiple prompts can be used to specify a single task, and propose to regularize prompt consistency, encouraging consistent predictions over this diverse set of prompts. Our method makes it possible to fine-tune the model either with extra unlabeled training data, or directly on test input at inference time in an unsupervised manner. In experiments, our approach outperforms the state-of-the-art zero-shot learner, T0 (Sanh et al., 2022), on 9 out of 11 datasets across 4 NLP tasks by up to 10.6 absolute points in terms of accuracy. The gains are often attained with a small number of unlabeled examples.",
      "year": 2022,
      "influentialCitationCount": 5,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2205.00049",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2613-2626"
      },
      "authors": [
        {
          "authorId": "2110714400",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "2378954",
          "name": "Xuezhe Ma"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "0a39442979d6e678dd36bb443ad529c14e86a86e",
      "externalIds": {
        "ArXiv": "2207.05987",
        "DBLP": "conf/iclr/Zhou0XJN23",
        "CorpusId": 252734952
      },
      "url": "https://www.semanticscholar.org/paper/0a39442979d6e678dd36bb443ad529c14e86a86e",
      "title": "DocPrompting: Generating Code by Retrieving the Docs",
      "abstract": "Publicly available source-code libraries are continuously growing and changing. This makes it impossible for models of code to keep current with all available APIs by simply training these models on existing code repositories. Thus, existing models inherently cannot generalize to using unseen functions and libraries, because these would never appear in the training data. In contrast, when human programmers use functions and libraries for the first time, they frequently refer to textual resources such as code manuals and documentation, to explore and understand the available functionality. Inspired by this observation, we introduce DocPrompting: a natural-language-to-code generation approach that explicitly leverages documentation by (1) retrieving the relevant documentation pieces given an NL intent, and (2) generating code based on the NL intent and the retrieved documentation. DocPrompting is general: it can be applied to any programming language and is agnostic to the underlying neural model. We demonstrate that DocPrompting consistently improves NL-to-code models: DocPrompting improves strong base models such as CodeT5 by 2.85% in pass@1 (52% relative gain) and 4.39% in pass@10 (30% relative gain) in execution-based evaluation on the popular Python CoNaLa benchmark; on a new Bash dataset tldr, DocPrompting improves CodeT5 and GPT-Neo1.3B by up to absolute 6.9% exact match.",
      "year": 2022,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": null,
      "authors": [
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "47051926",
          "name": "Uri Alon"
        },
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "1390877035",
          "name": "Zhiruo Wang"
        },
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "0cceebb6b796c210b55ebf6b98032b5c631db0e1",
      "externalIds": {
        "DBLP": "conf/naacl/FernandesFRSONM22",
        "ArXiv": "2205.00978",
        "ACL": "2022.naacl-main.100",
        "DOI": "10.48550/arXiv.2205.00978",
        "CorpusId": 248496443
      },
      "url": "https://www.semanticscholar.org/paper/0cceebb6b796c210b55ebf6b98032b5c631db0e1",
      "title": "Quality-Aware Decoding for Neural Machine Translation",
      "abstract": "Despite the progress in machine translation quality estimation and evaluation in the last years, decoding in neural machine translation (NMT) is mostly oblivious to this and centers around finding the most probable translation according to the model (MAP decoding), approximated with beam search. In this paper, we bring together these two lines of research and propose quality-aware decoding for NMT, by leveraging recent breakthroughs in reference-free and reference-based MT evaluation through various inference methods like N-best reranking and minimum Bayes risk decoding. We perform an extensive comparison of various possible candidate generation and ranking methods across four datasets and two model classes and find that quality-aware decoding consistently outperforms MAP-based decoding according both to state-of-the-art automatic metrics (COMET and BLEURT) and to human assessments.",
      "year": 2022,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2205.00978",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1396-1412"
      },
      "authors": [
        {
          "authorId": "2058640028",
          "name": "Patrick Fernandes"
        },
        {
          "authorId": "1748971692",
          "name": "Ant\u00f3nio Farinhas"
        },
        {
          "authorId": "15631652",
          "name": "Ricardo Rei"
        },
        {
          "authorId": "34876539",
          "name": "Jos\u00e9 G. C. de Souza"
        },
        {
          "authorId": "1988654955",
          "name": "Perez Ogayo"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400227478",
          "name": "Andr\u00e9 F. T. Martins"
        }
      ]
    },
    {
      "paperId": "15dd7e862b09de21fd67d5abb2b681293755ffca",
      "externalIds": {
        "ACL": "2022.naacl-main.330",
        "DBLP": "journals/corr/abs-2204-12632",
        "ArXiv": "2204.12632",
        "DOI": "10.48550/arXiv.2204.12632",
        "CorpusId": 248406097
      },
      "url": "https://www.semanticscholar.org/paper/15dd7e862b09de21fd67d5abb2b681293755ffca",
      "title": "Testing the Ability of Language Models to Interpret Figurative Language",
      "abstract": "Figurative and metaphorical language are commonplace in discourse, and figurative expressions play an important role in communication and cognition. However, figurative language has been a relatively under-studied area in NLP, and it remains an open question to what extent modern language models can interpret nonliteral phrases. To address this question, we introduce Fig-QA, a Winograd-style nonliteral language understanding task consisting of correctly interpreting paired figurative phrases with divergent meanings. We evaluate the performance of several state-of-the-art language models on this task, and find that although language models achieve performance significantly over chance, they still fall short of human performance, particularly in zero- or few-shot settings. This suggests that further work is needed to improve the nonliteral reasoning capabilities of language models.",
      "year": 2022,
      "influentialCitationCount": 7,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2204.12632",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2204.12632",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1381444447",
          "name": "Emmy Liu"
        },
        {
          "authorId": "31248497",
          "name": "Chenxuan Cui"
        },
        {
          "authorId": "2163585699",
          "name": "Kenneth Zheng"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "19b7c860128e5461d451b3d15f3836a9e1680ffc",
      "externalIds": {
        "DBLP": "journals/tosem/DramkoLYSANVG23",
        "DOI": "10.1145/3546946",
        "CorpusId": 250925608
      },
      "url": "https://www.semanticscholar.org/paper/19b7c860128e5461d451b3d15f3836a9e1680ffc",
      "title": "DIRE and its Data: Neural Decompiled Variable Renamings with Respect to Software Class",
      "abstract": "The decompiler is one of the most common tools for examining executable binaries without the corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Unfortunately, decompiler output is far from readable because the decompilation process is often incomplete. State-of-the-art techniques use machine learning to predict missing information like variable names. While these approaches are often able to suggest good variable names in context, no existing work examines how the selection of training data influences these machine learning models. We investigate how data provenance and the quality of training data affect performance, and how well, if at all, trained models generalize across software domains. We focus on the variable renaming problem using one such machine learning model, DIRE. We first describe DIRE in detail and the accompanying technique used to generate training data from raw code. We also evaluate DIRE\u2019s overall performance without respect to data quality. Next, we show how training on more popular, possibly higher quality code (measured using GitHub stars) leads to a more generalizable model because popular code tends to have more diverse variable names. Finally, we evaluate how well DIRE predicts domain-specific identifiers, propose a modification to incorporate domain information, and show that it can predict identifiers in domain-specific scenarios 23% more frequently than the original DIRE model.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://dl.acm.org/doi/pdf/10.1145/3546946",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "32",
        "pages": "1 - 34",
        "name": "ACM Transactions on Software Engineering and Methodology"
      },
      "authors": [
        {
          "authorId": "2178374284",
          "name": "Luke Dramko"
        },
        {
          "authorId": "51119916",
          "name": "Jeremy Lacomis"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "32804915",
          "name": "Edward J. Schwartz"
        },
        {
          "authorId": "3216345",
          "name": "Miltiadis Allamanis"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2434621",
          "name": "Bogdan Vasilescu"
        },
        {
          "authorId": "133770966",
          "name": "Claire Le Goues"
        }
      ]
    },
    {
      "paperId": "1bed34f2c23b97fd18de359cf62cd92b3ba612c3",
      "externalIds": {
        "ArXiv": "2212.10481",
        "DBLP": "conf/emnlp/WangZFN23",
        "DOI": "10.48550/arXiv.2212.10481",
        "CorpusId": 254877069
      },
      "url": "https://www.semanticscholar.org/paper/1bed34f2c23b97fd18de359cf62cd92b3ba612c3",
      "title": "Execution-Based Evaluation for Open-Domain Code Generation",
      "abstract": "To extend the scope of coding queries to more realistic settings, we propose ODEX, the first Open-Domain EXecution-based natural language (NL) to Python code generation dataset. ODEX has 945 NL-Code pairs spanning 79 diverse libraries, along with 1,707 human-written test cases for execution. Our NL-Code pairs are harvested from StackOverflow forums to encourage natural and practical coding queries. Moreover, ODEX supports four natural languages as intents, in English, Spanish, Japanese, and Russian. ODEX unveils intriguing behavioral differences among top-performing code language models (LM). While CODEX achieves better overall results, CODEGEN improves effectively via scaling -- CODEGEN 6.1B performs comparably with CODEX 12B. Both models show substantial gaps between open and closed domains, but CODEGEN gaps tend to decrease with model size while CODEX gaps increase. We release ODEX to facilitate research into open-domain problems for the code generation community.",
      "year": 2022,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2212.10481",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1271-1290"
      },
      "authors": [
        {
          "authorId": "1390877035",
          "name": "Zhiruo Wang"
        },
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "47070750",
          "name": "Daniel Fried"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "1ec16018e9152c50bbaed1d49c3077b9fb6d2838",
      "externalIds": {
        "DBLP": "conf/emnlp/YinN22",
        "ArXiv": "2202.10419",
        "ACL": "2022.emnlp-main.14",
        "DOI": "10.18653/v1/2022.emnlp-main.14",
        "CorpusId": 247011700
      },
      "url": "https://www.semanticscholar.org/paper/1ec16018e9152c50bbaed1d49c3077b9fb6d2838",
      "title": "Interpreting Language Models with Contrastive Explanations",
      "abstract": "Model interpretability methods are often used to explain NLP model decisions on tasks such as text classification, where the output space is relatively small. However, when applied to language generation, where the output space often consists of tens of thousands of tokens, these methods are unable to provide informative explanations. Language models must consider various features to predict a token, such as its part of speech, number, tense, or semantics.Existing explanation methods conflate evidence for all these features into a single explanation, which is less interpretable for human understanding.To disentangle the different decisions in language modeling, we focus on explaining language models contrastively: we look for salient input tokens that explain why the model predicted one token instead of another. We demonstrate that contrastive explanations are quantifiably better than non-contrastive explanations in verifying major grammatical phenomena, and that they significantly improve contrastive model simulatability for human observers. We also identify groups of contrastive decisions where the model uses similar evidence, and we are able to characterize what input tokens models use during various language generation decisions.",
      "year": 2022,
      "influentialCitationCount": 5,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2022.emnlp-main.14.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "184-198"
      },
      "authors": [
        {
          "authorId": "1602993448",
          "name": "Kayo Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "1fec9a1c1d28228b1d23874c74de7315928dea6f",
      "externalIds": {
        "DBLP": "conf/emnlp/WangWN22",
        "ArXiv": "2211.06127",
        "ACL": "2022.emnlp-main.621",
        "DOI": "10.48550/arXiv.2211.06127",
        "CorpusId": 253499206
      },
      "url": "https://www.semanticscholar.org/paper/1fec9a1c1d28228b1d23874c74de7315928dea6f",
      "title": "English Contrastive Learning Can Learn Universal Cross-lingual Sentence Embeddings",
      "abstract": "Universal cross-lingual sentence embeddings map semantically similar cross-lingual sentences into a shared embedding space. Aligning cross-lingual sentence embeddings usually requires supervised cross-lingual parallel sentences. In this work, we propose mSimCSE, which extends SimCSE to multilingual settings and reveal that contrastive learning on English data can surprisingly learn high-quality universal cross-lingual sentence embeddings without any parallel data.In unsupervised and weakly supervised settings, mSimCSE significantly improves previous sentence embedding methods on cross-lingual retrieval and multilingual STS tasks. The performance of unsupervised mSimCSE is comparable to fully supervised methods in retrieving low-resource languages and multilingual STS.The performance can be further enhanced when cross-lingual NLI data is available.",
      "year": 2022,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2211.06127",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2211.06127",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "46394797",
          "name": "Yau-Shian Wang"
        },
        {
          "authorId": "2190699139",
          "name": "Ashley Wu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "225caf6f074498f0a76f35ffe43e8baa60fdac7c",
      "externalIds": {
        "ACL": "2023.acl-long.673",
        "ArXiv": "2212.10726",
        "DBLP": "journals/corr/abs-2212-10726",
        "DOI": "10.48550/arXiv.2212.10726",
        "CorpusId": 254926550
      },
      "url": "https://www.semanticscholar.org/paper/225caf6f074498f0a76f35ffe43e8baa60fdac7c",
      "title": "Beyond Contrastive Learning: A Variational Generative Model for Multilingual Retrieval",
      "abstract": "Contrastive learning has been successfully used for retrieval of semantically aligned sentences, but it often requires large batch sizes or careful engineering to work well. In this paper, we instead propose a generative model for learning multilingual text embeddings which can be used to retrieve or score sentence pairs. Our model operates on parallel data in N languages and, through an approximation we introduce, efficiently encourages source separation in this multilingual setting, separating semantic information that is shared between translations from stylistic or language-specific variation. We show careful large-scale comparisons between contrastive and generation-based approaches for learning multilingual text embeddings, a comparison that has not been done to the best of our knowledge despite the popularity of these approaches. We evaluate this method on a suite of tasks including semantic similarity, bitext mining, and cross-lingual question retrieval - the last of which we introduce in this paper. Overall, our model outperforms both a strong contrastive and generative baseline on these tasks.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2212.10726",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "12044-12066"
      },
      "authors": [
        {
          "authorId": "1771118",
          "name": "J. Wieting"
        },
        {
          "authorId": "144797264",
          "name": "J. Clark"
        },
        {
          "authorId": "50056360",
          "name": "William W. Cohen"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        }
      ]
    },
    {
      "paperId": "37ba5946527629cf538a4675476f824da99dfe35",
      "externalIds": {
        "DBLP": "journals/corr/abs-2205-09843",
        "ACL": "2022.suki-1.5",
        "ArXiv": "2205.09843",
        "DOI": "10.48550/arXiv.2205.09843",
        "CorpusId": 248965423
      },
      "url": "https://www.semanticscholar.org/paper/37ba5946527629cf538a4675476f824da99dfe35",
      "title": "Table Retrieval May Not Necessitate Table-specific Model Design",
      "abstract": "Tables are an important form of structured data for both human and machine readers alike, providing answers to questions that cannot, or cannot easily, be found in texts. Recent work has designed special models and training paradigms for table-related tasks such as table-based question answering and table retrieval. Though effective, they add complexity in both modeling and data acquisition compared to generic text solutions and obscure which elements are truly beneficial. In this work, we focus on the task of table retrieval, and ask: \u201cis table-specific model design necessary for table retrieval, or can a simpler text-based model be effectively used to achieve a similar result?\u2019\u2019 First, we perform an analysis on a table-based portion of the Natural Questions dataset (NQ-table), and find that structure plays a negligible role in more than 70% of the cases. Based on this, we experiment with a general Dense Passage Retriever (DPR) based on text and a specialized Dense Table Retriever (DTR) that uses table-specific model designs. We find that DPR performs well without any table-specific design and training, and even achieves superior results compared to DTR when fine-tuned on properly linearized tables. We then experiment with three modules to explicitly encode table structures, namely auxiliary row/column embeddings, hard attention masks, and soft relation-based attention biases. However, none of these yielded significant improvements, suggesting that table-specific model design may not be necessary for table retrieval.",
      "year": 2022,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2205.09843",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2205.09843",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1390877035",
          "name": "Zhiruo Wang"
        },
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "144287919",
          "name": "Eric Nyberg"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "37e2c7c89325a1a4685a46ff830fe7ecca8f1f80",
      "externalIds": {
        "DBLP": "journals/corr/abs-2204-10810",
        "ArXiv": "2204.10810",
        "DOI": "10.48550/arXiv.2204.10810",
        "CorpusId": 248366387
      },
      "url": "https://www.semanticscholar.org/paper/37e2c7c89325a1a4685a46ff830fe7ecca8f1f80",
      "title": "Learning to Scaffold: Optimizing Model Explanations for Teaching",
      "abstract": "Modern machine learning models are opaque, and as a result there is a burgeoning academic subfield on methods that explain these models' behavior. However, what is the precise goal of providing such explanations, and how can we demonstrate that explanations achieve this goal? Some research argues that explanations should help teach a student (either human or machine) to simulate the model being explained, and that the quality of explanations can be measured by the simulation accuracy of students on unexplained examples. In this work, leveraging meta-learning techniques, we extend this idea to improve the quality of the explanations themselves, specifically by optimizing explanations such that student models more effectively learn to simulate the original model. We train models on three natural language processing and computer vision tasks, and find that students trained with explanations extracted with our framework are able to simulate the teacher significantly more effectively than ones produced with previous methods. Through human annotations and a user study, we further find that these learned explanations more closely align with how humans would explain the required decisions in these tasks. Our code is available at https://github.com/coderpat/learning-scaffold",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2204.10810",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2204.10810",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2058640028",
          "name": "Patrick Fernandes"
        },
        {
          "authorId": "145188499",
          "name": "Marcos Vin\u00edcius Treviso"
        },
        {
          "authorId": "2064506371",
          "name": "Danish Pruthi"
        },
        {
          "authorId": "2113367250",
          "name": "Andr\u00e9 F. T. Martins"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "39e40821b7207125e54e6ed7112e55cd38c6f0c3",
      "externalIds": {
        "DBLP": "journals/corr/abs-2210-07128",
        "ACL": "2022.emnlp-main.90",
        "ArXiv": "2210.07128",
        "DOI": "10.48550/arXiv.2210.07128",
        "CorpusId": 252873120
      },
      "url": "https://www.semanticscholar.org/paper/39e40821b7207125e54e6ed7112e55cd38c6f0c3",
      "title": "Language Models of Code are Few-Shot Commonsense Learners",
      "abstract": "We address the general task of structured commonsense reasoning: given a natural language input, the goal is to generate a graph such as an event or a reasoning-graph.To employ large language models (LMs) for this task, existing approaches \u2018serialize\u2019 the output graph as a flat list of nodes and edges.Although feasible, these serialized graphs strongly deviate from the natural language corpora that LMs were pre-trained on, hindering LMs from generating them correctly. In this paper, we show that when we instead frame structured commonsense reasoning tasks as code generation tasks, pre-trained LMs of code are better structured commonsense reasoners than LMs of natural language, even when the downstream task does not involve source code at all.We demonstrate our approach across three diverse structured commonsense reasoning tasks. In all these natural language tasks, we show that using our approach, a code generation LM (codex) outperforms natural-LMs that are fine-tuned on the target task (T5) and other strong LMs such as GPT-3 in the few-shot setting.",
      "year": 2022,
      "influentialCitationCount": 5,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2210.07128",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2210.07128",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "21626987",
          "name": "Aman Madaan"
        },
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "47051926",
          "name": "Uri Alon"
        },
        {
          "authorId": "46286308",
          "name": "Yiming Yang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "3a1fa0785855716f88d017f587b68bbb81a34bc1",
      "externalIds": {
        "ACL": "2022.naacl-main.68",
        "DBLP": "conf/naacl/JiangMHNC22",
        "ArXiv": "2207.03637",
        "DOI": "10.48550/arXiv.2207.03637",
        "CorpusId": 250390443
      },
      "url": "https://www.semanticscholar.org/paper/3a1fa0785855716f88d017f587b68bbb81a34bc1",
      "title": "OmniTab: Pretraining with Natural and Synthetic Data for Few-shot Table-based Question Answering",
      "abstract": "The information in tables can be an important complement to text, making table-based question answering (QA) systems of great value. The intrinsic complexity of handling tables often adds an extra burden to both model design and data annotation. In this paper, we aim to develop a simple table-based QA model with minimal annotation effort. Motivated by the fact that table-based QA requires both alignment between questions and tables and the ability to perform complicated reasoning over multiple table elements, we propose an omnivorous pretraining approach that consumes both natural and synthetic data to endow models with these respective abilities. Specifically, given freely available tables, we leverage retrieval to pair them with relevant natural sentences for mask-based pretraining, and synthesize NL questions by converting SQL sampled from tables for pretraining with a QA loss. We perform extensive experiments in both few-shot and full settings, and the results clearly demonstrate the superiority of our model OmniTab, with the best multitasking approach achieving an absolute gain of 16.2% and 2.7% in 128-shot and full settings respectively, also establishing a new state-of-the-art on WikiTableQuestions. Detailed ablations and analyses reveal different characteristics of natural and synthetic data, shedding light on future directions in omnivorous pretraining.",
      "year": 2022,
      "influentialCitationCount": 7,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2207.03637",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "932-942"
      },
      "authors": [
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "145469202",
          "name": "Yi Mao"
        },
        {
          "authorId": "50462546",
          "name": "Pengcheng He"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2109136147",
          "name": "Weizhu Chen"
        }
      ]
    },
    {
      "paperId": "46b1616ff969c9e9c0b768ee37720156dfe4aa37",
      "externalIds": {
        "ArXiv": "2206.05154",
        "DBLP": "journals/corr/abs-2206-05154",
        "DOI": "10.48550/arXiv.2206.05154",
        "CorpusId": 249605802
      },
      "url": "https://www.semanticscholar.org/paper/46b1616ff969c9e9c0b768ee37720156dfe4aa37",
      "title": "Teacher Perception of Automatically Extracted Grammar Concepts for L2 Language Learning",
      "abstract": "One of the challenges in language teaching is how best to organize rules regarding syntax, semantics, or phonology in a meaningful manner. This not only requires content creators to have pedagogical skills, but also have that language's deep understanding. While comprehensive materials to develop such curricula are available in English and some broadly spoken languages, for many other languages, teachers need to manually create them in response to their students' needs. This is challenging because i) it requires that such experts be accessible and have the necessary resources, and ii) describing all the intricacies of a language is time-consuming and prone to omission. In this work, we aim to facilitate this process by automatically discovering and visualizing grammar descriptions. We extract descriptions from a natural text corpus that answer questions about morphosyntax (learning of word order, agreement, case marking, or word formation) and semantics (learning of vocabulary). We apply this method for teaching two Indian languages, Kannada and Marathi, which, unlike English, do not have well-developed resources for second language learning. To assess the perceived utility of the extracted material, we enlist the help of language educators from schools in North America to perform a manual evaluation, who find the materials have potential to be used for their lesson preparation and learner evaluation.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2206.05154",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2206.05154",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "51250894",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "2169938055",
          "name": "Arun Sampath"
        },
        {
          "authorId": "97568042",
          "name": "Ashwin Sheshadri"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "485c0bad66bff3a4ce70adafaf2fa02b8b1e64ff",
      "externalIds": {
        "ACL": "2022.iwslt-1.27",
        "DBLP": "conf/iwslt/YanFDSPBWNW22",
        "DOI": "10.18653/v1/2022.iwslt-1.27",
        "CorpusId": 248780518
      },
      "url": "https://www.semanticscholar.org/paper/485c0bad66bff3a4ce70adafaf2fa02b8b1e64ff",
      "title": "CMU\u2019s IWSLT 2022 Dialect Speech Translation System",
      "abstract": "This paper describes CMU\u2019s submissions to the IWSLT 2022 dialect speech translation (ST) shared task for translating Tunisian-Arabic speech to English text. We use additional paired Modern Standard Arabic data (MSA) to directly improve the speech recognition (ASR) and machine translation (MT) components of our cascaded systems. We also augment the paired ASR data with pseudo translations via sequence-level knowledge distillation from an MT model and use these artificial triplet ST data to improve our end-to-end (E2E) systems. Our E2E models are based on the Multi-Decoder architecture with searchable hidden intermediates. We extend the Multi-Decoder by orienting the speech encoder towards the target language by applying ST supervision as hierarchical connectionist temporal classification (CTC) multi-task. During inference, we apply joint decoding of the ST CTC and ST autoregressive decoder branches of our modified Multi-Decoder. Finally, we apply ROVER voting, posterior combination, and minimum bayes-risk decoding with combined N-best lists to ensemble our various cascaded and E2E systems. Our best systems reached 20.8 and 19.5 BLEU on test2 (blind) and test1 respectively. Without any additional MSA data, we reached 20.4 and 19.2 on the same test sets.",
      "year": 2022,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2022.iwslt-1.27.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "298-307"
      },
      "authors": [
        {
          "authorId": "2087059555",
          "name": "Brian Yan"
        },
        {
          "authorId": "2058640028",
          "name": "Patrick Fernandes"
        },
        {
          "authorId": "35186886",
          "name": "Siddharth Dalmia"
        },
        {
          "authorId": "1485531923",
          "name": "Jiatong Shi"
        },
        {
          "authorId": "2111014429",
          "name": "Yifan Peng"
        },
        {
          "authorId": "2142561945",
          "name": "Dan Berrebbi"
        },
        {
          "authorId": "3277494",
          "name": "Xinyi Wang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1746678",
          "name": "Shinji Watanabe"
        }
      ]
    },
    {
      "paperId": "4e9328b2801e158647dff69606ed47d47045eca8",
      "externalIds": {
        "ArXiv": "2202.12875",
        "ACL": "2022.acl-demo.18",
        "DBLP": "conf/acl/XiaoFYVLLNL22",
        "DOI": "10.18653/v1/2022.acl-demo.18",
        "CorpusId": 247154685
      },
      "url": "https://www.semanticscholar.org/paper/4e9328b2801e158647dff69606ed47d47045eca8",
      "title": "DataLab: A Platform for Data Analysis and Intervention",
      "abstract": "Despite data\u2019s crucial role in machine learning, most existing tools and research tend to focus on systems on top of existing data rather than how to interpret and manipulate data.In this paper, we propose DataLab, a unified data-oriented platform that not only allows users to interactively analyze the characteristics of data but also provides a standardized interface so that many data processing operations can be provided within a unified interface. Additionally, in view of the ongoing surge in the proliferation of datasets, DataLab has features for dataset recommendation and global vision analysis that help researchers form a better view of the data ecosystem. So far, DataLab covers 1,300 datasets and 3,583 of its transformed version, where 313 datasets support different types of analysis (e.g., with respect to gender bias) with the help of 119M samples annotated by 318 feature functions. DataLab is under active development and will be supported going forward. We have released a web platform, web API, Python SDK, and PyPI published package, which hopefully, can meet the diverse needs of researchers.",
      "year": 2022,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2022.acl-demo.18.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2202.12875",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2116642640",
          "name": "Yanghua Xiao"
        },
        {
          "authorId": "41037252",
          "name": "Jinlan Fu"
        },
        {
          "authorId": "30300197",
          "name": "Weizhe Yuan"
        },
        {
          "authorId": "2061499362",
          "name": "Vijay Viswanathan"
        },
        {
          "authorId": "2283974457",
          "name": "Zhoumianze Liu"
        },
        {
          "authorId": "2108176413",
          "name": "Yixin Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        }
      ]
    },
    {
      "paperId": "51836dfa1542277ed982612caa90ecf31ead4ba8",
      "externalIds": {
        "DBLP": "journals/corr/abs-2204-06340",
        "ArXiv": "2204.06340",
        "DOI": "10.48550/arXiv.2204.06340",
        "CorpusId": 247570285
      },
      "url": "https://www.semanticscholar.org/paper/51836dfa1542277ed982612caa90ecf31ead4ba8",
      "title": "Distributionally Robust Models with Parametric Likelihood Ratios",
      "abstract": "As machine learning models are deployed ever more broadly, it becomes increasingly important that they are not only able to perform well on their training distribution, but also yield accurate predictions when confronted with distribution shift. The Distributionally Robust Optimization (DRO) framework proposes to address this issue by training models to minimize their expected risk under a collection of distributions, to imitate test-time shifts. This is most commonly achieved by instance-level re-weighting of the training objective to emulate the likelihood ratio with possible test distributions, which allows for estimating their empirical risk via importance sampling (assuming that they are subpopulations of the training distribution). However, re-weighting schemes in the literature are usually limited due to the difficulty of keeping the optimization problem tractable and the complexity of enforcing normalization constraints. In this paper, we show that three simple ideas -- mini-batch level normalization, a KL penalty and simultaneous gradient updates -- allow us to train models with DRO using a broader class of parametric likelihood ratios. In a series of experiments on both image and text classification benchmarks, we find that models trained with the resulting parametric adversaries are consistently more robust to subpopulation shifts when compared to other DRO approaches, and that the method performs reliably well with little hyper-parameter tuning. Code to reproduce our experiments can be found at https://github.com/pmichel31415/P-DRO.",
      "year": 2022,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2204.06340",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2204.06340",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "2117567142",
          "name": "Tatsunori Hashimoto"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "55b901b3f5a0ec6788cbad0c0acdd4aa0f35c72f",
      "externalIds": {
        "DBLP": "conf/iclr/DeryMKNT23",
        "ArXiv": "2205.14082",
        "DOI": "10.48550/arXiv.2205.14082",
        "CorpusId": 249152128
      },
      "url": "https://www.semanticscholar.org/paper/55b901b3f5a0ec6788cbad0c0acdd4aa0f35c72f",
      "title": "AANG: Automating Auxiliary Learning",
      "abstract": "Auxiliary objectives, supplementary learning signals that are introduced to help aid learning on data-starved or highly complex end-tasks, are commonplace in machine learning. Whilst much work has been done to formulate useful auxiliary objectives, their construction is still an art which proceeds by slow and tedious hand-design. Intuition for how and when these objectives improve end-task performance has also had limited theoretical backing. In this work, we present an approach for automatically generating a suite of auxiliary objectives. We achieve this by deconstructing existing objectives within a novel unified taxonomy, identifying connections between them, and generating new ones based on the uncovered structure. Next, we theoretically formalize widely-held intuitions about how auxiliary learning improves generalization on the end-task. This leads us to a principled and efficient algorithm for searching the space of generated objectives to find those most useful to a specified end-task. With natural language processing (NLP) as our domain of study, we demonstrate that our automated auxiliary learning pipeline leads to strong improvements over competitive baselines across continued training experiments on a pre-trained model on 5 NLP tasks.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2205.14082",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2205.14082",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "32273391",
          "name": "L. Dery"
        },
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "10398264",
          "name": "M. Khodak"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145532827",
          "name": "Ameet Talwalkar"
        }
      ]
    },
    {
      "paperId": "58c9838b45edad4b7fc11bd7119f113ea5a3d7f1",
      "externalIds": {
        "ACL": "2022.acl-long.207",
        "DBLP": "conf/acl/LiuLRN22",
        "ArXiv": "2203.16804",
        "DOI": "10.48550/arXiv.2203.16804",
        "CorpusId": 247839752
      },
      "url": "https://www.semanticscholar.org/paper/58c9838b45edad4b7fc11bd7119f113ea5a3d7f1",
      "title": "BRIO: Bringing Order to Abstractive Summarization",
      "abstract": "Abstractive summarization models are commonly trained using maximum likelihood estimation, which assumes a deterministic (one-point) target distribution in which an ideal model will assign all the probability mass to the reference summary. This assumption may lead to performance degradation during inference, where the model needs to compare several system-generated (candidate) summaries that have deviated from the reference summary. To address this problem, we propose a novel training paradigm which assumes a non-deterministic distribution so that different candidate summaries are assigned probability mass according to their quality. Our method achieves a new state-of-the-art result on the CNN/DailyMail (47.78 ROUGE-1) and XSum (49.07 ROUGE-1) datasets. Further analysis also shows that our model can estimate probabilities of candidate summaries that are more correlated with their level of quality.",
      "year": 2022,
      "influentialCitationCount": 30,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2203.16804",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2890-2903"
      },
      "authors": [
        {
          "authorId": "2108176413",
          "name": "Yixin Liu"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "9215251",
          "name": "Dragomir R. Radev"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "5defbecd8391f93aabda11c63122d1a00a28222e",
      "externalIds": {
        "DBLP": "conf/emnlp/QinYN023",
        "ArXiv": "2212.05726",
        "DOI": "10.48550/arXiv.2212.05726",
        "CorpusId": 254564068
      },
      "url": "https://www.semanticscholar.org/paper/5defbecd8391f93aabda11c63122d1a00a28222e",
      "title": "T5Score: Discriminative Fine-tuning of Generative Evaluation Metrics",
      "abstract": "Modern embedding-based metrics for evaluation of generated text generally fall into one of two paradigms: discriminative metrics that are trained to directly predict which outputs are of higher quality according to supervised human annotations, and generative metrics that are trained to evaluate text based on the probabilities of a generative model. Both have their advantages; discriminative metrics are able to directly optimize for the problem of distinguishing between good and bad outputs, while generative metrics can be trained using abundant raw text. In this paper, we present a framework that combines the best of both worlds, using both supervised and unsupervised signals from whatever data we have available. We operationalize this idea by training T5Score, a metric that uses these training signals with mT5 as the backbone. We perform an extensive empirical comparison with other existing metrics on 5 datasets, 19 languages and 280 systems, demonstrating the utility of our method. Experimental results show that: T5Score achieves the best performance on all datasets against existing top-scoring metrics at the segment level. We release our code and models at https://github.com/qinyiwei/T5Score.",
      "year": 2022,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2212.05726",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2212.05726",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2143528602",
          "name": "Yiwei Qin"
        },
        {
          "authorId": "30300197",
          "name": "Weizhe Yuan"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        }
      ]
    },
    {
      "paperId": "60bff5a4527141599d8e05904baf96410541f8a9",
      "externalIds": {
        "DBLP": "conf/emnlp/ReidN22",
        "ArXiv": "2205.12374",
        "DOI": "10.48550/arXiv.2205.12374",
        "CorpusId": 249062636
      },
      "url": "https://www.semanticscholar.org/paper/60bff5a4527141599d8e05904baf96410541f8a9",
      "title": "Learning to Model Editing Processes",
      "abstract": "Most existing sequence generation models produce outputs in one pass, usually left-to-right. However, this is in contrast with a more natural approach that humans use in generating content; iterative refinement and editing. Recent work has introduced edit-based models for various tasks (such as neural machine translation and text style transfer), but these generally model a single edit step. In this work, we propose modeling editing processes, modeling the whole process of iteratively generating sequences. We form a conceptual framework to describe the likelihood of multi-step edits, and describe neural models that can learn a generative model of sequences based on these multistep edits. We introduce baseline results and metrics on this task, finding that modeling editing processes improves performance on a variety of axes on both our proposed task and related downstream tasks compared to previous single-step models of edits.",
      "year": 2022,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2205.12374",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3822-3832"
      },
      "authors": [
        {
          "authorId": "1557386977",
          "name": "Machel Reid"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",
      "externalIds": {
        "ArXiv": "2211.10435",
        "DBLP": "journals/corr/abs-2211-10435",
        "DOI": "10.48550/arXiv.2211.10435",
        "CorpusId": 253708270
      },
      "url": "https://www.semanticscholar.org/paper/6c1e1cc1e0e1f8fd026fe517607b2d4535565fa7",
      "title": "PAL: Program-aided Language Models",
      "abstract": "Large language models (LLMs) have recently demonstrated an impressive ability to perform arithmetic and symbolic reasoning tasks, when provided with a few examples at test time (\"few-shot prompting\"). Much of this success can be attributed to prompting methods such as\"chain-of-thought'', which employ LLMs for both understanding the problem description by decomposing it into steps, as well as solving each step of the problem. While LLMs seem to be adept at this sort of step-by-step decomposition, LLMs often make logical and arithmetic mistakes in the solution part, even when the problem is decomposed correctly. In this paper, we present Program-Aided Language models (PAL): a novel approach that uses the LLM to read natural language problems and generate programs as the intermediate reasoning steps, but offloads the solution step to a runtime such as a Python interpreter. With PAL, decomposing the natural language problem into runnable steps remains the only learning task for the LLM, while solving is delegated to the interpreter. We demonstrate this synergy between a neural LLM and a symbolic interpreter across 13 mathematical, symbolic, and algorithmic reasoning tasks from BIG-Bench Hard and other benchmarks. In all these natural language reasoning tasks, generating code using an LLM and reasoning using a Python interpreter leads to more accurate results than much larger models. For example, PAL using Codex achieves state-of-the-art few-shot accuracy on the GSM8K benchmark of math word problems, surpassing PaLM-540B which uses chain-of-thought by absolute 15% top-1. Our code and data are publicly available at http://reasonwithpal.com/ .",
      "year": 2022,
      "influentialCitationCount": 45,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2211.10435",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2211.10435",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "49715441",
          "name": "Luyu Gao"
        },
        {
          "authorId": "21626987",
          "name": "Aman Madaan"
        },
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "47051926",
          "name": "Uri Alon"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "46286308",
          "name": "Yiming Yang"
        },
        {
          "authorId": "144987107",
          "name": "Jamie Callan"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "70dd68c07b322b68836eded1fb4f78c0efcad685",
      "externalIds": {
        "DBLP": "journals/corr/abs-2210-07111",
        "ACL": "2023.findings-eacl.128",
        "ArXiv": "2210.07111",
        "DOI": "10.48550/arXiv.2210.07111",
        "CorpusId": 252873220
      },
      "url": "https://www.semanticscholar.org/paper/70dd68c07b322b68836eded1fb4f78c0efcad685",
      "title": "A Multi-dimensional Evaluation of Tokenizer-free Multilingual Pretrained Models",
      "abstract": "Recent works on tokenizer-free multilingual pretrained models show promising results in improving cross-lingual transfer and reducing engineering overhead compared to subword-based alternatives.However, previous work mainly focuses on reporting accuracy on a limited set of tasks and data settings, placing less emphasis on other important factors when tuning and deploying the models in practice, such as memory usage, inference speed, and finetuning data efficiency. We attempt to fill this gap by performing a comprehensive empirical comparison of multilingual tokenizer-free and subword-based models considering the various dimensions. Surprisingly, we find that subword-based models might still be the most practical choice in many settings, achieving better performance for lower inference latency and memory usage. Based on these results, we encourage future work in tokenizer-free methods to consider these factors when designing and evaluating new models.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2210.07111",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2210.07111",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2110825948",
          "name": "Jimin Sun"
        },
        {
          "authorId": "2058640028",
          "name": "Patrick Fernandes"
        },
        {
          "authorId": "3277494",
          "name": "Xinyi Wang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "70e91e16eb321067d9402710e14a40cf28311f73",
      "externalIds": {
        "DBLP": "journals/corr/abs-2209-10655",
        "ArXiv": "2209.10655",
        "DOI": "10.48550/arXiv.2209.10655",
        "CorpusId": 252439127
      },
      "url": "https://www.semanticscholar.org/paper/70e91e16eb321067d9402710e14a40cf28311f73",
      "title": "Mega: Moving Average Equipped Gated Attention",
      "abstract": "The design choices in the Transformer attention mechanism, including weak inductive bias and quadratic computational complexity, have limited its application for modeling long sequences. In this paper, we introduce Mega, a simple, theoretically grounded, single-head gated attention mechanism equipped with (exponential) moving average to incorporate inductive bias of position-aware local dependencies into the position-agnostic attention mechanism. We further propose a variant of Mega that offers linear time and space complexity yet yields only minimal quality loss, by efficiently splitting the whole sequence into multiple chunks with fixed length. Extensive experiments on a wide range of sequence modeling benchmarks, including the Long Range Arena, neural machine translation, auto-regressive language modeling, and image and speech classification, show that Mega achieves significant improvements over other sequence models, including variants of Transformers and recent state space models.",
      "year": 2022,
      "influentialCitationCount": 19,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2209.10655",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2209.10655",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2378954",
          "name": "Xuezhe Ma"
        },
        {
          "authorId": "2110714400",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "97791350",
          "name": "Xiang Kong"
        },
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "1970583",
          "name": "Liangke Gui"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143823227",
          "name": "Jonathan May"
        },
        {
          "authorId": "1982950",
          "name": "Luke Zettlemoyer"
        }
      ]
    },
    {
      "paperId": "7259ca612de44763678a30114b02d37c00280c86",
      "externalIds": {
        "DBLP": "conf/emnlp/AdelaniNRRBPLAM22",
        "ACL": "2022.emnlp-main.298",
        "ArXiv": "2210.12391",
        "DOI": "10.48550/arXiv.2210.12391",
        "CorpusId": 253098583
      },
      "url": "https://www.semanticscholar.org/paper/7259ca612de44763678a30114b02d37c00280c86",
      "title": "MasakhaNER 2.0: Africa-centric Transfer Learning for Named Entity Recognition",
      "abstract": "African languages are spoken by over a billion people, but they are under-represented in NLP research and development. Multiple challenges exist, including the limited availability of annotated training and evaluation datasets as well as the lack of understanding of which settings, languages, and recently proposed methods like cross-lingual transfer will be effective. In this paper, we aim to move towards solutions for these challenges, focusing on the task of named entity recognition (NER). We present the creation of the largest to-date human-annotated NER dataset for 20 African languages. We study the behaviour of state-of-the-art cross-lingual transfer methods in an Africa-centric setting, empirically demonstrating that the choice of source transfer language significantly affects performance. While much previous work defaults to using English as the source language, our results show that choosing the best transfer language improves zero-shot F1 scores by an average of 14% over 20 languages as compared to using English.",
      "year": 2022,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2210.12391",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "4488-4508"
      },
      "authors": [
        {
          "authorId": "2518906",
          "name": "David Ifeoluwa Adelani"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2124014463",
          "name": "Sebastian Ruder"
        },
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "2151088783",
          "name": "Michael Beukman"
        },
        {
          "authorId": "1414156453",
          "name": "Chester Palen-Michel"
        },
        {
          "authorId": "1737047",
          "name": "Constantine Lignos"
        },
        {
          "authorId": "122367036",
          "name": "Jesujoba Oluwadara Alabi"
        },
        {
          "authorId": "7744881",
          "name": "Shamsuddeen Hassan Muhammad"
        },
        {
          "authorId": "1801259",
          "name": "Peter Nabende"
        },
        {
          "authorId": "1686278430",
          "name": "Cheikh M. Bamba Dione"
        },
        {
          "authorId": "2164122773",
          "name": "Andiswa Bukula"
        },
        {
          "authorId": "2140114498",
          "name": "Rooweither Mabuya"
        },
        {
          "authorId": "1591111757",
          "name": "Bonaventure F. P. Dossou"
        },
        {
          "authorId": "1591123568",
          "name": "Blessing K. Sibanda"
        },
        {
          "authorId": "1395556657",
          "name": "Happy Buzaaba"
        },
        {
          "authorId": "1412684911",
          "name": "Jonathan Mukiibi"
        },
        {
          "authorId": "134013136",
          "name": "Godson Kalipe"
        },
        {
          "authorId": "2056774640",
          "name": "Derguene Mbaye"
        },
        {
          "authorId": "1858776532",
          "name": "Amelia Taylor"
        },
        {
          "authorId": "6277948",
          "name": "F. Kabore"
        },
        {
          "authorId": "1591176064",
          "name": "Chris C. Emezue"
        },
        {
          "authorId": "2056773747",
          "name": "Anuoluwapo Aremu"
        },
        {
          "authorId": "1988654955",
          "name": "Perez Ogayo"
        },
        {
          "authorId": "4420343",
          "name": "C. Gitau"
        },
        {
          "authorId": "2164122776",
          "name": "Edwin Munkoh-Buabeng"
        },
        {
          "authorId": "2164123106",
          "name": "V. M. Koagne"
        },
        {
          "authorId": "1602867250",
          "name": "A. Tapo"
        },
        {
          "authorId": "2188733140",
          "name": "Tebogo Macucwa"
        },
        {
          "authorId": "1841137457",
          "name": "V. Marivate"
        },
        {
          "authorId": "146632396",
          "name": "Elvis Mboning"
        },
        {
          "authorId": "2352354",
          "name": "T. Gwadabe"
        },
        {
          "authorId": "51221489",
          "name": "Tosin P. Adewumi"
        },
        {
          "authorId": "1452686038",
          "name": "Orevaoghene Ahia"
        },
        {
          "authorId": "1411005375",
          "name": "J. Nakatumba-Nabende"
        },
        {
          "authorId": "2188733779",
          "name": "Neo L. Mokono"
        },
        {
          "authorId": "50203736",
          "name": "Ignatius M Ezeani"
        },
        {
          "authorId": "73054967",
          "name": "C. Chukwuneke"
        },
        {
          "authorId": "2056770646",
          "name": "Mofetoluwa Adeyemi"
        },
        {
          "authorId": "2114722803",
          "name": "Gilles Hacheme"
        },
        {
          "authorId": "1429833598",
          "name": "Idris Abdulmumin"
        },
        {
          "authorId": "2166106776",
          "name": "Odunayo Ogundepo"
        },
        {
          "authorId": "2164122749",
          "name": "Oreen Yousuf"
        },
        {
          "authorId": "2188733769",
          "name": "Tatiana Moteu Ngoli"
        },
        {
          "authorId": "2561225",
          "name": "D. Klakow"
        }
      ]
    },
    {
      "paperId": "7e43dad7fbae3a7db47adc6b89c76acbd2fb225f",
      "externalIds": {
        "ACL": "2022.acl-long.214",
        "ArXiv": "2203.07264",
        "DBLP": "conf/acl/ZhouZY0YCN22",
        "DOI": "10.48550/arXiv.2203.07264",
        "CorpusId": 247446828
      },
      "url": "https://www.semanticscholar.org/paper/7e43dad7fbae3a7db47adc6b89c76acbd2fb225f",
      "title": "Show Me More Details: Discovering Hierarchies of Procedures from Semi-structured Web Data",
      "abstract": "Procedures are inherently hierarchical. To \u201cmake videos\u201d, one may need to \u201cpurchase a camera\u201d, which in turn may require one to \u201cset a budget\u201d. While such hierarchical knowledge is critical for reasoning about complex procedures, most existing work has treated procedures as shallow structures without modeling the parent-child relation. In this work, we attempt to construct an open-domain hierarchical knowledge-base (KB) of procedures based on wikiHow, a website containing more than 110k instructional articles, each documenting the steps to carry out a complex procedure. To this end, we develop a simple and efficient method that links steps (e.g., \u201cpurchase a camera\u201d) in an article to other articles with similar goals (e.g., \u201chow to choose a camera\u201d), recursively constructing the KB. Our method significantly outperforms several strong baselines according to automatic evaluation, human judgment, and application to downstream tasks such as instructional video retrieval.",
      "year": 2022,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2203.07264",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2203.07264",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "72436283",
          "name": "Li Zhang"
        },
        {
          "authorId": "2109409802",
          "name": "Yue Yang"
        },
        {
          "authorId": "1904906987",
          "name": "Qing Lyu"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "1763608",
          "name": "Chris Callison-Burch"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "87126a964ed14d0d2207747fc732b197e2fc9493",
      "externalIds": {
        "ArXiv": "2212.02027",
        "ACL": "2022.emnlp-main.149",
        "DBLP": "conf/emnlp/JiangGWADCN22",
        "DOI": "10.48550/arXiv.2212.02027",
        "CorpusId": 254246471
      },
      "url": "https://www.semanticscholar.org/paper/87126a964ed14d0d2207747fc732b197e2fc9493",
      "title": "Retrieval as Attention: End-to-end Learning of Retrieval and Reading within a Single Transformer",
      "abstract": "Systems for knowledge-intensive tasks such as open-domain question answering (QA) usually consist of two stages: efficient retrieval of relevant documents from a large corpus and detailed reading of the selected documents. This is usually done through two separate models, a retriever that encodes the query and finds nearest neighbors, and a reader based on Transformers. These two components are usually modeled separately, which necessitates a cumbersome implementation and is awkward to optimize in an end-to-end fashion. In this paper, we revisit this design and eschew the separate architecture and training in favor of a single Transformer that performs retrieval as attention (RAA), and end-to-end training solely based on supervision from the end QA task. We demonstrate for the first time that an end-to-end trained single Transformer can achieve both competitive retrieval and QA performance on in-domain datasets, matching or even slightly outperforming state-of-the-art dense retrievers and readers. Moreover, end-to-end adaptation of our model significantly boosts its performance on out-of-domain datasets in both supervised and unsupervised settings, making our model a simple and adaptable end-to-end solution for knowledge-intensive tasks.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2212.02027",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2212.02027",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "49715441",
          "name": "Luyu Gao"
        },
        {
          "authorId": "50007145",
          "name": "J. Araki"
        },
        {
          "authorId": "47929135",
          "name": "Haibo Ding"
        },
        {
          "authorId": "1390877035",
          "name": "Zhiruo Wang"
        },
        {
          "authorId": "144987107",
          "name": "Jamie Callan"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "9b534639bcadc9ad232b338e760c523a4d74c8de",
      "externalIds": {
        "ArXiv": "2203.13901",
        "DBLP": "journals/corr/abs-2203-13901",
        "DOI": "10.48550/arXiv.2203.13901",
        "CorpusId": 247762168
      },
      "url": "https://www.semanticscholar.org/paper/9b534639bcadc9ad232b338e760c523a4d74c8de",
      "title": "AUTOLEX: An Automatic Framework for Linguistic Exploration",
      "abstract": "Each language has its own complex systems of word, phrase, and sentence construction, the guiding principles of which are often summarized in grammar descriptions for the consumption of linguists or language learners. However, manual creation of such descriptions is a fraught process, as creating descriptions which describe the language in\"its own terms\"without bias or error requires both a deep understanding of the language at hand and linguistics as a whole. We propose an automatic framework AutoLEX that aims to ease linguists' discovery and extraction of concise descriptions of linguistic phenomena. Specifically, we apply this framework to extract descriptions for three phenomena: morphological agreement, case marking, and word order, across several languages. We evaluate the descriptions with the help of language experts and propose a method for automated evaluation when human evaluation is infeasible.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2203.13901",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2203.13901",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "51250894",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "38599655",
          "name": "Zaid A. W. Sheikh"
        },
        {
          "authorId": "3407646",
          "name": "David R. Mortensen"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "9eda819e51783f783d3ffd60fdbfac01ee1f4416",
      "externalIds": {
        "DBLP": "journals/corr/abs-2212-05740",
        "ArXiv": "2212.05740",
        "DOI": "10.48550/arXiv.2212.05740",
        "CorpusId": 252571103
      },
      "url": "https://www.semanticscholar.org/paper/9eda819e51783f783d3ffd60fdbfac01ee1f4416",
      "title": "Searching for Effective Multilingual Fine-Tuning Methods: A Case Study in Summarization",
      "abstract": "Recently, a large number of tuning strategies have been proposed to adapt pre-trained language models to downstream tasks. In this paper, we perform an extensive empirical evaluation of various tuning strategies for multilingual learning, particularly in the context of text summarization. Specifically, we explore the relative advantages of three families of multilingual tuning strategies (a total of five models) and empirically evaluate them for summarization over 45 languages. Experimentally, we not only established a new state-of-the-art on the XL-Sum dataset but also derive a series of observations that hopefully can provide hints for future research on the design of multilingual tuning strategies.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2212.05740",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2212.05740",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2143528602",
          "name": "Yiwei Qin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        }
      ]
    },
    {
      "paperId": "9eea41c0341e15e97062dd771515378cfba29019",
      "externalIds": {
        "DBLP": "conf/emnlp/BertschNG22",
        "ArXiv": "2210.15462",
        "DOI": "10.48550/arXiv.2210.15462",
        "CorpusId": 253157685
      },
      "url": "https://www.semanticscholar.org/paper/9eea41c0341e15e97062dd771515378cfba29019",
      "title": "He Said, She Said: Style Transfer for Shifting the Perspective of Dialogues",
      "abstract": "In this work, we define a new style transfer task: perspective shift, which reframes a dialogue from informal first person to a formal third person rephrasing of the text. This task requires challenging coreference resolution, emotion attribution, and interpretation of informal text. We explore several baseline approaches and discuss further directions on this task when applied to short dialogues. As a sample application, we demonstrate that applying perspective shifting to a dialogue summarization dataset (SAMSum) substantially improves the zero-shot performance of extractive news summarization models on this data. Additionally, supervised extractive models perform better when trained on perspective shifted data than on the original dialogues. We release our code publicly.",
      "year": 2022,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2210.15462",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "4823-4840"
      },
      "authors": [
        {
          "authorId": "2138301112",
          "name": "Amanda Bertsch"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1762110",
          "name": "Matthew R. Gormley"
        }
      ]
    },
    {
      "paperId": "a8fffb507d1790851550af5e4ebdd06e5bae1cee",
      "externalIds": {
        "DBLP": "journals/corr/abs-2203-09435",
        "ACL": "2022.acl-long.61",
        "ArXiv": "2203.09435",
        "DOI": "10.48550/arXiv.2203.09435",
        "CorpusId": 247518760
      },
      "url": "https://www.semanticscholar.org/paper/a8fffb507d1790851550af5e4ebdd06e5bae1cee",
      "title": "Expanding Pretrained Models to Thousands More Languages via Lexicon-based Adaptation",
      "abstract": "The performance of multilingual pretrained models is highly dependent on the availability of monolingual or parallel text present in a target language. Thus, the majority of the world\u2019s languages cannot benefit from recent progress in NLP as they have no or limited textual data. To expand possibilities of using NLP technology in these under-represented languages, we systematically study strategies that relax the reliance on conventional language resources through the use of bilingual lexicons, an alternative resource with much better language coverage. We analyze different strategies to synthesize textual or labeled data using lexicons, and how this data can be combined with monolingual or parallel text when available. For 19 under-represented languages across 3 tasks, our methods lead to consistent improvements of up to 5 and 15 points with and without extra monolingual text respectively. Overall, our study highlights how NLP methods can be adapted to thousands more languages that are under-served by current technology.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2203.09435",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "863-877"
      },
      "authors": [
        {
          "authorId": "3277494",
          "name": "Xinyi Wang"
        },
        {
          "authorId": "2124014463",
          "name": "Sebastian Ruder"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "b270bccb8ceeaa23f66c5b8746e8bdc489035e6d",
      "externalIds": {
        "DBLP": "conf/icml/SclarNB22",
        "CorpusId": 250340768
      },
      "url": "https://www.semanticscholar.org/paper/b270bccb8ceeaa23f66c5b8746e8bdc489035e6d",
      "title": "Symmetric Machine Theory of Mind",
      "abstract": "Theory of mind, the ability to model others\u2019 thoughts and desires, is a cornerstone of human social intelligence. This makes it an important challenge for the machine learning community, but previous works mainly attempt to design agents that model the \u201cmental state\u201d of others as passive observers or in specific predefined roles, such as in speaker-listener scenarios. In contrast, we propose to model machine theory of mind in a more general symmetric scenario. We introduce a multi-agent environment SymmToM where, like in real life, all agents can speak, listen, see other agents, and move freely through the world. Effective strategies to maximize an agent\u2019s reward require it to develop a theory of mind. We show that reinforcement learning agents that model the mental states of others achieve significant performance improvements over agents with no such theory of mind model. Importantly, our best agents still fail to achieve performance comparable to agents with access to the gold-standard mental state of other agents, demonstrating that the modeling of theory of mind in multi-agent scenarios is very much an open challenge. Code can be found at https: //github.com/msclar/symmtom.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "19450-19466"
      },
      "authors": [
        {
          "authorId": "1947172233",
          "name": "Melanie Sclar"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3312309",
          "name": "Yonatan Bisk"
        }
      ]
    },
    {
      "paperId": "b31b21d0750e849badfe76000e8170482f32b9be",
      "externalIds": {
        "DBLP": "journals/corr/abs-2207-05987",
        "DOI": "10.48550/arXiv.2207.05987",
        "CorpusId": 250492900
      },
      "url": "https://www.semanticscholar.org/paper/b31b21d0750e849badfe76000e8170482f32b9be",
      "title": "DocCoder: Generating Code by Retrieving and Reading Docs",
      "abstract": "Natural-language-to-code models learn to generate a code snippet given a natural language (NL) intent. However, the rapid growth of both publicly available and proprietary libraries and functions makes it impossible to cover all APIs using training examples, as new libraries and functions are introduced daily. Thus, existing models inherently cannot generalize to using unseen functions and libraries merely through incorporating them into the training data. In contrast, when human programmers write programs, they frequently refer to textual resources such as code manuals, documentation, and tutorials, to explore and understand available library functionality. Inspired by this observation, we introduce DocCoder : an approach that explicitly leverages code manuals and documentation by (1) retrieving the relevant documentation given the NL intent, and (2) generating the code based on the NL intent and the retrieved documentation. Our approach is general, can be applied to any programming language, and is agnostic to the underlying neural model. We demonstrate that DocCoder consistently improves NL-to-code models: DocCoder achieves 11x higher exact match accuracy than strong baselines on a new Bash dataset tldr ; on the popular Python CoNaLa benchmark, DocCoder improves over strong baselines by 1.65 BLEU. 1",
      "year": 2022,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2207.05987",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2207.05987",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "47051926",
          "name": "Uri Alon"
        },
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "b32a6f6ef7dd775e0f876b4713ceccebc56e651e",
      "externalIds": {
        "DBLP": "conf/pldi/Xu0NH22",
        "ArXiv": "2202.13169",
        "DOI": "10.1145/3520312.3534862",
        "CorpusId": 247158549
      },
      "url": "https://www.semanticscholar.org/paper/b32a6f6ef7dd775e0f876b4713ceccebc56e651e",
      "title": "A systematic evaluation of large language models of code",
      "abstract": "Large language models (LMs) of code have recently shown tremendous promise in completing code and synthesizing code from natural language descriptions. However, the current state-of-the-art code LMs (e.g., Codex) are not publicly available, leaving many questions about their model and data design decisions. We aim to fill in some of these blanks through a systematic evaluation of the largest existing models: Codex, GPT-J, GPT-Neo, GPT-NeoX-20B, and CodeParrot, across various programming languages. Although Codex itself is not open-source, we find that existing opensource models do achieve close results in some programming languages, although targeted mainly for natural language modeling. We further identify an important missing piece in the form of a large open-source model trained exclusively on a multi-lingual corpus of code. We release a new model, PolyCoder, with 2.7B parameters based on the GPT-2 architecture, that was trained on 249GB of code across 12 programming languages on a single machine. In the C programming language, PolyCoder outperforms all models including Codex. Our trained models are open-source and publicly available at https://github.com/VHellendoorn/Code-LMs, which enables future research and application in this area. We have an online appendix at https://arxiv.org/abs/2202.13169.",
      "year": 2022,
      "influentialCitationCount": 26,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://dl.acm.org/doi/pdf/10.1145/3520312.3534862",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "name": "Proceedings of the 6th ACM SIGPLAN International Symposium on Machine Programming"
      },
      "authors": [
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "47051926",
          "name": "Uri Alon"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "22747364",
          "name": "V. Hellendoorn"
        }
      ]
    },
    {
      "paperId": "bdc91a1fafa1b783149101f72547c96ee9b531d0",
      "externalIds": {
        "CorpusId": 254686201
      },
      "url": "https://www.semanticscholar.org/paper/bdc91a1fafa1b783149101f72547c96ee9b531d0",
      "title": "Scaling Up Multilingual Evaluation",
      "abstract": "The SUMEval Workshop\u2019s shared task in-volved predicting performance of multilingual PLMs across multiple languages when these models are fine-tuned with varying amounts of data in different languages. The training data was provided for performances of two multilingual models on four NLP tasks, and a baseline was shared with the participants to get started. For test data, the task had two variants for evaluation, non-surprise version where the performance was to be predicted for languages seen in the training data but with unseen configurations, and surprise version where the languages were unseen during the training. A total of five teams participated in the shared task with 15 submissions overall. The participants proposed addition of new features, feature engineering techniques and trained an ensemble of regression models for the task. The best performing team had an improvement of 64% in MAE over the shared baseline for the non-surprise variant, and a 17% improvement for the surprise variant.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "52154863",
          "name": "Kabir Ahuja"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "27419446",
          "name": "Barun Patra"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143990839",
          "name": "M. Choudhury"
        },
        {
          "authorId": "2159556964",
          "name": "Shantanu Patankar"
        },
        {
          "authorId": "2087839062",
          "name": "Omkar Gokhale"
        },
        {
          "authorId": "2131857526",
          "name": "Onkar Litake"
        },
        {
          "authorId": "2123094801",
          "name": "Aditya Mandke"
        },
        {
          "authorId": "30790244",
          "name": "Dipali Kadam"
        },
        {
          "authorId": "66514001",
          "name": "Syeda Sabrina Akter"
        }
      ]
    },
    {
      "paperId": "be245094cad78304dfae4b00d1e44b3b90898e71",
      "externalIds": {
        "DBLP": "journals/frai/KannEMOORFGCLRRMCNPSV22",
        "PubMedCentral": "9755662",
        "DOI": "10.3389/frai.2022.995667",
        "CorpusId": 254129330,
        "PubMed": "36530357"
      },
      "url": "https://www.semanticscholar.org/paper/be245094cad78304dfae4b00d1e44b3b90898e71",
      "title": "AmericasNLI: Machine translation and natural language inference systems for Indigenous languages of the Americas",
      "abstract": "Little attention has been paid to the development of human language technology for truly low-resource languages\u2014i.e., languages with limited amounts of digitally available text data, such as Indigenous languages. However, it has been shown that pretrained multilingual models are able to perform crosslingual transfer in a zero-shot setting even for low-resource languages which are unseen during pretraining. Yet, prior work evaluating performance on unseen languages has largely been limited to shallow token-level tasks. It remains unclear if zero-shot learning of deeper semantic tasks is possible for unseen languages. To explore this question, we present AmericasNLI, a natural language inference dataset covering 10 Indigenous languages of the Americas. We conduct experiments with pretrained models, exploring zero-shot learning in combination with model adaptation. Furthermore, as AmericasNLI is a multiway parallel dataset, we use it to benchmark the performance of different machine translation models for those languages. Finally, using a standard transformer model, we explore translation-based approaches for natural language inference. We find that the zero-shot performance of pretrained models without adaptation is poor for all languages in AmericasNLI, but model adaptation via continued pretraining results in improvements. All machine translation models are rather weak, but, surprisingly, translation-based approaches to natural language inference outperform all other models on that task.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.frontiersin.org/articles/10.3389/frai.2022.995667/pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science",
        "Medicine"
      ],
      "journal": {
        "volume": "5",
        "name": "Frontiers in Artificial Intelligence"
      },
      "authors": [
        {
          "authorId": "3422953",
          "name": "Katharina Kann"
        },
        {
          "authorId": "146057134",
          "name": "Abteen Ebrahimi"
        },
        {
          "authorId": "153151470",
          "name": "Manuel Mager"
        },
        {
          "authorId": "65775345",
          "name": "Arturo Oncevay"
        },
        {
          "authorId": "118344292",
          "name": "J. Ortega"
        },
        {
          "authorId": "40659617",
          "name": "Annette Rios Gonzales"
        },
        {
          "authorId": "144270981",
          "name": "Angela Fan"
        },
        {
          "authorId": "1409305289",
          "name": "Ximena Gutierrez-Vasques"
        },
        {
          "authorId": "2287191",
          "name": "Luis Chiruzzo"
        },
        {
          "authorId": "1419526006",
          "name": "G. Gim\u00e9nez-Lugo"
        },
        {
          "authorId": "2078503669",
          "name": "Ricardo Ramos"
        },
        {
          "authorId": "1403616824",
          "name": "Ivan Vladimir Meza Ruiz"
        },
        {
          "authorId": "51125557",
          "name": "Elisabeth Mager"
        },
        {
          "authorId": "113810201",
          "name": "Vishrav Chaudhary"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "6214002",
          "name": "Alexis Palmer"
        },
        {
          "authorId": "1405433180",
          "name": "Rolando Coto-Solano"
        },
        {
          "authorId": "4160376",
          "name": "Ngoc Thang Vu"
        }
      ]
    },
    {
      "paperId": "c33e99e90d066319866de9e0768e01b83360d1ab",
      "externalIds": {
        "DBLP": "journals/corr/abs-2208-11024",
        "ArXiv": "2208.11024",
        "DOI": "10.48550/arXiv.2208.11024",
        "CorpusId": 251741364
      },
      "url": "https://www.semanticscholar.org/paper/c33e99e90d066319866de9e0768e01b83360d1ab",
      "title": "KGxBoard: Explainable and Interactive Leaderboard for Evaluation of Knowledge Graph Completion Models",
      "abstract": "Knowledge Graphs (KGs) store information in the form of (head, predicate, tail)-triples. To augment KGs with new knowledge, researchers proposed models for KG Completion (KGC) tasks such as link prediction; i.e., answering (h; p; ?) or (?; p; t) queries. Such models are usually evaluated with averaged metrics on a held-out test set. While useful for tracking progress, averaged single-score metrics cannot reveal what exactly a model has learned -- or failed to learn. To address this issue, we propose KGxBoard: an interactive framework for performing fine-grained evaluation on meaningful subsets of the data, each of which tests individual and interpretable capabilities of a KGC model. In our experiments, we highlight the findings that we discovered with the use of KGxBoard, which would have been impossible to detect with standard averaged single-score metrics.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2208.11024",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "338-350"
      },
      "authors": [
        {
          "authorId": "2091675826",
          "name": "Haris Widjaja"
        },
        {
          "authorId": "24868638",
          "name": "Kiril Gashteovski"
        },
        {
          "authorId": "2037487498",
          "name": "Wiem Ben Rim"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "3200180",
          "name": "Christopher Malon"
        },
        {
          "authorId": "8792454",
          "name": "Daniel Ruffinelli"
        },
        {
          "authorId": "19752252",
          "name": "Carolin (Haas) Lawrence"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "d6b7bc4e7968398101d8d9b4da7d4a0186763ff3",
      "externalIds": {
        "DBLP": "journals/corr/abs-2203-08388",
        "ACL": "2023.findings-eacl.20",
        "ArXiv": "2203.08388",
        "DOI": "10.48550/arXiv.2203.08388",
        "CorpusId": 247475896
      },
      "url": "https://www.semanticscholar.org/paper/d6b7bc4e7968398101d8d9b4da7d4a0186763ff3",
      "title": "MCoNaLa: A Benchmark for Code Generation from Multiple Natural Languages",
      "abstract": "While there has been a recent burgeoning of applications at the intersection of natural and programming languages, such as code generation and code summarization, these applications are usually English-centric. This creates a barrier for program developers who are not proficient in English. To mitigate this gap in technology development across languages, we propose a multilingual dataset, MCoNaLa, to benchmark code generation from natural language commands extending beyond English. Modeled off of the methodology from the English Code/Natural Language Challenge (CoNaLa) dataset, we annotated a total of 896 NL-Code pairs in three languages: Spanish, Japanese, and Russian. We present a systematic evaluation on MCoNaLa by testing state-of-the-art code generation systems. Although the difficulties vary across three languages, all systems lag significantly behind their English counterparts, revealing the challenges in adapting code generation to new languages.",
      "year": 2022,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2203.08388",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "265-273"
      },
      "authors": [
        {
          "authorId": "1390877035",
          "name": "Zhiruo Wang"
        },
        {
          "authorId": "2158994691",
          "name": "Grace Cuenca"
        },
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "d9212b207e49a3aa6806fb2ddadb303b7b1d47a8",
      "externalIds": {
        "DBLP": "journals/corr/abs-2109-08214",
        "ACL": "2022.suki-1.8",
        "DOI": "10.18653/v1/2022.suki-1.8",
        "CorpusId": 263863086
      },
      "url": "https://www.semanticscholar.org/paper/d9212b207e49a3aa6806fb2ddadb303b7b1d47a8",
      "title": "Hierarchical Control of Situated Agents through Natural Language",
      "abstract": "When humans perform a particular task, they do so hierarchically: splitting higher-level tasks into smaller sub-tasks. However, most works on natural language (NL) command of situated agents have treated the procedures to be executed as flat sequences of simple actions, or any hierarchies of procedures have been shallow at best. In this paper, we propose a formalism of procedures as programs, a method for representing hierarchical procedural knowledge for agent command and control aimed at enabling easy application to various scenarios. We further propose a modeling paradigm of hierarchical modular networks, which consist of a planner and reactors that convert NL intents to predictions of executable programs and probe the environment for information necessary to complete the program execution. We instantiate this framework on the IQA and ALFRED datasets for NL instruction following. Our model outperforms reactive baselines by a large margin on both datasets. We also demonstrate that our framework is more data-efficient, and that it allows for fast iterative development.",
      "year": 2022,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2022.suki-1.8.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2109.08214",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "2055975657",
          "name": "P. Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "ddedad67de4aa720f7b450605c73e197e9aa912d",
      "externalIds": {
        "ArXiv": "2207.00688",
        "DBLP": "journals/corr/abs-2207-00688",
        "DOI": "10.48550/arXiv.2207.00688",
        "CorpusId": 250264199
      },
      "url": "https://www.semanticscholar.org/paper/ddedad67de4aa720f7b450605c73e197e9aa912d",
      "title": "Building African Voices",
      "abstract": "Modern speech synthesis techniques can produce natural-sounding speech given sufficient high-quality data and compute resources. However, such data is not readily available for many languages. This paper focuses on speech synthesis for low-resourced African languages, from corpus creation to sharing and deploying the Text-to-Speech (TTS) systems. We first create a set of general-purpose instructions on building speech synthesis systems with minimum technological resources and subject-matter expertise. Next, we create new datasets and curate datasets from\"found\"data (existing recordings) through a participatory approach while considering accessibility, quality, and breadth. We demonstrate that we can develop synthesizers that generate intelligible speech with 25 minutes of created speech, even when recorded in suboptimal environments. Finally, we release the speech data, code, and trained voices for 12 African languages to support researchers and developers.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2207.00688",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "journal": {
        "volume": "abs/2207.00688",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1988654955",
          "name": "Perez Ogayo"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1690706",
          "name": "A. Black"
        }
      ]
    },
    {
      "paperId": "ea77d7f930761a0a462d70de70318487b06e3d5d",
      "externalIds": {
        "ACL": "2022.sumeval-1.1",
        "CorpusId": 253762003
      },
      "url": "https://www.semanticscholar.org/paper/ea77d7f930761a0a462d70de70318487b06e3d5d",
      "title": "The SUMEval 2022 Shared Task on Performance Prediction of Multilingual Pre-trained Language Models",
      "abstract": "The SUMEval Workshop\u2019s shared task in-volved predicting performance of multilingual PLMs across multiple languages when these models are fine-tuned with varying amounts of data in different languages. The training data was provided for performances of two multilingual models on four NLP tasks, and a baseline was shared with the participants to get started. For test data, the task had two variants for evaluation, non-surprise version where the performance was to be predicted for languages seen in the training data but with unseen configurations, and surprise version where the languages were unseen during the training. A total of five teams participated in the shared task with 15 submissions overall. The participants proposed addition of new features, feature engineering techniques and trained an ensemble of regression models for the task. The best performing team had an improvement of 64% in MAE over the shared baseline for the non-surprise variant, and a 17% improvement for the surprise variant.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "52154863",
          "name": "Kabir Ahuja"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "27419446",
          "name": "Barun Patra"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143990839",
          "name": "M. Choudhury"
        },
        {
          "authorId": "34725175",
          "name": "Sandipan Dandapat"
        },
        {
          "authorId": "3010457",
          "name": "Sunayana Sitaram"
        },
        {
          "authorId": "113810201",
          "name": "Vishrav Chaudhary"
        }
      ]
    },
    {
      "paperId": "eac5e5ef2bb1d158645ee8ae8f3e167767316b46",
      "externalIds": {
        "DBLP": "journals/corr/abs-2210-04234",
        "ACL": "2022.coling-1.152",
        "ArXiv": "2210.04234",
        "DOI": "10.48550/arXiv.2210.04234",
        "CorpusId": 252780709
      },
      "url": "https://www.semanticscholar.org/paper/eac5e5ef2bb1d158645ee8ae8f3e167767316b46",
      "title": "Understanding and Improving Zero-shot Multi-hop Reasoning in Generative Question Answering",
      "abstract": "Generative question answering (QA) models generate answers to questions either solely based on the parameters of the model (the closed-book setting) or additionally retrieving relevant evidence (the open-book setting). Generative QA models can answer some relatively complex questions, but the mechanism through which they do so is still poorly understood. We perform several studies aimed at better understanding the multi-hop reasoning capabilities of generative QA models. First, we decompose multi-hop questions into multiple corresponding single-hop questions, and find marked inconsistency in QA models\u2019 answers on these pairs of ostensibly identical question chains. Second, we find that models lack zero-shot multi-hop reasoning ability: when trained only on single-hop questions, models generalize poorly to multi-hop questions. Finally, we demonstrate that it is possible to improve models\u2019 zero-shot multi-hop reasoning capacity through two methods that approximate real multi-hop natural language (NL) questions by training on either concatenation of single-hop questions or logical forms (SPARQL). In sum, these results demonstrate that multi-hop reasoning does not emerge naturally in generative QA models, but can be encouraged by advances in training or modeling techniques. Code is available at https://github.com/jzbjyb/multihop.",
      "year": 2022,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2210.04234",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1765-1775"
      },
      "authors": [
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "50007145",
          "name": "J. Araki"
        },
        {
          "authorId": "47929135",
          "name": "Haibo Ding"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "ee0d1cdd807b54712f25e2060f4eeda8206f3324",
      "externalIds": {
        "DBLP": "journals/corr/abs-2210-05200",
        "ArXiv": "2210.05200",
        "ACL": "2023.eacl-main.119",
        "DOI": "10.48550/arXiv.2210.05200",
        "CorpusId": 252815434
      },
      "url": "https://www.semanticscholar.org/paper/ee0d1cdd807b54712f25e2060f4eeda8206f3324",
      "title": "CTC Alignments Improve Autoregressive Translation",
      "abstract": "Connectionist Temporal Classification (CTC) is a widely used approach for automatic speech recognition (ASR) that performs conditionally independent monotonic alignment. However for translation, CTC exhibits clear limitations due to the contextual and non-monotonic nature of the task and thus lags behind attentional decoder approaches in terms of translation quality. In this work, we argue that CTC does in fact make sense for translation if applied in a joint CTC/attention framework wherein CTC\u2019s core properties can counteract several key weaknesses of pure-attention models during training and decoding. To validate this conjecture, we modify the Hybrid CTC/Attention model originally proposed for ASR to support text-to-text translation (MT) and speech-to-text translation (ST). Our proposed joint CTC/attention models outperform pure-attention baselines across six benchmark translation tasks.",
      "year": 2022,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2210.05200",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "journal": {
        "pages": "1615-1631"
      },
      "authors": [
        {
          "authorId": "2087059555",
          "name": "Brian Yan"
        },
        {
          "authorId": "35186886",
          "name": "Siddharth Dalmia"
        },
        {
          "authorId": "46722767",
          "name": "Yosuke Higuchi"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1740721",
          "name": "Florian Metze"
        },
        {
          "authorId": "1690706",
          "name": "A. Black"
        },
        {
          "authorId": "1746678",
          "name": "Shinji Watanabe"
        }
      ]
    },
    {
      "paperId": "ee5895aa70bb4dbbdda256e23cada2d2ad1b15ae",
      "externalIds": {
        "DBLP": "journals/corr/abs-2210-03575",
        "ArXiv": "2210.03575",
        "ACL": "2022.emnlp-main.617",
        "DOI": "10.48550/arXiv.2210.03575",
        "CorpusId": 252762278
      },
      "url": "https://www.semanticscholar.org/paper/ee5895aa70bb4dbbdda256e23cada2d2ad1b15ae",
      "title": "Are representations built from the ground up? An empirical examination of local composition in language models",
      "abstract": "Compositionality, the phenomenon where the meaning of a phrase can be derived from its constituent parts, is a hallmark of human language. At the same time, many phrases are non-compositional, carrying a meaning beyond that of each part in isolation. Representing both of these types of phrases is critical for language understanding, but it is an open question whether modern language models (LMs) learn to do so; in this work we examine this question. We first formulate a problem of predicting the LM-internal representations of longer phrases given those of their constituents. We find that the representation of a parent phrase can be predicted with some accuracy given an affine transformation of its children. While we would expect the predictive accuracy to correlate with human judgments of semantic compositionality, we find this is largely not the case, indicating that LMs may not accurately distinguish between compositional and non-compositional phrases. We perform a variety of analyses, shedding light on when different varieties of LMs do and do not generate compositional representations, and discuss implications for future modeling work.",
      "year": 2022,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://arxiv.org/pdf/2210.03575",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2210.03575",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1381444447",
          "name": "Emmy Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "f0b8479d2d09dda3a07aa6fc2149dbfcd0f4608b",
      "externalIds": {
        "DBLP": "journals/corr/abs-2210-16886",
        "ArXiv": "2210.16886",
        "DOI": "10.48550/arXiv.2210.16886",
        "CorpusId": 253237066
      },
      "url": "https://www.semanticscholar.org/paper/f0b8479d2d09dda3a07aa6fc2149dbfcd0f4608b",
      "title": "DiffusER: Discrete Diffusion via Edit-based Reconstruction",
      "abstract": "In text generation, models that generate text from scratch one token at a time are currently the dominant paradigm. Despite being performant, these models lack the ability to revise existing text, which limits their usability in many practical scenarios. We look to address this, with DiffusER (Diffusion via Edit-based Reconstruction), a new edit-based generative model for text based on denoising diffusion models -- a class of models that use a Markov chain of denoising steps to incrementally generate data. DiffusER is not only a strong generative model in general, rivalling autoregressive models on several tasks spanning machine translation, summarization, and style transfer; it can also perform other varieties of generation that standard autoregressive models are not well-suited for. For instance, we demonstrate that DiffusER makes it possible for a user to condition generation on a prototype, or an incomplete sequence, and continue revising based on previous edit steps.",
      "year": 2022,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2210.16886",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2210.16886",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1557386977",
          "name": "Machel Reid"
        },
        {
          "authorId": "22747364",
          "name": "V. Hellendoorn"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "f92535edac9d1c735feabdb4d94c1157f12d899c",
      "externalIds": {
        "ArXiv": "2201.12431",
        "DBLP": "conf/icml/0002XHSRN22",
        "CorpusId": 246431219
      },
      "url": "https://www.semanticscholar.org/paper/f92535edac9d1c735feabdb4d94c1157f12d899c",
      "title": "Neuro-Symbolic Language Modeling with Automaton-augmented Retrieval",
      "abstract": "Retrieval-based language models (R-LM) model the probability of natural language text by combining a standard language model (LM) with examples retrieved from an external datastore at test time. While effective, a major bottleneck of using these models in practice is the computationally costly datastore search, which can be performed as frequently as every time step. In this paper, we present RetoMaton - retrieval automaton - which approximates the datastore search, based on (1) saving pointers between consecutive datastore entries, and (2) clustering of entries into\"states\". This effectively results in a weighted finite automaton built on top of the datastore, instead of representing the datastore as a flat list. The creation of the automaton is unsupervised, and a RetoMaton can be constructed from any text collection: either the original training corpus or from another domain. Traversing this automaton at inference time, in parallel to the LM inference, reduces its perplexity by up to 1.85, or alternatively saves up to 83% of the nearest neighbor searches over $k$NN-LM (Khandelwal et al., 2020) without hurting perplexity. Our code and trained models are available at https://github.com/neulab/retomaton .",
      "year": 2022,
      "influentialCitationCount": 4,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "468-485"
      },
      "authors": [
        {
          "authorId": "47051926",
          "name": "Uri Alon"
        },
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "2072419570",
          "name": "Sudipta Sengupta"
        },
        {
          "authorId": "144590225",
          "name": "D. Roth"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "05dd473258250c046541afeceaa2b9b3d1070a07",
      "externalIds": {
        "ACL": "2021.wmt-1.117",
        "DBLP": "journals/corr/abs-2106-11375",
        "ArXiv": "2106.11375",
        "CorpusId": 235593405
      },
      "url": "https://www.semanticscholar.org/paper/05dd473258250c046541afeceaa2b9b3d1070a07",
      "title": "Phrase-level Active Learning for Neural Machine Translation",
      "abstract": "Neural machine translation (NMT) is sensitive to domain shift. In this paper, we address this problem in an active learning setting where we can spend a given budget on translating in-domain data, and gradually fine-tune a pre-trained out-of-domain NMT model on the newly translated data. Existing active learning methods for NMT usually select sentences based on uncertainty scores, but these methods require costly translation of full sentences even when only one or two key phrases within the sentence are informative. To address this limitation, we re-examine previous work from the phrase-based machine translation (PBMT) era that selected not full sentences, but rather individual phrases. However, while incorporating these phrases into PBMT systems was relatively simple, it is less trivial for NMT systems, which need to be trained on full sequences to capture larger structural properties of sentences unique to the new domain. To overcome these hurdles, we propose to select both full sentences and individual phrases from unlabelled data in the new domain for routing to human translators. In a German-English translation task, our active learning approach achieves consistent improvements over uncertainty-based sentence selection methods, improving up to 1.2 BLEU score over strong active learning baselines.",
      "year": 2021,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2106.11375",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "06d21b10e0f85ea38e23f99efd38a770e455863c",
      "externalIds": {
        "ArXiv": "2104.15114",
        "DBLP": "journals/corr/abs-2104-15114",
        "DOI": "10.18653/v1/2022.emnlp-demos.38",
        "CorpusId": 233476516
      },
      "url": "https://www.semanticscholar.org/paper/06d21b10e0f85ea38e23f99efd38a770e455863c",
      "title": "Paraphrastic Representations at Scale",
      "abstract": "We present a system that allows users to train their own state-of-the-art paraphrastic sentence representations in a variety of languages. We also release trained models for English, Arabic, German, French, Spanish, Russian, Turkish, and Chinese. We train these models on large amounts of data, achieving significantly improved performance from the original papers proposing the methods on a suite of monolingual semantic similarity, cross-lingual semantic similarity, and bitext mining tasks. Moreover, the resulting models surpass all prior work on unsupervised semantic textual similarity, significantly outperforming even BERT-based models like Sentence-BERT (Reimers and Gurevych, 2019). Additionally, our models are orders of magnitude faster than prior work and can be used on CPU with little difference in inference speed (even improved speed over GPU when using more CPU cores), making these models an attractive choice for users without access to GPUs or for use on embedded devices. Finally, we add significantly increased functionality to the code bases for training paraphrastic sentence models, easing their use for both inference and for training them for any desired language with parallel data. We also include code to automatically download and preprocess training data.",
      "year": 2021,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2022.emnlp-demos.38.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "379-388"
      },
      "authors": [
        {
          "authorId": "1771118",
          "name": "J. Wieting"
        },
        {
          "authorId": "1700980",
          "name": "Kevin Gimpel"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        }
      ]
    },
    {
      "paperId": "06e36261b21af2943e464a562c92c09dac292a82",
      "externalIds": {
        "DBLP": "conf/uss/ChenLSGNV22",
        "ArXiv": "2108.06363",
        "CorpusId": 237091166
      },
      "url": "https://www.semanticscholar.org/paper/06e36261b21af2943e464a562c92c09dac292a82",
      "title": "Augmenting Decompiler Output with Learned Variable Names and Types",
      "abstract": "A common tool used by security professionals for reverse-engineering binaries found in the wild is the decompiler. A decompiler attempts to reverse compilation, transforming a binary to a higher-level language such as C. High-level languages ease reasoning about programs by providing useful abstractions such as loops, typed variables, and comments, but these abstractions are lost during compilation. Decompilers are able to deterministically reconstruct structural properties of code, but comments, variable names, and custom variable types are technically impossible to recover. In this paper we present DIRTY (DecompIled variable ReTYper), a novel technique for improving the quality of decompiler output that automatically generates meaningful variable names and types. Empirical evaluation on a novel dataset of C code mined from GitHub shows that DIRTY outperforms prior work approaches by a sizable margin, recovering the original names written by developers 66.4% of the time and the original types 75.8% of the time.",
      "year": 2021,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2108.06363",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "50282546",
          "name": "Qibin Chen"
        },
        {
          "authorId": "51119916",
          "name": "Jeremy Lacomis"
        },
        {
          "authorId": "32804915",
          "name": "Edward J. Schwartz"
        },
        {
          "authorId": "2957803",
          "name": "Claire Le Goues"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2434621",
          "name": "Bogdan Vasilescu"
        }
      ]
    },
    {
      "paperId": "0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d",
      "externalIds": {
        "ArXiv": "2109.04212",
        "ACL": "2021.emnlp-main.461",
        "DBLP": "conf/emnlp/HeNB21",
        "DOI": "10.18653/v1/2021.emnlp-main.461",
        "CorpusId": 237452184
      },
      "url": "https://www.semanticscholar.org/paper/0c47eb31b2dd76d8dc986173a1d3f00da1c9c74d",
      "title": "Efficient Nearest Neighbor Language Models",
      "abstract": "Non-parametric neural language models (NLMs) learn predictive distributions of text utilizing an external datastore, which allows them to learn through explicitly memorizing the training datapoints. While effective, these models often require retrieval from a large datastore at test time, significantly increasing the inference overhead and thus limiting the deployment of non-parametric NLMs in practical applications. In this paper, we take the recently proposed k-nearest neighbors language model as an example, exploring methods to improve its efficiency along various dimensions. Experiments on the standard WikiText-103 benchmark and domain-adaptation datasets show that our methods are able to achieve up to a 6x speed-up in inference speed while retaining comparable performance. The empirical analysis we present may provide guidelines for future research seeking to develop or deploy more efficient non-parametric NLMs.",
      "year": 2021,
      "influentialCitationCount": 9,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.emnlp-main.461.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "5703-5714"
      },
      "authors": [
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        }
      ]
    },
    {
      "paperId": "1756376bf7cf0d0a7bec881d663b57907a361ecf",
      "externalIds": {
        "ArXiv": "2101.12087",
        "DBLP": "conf/iclr/YaoXYSN21",
        "MAG": "3119514078",
        "CorpusId": 231719413
      },
      "url": "https://www.semanticscholar.org/paper/1756376bf7cf0d0a7bec881d663b57907a361ecf",
      "title": "Learning Structural Edits via Incremental Tree Transformations",
      "abstract": "While most neural generative models generate outputs in a single pass, the human creative process is usually one of iterative building and refinement. Recent work has proposed models of editing processes, but these mostly focus on editing sequential data and/or only model a single editing pass. In this paper, we present a generic model for incremental editing of structured data (i.e. ''structural edits''). Particularly, we focus on tree-structured data, taking abstract syntax trees of computer programs as our canonical example. Our editor learns to iteratively generate tree edits (e.g. deleting or adding a subtree) and applies them to the partially edited data, thereby the entire editing process can be formulated as consecutive, incremental tree transformations. To show the unique benefits of modeling tree edits directly, we further propose a novel edit encoder for learning to represent edits, as well as an imitation learning method that allows the editor to be more robust. We evaluate our proposed editor on two source code edit datasets, where results show that, with the proposed edit encoder, our editor significantly improves accuracy over previous approaches that generate the edited program directly in one pass. Finally, we demonstrate that training our editor to imitate experts and correct its mistakes dynamically can further improve its performance.",
      "year": 2021,
      "influentialCitationCount": 4,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2101.12087",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "3366595",
          "name": "Ziyu Yao"
        },
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "1515546612",
          "name": "Huan Sun"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "1838cbd2eee0a555ab7e850eff1fce69d62acb95",
      "externalIds": {
        "ArXiv": "2104.07908",
        "MAG": "3167030571",
        "DBLP": "conf/naacl/XiaZMSNA21",
        "ACL": "2021.naacl-main.42",
        "DOI": "10.18653/V1/2021.NAACL-MAIN.42",
        "CorpusId": 233289455
      },
      "url": "https://www.semanticscholar.org/paper/1838cbd2eee0a555ab7e850eff1fce69d62acb95",
      "title": "MetaXL: Meta Representation Transformation for Low-resource Cross-lingual Learning",
      "abstract": "The combination of multilingual pre-trained representations and cross-lingual transfer learning is one of the most effective methods for building functional NLP systems for low-resource languages. However, for extremely low-resource languages without large-scale monolingual corpora for pre-training or sufficient annotated data for fine-tuning, transfer learning remains an understudied and challenging task. Moreover, recent work shows that multilingual representations are surprisingly disjoint across languages, bringing additional challenges for transfer onto extremely low-resource languages. In this paper, we propose MetaXL, a meta-learning based framework that learns to transform representations judiciously from auxiliary languages to a target one and brings their representation spaces closer for effective transfer. Extensive experiments on real-world low-resource languages \u2013 without access to large-scale monolingual corpora or large amounts of labeled data \u2013 for tasks like cross-lingual sentiment analysis and named entity recognition show the effectiveness of our approach. Code for MetaXL is publicly available at github.com/microsoft/MetaXL.",
      "year": 2021,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.naacl-main.42.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2104.07908",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "67284811",
          "name": "Mengzhou Xia"
        },
        {
          "authorId": "2250250",
          "name": "Guoqing Zheng"
        },
        {
          "authorId": "1777140",
          "name": "Subhabrata Mukherjee"
        },
        {
          "authorId": "1703337",
          "name": "Milad Shokouhi"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2072795428",
          "name": "A. Awadallah"
        }
      ]
    },
    {
      "paperId": "18dc7f5937d85128447905f5b8e63e9adfb6a310",
      "externalIds": {
        "DBLP": "journals/corr/abs-2109-07446",
        "ACL": "2023.acl-long.36",
        "ArXiv": "2109.07446",
        "DOI": "10.18653/v1/2023.acl-long.36",
        "CorpusId": 237513464
      },
      "url": "https://www.semanticscholar.org/paper/18dc7f5937d85128447905f5b8e63e9adfb6a310",
      "title": "When Does Translation Require Context? A Data-driven, Multilingual Exploration",
      "abstract": "Although proper handling of discourse significantly contributes to the quality of machine translation (MT), these improvements are not adequately measured in common translation quality metrics. Recent works in context-aware MT attempt to target a small set of discourse phenomena during evaluation, however not in a fully systematic way. In this paper, we develop the Multilingual Discourse-Aware (MuDA) benchmark, a series of taggers that identify and evaluate model performance on discourse phenomena in any given dataset. The choice of phenomena is inspired by a novel methodology to systematically identify translations that require context. This methodology confirms the difficulty of previously studied phenomena while uncovering others which were not previously addressed. We find that commonly studied context-aware MT models make only marginal improvements over context-agnostic models, which suggests these models do not handle these ambiguities effectively. We release code and data for 14 language pairs to encourage the MT community to focus on accurately capturing discourse phenomena. Code available at https://github.com/neulab/contextual-mt",
      "year": 2021,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2023.acl-long.36.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "606-626"
      },
      "authors": [
        {
          "authorId": "1602993448",
          "name": "Kayo Yin"
        },
        {
          "authorId": "2058640028",
          "name": "Patrick Fernandes"
        },
        {
          "authorId": "1400227478",
          "name": "Andr\u00e9 F. T. Martins"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "1cf2e9e198feef3893da2800a7949f6880ddc084",
      "externalIds": {
        "DBLP": "conf/acl/LiuFXYCDLYN21",
        "ACL": "2021.acl-demo.34",
        "ArXiv": "2104.06387",
        "DOI": "10.18653/v1/2021.acl-demo.34",
        "CorpusId": 233219948
      },
      "url": "https://www.semanticscholar.org/paper/1cf2e9e198feef3893da2800a7949f6880ddc084",
      "title": "ExplainaBoard: An Explainable Leaderboard for NLP",
      "abstract": "With the rapid development of NLP research, leaderboards have emerged as one tool to track the performance of various systems on various NLP tasks. They are effective in this goal to some extent, but generally present a rather simplistic one-dimensional view of the submitted systems, communicated only through holistic accuracy numbers. In this paper, we present a new conceptualization and implementation of NLP evaluation: the ExplainaBoard, which in addition to inheriting the functionality of the standard leaderboard, also allows researchers to (i) diagnose strengths and weaknesses of a single system (e.g. what is the best-performing system bad at?) (ii) interpret relationships between multiple systems. (e.g. where does system A outperform system B? What if we combine systems A, B and C?) and (iii) examine prediction results closely (e.g. what are common errors made by multiple systems or in what contexts do particular errors occur?). So far, ExplainaBoard covers more than 400 systems, 50 datasets, 40 languages, and 12 tasks. We not only released an online platform at the website but also make our evaluation tool an API with MIT Licence at Github and PyPi that allows users to conveniently assess their models offline. We additionally release all output files from systems that we have run or collected to motivate \u201coutput-driven\u201d research in the future.",
      "year": 2021,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.acl-demo.34.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "280-289"
      },
      "authors": [
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "41037252",
          "name": "Jinlan Fu"
        },
        {
          "authorId": "2116642640",
          "name": "Yanghua Xiao"
        },
        {
          "authorId": "30300197",
          "name": "Weizhe Yuan"
        },
        {
          "authorId": "46923811",
          "name": "Shuaichen Chang"
        },
        {
          "authorId": "2087363104",
          "name": "Junqi Dai"
        },
        {
          "authorId": "2108176413",
          "name": "Yixin Liu"
        },
        {
          "authorId": "1796245019",
          "name": "Zihuiwen Ye"
        },
        {
          "authorId": "14199369",
          "name": "Zi-Yi Dou"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "1f98c3cc04295bff64b8bc56ea604af675d95c44",
      "externalIds": {
        "DBLP": "conf/acl/0001HCN22",
        "ArXiv": "2111.07393",
        "ACL": "2022.acl-long.123",
        "DOI": "10.18653/v1/2022.acl-long.123",
        "CorpusId": 244117199
      },
      "url": "https://www.semanticscholar.org/paper/1f98c3cc04295bff64b8bc56ea604af675d95c44",
      "title": "DEEP: DEnoising Entity Pre-training for Neural Machine Translation",
      "abstract": "It has been shown that machine translation models usually generate poor translations for named entities that are infrequent in the training corpus. Earlier named entity translation methods mainly focus on phonetic transliteration, which ignores the sentence context for translation and is limited in domain and language coverage. To address this limitation, we propose DEEP, a DEnoising Entity Pre-training method that leverages large amounts of monolingual data and a knowledge base to improve named entity translation accuracy within sentences. Besides, we investigate a multi-task learning strategy that finetunes a pre-trained neural machine translation model on both entity-augmented monolingual data and parallel data to further improve entity translation. Experimental results on three language pairs demonstrate that DEEP results in significant improvements over strong denoising auto-encoding baselines, with a gain of up to 1.3 BLEU and up to 9.2 entity accuracy points for English-Russian translation.",
      "year": 2021,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2022.acl-long.123.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2111.07393",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "145919378",
          "name": "Junjie Hu"
        },
        {
          "authorId": "50376014",
          "name": "Hiroaki Hayashi"
        },
        {
          "authorId": "1979489",
          "name": "Kyunghyun Cho"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "21c9c624bc328686cef4bb1f80a786a5027d8886",
      "externalIds": {
        "ACL": "2021.acl-long.59",
        "ArXiv": "2106.01560",
        "DBLP": "journals/corr/abs-2106-01560",
        "DOI": "10.18653/v1/2021.acl-long.59",
        "CorpusId": 235313529
      },
      "url": "https://www.semanticscholar.org/paper/21c9c624bc328686cef4bb1f80a786a5027d8886",
      "title": "CitationIE: Leveraging the Citation Graph for Scientific Information Extraction",
      "abstract": "Automatically extracting key information from scientific documents has the potential to help scientists work more efficiently and accelerate the pace of scientific progress. Prior work has considered extracting document-level entity clusters and relations end-to-end from raw scientific text, which can improve literature search and help identify methods and materials for a given problem. Despite the importance of this task, most existing works on scientific information extraction (SciIE) consider extraction solely based on the content of an individual paper, without considering the paper\u2019s place in the broader literature. In contrast to prior work, we augment our text representations by leveraging a complementary source of document context: the citation graph of referential links between citing and cited papers. On a test set of English-language scientific documents, we show that simple ways of utilizing the structure and content of the citation graph can each lead to significant gains in different scientific information extraction tasks. When these tasks are combined, we observe a sizable improvement in end-to-end information extraction over the state-of-the-art, suggesting the potential for future work along this direction. We release software tools to facilitate citation-aware SciIE development.",
      "year": 2021,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.acl-long.59.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "719-731"
      },
      "authors": [
        {
          "authorId": "2061499362",
          "name": "Vijay Viswanathan"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        }
      ]
    },
    {
      "paperId": "25efc17ba82ba4af29f2e03868de74e1ea66d025",
      "externalIds": {
        "ArXiv": "2103.08849",
        "MAG": "3171927989",
        "DBLP": "journals/corr/abs-2103-08849",
        "ACL": "2021.naacl-main.195",
        "DOI": "10.18653/V1/2021.NAACL-MAIN.195",
        "CorpusId": 232240219
      },
      "url": "https://www.semanticscholar.org/paper/25efc17ba82ba4af29f2e03868de74e1ea66d025",
      "title": "Multilingual Multimodal Pre-training for Zero-Shot Cross-Lingual Transfer of Vision-Language Models",
      "abstract": "This paper studies zero-shot cross-lingual transfer of vision-language models. Specifically, we focus on multilingual text-to-video search and propose a Transformer-based model that learns contextual multilingual multimodal embeddings. Under a zero-shot setting, we empirically demonstrate that performance degrades significantly when we query the multilingual text-video model with non-English sentences. To address this problem, we introduce a multilingual multimodal pre-training strategy, and collect a new multilingual instructional video dataset (Multi-HowTo100M) for pre-training. Experiments on VTT show that our method significantly improves video search in non-English languages without additional annotations. Furthermore, when multilingual annotations are available, our method outperforms recent baselines by a large margin in multilingual text-to-video search on VTT and VATEX; as well as in multilingual text-to-image search on Multi30K. Our model and Multi-HowTo100M is available at http://github.com/berniebear/Multi-HT100M.",
      "year": 2021,
      "influentialCitationCount": 11,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.naacl-main.195.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2103.08849",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2319973",
          "name": "Po-Yao (Bernie) Huang"
        },
        {
          "authorId": "1379929116",
          "name": "Mandela Patrick"
        },
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1740721",
          "name": "Florian Metze"
        },
        {
          "authorId": "145788702",
          "name": "A. Hauptmann"
        }
      ]
    },
    {
      "paperId": "28692beece311a90f5fa1ca2ec9d0c2ce293d069",
      "externalIds": {
        "DBLP": "journals/csur/LiuYFJHN23",
        "ArXiv": "2107.13586",
        "DOI": "10.1145/3560815",
        "CorpusId": 236493269
      },
      "url": "https://www.semanticscholar.org/paper/28692beece311a90f5fa1ca2ec9d0c2ce293d069",
      "title": "Pre-train, Prompt, and Predict: A Systematic Survey of Prompting Methods in Natural Language Processing",
      "abstract": "This article surveys and organizes research works in a new paradigm in natural language processing, which we dub \u201cprompt-based learning.\u201d Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y|x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x\u2032 that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x\u0302, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: It allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this article, we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g., the choice of pre-trained language models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts but also release other resources, e.g., a website NLPedia\u2013Pretrain including constantly updated survey and paperlist.",
      "year": 2021,
      "influentialCitationCount": 169,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://dl.acm.org/doi/pdf/10.1145/3560815",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "55",
        "pages": "1 - 35",
        "name": "ACM Computing Surveys"
      },
      "authors": [
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "30300197",
          "name": "Weizhe Yuan"
        },
        {
          "authorId": "41037252",
          "name": "Jinlan Fu"
        },
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "50376014",
          "name": "Hiroaki Hayashi"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "29263fa3632951be0ca617988d7c9ce651e74393",
      "externalIds": {
        "ACL": "2022.findings-acl.218",
        "ArXiv": "2110.08130",
        "DBLP": "journals/corr/abs-2110-08130",
        "DOI": "10.18653/v1/2022.findings-acl.218",
        "CorpusId": 235416074
      },
      "url": "https://www.semanticscholar.org/paper/29263fa3632951be0ca617988d7c9ce651e74393",
      "title": "Breaking Down Multilingual Machine Translation",
      "abstract": "While multilingual training is now an essential ingredient in machine translation (MT) systems, recent work has demonstrated that it has different effects in different multilingual settings, such as many-to-one, one-to-many, and many-to-many learning. These training settings expose the encoder and the decoder in a machine translation model with different data distributions. In this paper, we examine how different varieties of multilingual training contribute to learning these two components of the MT model. Specifically, we compare bilingual models with encoders and/or decoders initialized by multilingual training. We show that multilingual training is beneficial to encoders in general, while it only benefits decoders for low-resource languages (LRLs). We further find the important attention heads for each language pair and compare their correlations during inference. Our analysis sheds light on how multilingual translation models work and also enables us to propose methods to improve performance by training with highly related languages. Our many-to-one models for high-resource languages and one-to-many models for LRL outperform the best results reported by Aharoni et al. (2019).",
      "year": 2021,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2022.findings-acl.218.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2766-2780"
      },
      "authors": [
        {
          "authorId": "153033862",
          "name": "Ting-Rui Chiang"
        },
        {
          "authorId": "2109381394",
          "name": "Yi-Pei Chen"
        },
        {
          "authorId": "47999368",
          "name": "Yi-Ting Yeh"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "2b9762e91305986ac8a2d624d0a69521304405f3",
      "externalIds": {
        "ArXiv": "2104.07412",
        "ACL": "2021.emnlp-main.802",
        "DBLP": "journals/corr/abs-2104-07412",
        "DOI": "10.18653/v1/2021.emnlp-main.802",
        "CorpusId": 233241004
      },
      "url": "https://www.semanticscholar.org/paper/2b9762e91305986ac8a2d624d0a69521304405f3",
      "title": "XTREME-R: Towards More Challenging and Nuanced Multilingual Evaluation",
      "abstract": "Machine learning has brought striking advances in multilingual natural language processing capabilities over the past year. For example, the latest techniques have improved the state-of-the-art performance on the XTREME multilingual benchmark by more than 13 points. While a sizeable gap to human-level performance remains, improvements have been easier to achieve in some tasks than in others. This paper analyzes the current state of cross-lingual transfer learning and summarizes some lessons learned. In order to catalyze meaningful progress, we extend XTREME to XTREME-R, which consists of an improved set of ten natural language understanding tasks, including challenging language-agnostic retrieval tasks, and covers 50 typologically diverse languages. In addition, we provide a massively multilingual diagnostic suite and fine-grained multi-dataset evaluation capabilities through an interactive public leaderboard to gain a better understanding of such models.",
      "year": 2021,
      "influentialCitationCount": 23,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.emnlp-main.802.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2104.07412",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2884561",
          "name": "Sebastian Ruder"
        },
        {
          "authorId": "40832517",
          "name": "Noah Constant"
        },
        {
          "authorId": "35025872",
          "name": "Jan A. Botha"
        },
        {
          "authorId": "9356387",
          "name": "Aditya Siddhant"
        },
        {
          "authorId": "2345617",
          "name": "Orhan Firat"
        },
        {
          "authorId": "41037252",
          "name": "Jinlan Fu"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145657834",
          "name": "Melvin Johnson"
        }
      ]
    },
    {
      "paperId": "3ea8767b852253e2636b6e57925be7fcc1d739df",
      "externalIds": {
        "ArXiv": "2110.06733",
        "DBLP": "journals/corr/abs-2110-06733",
        "ACL": "2022.acl-long.376",
        "DOI": "10.18653/v1/2022.acl-long.376",
        "CorpusId": 238744120
      },
      "url": "https://www.semanticscholar.org/paper/3ea8767b852253e2636b6e57925be7fcc1d739df",
      "title": "Systematic Inequalities in Language Technology Performance across the World\u2019s Languages",
      "abstract": "Natural language processing (NLP) systems have become a central technology in communication, education, medicine, artificial intelligence, and many other domains of research and development. While the performance of NLP methods has grown enormously over the last decade, this progress has been restricted to a minuscule subset of the world\u2019s \\approx6,500 languages. We introduce a framework for estimating the global utility of language technologies as revealed in a comprehensive snapshot of recent publications in NLP. Our analyses involve the field at large, but also more in-depth studies on both user-facing technologies (machine translation, language understanding, question answering, text-to-speech synthesis) as well as foundational NLP tasks (dependency parsing, morphological inflection). In the process, we (1) quantify disparities in the current state of NLP research, (2) explore some of its associated societal and academic factors, and (3) produce tailored recommendations for evidence-based policy making aimed at promoting more global and equitable language technologies. Data and code to reproduce the findings discussed in this paper areavailable on GitHub (https://github.com/neubig/globalutility).",
      "year": 2021,
      "influentialCitationCount": 5,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2022.acl-long.376.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2110.06733",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "6894443",
          "name": "Dami\u00e1n E. Blasi"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "43a87867fe6bf4eb920f97fc753be4b727308923",
      "externalIds": {
        "DBLP": "journals/corr/abs-2110-04366",
        "ArXiv": "2110.04366",
        "CorpusId": 238583580
      },
      "url": "https://www.semanticscholar.org/paper/43a87867fe6bf4eb920f97fc753be4b727308923",
      "title": "Towards a Unified View of Parameter-Efficient Transfer Learning",
      "abstract": "Fine-tuning large pre-trained language models on downstream tasks has become the de-facto learning paradigm in NLP. However, conventional approaches fine-tune all the parameters of the pre-trained model, which becomes prohibitive as the model size and the number of tasks grow. Recent work has proposed a variety of parameter-efficient transfer learning methods that only fine-tune a small number of (extra) parameters to attain strong performance. While effective, the critical ingredients for success and the connections among the various methods are poorly understood. In this paper, we break down the design of state-of-the-art parameter-efficient transfer learning methods and present a unified framework that establishes connections between them. Specifically, we re-frame them as modifications to specific hidden states in pre-trained models, and define a set of design dimensions along which different methods vary, such as the function to compute the modification and the position to apply the modification. Through comprehensive empirical studies across machine translation, text summarization, language understanding, and text classification benchmarks, we utilize the unified view to identify important design choices in previous methods. Furthermore, our unified framework enables the transfer of design elements across different approaches, and as a result we are able to instantiate new parameter-efficient fine-tuning methods that tune less parameters than previous methods while being more effective, achieving comparable results to fine-tuning all parameters on all four tasks.",
      "year": 2021,
      "influentialCitationCount": 91,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2110.04366",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "2110714400",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "2378954",
          "name": "Xuezhe Ma"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "47c740e858d3dfec0bf95104600851a8a2bec9ff",
      "externalIds": {
        "DBLP": "conf/icse/ChenLSNVG22",
        "ArXiv": "2112.02650",
        "DOI": "10.1145/3510003.3510162",
        "CorpusId": 244909169
      },
      "url": "https://www.semanticscholar.org/paper/47c740e858d3dfec0bf95104600851a8a2bec9ff",
      "title": "VarCLR: Variable Semantic Representation Pre-training via Contrastive Learning",
      "abstract": "Variable names are critical for conveying intended program behavior. Machine learning-based program analysis methods use variable name representations for a wide range of tasks, such as suggesting new variable names and bug detection. Ideally, such methods could capture semantic relationships between names beyond syntactic similarity, e.g., the fact that the names average and mean are similar. Unfortunately, previous work has found that even the best of previous representation approaches primarily capture \u201crelatedness\u201d (whether two variables are linked at all), rather than \u201csimilarity\u201d (whether they actually have the same meaning). We propose Varclr, a new approach for learning semantic representations of variable names that effectively captures variable similarity in this stricter sense. We observe that this problem is an excellent fit for contrastive learning, which aims to minimize the distance between explicitly similar inputs, while maximizing the distance between dissimilar inputs. This requires labeled training data, and thus we construct a novel, weakly-supervised variable renaming dataset mined from GitHub edits. We show that Varclr enables the effective application of sophisticated, general-purpose language models like BERT, to variable name representation and thus also to related downstream tasks like variable name similarity search or spelling correction. Varclr produces models that significantly outperform the state-of-the-art on IDBENCH, an existing benchmark that explicitly captures variable similarity (as distinct from relatedness). Finally, we contribute a release of all data, code, and pre-trained models, aiming to provide a drop-in replacement for variable representations used in either existing or future program analyses that rely on variable names.",
      "year": 2021,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://dl.acm.org/doi/pdf/10.1145/3510003.3510162",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2327-2339",
        "name": "2022 IEEE/ACM 44th International Conference on Software Engineering (ICSE)"
      },
      "authors": [
        {
          "authorId": "50282546",
          "name": "Qibin Chen"
        },
        {
          "authorId": "51119916",
          "name": "Jeremy Lacomis"
        },
        {
          "authorId": "32804915",
          "name": "Edward J. Schwartz"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2434621",
          "name": "Bogdan Vasilescu"
        },
        {
          "authorId": "2957803",
          "name": "Claire Le Goues"
        }
      ]
    },
    {
      "paperId": "4a160efbe80c38cd5eb2f92c7c095b49b113397d",
      "externalIds": {
        "ArXiv": "2101.11149",
        "DBLP": "journals/tosem/XuVN22",
        "DOI": "10.1145/3487569",
        "CorpusId": 231718679
      },
      "url": "https://www.semanticscholar.org/paper/4a160efbe80c38cd5eb2f92c7c095b49b113397d",
      "title": "In-IDE Code Generation from Natural Language: Promise and Challenges",
      "abstract": "A great part of software development involves conceptualizing or communicating the underlying procedures and logic that needs to be expressed in programs. One major difficulty of programming is turning concept into code, especially when dealing with the APIs of unfamiliar libraries. Recently, there has been a proliferation of machine learning methods for code generation and retrieval from natural language queries, but these have primarily been evaluated purely based on retrieval accuracy or overlap of generated code with developer-written code, and the actual effect of these methods on the developer workflow is surprisingly unattested. In this article, we perform the first comprehensive investigation of the promise and challenges of using such technology inside the PyCharm IDE, asking, \u201cAt the current state of technology does it improve developer productivity or accuracy, how does it affect the developer experience, and what are the remaining gaps and challenges?\u201d To facilitate the study, we first develop a plugin for the PyCharm IDE that implements a hybrid of code generation and code retrieval functionality, and we orchestrate virtual environments to enable collection of many user events (e.g., web browsing, keystrokes, fine-grained code edits). We ask developers with various backgrounds to complete 7 varieties of 14 Python programming tasks ranging from basic file manipulation to machine learning or data visualization, with or without the help of the plugin. While qualitative surveys of developer experience are largely positive, quantitative results with regards to increased productivity, code quality, or program correctness are inconclusive. Further analysis identifies several pain points that could improve the effectiveness of future machine learning-based code generation/retrieval developer assistants and demonstrates when developers prefer code generation over code retrieval and vice versa. We release all data and software to pave the road for future empirical studies on this topic, as well as development of better code generation models.",
      "year": 2021,
      "influentialCitationCount": 9,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://dl.acm.org/doi/pdf/10.1145/3487569",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "31",
        "pages": "1 - 47",
        "name": "ACM Transactions on Software Engineering and Methodology (TOSEM)"
      },
      "authors": [
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "2434621",
          "name": "Bogdan Vasilescu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "4e77a4d4bcc09f6b2f3bcb790d348f4dfdbf427b",
      "externalIds": {
        "ArXiv": "2109.07437",
        "DBLP": "journals/corr/abs-2109-07437",
        "CorpusId": 237513568
      },
      "url": "https://www.semanticscholar.org/paper/4e77a4d4bcc09f6b2f3bcb790d348f4dfdbf427b",
      "title": "Should We Be Pre-training? An Argument for End-task Aware Training as an Alternative",
      "abstract": "In most settings of practical concern, machine learning practitioners know in advance what end-task they wish to boost with auxiliary tasks. However, widely used methods for leveraging auxiliary data like pre-training and its continued-pretraining variant are end-task agnostic: they rarely, if ever, exploit knowledge of the target task. We study replacing end-task agnostic continued training of pre-trained language models with end-task aware training of said models. We argue that for sufficiently important end-tasks, the benefits of leveraging auxiliary data in a task-aware fashion can justify forgoing the traditional approach of obtaining generic, end-task agnostic representations as with (continued) pre-training. On three different low-resource NLP tasks from two domains, we demonstrate that multi-tasking the end-task and auxiliary objectives results in significantly better downstream task performance than the widely-used task-agnostic continued pre-training paradigm of Gururangan et al. (2020). We next introduce an online meta-learning algorithm that learns a set of multi-task weights to better balance among our multiple auxiliary objectives, achieving further improvements on end-task performance and data efficiency.",
      "year": 2021,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2109.07437",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "32273391",
          "name": "L. Dery"
        },
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "145532827",
          "name": "Ameet Talwalkar"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "59cbd70b9bcdb4a15915c11b47bfcd93319d82c6",
      "externalIds": {
        "DBLP": "conf/aaai/AroraPSCLN22",
        "ArXiv": "2112.09669",
        "DOI": "10.1609/aaai.v36i5.20464",
        "CorpusId": 245329778
      },
      "url": "https://www.semanticscholar.org/paper/59cbd70b9bcdb4a15915c11b47bfcd93319d82c6",
      "title": "Explain, Edit, and Understand: Rethinking User Study Design for Evaluating Model Explanations",
      "abstract": "In attempts to \"explain\" predictions of machine learning models, researchers have proposed hundreds of techniques for attributing predictions to features that are deemed important. While these attributions are often claimed to hold the potential to improve human \"understanding\" of the models, surprisingly little work explicitly evaluates progress towards this aspiration. In this paper, we conduct a crowdsourcing study, where participants interact with deception detection models that have been trained to distinguish between genuine and fake hotel reviews. They are challenged both to simulate the model on fresh reviews, and to edit reviews with the goal of lowering the probability of the originally predicted class. Successful manipulations would lead to an adversarial example. During the training (but not the test) phase, input spans are highlighted to communicate salience. Through our evaluation, we observe that for a linear bag-of-words model, participants with access to the feature coefficients during training are able to cause a larger reduction in model confidence in the testing phase when compared to the no-explanation control. For the BERT-based classifier, popular local explanations do not improve their ability to reduce the model confidence over the no-explanation case. Remarkably, when the explanation for the BERT model is given by the (global) attributions of a linear model trained to imitate the BERT model, people can effectively manipulate the model.",
      "year": 2021,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/20464/20223",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "5277-5285"
      },
      "authors": [
        {
          "authorId": "72401599",
          "name": "Siddhant Arora"
        },
        {
          "authorId": "2064506371",
          "name": "Danish Pruthi"
        },
        {
          "authorId": "2464164",
          "name": "N. Sadeh"
        },
        {
          "authorId": "50056360",
          "name": "William W. Cohen"
        },
        {
          "authorId": "32219137",
          "name": "Zachary Chase Lipton"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "5ede529879d162d2779d410a5775d3f6cd6be3f4",
      "externalIds": {
        "DBLP": "journals/corr/abs-2103-10282",
        "ArXiv": "2103.10282",
        "CorpusId": 232269775
      },
      "url": "https://www.semanticscholar.org/paper/5ede529879d162d2779d410a5775d3f6cd6be3f4",
      "title": "Modeling the Second Player in Distributionally Robust Optimization",
      "abstract": "Distributionally robust optimization (DRO) provides a framework for training machine learning models that are able to perform well on a collection of related data distributions (the\"uncertainty set\"). This is done by solving a min-max game: the model is trained to minimize its maximum expected loss among all distributions in the uncertainty set. While careful design of the uncertainty set is critical to the success of the DRO procedure, previous work has been limited to relatively simple alternatives that keep the min-max optimization problem exactly tractable, such as $f$-divergence balls. In this paper, we argue instead for the use of neural generative models to characterize the worst-case distribution, allowing for more flexible and problem-specific selection of the uncertainty set. However, while simple conceptually, this approach poses a number of implementation and optimization challenges. To circumvent these issues, we propose a relaxation of the KL-constrained inner maximization objective that makes the DRO problem more amenable to gradient-based optimization of large scale generative models, and develop model selection heuristics to guide hyper-parameter search. On both toy settings and realistic NLP tasks, we find that the proposed approach yields models that are more robust than comparable baselines.",
      "year": 2021,
      "influentialCitationCount": 5,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2103.10282",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "3056528",
          "name": "Tatsunori B. Hashimoto"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "69817744fc9c4ba8062c780d295983903194d20e",
      "externalIds": {
        "ArXiv": "2105.07476",
        "DBLP": "journals/corr/abs-2105-07476",
        "ACL": "2021.mtsummit-at4ssl.1",
        "CorpusId": 234742622
      },
      "url": "https://www.semanticscholar.org/paper/69817744fc9c4ba8062c780d295983903194d20e",
      "title": "Data Augmentation for Sign Language Gloss Translation",
      "abstract": "Sign language translation (SLT) is often decomposed into video-to-gloss recognition and gloss to-text translation, where a gloss is a sequence of transcribed spoken-language words in the order in which they are signed. We focus here on gloss-to-text translation, which we treat as a low-resource neural machine translation (NMT) problem. However, unlike traditional low resource NMT, gloss-to-text translation differs because gloss-text pairs often have a higher lexical overlap and lower syntactic overlap than pairs of spoken languages. We exploit this lexical overlap and handle syntactic divergence by proposing two rule-based heuristics that generate pseudo-parallel gloss-text pairs from monolingual spoken language text. By pre-training on this synthetic data, we improve translation from American Sign Language (ASL) to English and German Sign Language (DGS) to German by up to 3.14 and 2.20 BLEU, respectively.",
      "year": 2021,
      "influentialCitationCount": 5,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2105.07476",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "73769459",
          "name": "Amit Moryossef"
        },
        {
          "authorId": "1602993448",
          "name": "Kayo Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "79775260",
          "name": "Yoav Goldberg"
        }
      ]
    },
    {
      "paperId": "6a9394e5d49c1251c0fb6d7fb0c0813d26c6a907",
      "externalIds": {
        "ACL": "2022.acl-long.103",
        "ArXiv": "2110.08381",
        "DBLP": "journals/corr/abs-2110-08381",
        "DOI": "10.18653/v1/2022.acl-long.103",
        "CorpusId": 239016682
      },
      "url": "https://www.semanticscholar.org/paper/6a9394e5d49c1251c0fb6d7fb0c0813d26c6a907",
      "title": "On The Ingredients of an Effective Zero-shot Semantic Parser",
      "abstract": "Semantic parsers map natural language utterances into meaning representations (e.g., programs). Such models are typically bottlenecked by the paucity of training data due to the required laborious annotation efforts. Recent studies have performed zero-shot learning by synthesizing training examples of canonical utterances and programs from a grammar, and further paraphrasing these utterances to improve linguistic diversity. However, such synthetic examples cannot fully capture patterns in real data. In this paper we analyze zero-shot parsers through the lenses of the language and logical gaps (Herzig and Berant, 2019), which quantify the discrepancy of language and programmatic patterns between the canonical examples and real-world user-issued ones. We propose bridging these gaps using improved grammars, stronger paraphrasers, and efficient learning methods using canonical examples that most likely reflect real user intents. Our model achieves strong performance on two semantic parsing benchmarks (Scholar, Geo) with zero labeled data.",
      "year": 2021,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2022.acl-long.103.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2110.08381",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2055975657",
          "name": "P. Yin"
        },
        {
          "authorId": "1771118",
          "name": "J. Wieting"
        },
        {
          "authorId": "2707234",
          "name": "Avirup Sil"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "714fc6626c527e05f2a31626d067d46520c6740e",
      "externalIds": {
        "ArXiv": "2106.07171",
        "DBLP": "journals/corr/abs-2106-07171",
        "CorpusId": 235421683
      },
      "url": "https://www.semanticscholar.org/paper/714fc6626c527e05f2a31626d067d46520c6740e",
      "title": "Examining and Combating Spurious Features under Distribution Shift",
      "abstract": "A central goal of machine learning is to learn robust representations that capture the causal relationship between inputs features and output labels. However, minimizing empirical risk over finite or biased datasets often results in models latching on to spurious correlations between the training input/output pairs that are not fundamental to the problem at hand. In this paper, we define and analyze robust and spurious representations using the information-theoretic concept of minimal sufficient statistics. We prove that even when there is only bias of the input distribution (i.e. covariate shift), models can still pick up spurious features from their training data. Group distributionally robust optimization (DRO) provides an effective tool to alleviate covariate shift by minimizing the worst-case training loss over a set of pre-defined groups. Inspired by our analysis, we demonstrate that group DRO can fail when groups do not directly account for various spurious correlations that occur in the data. To address this, we further propose to minimize the worst-case losses over a more flexible set of distributions that are defined on the joint distribution of groups and instances, instead of treating each group as a whole at optimization time. Through extensive experiments on one image and two language tasks, we show that our model is significantly more robust than comparable baselines under various partitions. Our code is available at https://github.com/violet-zct/group-conditional-DRO.",
      "year": 2021,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2106.07171",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2110714400",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "2378954",
          "name": "Xuezhe Ma"
        },
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "72d89aa7cd77c3f22a667f2b0707758eb8d52a7a",
      "externalIds": {
        "DBLP": "conf/acl/YinFPCMN20",
        "ArXiv": "2105.06977",
        "ACL": "2021.acl-long.65",
        "DOI": "10.18653/v1/2021.acl-long.65",
        "CorpusId": 234680504
      },
      "url": "https://www.semanticscholar.org/paper/72d89aa7cd77c3f22a667f2b0707758eb8d52a7a",
      "title": "Do Context-Aware Translation Models Pay the Right Attention?",
      "abstract": "Context-aware machine translation models are designed to leverage contextual information, but often fail to do so. As a result, they inaccurately disambiguate pronouns and polysemous words that require context for resolution. In this paper, we ask several questions: What contexts do human translators use to resolve ambiguous words? Are models paying large amounts of attention to the same context? What if we explicitly train them to do so? To answer these questions, we introduce SCAT (Supporting Context for Ambiguous Translations), a new English-French dataset comprising supporting context words for 14K translations that professional translators found useful for pronoun disambiguation. Using SCAT, we perform an in-depth analysis of the context used to disambiguate, examining positional and lexical characteristics of the supporting words. Furthermore, we measure the degree of alignment between the model\u2019s attention scores and the supporting context from SCAT, and apply a guided attention strategy to encourage agreement between the two.",
      "year": 2021,
      "influentialCitationCount": 7,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.acl-long.65.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "788-801"
      },
      "authors": [
        {
          "authorId": "1602993448",
          "name": "Kayo Yin"
        },
        {
          "authorId": "2058640028",
          "name": "Patrick Fernandes"
        },
        {
          "authorId": "2064506371",
          "name": "Danish Pruthi"
        },
        {
          "authorId": "51250894",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "145644643",
          "name": "Andr\u00e9 F. T. Martins"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "7b5b15279e5a52439614f886b79fa33f4b88bfb2",
      "externalIds": {
        "DBLP": "conf/emnlp/WangTRN21",
        "ArXiv": "2109.04877",
        "DOI": "10.18653/v1/2021.findings-emnlp.63",
        "CorpusId": 237485466
      },
      "url": "https://www.semanticscholar.org/paper/7b5b15279e5a52439614f886b79fa33f4b88bfb2",
      "title": "Efficient Test Time Adapter Ensembling for Low-resource Language Varieties",
      "abstract": "Adapters are light-weight modules that allow parameter-efficient fine-tuning of pretrained models. Specialized language and task adapters have recently been proposed to facilitate cross-lingual transfer of multilingual pretrained models (Pfeiffer et al., 2020b). However, this approach requires training a separate language adapter for every language one wishes to support, which can be impractical for languages with limited data. An intuitive solution is to use a related language adapter for the new language variety, but we observe that this solution can lead to sub-optimal performance. In this paper, we aim to improve the robustness of language adapters to uncovered languages without training new adapters. We find that ensembling multiple existing language adapters makes the fine-tuned model significantly more robust to other language varieties not included in these adapters. Building upon this observation, we propose Entropy Minimized Ensemble of Adapters (EMEA), a method that optimizes the ensemble weights of the pretrained language adapters for each test sentence by minimizing the entropy of its predictions. Experiments on three diverse groups of language varieties show that our method leads to significant improvements on both named entity recognition and part-of-speech tagging across all languages.",
      "year": 2021,
      "influentialCitationCount": 8,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.findings-emnlp.63.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "730-737"
      },
      "authors": [
        {
          "authorId": null,
          "name": "Xinyi Wang"
        },
        {
          "authorId": "145317727",
          "name": "Yulia Tsvetkov"
        },
        {
          "authorId": "2124014463",
          "name": "Sebastian Ruder"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "821532ecef5bc2252823b190c35f1e4c44ddc41c",
      "externalIds": {
        "ArXiv": "2101.08231",
        "DBLP": "journals/corr/abs-2101-08231",
        "ACL": "2021.eacl-main.181",
        "DOI": "10.18653/v1/2021.eacl-main.181",
        "CorpusId": 231648372
      },
      "url": "https://www.semanticscholar.org/paper/821532ecef5bc2252823b190c35f1e4c44ddc41c",
      "title": "Word Alignment by Fine-tuning Embeddings on Parallel Corpora",
      "abstract": "Word alignment over parallel corpora has a wide variety of applications, including learning translation lexicons, cross-lingual transfer of language processing tools, and automatic evaluation or analysis of translation outputs. The great majority of past work on word alignment has worked by performing unsupervised learning on parallel text. Recently, however, other work has demonstrated that pre-trained contextualized word embeddings derived from multilingually trained language models (LMs) prove an attractive alternative, achieving competitive results on the word alignment task even in the absence of explicit training on parallel data. In this paper, we examine methods to marry the two approaches: leveraging pre-trained LMs but fine-tuning them on parallel text with objectives designed to improve alignment quality, and proposing methods to effectively extract alignments from these fine-tuned models. We perform experiments on five language pairs and demonstrate that our model can consistently outperform previous state-of-the-art models of all varieties. In addition, we demonstrate that we are able to train multilingual word aligners that can obtain robust performance on different language pairs.",
      "year": 2021,
      "influentialCitationCount": 28,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.eacl-main.181.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2112-2128"
      },
      "authors": [
        {
          "authorId": "14199369",
          "name": "Zi-Yi Dou"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "83145b7a391b792e24d8d38f74ed6b6ae7a149dc",
      "externalIds": {
        "ArXiv": "2105.03482",
        "DBLP": "conf/acl/FernandesYNM20",
        "ACL": "2021.acl-long.505",
        "DOI": "10.18653/v1/2021.acl-long.505",
        "CorpusId": 234342704
      },
      "url": "https://www.semanticscholar.org/paper/83145b7a391b792e24d8d38f74ed6b6ae7a149dc",
      "title": "Measuring and Increasing Context Usage in Context-Aware Machine Translation",
      "abstract": "Recent work in neural machine translation has demonstrated both the necessity and feasibility of using inter-sentential context, context from sentences other than those currently being translated. However, while many current methods present model architectures that theoretically can use this extra context, it is often not clear how much they do actually utilize it at translation time. In this paper, we introduce a new metric, conditional cross-mutual information, to quantify usage of context by these models. Using this metric, we measure how much document-level machine translation systems use particular varieties of context. We find that target context is referenced more than source context, and that including more context has a diminishing affect on results. We then introduce a new, simple training method, context-aware word dropout, to increase the usage of context by context-aware models. Experiments show that our method not only increases context usage, but also improves the translation quality according to metrics such as BLEU and COMET, as well as performance on anaphoric pronoun resolution and lexical cohesion contrastive datasets.",
      "year": 2021,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.acl-long.505.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2105.03482",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2058640028",
          "name": "Patrick Fernandes"
        },
        {
          "authorId": "1602993448",
          "name": "Kayo Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145644643",
          "name": "Andr\u00e9 F. T. Martins"
        }
      ]
    },
    {
      "paperId": "8512718bafa447f9b433da9e809215dfc28b6b28",
      "externalIds": {
        "ArXiv": "2102.05486",
        "DBLP": "conf/eacl/YeLFN21",
        "ACL": "2021.eacl-main.324",
        "DOI": "10.18653/v1/2021.eacl-main.324",
        "CorpusId": 231861672
      },
      "url": "https://www.semanticscholar.org/paper/8512718bafa447f9b433da9e809215dfc28b6b28",
      "title": "Towards More Fine-grained and Reliable NLP Performance Prediction",
      "abstract": "Performance prediction, the task of estimating a system\u2019s performance without performing experiments, allows us to reduce the experimental burden caused by the combinatorial explosion of different datasets, languages, tasks, and models. In this paper, we make two contributions to improving performance prediction for NLP tasks. First, we examine performance predictors not only for holistic measures of accuracy like F1 or BLEU, but also fine-grained performance measures such as accuracy over individual classes of examples. Second, we propose methods to understand the reliability of a performance prediction model from two angles: confidence intervals and calibration. We perform an analysis of four types of NLP tasks, and both demonstrate the feasibility of fine-grained performance prediction and the necessity to perform reliability analysis for performance prediction methods in the future.",
      "year": 2021,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.eacl-main.324.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2102.05486",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1796245019",
          "name": "Zihuiwen Ye"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "41037252",
          "name": "Jinlan Fu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "879d8318be60d36d0132ef1181f265afddcdcf21",
      "externalIds": {
        "ArXiv": "2111.02622",
        "DBLP": "journals/corr/abs-2111-02622",
        "DOI": "10.1162/tacl_a_00427",
        "CorpusId": 242757756
      },
      "url": "https://www.semanticscholar.org/paper/879d8318be60d36d0132ef1181f265afddcdcf21",
      "title": "Lexically Aware Semi-Supervised Learning for OCR Post-Correction",
      "abstract": "Abstract Much of the existing linguistic data in many languages of the world is locked away in non- digitized books and documents. Optical character recognition (OCR) can be used to produce digitized text, and previous work has demonstrated the utility of neural post-correction methods that improve the results of general- purpose OCR systems on recognition of less- well-resourced languages. However, these methods rely on manually curated post- correction data, which are relatively scarce compared to the non-annotated raw images that need to be digitized. In this paper, we present a semi-supervised learning method that makes it possible to utilize these raw images to improve performance, specifically through the use of self-training, a technique where a model is iteratively trained on its own outputs. In addition, to enforce consistency in the recognized vocabulary, we introduce a lexically aware decoding method that augments the neural post-correction model with a count-based language model constructed from the recognized texts, implemented using weighted finite-state automata (WFSA) for efficient and effective decoding. Results on four endangered languages demonstrate the utility of the proposed method, with relative error reductions of 15%\u201329%, where we find the combination of self-training and lexically aware decoding essential for achieving consistent improvements.1",
      "year": 2021,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00427/1974763/tacl_a_00427.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "9",
        "pages": "1285-1302",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "102758386",
          "name": "Daisy Rosenblum"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "88dbde378e9ac5c25fc7d78f5da147223e8d34d4",
      "externalIds": {
        "ACL": "2021.naacl-main.225",
        "MAG": "3168664364",
        "DBLP": "conf/naacl/YinFNPPSTA21",
        "DOI": "10.18653/V1/2021.NAACL-MAIN.225",
        "CorpusId": 235097473
      },
      "url": "https://www.semanticscholar.org/paper/88dbde378e9ac5c25fc7d78f5da147223e8d34d4",
      "title": "Compositional Generalization for Neural Semantic Parsing via Span-level Supervised Attention",
      "abstract": "We describe a span-level supervised attention loss that improves compositional generalization in semantic parsers. Our approach builds on existing losses that encourage attention maps in neural sequence-to-sequence models to imitate the output of classical word alignment algorithms. Where past work has used word-level alignments, we focus on spans; borrowing ideas from phrase-based machine translation, we align subtrees in semantic parses to spans of input sentences, and encourage neural attention mechanisms to mimic these alignments. This method improves the performance of transformers, RNNs, and structured decoders on three benchmarks of compositional generalization.",
      "year": 2021,
      "influentialCitationCount": 7,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.naacl-main.225.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2810-2823"
      },
      "authors": [
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "145204655",
          "name": "Hao Fang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "13461242",
          "name": "Adam Pauls"
        },
        {
          "authorId": "144888672",
          "name": "Emmanouil Antonios Platanios"
        },
        {
          "authorId": "1758652",
          "name": "Yu Su"
        },
        {
          "authorId": "2140419664",
          "name": "Sam Thomson"
        },
        {
          "authorId": "2112400",
          "name": "Jacob Andreas"
        }
      ]
    },
    {
      "paperId": "8b20173b98914f36302389e4c761c334fe867dcd",
      "externalIds": {
        "ACL": "2021.emnlp-main.570",
        "DBLP": "journals/corr/abs-2103-16590",
        "ArXiv": "2103.16590",
        "DOI": "10.18653/v1/2021.emnlp-main.570",
        "CorpusId": 232428003
      },
      "url": "https://www.semanticscholar.org/paper/8b20173b98914f36302389e4c761c334fe867dcd",
      "title": "Evaluating the Morphosyntactic Well-formedness of Generated Texts",
      "abstract": "Text generation systems are ubiquitous in natural language processing applications. However, evaluation of these systems remains a challenge, especially in multilingual settings. In this paper, we propose L\u2019AMBRE \u2013 a metric to evaluate the morphosyntactic well-formedness of text using its dependency parse and morphosyntactic rules of the language. We present a way to automatically extract various rules governing morphosyntax directly from dependency treebanks. To tackle the noisy outputs from text generation systems, we propose a simple methodology to train robust parsers. We show the effectiveness of our metric on the task of machine translation through a diachronic study of systems translating into morphologically-rich languages.",
      "year": 2021,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.emnlp-main.570.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2103.16590",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "51132476",
          "name": "Adithya Pratapa"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "51250894",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "3407646",
          "name": "David R. Mortensen"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145317727",
          "name": "Yulia Tsvetkov"
        }
      ]
    },
    {
      "paperId": "8ea737a2e35da3ebb20ade656882300ae57b816b",
      "externalIds": {
        "ArXiv": "2109.08214",
        "CorpusId": 237562868
      },
      "url": "https://www.semanticscholar.org/paper/8ea737a2e35da3ebb20ade656882300ae57b816b",
      "title": "Procedures as Programs: Hierarchical Control of Situated Agents through Natural Language",
      "abstract": "When humans conceive how to perform a particular task, they do so hierarchically: splitting higher-level tasks into smaller sub-tasks. However, in the literature on natural language (NL) command of situated agents, most works have treated the procedures to be executed as flat sequences of simple actions, or any hierarchies of procedures have been shallow at best. In this paper, we propose a formalism of procedures as programs, a powerful yet intuitive method of representing hierarchical procedural knowledge for agent command and control. We further propose a modeling paradigm of hierarchical modular networks, which consist of a planner and reactors that convert NL intents to predictions of executable programs and probe the environment for information necessary to complete the program execution. We instantiate this framework on the IQA and ALFRED datasets for NL instruction following. Our model outperforms reactive baselines by a large margin on both datasets. We also demonstrate that our framework is more data-efficient, and that it allows for fast iterative development.",
      "year": 2021,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": null,
      "authors": [
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "9aa8f00815ff2b48460c2865a91e4ae12150bcc3",
      "externalIds": {
        "ArXiv": "2109.13498",
        "DBLP": "journals/corr/abs-2109-13498",
        "CorpusId": 238198487
      },
      "url": "https://www.semanticscholar.org/paper/9aa8f00815ff2b48460c2865a91e4ae12150bcc3",
      "title": "Learning to Superoptimize Real-world Programs",
      "abstract": "Program optimization is the process of modifying software to execute more efficiently. Superoptimizers attempt to find the optimal program by employing significantly more expensive search and constraint solving techniques. Generally, these methods do not scale well to programs in real development scenarios, and as a result, superoptimization has largely been confined to small-scale, domain-specific, and/or synthetic program benchmarks. In this paper, we propose a framework to learn to superoptimize real-world programs by using neural sequence-to-sequence models. We created a dataset consisting of over 25K real-world x86-64 assembly functions mined from open-source projects and propose an approach, Self Imitation Learning for Optimization (SILO) that is easy to implement and outperforms a standard policy gradient learning approach on our dataset. Our method, SILO, superoptimizes 5.9% of our test set when compared with the gcc version 10.3 compiler's aggressive optimization level -O3. We also report that SILO's rate of superoptimization on our test set is over five times that of a standard policy gradient approach and a model pre-trained on compiler optimization demonstration.",
      "year": 2021,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2109.13498",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2129995371",
          "name": "Alex Shypula"
        },
        {
          "authorId": "2055975657",
          "name": "P. Yin"
        },
        {
          "authorId": "51119916",
          "name": "Jeremy Lacomis"
        },
        {
          "authorId": "2957803",
          "name": "Claire Le Goues"
        },
        {
          "authorId": "32291852",
          "name": "Edward N. Schwartz"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "9b3a071734cfecb2880105d84bde6791b48bf4a3",
      "externalIds": {
        "DBLP": "journals/corr/abs-2109-04020",
        "ArXiv": "2109.04020",
        "ACL": "2021.emnlp-main.458",
        "DOI": "10.18653/v1/2021.emnlp-main.458",
        "CorpusId": 237454532
      },
      "url": "https://www.semanticscholar.org/paper/9b3a071734cfecb2880105d84bde6791b48bf4a3",
      "title": "Distributionally Robust Multilingual Machine Translation",
      "abstract": "Multilingual neural machine translation (MNMT) learns to translate multiple language pairs with a single model, potentially improving both the accuracy and the memory-efficiency of deployed models. However, the heavy data imbalance between languages hinders the model from performing uniformly across language pairs. In this paper, we propose a new learning objective for MNMT based on distributionally robust optimization, which minimizes the worst-case expected loss over the set of language pairs. We further show how to practically optimize this objective for large translation corpora using an iterated best response scheme, which is both effective and incurs negligible additional computational cost compared to standard empirical risk minimization. We perform extensive experiments on three sets of languages from two datasets and show that our method consistently outperforms strong baseline methods in terms of average and per-language performance under both many-to-one and one-to-many translation settings.",
      "year": 2021,
      "influentialCitationCount": 5,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.emnlp-main.458.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2109.04020",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2110714400",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "153461979",
          "name": "Daniel L\u00e9vy"
        },
        {
          "authorId": "2116235416",
          "name": "Xian Li"
        },
        {
          "authorId": "2320509",
          "name": "Marjan Ghazvininejad"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "a6a7724763d8adba466519489b0b9d209e7f2d15",
      "externalIds": {
        "DBLP": "conf/nips/YuanNL21",
        "ArXiv": "2106.11520",
        "CorpusId": 235593404
      },
      "url": "https://www.semanticscholar.org/paper/a6a7724763d8adba466519489b0b9d209e7f2d15",
      "title": "BARTScore: Evaluating Generated Text as Text Generation",
      "abstract": "A wide variety of NLP applications, such as machine translation, summarization, and dialog, involve text generation. One major challenge for these applications is how to evaluate whether such generated texts are actually fluent, accurate, or effective. In this work, we conceptualize the evaluation of generated text as a text generation problem, modeled using pre-trained sequence-to-sequence models. The general idea is that models trained to convert the generated text to/from a reference output or the source text will achieve higher scores when the generated text is better. We operationalize this idea using BART, an encoder-decoder based pre-trained model, and propose a metric BARTScore with a number of variants that can be flexibly applied in an unsupervised fashion to evaluation of text from different perspectives (e.g. informativeness, fluency, or factuality). BARTScore is conceptually simple and empirically effective. It can outperform existing top-scoring metrics in 16 of 22 test settings, covering evaluation of 16 datasets (e.g., machine translation, text summarization) and 7 different perspectives (e.g., informativeness, factuality). Code to calculate BARTScore is available at https://github.com/neulab/BARTScore, and we have released an interactive leaderboard for meta-evaluation at http://explainaboard.nlpedia.ai/leaderboard/task-meval/ on the ExplainaBoard platform, which allows us to interactively understand the strengths, weaknesses, and complementarity of each metric.",
      "year": 2021,
      "influentialCitationCount": 95,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2106.11520",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "30300197",
          "name": "Weizhe Yuan"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        }
      ]
    },
    {
      "paperId": "a77643bff6f50ccc4f80ec081e4d078a2e788ae7",
      "externalIds": {
        "ArXiv": "2103.08490",
        "DBLP": "journals/corr/abs-2103-08490",
        "MAG": "3166790124",
        "ACL": "2021.naacl-main.40",
        "DOI": "10.18653/V1/2021.NAACL-MAIN.40",
        "CorpusId": 232233194
      },
      "url": "https://www.semanticscholar.org/paper/a77643bff6f50ccc4f80ec081e4d078a2e788ae7",
      "title": "Multi-view Subword Regularization",
      "abstract": "Multilingual pretrained representations generally rely on subword segmentation algorithms to create a shared multilingual vocabulary. However, standard heuristic algorithms often lead to sub-optimal segmentation, especially for languages with limited amounts of data. In this paper, we take two major steps towards alleviating this problem. First, we demonstrate empirically that applying existing subword regularization methods (Kudo, 2018; Provilkov et al., 2020) during fine-tuning of pre-trained multilingual representations improves the effectiveness of cross-lingual transfer. Second, to take full advantage of different possible input segmentations, we propose Multi-view Subword Regularization (MVR), a method that enforces the consistency of predictors between using inputs tokenized by the standard and probabilistic segmentations. Results on the XTREME multilingual benchmark (Hu et al., 2020) show that MVR brings consistent improvements of up to 2.5 points over using standard segmentation algorithms.",
      "year": 2021,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.naacl-main.40.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "473-482"
      },
      "authors": [
        {
          "authorId": null,
          "name": "Xinyi Wang"
        },
        {
          "authorId": "2884561",
          "name": "Sebastian Ruder"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "ab364a00a356e5cb8c9e9b144b57e0517bce56f6",
      "externalIds": {
        "ACL": "2021.emnlp-main.99",
        "DBLP": "journals/corr/abs-2109-04715",
        "ArXiv": "2109.04715",
        "DOI": "10.18653/v1/2021.emnlp-main.99",
        "CorpusId": 237485544
      },
      "url": "https://www.semanticscholar.org/paper/ab364a00a356e5cb8c9e9b144b57e0517bce56f6",
      "title": "AfroMT: Pretraining Strategies and Reproducible Benchmarks for Translation of 8 African Languages",
      "abstract": "Reproducible benchmarks are crucial in driving progress of machine translation research. However, existing machine translation benchmarks have been mostly limited to high-resource or well-represented languages. Despite an increasing interest in low-resource machine translation, there are no standardized reproducible benchmarks for many African languages, many of which are used by millions of speakers but have less digitized textual data. To tackle these challenges, we propose AfroMT, a standardized, clean, and reproducible machine translation benchmark for eight widely spoken African languages. We also develop a suite of analysis tools for system diagnosis taking into account the unique properties of these languages. Furthermore, we explore the newly considered case of low-resource focused pretraining and develop two novel data augmentation-based strategies, leveraging word-level alignment information and pseudo-monolingual data for pretraining multilingual sequence-to-sequence models. We demonstrate significant improvements when pretraining on 11 languages, with gains of up to 2 BLEU points over strong baselines. We also show gains of up to 12 BLEU points over cross-lingual transfer baselines in data-constrained scenarios. All code and pretrained models will be released as further steps towards larger reproducible benchmarks for African languages.",
      "year": 2021,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.emnlp-main.99.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1306-1320"
      },
      "authors": [
        {
          "authorId": "1557386977",
          "name": "Machel Reid"
        },
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "49484314",
          "name": "Y. Matsuo"
        }
      ]
    },
    {
      "paperId": "add0dd0ab2a408354fa2d89cc492b4d2ab26845d",
      "externalIds": {
        "ACL": "2022.acl-long.435",
        "ArXiv": "2104.08726",
        "DBLP": "conf/acl/EbrahimiMOCCFOR22",
        "DOI": "10.18653/v1/2022.acl-long.435",
        "CorpusId": 233296177
      },
      "url": "https://www.semanticscholar.org/paper/add0dd0ab2a408354fa2d89cc492b4d2ab26845d",
      "title": "AmericasNLI: Evaluating Zero-shot Natural Language Understanding of Pretrained Multilingual Models in Truly Low-resource Languages",
      "abstract": "Pretrained multilingual models are able to perform cross-lingual transfer in a zero-shot setting, even for languages unseen during pretraining. However, prior work evaluating performance on unseen languages has largely been limited to low-level, syntactic tasks, and it remains unclear if zero-shot learning of high-level, semantic tasks is possible for unseen languages. To explore this question, we present AmericasNLI, an extension of XNLI (Conneau et al., 2018) to 10 Indigenous languages of the Americas. We conduct experiments with XLM-R, testing multiple zero-shot and translation-based approaches. Additionally, we explore model adaptation via continued pretraining and provide an analysis of the dataset by considering hypothesis-only models. We find that XLM-R\u2019s zero-shot performance is poor for all 10 languages, with an average performance of 38.48%. Continued pretraining offers improvements, with an average accuracy of 43.85%. Surprisingly, training on poorly translated data by far outperforms all other methods with an accuracy of 49.12%.",
      "year": 2021,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2022.acl-long.435.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2104.08726",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "146057134",
          "name": "Abteen Ebrahimi"
        },
        {
          "authorId": "153151470",
          "name": "Manuel Mager"
        },
        {
          "authorId": "65775345",
          "name": "Arturo Oncevay"
        },
        {
          "authorId": "113810201",
          "name": "Vishrav Chaudhary"
        },
        {
          "authorId": "2287191",
          "name": "Luis Chiruzzo"
        },
        {
          "authorId": "144270981",
          "name": "Angela Fan"
        },
        {
          "authorId": "2056819421",
          "name": "John E. Ortega"
        },
        {
          "authorId": "2078503669",
          "name": "Ricardo Ramos"
        },
        {
          "authorId": "40659617",
          "name": "Annette Rios Gonzales"
        },
        {
          "authorId": "2078503824",
          "name": "Ivan Vladimir"
        },
        {
          "authorId": "2084206806",
          "name": "Gustavo A. Gim'enez-Lugo"
        },
        {
          "authorId": "51125557",
          "name": "Elisabeth Mager"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "6214002",
          "name": "Alexis Palmer"
        },
        {
          "authorId": "2078502861",
          "name": "Rolando A. Coto Solano"
        },
        {
          "authorId": "2008190903",
          "name": "Ngoc Thang Vu"
        },
        {
          "authorId": "3422953",
          "name": "Katharina Kann"
        }
      ]
    },
    {
      "paperId": "af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30",
      "externalIds": {
        "ArXiv": "2107.05697",
        "DBLP": "journals/corr/abs-2107-05697",
        "CorpusId": 235826075
      },
      "url": "https://www.semanticscholar.org/paper/af0adbaa0c1ea6abaed4b3d21f1dc4121c35fb30",
      "title": "Few-shot Language Coordination by Modeling Theory of Mind",
      "abstract": "$\\textit{No man is an island.}$ Humans communicate with a large community by coordinating with different interlocutors within short conversations. This ability has been understudied by the research on building neural communicative agents. We study the task of few-shot $\\textit{language coordination}$: agents quickly adapting to their conversational partners' language abilities. Different from current communicative agents trained with self-play, we require the lead agent to coordinate with a $\\textit{population}$ of agents with different linguistic abilities, quickly adapting to communicate with unseen agents in the population. This requires the ability to model the partner's beliefs, a vital component of human communication. Drawing inspiration from theory-of-mind (ToM; Premack&Woodruff (1978)), we study the effect of the speaker explicitly modeling the listeners' mental states. The speakers, as shown in our experiments, acquire the ability to predict the reactions of their partner, which helps it generate instructions that concisely express its communicative goal. We examine our hypothesis that the instructions generated with ToM modeling yield better communication performance in both a referential game and a language navigation task. Positive results from our experiments hint at the importance of explicitly modeling communication as a socio-pragmatic progress.",
      "year": 2021,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2107.05697",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "144459859",
          "name": "Hao Zhu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3312309",
          "name": "Yonatan Bisk"
        }
      ]
    },
    {
      "paperId": "b57da3ccf214e8dad49116c8db9590c2c89629f5",
      "externalIds": {
        "DBLP": "journals/tacl/AdelaniANDKLPBR21",
        "ArXiv": "2103.11811",
        "DOI": "10.1162/tacl_a_00416",
        "CorpusId": 232307797
      },
      "url": "https://www.semanticscholar.org/paper/b57da3ccf214e8dad49116c8db9590c2c89629f5",
      "title": "MasakhaNER: Named Entity Recognition for African Languages",
      "abstract": "Abstract We take a step towards addressing the under- representation of the African continent in NLP research by bringing together different stakeholders to create the first large, publicly available, high-quality dataset for named entity recognition (NER) in ten African languages. We detail the characteristics of these languages to help researchers and practitioners better understand the challenges they pose for NER tasks. We analyze our datasets and conduct an extensive empirical evaluation of state- of-the-art methods across both supervised and transfer learning settings. Finally, we release the data, code, and models to inspire future research on African NLP.1",
      "year": 2021,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00416/1966201/tacl_a_00416.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "9",
        "pages": "1116-1131",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "2518906",
          "name": "David Ifeoluwa Adelani"
        },
        {
          "authorId": "145164877",
          "name": "Jade Z. Abbott"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1432838153",
          "name": "Daniel D'souza"
        },
        {
          "authorId": "3422710",
          "name": "Julia Kreutzer"
        },
        {
          "authorId": "1737047",
          "name": "Constantine Lignos"
        },
        {
          "authorId": "1414156453",
          "name": "Chester Palen-Michel"
        },
        {
          "authorId": "1395556657",
          "name": "Happy Buzaaba"
        },
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "2884561",
          "name": "Sebastian Ruder"
        },
        {
          "authorId": "2153374",
          "name": "Stephen Mayhew"
        },
        {
          "authorId": "2052358080",
          "name": "Israel Abebe Azime"
        },
        {
          "authorId": "7744881",
          "name": "Shamsuddeen Hassan Muhammad"
        },
        {
          "authorId": "1591176064",
          "name": "Chris C. Emezue"
        },
        {
          "authorId": "1411005375",
          "name": "J. Nakatumba-Nabende"
        },
        {
          "authorId": "1988654955",
          "name": "Perez Ogayo"
        },
        {
          "authorId": "2056773747",
          "name": "Anuoluwapo Aremu"
        },
        {
          "authorId": "2138515650",
          "name": "Catherine Gitau"
        },
        {
          "authorId": "2056774640",
          "name": "Derguene Mbaye"
        },
        {
          "authorId": "122367036",
          "name": "Jesujoba Oluwadara Alabi"
        },
        {
          "authorId": "3084761",
          "name": "Seid Muhie Yimam"
        },
        {
          "authorId": "2352354",
          "name": "T. Gwadabe"
        },
        {
          "authorId": "3452783",
          "name": "I. Ezeani"
        },
        {
          "authorId": "1988673056",
          "name": "Andre Niyongabo Rubungo"
        },
        {
          "authorId": "1412684911",
          "name": "Jonathan Mukiibi"
        },
        {
          "authorId": "90459153",
          "name": "V. Otiende"
        },
        {
          "authorId": "2570169",
          "name": "Iroro Orife"
        },
        {
          "authorId": "2058260775",
          "name": "Davis David"
        },
        {
          "authorId": "2056776523",
          "name": "Samba Ngom"
        },
        {
          "authorId": "51221489",
          "name": "Tosin P. Adewumi"
        },
        {
          "authorId": "1929390",
          "name": "Paul Rayson"
        },
        {
          "authorId": "2056770646",
          "name": "Mofetoluwa Adeyemi"
        },
        {
          "authorId": "2056776794",
          "name": "Gerald Muriuki"
        },
        {
          "authorId": "2056775833",
          "name": "E. Anebi"
        },
        {
          "authorId": "73054967",
          "name": "C. Chukwuneke"
        },
        {
          "authorId": "51991672",
          "name": "N. Odu"
        },
        {
          "authorId": "2056776516",
          "name": "Eric Peter Wairagala"
        },
        {
          "authorId": "1483741505",
          "name": "S. Oyerinde"
        },
        {
          "authorId": "2056776870",
          "name": "Clemencia Siro"
        },
        {
          "authorId": "2056776512",
          "name": "Tobius Saul Bateesa"
        },
        {
          "authorId": "2056776668",
          "name": "Temilola Oloyede"
        },
        {
          "authorId": "2006446445",
          "name": "Yvonne Wambui"
        },
        {
          "authorId": "2056776544",
          "name": "Victor Akinode"
        },
        {
          "authorId": "2056773676",
          "name": "Deborah Nabagereka"
        },
        {
          "authorId": "2056775014",
          "name": "Maurice Katusiime"
        },
        {
          "authorId": "2054106142",
          "name": "Ayodele Awokoya"
        },
        {
          "authorId": "2056773569",
          "name": "Mouhamadane Mboup"
        },
        {
          "authorId": "2056776339",
          "name": "D. Gebreyohannes"
        },
        {
          "authorId": "2056774874",
          "name": "Henok Tilaye"
        },
        {
          "authorId": "1720801982",
          "name": "Kelechi Nwaike"
        },
        {
          "authorId": "2056773653",
          "name": "Degaga Wolde"
        },
        {
          "authorId": "49488546",
          "name": "A. Faye"
        },
        {
          "authorId": "1591123568",
          "name": "Blessing K. Sibanda"
        },
        {
          "authorId": "1452686038",
          "name": "Orevaoghene Ahia"
        },
        {
          "authorId": "1591111757",
          "name": "Bonaventure F. P. Dossou"
        },
        {
          "authorId": "1452683268",
          "name": "Kelechi Ogueji"
        },
        {
          "authorId": "2076531470",
          "name": "T. Diop"
        },
        {
          "authorId": "144931688",
          "name": "A. Diallo"
        },
        {
          "authorId": "146001089",
          "name": "Adewale Akinfaderin"
        },
        {
          "authorId": "9335012",
          "name": "T. Marengereke"
        },
        {
          "authorId": "1486204986",
          "name": "Salomey Osei"
        }
      ]
    },
    {
      "paperId": "bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713",
      "externalIds": {
        "DBLP": "conf/interspeech/SiminyuLAMMN21",
        "ArXiv": "2104.01624",
        "DOI": "10.21437/interspeech.2021-1434",
        "CorpusId": 233025476
      },
      "url": "https://www.semanticscholar.org/paper/bad5d2d6d1f3282ebbcb602a6f3a5dd9488fd713",
      "title": "Phoneme Recognition through Fine Tuning of Phonetic Representations: a Case Study on Luhya Language Varieties",
      "abstract": "Models pre-trained on multiple languages have shown significant promise for improving speech recognition, particularly for low-resource languages. In this work, we focus on phoneme recognition using Allosaurus, a method for multilingual recognition based on phonetic annotation, which incorporates phonological knowledge through a language-dependent allophone layer that associates a universal narrow phone-set with the phonemes that appear in each language. To evaluate in a challenging real-world scenario, we curate phone recognition datasets for Bukusu and Saamia, two varieties of the Luhya language cluster of western Kenya and eastern Uganda. To our knowledge, these datasets are the first of their kind. We carry out similar experiments on the dataset of an endangered Tangkhulic language, East Tusom, a Tibeto-Burman language variety spoken mostly in India. We explore both zero-shot and few-shot recognition by fine-tuning using datasets of varying sizes (10 to 1000 utterances). We find that fine-tuning of Allosaurus, even with just 100 utterances, leads to significant improvements in phone error rates.",
      "year": 2021,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2104.01624",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "271-275"
      },
      "authors": [
        {
          "authorId": "1591111860",
          "name": "Kathleen Siminyu"
        },
        {
          "authorId": "47058260",
          "name": "Xinjian Li"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "3407646",
          "name": "David R. Mortensen"
        },
        {
          "authorId": "10441693",
          "name": "M. Marlo"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "c6bb04f3d8000b7e800f6359082de39548c7da79",
      "externalIds": {
        "DBLP": "journals/corr/abs-2110-02870",
        "ArXiv": "2110.02870",
        "CorpusId": 238408095
      },
      "url": "https://www.semanticscholar.org/paper/c6bb04f3d8000b7e800f6359082de39548c7da79",
      "title": "Capturing Structural Locality in Non-parametric Language Models",
      "abstract": "Structural locality is a ubiquitous feature of real-world datasets, wherein data points are organized into local hierarchies. Some examples include topical clusters in text or project hierarchies in source code repositories. In this paper, we explore utilizing this structural locality within non-parametric language models, which generate sequences that reference retrieved examples from an external source. We propose a simple yet effective approach for adding locality information into such models by adding learned parameters that improve the likelihood of retrieving examples from local neighborhoods. Experiments on two different domains, Java source code and Wikipedia text, demonstrate that locality features improve model efficacy over models without access to these features, with interesting differences. We also perform an analysis of how and where locality features contribute to improved performance and why the traditionally used contextual similarity metrics alone are not enough to grasp the locality structure.",
      "year": 2021,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2110.02870",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "2109932032",
          "name": "Junxian He"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "22747364",
          "name": "V. Hellendoorn"
        }
      ]
    },
    {
      "paperId": "cefd3993db4d065b95ab8f105452fb728c02b60e",
      "externalIds": {
        "DBLP": "journals/jair/YuanLN22",
        "ArXiv": "2102.00176",
        "DOI": "10.1613/jair.1.12862",
        "CorpusId": 231740610
      },
      "url": "https://www.semanticscholar.org/paper/cefd3993db4d065b95ab8f105452fb728c02b60e",
      "title": "Can We Automate Scientific Reviewing?",
      "abstract": "The rapid development of science and technology has been accompanied by an exponential growth in peer-reviewed scientific publications. At the same time, the review of each paper is a laborious process that must be carried out by subject matter experts. Thus, providing high-quality reviews of this growing number of papers is a significant challenge. In this work, we ask the question \u201ccan we automate scientific reviewing? \u201d, discussing the possibility of using natural language processing (NLP) models to generate peer reviews for scientific papers. Because it is non-trivial to define what a \u201cgood\u201d review is in the first place, we first discuss possible evaluation metrics that could be used to judge success in this task. We then focus on the machine learning domain and collect a dataset of papers in the domain, annotate them with different aspects of content covered in each review, and train targeted summarization models that take in papers as input and generate reviews as output. Comprehensive experimental results on the test set show that while system-generated reviews are comprehensive, touching upon more aspects of the paper than human-written reviews, the generated texts are less constructive and less factual than human-written reviews for all aspects except the explanation of the core ideas of the papers, which are largely factually correct. Given these results, we pose eight challenges in the pursuit of a good review generation system together with potential solutions, which, hopefully, will inspire more future research in this direction.\nWe make relevant resource publicly available for use by future research: https://github. com/neulab/ReviewAdvisor. In addition, while our conclusion is that the technology is not yet ready for use in high-stakes review settings we provide a system demo, ReviewAdvisor (http://review.nlpedia.ai/), showing the current capabilities and failings of state-of-the-art NLP models at this task (see demo screenshot in A.2). A review of this paper written by the system proposed in this paper can be found in A.1.",
      "year": 2021,
      "influentialCitationCount": 9,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://jair.org/index.php/jair/article/download/12862/26847",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2102.00176",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "30300197",
          "name": "Weizhe Yuan"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "d06493373421c86ba33dbb8834ccb725105a665f",
      "externalIds": {
        "DBLP": "journals/corr/abs-2109-06014",
        "ArXiv": "2109.06014",
        "ACL": "2021.emnlp-main.553",
        "DOI": "10.18653/v1/2021.emnlp-main.553",
        "CorpusId": 237492262
      },
      "url": "https://www.semanticscholar.org/paper/d06493373421c86ba33dbb8834ccb725105a665f",
      "title": "When is Wall a Pared and when a Muro?: Extracting Rules Governing Lexical Selection",
      "abstract": "Learning fine-grained distinctions between vocabulary items is a key challenge in learning a new language. For example, the noun \u201cwall\u201d has different lexical manifestations in Spanish \u2013 \u201cpared\u201d refers to an indoor wall while \u201cmuro\u201d refers to an outside wall. However, this variety of lexical distinction may not be obvious to non-native learners unless the distinction is explained in such a way. In this work, we present a method for automatically identifying fine-grained lexical distinctions, and extracting rules explaining these distinctions in a human- and machine-readable format. We confirm the quality of these extracted rules in a language learning setup for two languages, Spanish and Greek, where we use the rules to teach non-native speakers when to translate a given ambiguous word into its different possible translations.",
      "year": 2021,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.emnlp-main.553.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "6911-6929"
      },
      "authors": [
        {
          "authorId": "51250894",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "1602993448",
          "name": "Kayo Yin"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "d5ec188a5a39e504788c1fe33457eeb816a99f31",
      "externalIds": {
        "DBLP": "conf/conll/SuRZHWBN21",
        "ArXiv": "2109.09790",
        "ACL": "2021.conll-1.2",
        "DOI": "10.18653/v1/2021.conll-1.2",
        "CorpusId": 237581619
      },
      "url": "https://www.semanticscholar.org/paper/d5ec188a5a39e504788c1fe33457eeb816a99f31",
      "title": "Dependency Induction Through the Lens of Visual Perception",
      "abstract": "Most previous work on grammar induction focuses on learning phrasal or dependency structure purely from text. However, because the signal provided by text alone is limited, recently introduced visually grounded syntax models make use of multimodal information leading to improved performance in constituency grammar induction. However, as compared to dependency grammars, constituency grammars do not provide a straightforward way to incorporate visual information without enforcing language-specific heuristics. In this paper, we propose an unsupervised grammar induction model that leverages word concreteness and a structural vision-based heuristic to jointly learn constituency-structure and dependency-structure grammars. Our experiments find that concreteness is a strong indicator for learning dependency grammars, improving the direct attachment score (DAS) by over 50% as compared to state-of-the-art models trained on pure text. Next, we propose an extension of our model that leverages both word concreteness and visual semantic role labels in constituency and dependency parsing. Our experiments show that the proposed extension outperforms the current state-of-the-art visually grounded models in constituency parsing even with a smaller grammar size.",
      "year": 2021,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.conll-1.2.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "17-26"
      },
      "authors": [
        {
          "authorId": "153083809",
          "name": "Ruisi Su"
        },
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "144459859",
          "name": "Hao Zhu"
        },
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "2115553247",
          "name": "Xinyu Wang"
        },
        {
          "authorId": "3312309",
          "name": "Yonatan Bisk"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "e073049578b989958607130d21420d5632efc973",
      "externalIds": {
        "MAG": "3164777966",
        "ACL": "2021.americasnlp-1.23",
        "DOI": "10.18653/V1/2021.AMERICASNLP-1.23",
        "CorpusId": 235097180
      },
      "url": "https://www.semanticscholar.org/paper/e073049578b989958607130d21420d5632efc973",
      "title": "Findings of the AmericasNLP 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas",
      "abstract": "This paper presents the results of the 2021 Shared Task on Open Machine Translation for Indigenous Languages of the Americas. The shared task featured two independent tracks, and participants submitted machine translation systems for up to 10 indigenous languages. Overall, 8 teams participated with a total of 214 submissions. We provided training sets consisting of data collected from various sources, as well as manually translated sentences for the development and test sets. An official baseline trained on this data was also provided. Team submissions featured a variety of architectures, including both statistical and neural models, and for the majority of languages, many teams were able to considerably improve over the baseline. The best performing systems achieved 12.97 ChrF higher than baseline, when averaged across languages.",
      "year": 2021,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.americasnlp-1.23.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "pages": "202-217",
        "name": ""
      },
      "authors": [
        {
          "authorId": "153151470",
          "name": "Manuel Mager"
        },
        {
          "authorId": "65775345",
          "name": "Arturo Oncevay"
        },
        {
          "authorId": "146057134",
          "name": "Abteen Ebrahimi"
        },
        {
          "authorId": "2056819421",
          "name": "John E. Ortega"
        },
        {
          "authorId": "40659617",
          "name": "Annette Rios Gonzales"
        },
        {
          "authorId": "144270981",
          "name": "Angela Fan"
        },
        {
          "authorId": "1409305289",
          "name": "Ximena Gutierrez-Vasques"
        },
        {
          "authorId": "2287191",
          "name": "Luis Chiruzzo"
        },
        {
          "authorId": "1419526006",
          "name": "G. Gim\u00e9nez-Lugo"
        },
        {
          "authorId": "2078503669",
          "name": "Ricardo Ramos"
        },
        {
          "authorId": "1403616824",
          "name": "Ivan Vladimir Meza Ruiz"
        },
        {
          "authorId": "1405433180",
          "name": "Rolando Coto-Solano"
        },
        {
          "authorId": "6214002",
          "name": "Alexis Palmer"
        },
        {
          "authorId": "2101319814",
          "name": "Elisabeth Mager-Hois"
        },
        {
          "authorId": "113810201",
          "name": "Vishrav Chaudhary"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2008190903",
          "name": "Ngoc Thang Vu"
        },
        {
          "authorId": "3422953",
          "name": "Katharina Kann"
        }
      ]
    },
    {
      "paperId": "fcdac45272543b4f8b8eaa59d66044d1b7018494",
      "externalIds": {
        "DBLP": "conf/iclr/PhamWYN21",
        "ArXiv": "2102.07847",
        "CorpusId": 231933756
      },
      "url": "https://www.semanticscholar.org/paper/fcdac45272543b4f8b8eaa59d66044d1b7018494",
      "title": "Meta Back-translation",
      "abstract": "Back-translation is an effective strategy to improve the performance of Neural Machine Translation~(NMT) by generating pseudo-parallel data. However, several recent works have found that better translation quality of the pseudo-parallel data does not necessarily lead to better final translation models, while lower-quality but more diverse data often yields stronger results. In this paper, we propose a novel method to generate pseudo-parallel data from a pre-trained back-translation model. Our method is a meta-learning algorithm which adapts a pre-trained back-translation model so that the pseudo-parallel data it generates would train a forward-translation model to do well on a validation set. In our evaluations in both the standard datasets WMT En-De'14 and WMT En-Fr'14, as well as a multilingual translation setting, our method leads to significant improvements over strong baselines. Our code will be made available.",
      "year": 2021,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2102.07847",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "143950636",
          "name": "Hieu Pham"
        },
        {
          "authorId": null,
          "name": "Xinyi Wang"
        },
        {
          "authorId": "46286308",
          "name": "Yiming Yang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "0d360a1256ccdfca58cf98d12243df8407fd442d",
      "externalIds": {
        "MAG": "3035367371",
        "ACL": "2020.acl-main.249",
        "DBLP": "conf/acl/KuritaMN20",
        "ArXiv": "2004.06660",
        "DOI": "10.18653/v1/2020.acl-main.249",
        "CorpusId": 215754328
      },
      "url": "https://www.semanticscholar.org/paper/0d360a1256ccdfca58cf98d12243df8407fd442d",
      "title": "Weight Poisoning Attacks on Pretrained Models",
      "abstract": "Recently, NLP has seen a surge in the usage of large pre-trained models. Users download weights of models pre-trained on large datasets, then fine-tune the weights on a task of their choice. This raises the question of whether downloading untrusted pre-trained weights can pose a security threat. In this paper, we show that it is possible to construct \u201cweight poisoning\u201d attacks where pre-trained weights are injected with vulnerabilities that expose \u201cbackdoors\u201d after fine-tuning, enabling the attacker to manipulate the model prediction simply by injecting an arbitrary keyword. We show that by applying a regularization method which we call RIPPLe and an initialization procedure we call Embedding Surgery, such attacks are possible even with limited knowledge of the dataset and fine-tuning procedure. Our experiments on sentiment classification, toxicity detection, and spam detection show that this attack is widely applicable and poses a serious threat. Finally, we outline practical defenses against such attacks.",
      "year": 2020,
      "influentialCitationCount": 52,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.acl-main.249.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "volume": "abs/2004.06660",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "147225682",
          "name": "Keita Kurita"
        },
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "18f4ec53a4221a97e1482f091f41a23f3d873cf2",
      "externalIds": {
        "MAG": "3095722499",
        "DBLP": "conf/emnlp/PruthiDNL20",
        "ArXiv": "2011.01459",
        "ACL": "2020.findings-emnlp.353",
        "DOI": "10.18653/v1/2020.findings-emnlp.353",
        "CorpusId": 226237381
      },
      "url": "https://www.semanticscholar.org/paper/18f4ec53a4221a97e1482f091f41a23f3d873cf2",
      "title": "Weakly- and Semi-supervised Evidence Extraction",
      "abstract": "For many prediction tasks, stakeholders desire not only predictions but also supporting evidence that a human can use to verify its correctness. However, in practice, evidence annotations may only be available for a minority of training examples (if available at all). In this paper, we propose new methods to combine few evidence annotations (strong semi-supervision) with abundant document-level labels (weak supervision) for the task of evidence extraction. Evaluating on two classification tasks that feature evidence annotations, we find that our methods outperform baselines adapted from the interpretability literature to our task. Our approach yields gains with as few as hundred evidence annotations.",
      "year": 2020,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.353.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2011.01459",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "7880098",
          "name": "Danish Pruthi"
        },
        {
          "authorId": "34994191",
          "name": "Bhuwan Dhingra"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2126415179",
          "name": "Z. Lipton"
        }
      ]
    },
    {
      "paperId": "1bd43c91ecbf46098ef2b521c5367e849819960e",
      "externalIds": {
        "DBLP": "conf/emnlp/DouAN20",
        "ACL": "2020.emnlp-main.475",
        "MAG": "3015580305",
        "ArXiv": "2004.03672",
        "DOI": "10.18653/v1/2020.emnlp-main.475",
        "CorpusId": 215415842
      },
      "url": "https://www.semanticscholar.org/paper/1bd43c91ecbf46098ef2b521c5367e849819960e",
      "title": "Dynamic Data Selection and Weighting for Iterative Back-Translation",
      "abstract": "Back-translation has proven to be an effective method to utilize monolingual data in neural machine translation (NMT), and iteratively conducting back-translation can further improve the model performance. Selecting which monolingual data to back-translate is crucial, as we require that the resulting synthetic data are of high quality \\textit{and} reflect the target domain. To achieve these two goals, data selection and weighting strategies have been proposed, with a common practice being to select samples close to the target domain but also dissimilar to the average general-domain text. In this paper, we provide insights into this commonly used approach and generalize it to a dynamic curriculum learning strategy, which is applied to iterative back-translation models. In addition, we propose weighting strategies based on both the current quality of the sentence and its improvement over the previous iteration. We evaluate our models on domain adaptation, low-resource, and high-resource MT settings and on two language pairs. Experimental results demonstrate that our methods achieve improvements of up to 1.8 BLEU points over competitive baselines.",
      "year": 2020,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.emnlp-main.475.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2004.03672",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "14199369",
          "name": "Zi-Yi Dou"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "2232808cf3161ca4c434126e35f47ee33c0c8219",
      "externalIds": {
        "ACL": "2022.tacl-1.21",
        "DBLP": "journals/corr/abs-2012-00893",
        "MAG": "3110300144",
        "ArXiv": "2012.00893",
        "DOI": "10.1162/tacl_a_00465",
        "CorpusId": 227247654
      },
      "url": "https://www.semanticscholar.org/paper/2232808cf3161ca4c434126e35f47ee33c0c8219",
      "title": "Evaluating Explanations: How Much Do Explanations from the Teacher Aid Students?",
      "abstract": "While many methods purport to explain predictions by highlighting salient features, what aims these explanations serve and how they ought to be evaluated often go unstated. In this work, we introduce a framework to quantify the value of explanations via the accuracy gains that they confer on a student model trained to simulate a teacher model. Crucially, the explanations are available to the student during training, but are not available at test time. Compared with prior proposals, our approach is less easily gamed, enabling principled, automatic, model-agnostic evaluation of attributions. Using our framework, we compare numerous attribution methods for text classification and question answering, and observe quantitative differences that are consistent (to a moderate to high degree) across different student model architectures and learning strategies.1",
      "year": 2020,
      "influentialCitationCount": 16,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00465/2006971/tacl_a_00465.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "10",
        "pages": "359-375",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "7880098",
          "name": "Danish Pruthi"
        },
        {
          "authorId": "34994191",
          "name": "Bhuwan Dhingra"
        },
        {
          "authorId": "7353832",
          "name": "Livio Baldini Soares"
        },
        {
          "authorId": "123052390",
          "name": "Michael Collins"
        },
        {
          "authorId": "2126415179",
          "name": "Z. Lipton"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "50056360",
          "name": "William W. Cohen"
        }
      ]
    },
    {
      "paperId": "2507a6924007efbe0c3116048a85108398f23007",
      "externalIds": {
        "MAG": "3117342196",
        "DBLP": "conf/coling/ZhaoOANL20",
        "ACL": "2020.coling-main.471",
        "DOI": "10.18653/V1/2020.COLING-MAIN.471",
        "CorpusId": 227231816
      },
      "url": "https://www.semanticscholar.org/paper/2507a6924007efbe0c3116048a85108398f23007",
      "title": "Automatic Interlinear Glossing for Under-Resourced Languages Leveraging Translations",
      "abstract": "Interlinear Glossed Text (IGT) is a widely used format for encoding linguistic information in language documentation projects and scholarly papers. Manual production of IGT takes time and requires linguistic expertise. We attempt to address this issue by creating automatic glossing models, using modern multi-source neural models that additionally leverage easy-to-collect translations. We further explore cross-lingual transfer and a simple output length control mechanism, further refining our models. Evaluated on three challenging low-resource scenarios, our approach significantly outperforms a recent, state-of-the-art baseline, particularly improving on overall accuracy as well as lemma and tag recall.",
      "year": 2020,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.coling-main.471.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "5397-5408"
      },
      "authors": [
        {
          "authorId": "30676414",
          "name": "Xingyuan Zhao"
        },
        {
          "authorId": "2055633551",
          "name": "Satoru Ozaki"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1686960",
          "name": "Lori S. Levin"
        }
      ]
    },
    {
      "paperId": "2583e7e279e2969493c3290c8f300ab32da40bf9",
      "externalIds": {
        "MAG": "3014809049",
        "ArXiv": "2003.01343",
        "ACL": "2020.tacl-1.8",
        "DBLP": "journals/corr/abs-2003-01343",
        "DOI": "10.1162/tacl_a_00303",
        "CorpusId": 211818155
      },
      "url": "https://www.semanticscholar.org/paper/2583e7e279e2969493c3290c8f300ab32da40bf9",
      "title": "Improving Candidate Generation for Low-resource Cross-lingual Entity Linking",
      "abstract": "Cross-lingual entity linking (XEL) is the task of finding referents in a target-language knowledge base (KB) for mentions extracted from source-language texts. The first step of (X)EL is candidate generation, which retrieves a list of plausible candidate entities from the target-language KB for each mention. Approaches based on resources from Wikipedia have proven successful in the realm of relatively high-resource languages, but these do not extend well to low-resource languages with few, if any, Wikipedia pages. Recently, transfer learning methods have been shown to reduce the demand for resources in the low-resource languages by utilizing resources in closely related languages, but the performance still lags far behind their high-resource counterparts. In this paper, we first assess the problems faced by current entity candidate generation methods for low-resource XEL, then propose three improvements that (1) reduce the disconnect between entity mentions and KB entries, and (2) improve the robustness of the model to low-resource scenarios. The methods are simple, but effective: We experiment with our approach on seven XEL datasets and find that they yield an average gain of 16.9% in Top-30 gold candidate recall, compared with state-of-the-art baselines. Our improved model also yields an average gain of 7.9% in in-KB accuracy of end-to-end XEL.1",
      "year": 2020,
      "influentialCitationCount": 5,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00303/1923805/tacl_a_00303.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "8",
        "pages": "109-124",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "1518268393",
          "name": "Shruti Rijhawani"
        },
        {
          "authorId": "1771118",
          "name": "J. Wieting"
        },
        {
          "authorId": "143712374",
          "name": "J. Carbonell"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "33422275fbb9958f55419620697faf531482699b",
      "externalIds": {
        "DBLP": "journals/tacl/JiangADN21",
        "ArXiv": "2012.00955",
        "DOI": "10.1162/tacl_a_00407",
        "CorpusId": 235078802
      },
      "url": "https://www.semanticscholar.org/paper/33422275fbb9958f55419620697faf531482699b",
      "title": "How Can We Know When Language Models Know? On the Calibration of Language Models for Question Answering",
      "abstract": "Abstract Recent works have shown that language models (LM) capture different types of knowledge regarding facts or common sense. However, because no model is perfect, they still fail to provide appropriate answers in many cases. In this paper, we ask the question, \u201cHow can we know when language models know, with confidence, the answer to a particular query?\u201d We examine this question from the point of view of calibration, the property of a probabilistic model\u2019s predicted probabilities actually being well correlated with the probabilities of correctness. We examine three strong generative models\u2014T5, BART, and GPT-2\u2014and study whether their probabilities on QA tasks are well calibrated, finding the answer is a relatively emphatic no. We then examine methods to calibrate such models to make their confidence scores correlate better with the likelihood of correctness through fine-tuning, post-hoc probability modification, or adjustment of the predicted outputs or inputs. Experiments on a diverse range of datasets demonstrate the effectiveness of our methods. We also perform analysis to study the strengths and limitations of these methods, shedding light on further improvements that may be made in methods for calibrating LMs. We have released the code at https://github.com/jzbjyb/lm-calibration.",
      "year": 2020,
      "influentialCitationCount": 22,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00407/1962628/tacl_a_00407.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "9",
        "pages": "962-977",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "50007145",
          "name": "J. Araki"
        },
        {
          "authorId": "47929135",
          "name": "Haibo Ding"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "358d7d6333d3edd530e37efd8004cb9da8cfd5d4",
      "externalIds": {
        "ACL": "2020.nlpbt-1.4",
        "MAG": "3022767972",
        "DBLP": "journals/corr/abs-2005-00706",
        "ArXiv": "2005.00706",
        "DOI": "10.18653/v1/2020.nlpbt-1.4",
        "CorpusId": 215777958
      },
      "url": "https://www.semanticscholar.org/paper/358d7d6333d3edd530e37efd8004cb9da8cfd5d4",
      "title": "A Benchmark for Structured Procedural Knowledge Extraction from Cooking Videos",
      "abstract": "Watching instructional videos are often used to learn about procedures. Video captioning is one way of automatically collecting such knowledge. However, it provides only an indirect, overall evaluation of multimodal models with no finer-grained quantitative measure of what they have learned. We propose instead, a benchmark of structured procedural knowledge extracted from cooking videos. This work is complementary to existing tasks, but requires models to produce interpretable structured knowledge in the form of verb-argument tuples. Our manually annotated open-vocabulary resource includes 356 instructional cooking videos and 15,523 video clip/sentence-level annotations. Our analysis shows that the proposed task is challenging and standard modeling approaches like unsupervised segmentation, semantic role labeling, and visual action detection perform poorly when forced to predict every action of a procedure in a structured form.",
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.nlpbt-1.4.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2005.00706",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "144906579",
          "name": "Lei Ji"
        },
        {
          "authorId": "119700639",
          "name": "Botian Shi"
        },
        {
          "authorId": "3109653",
          "name": "Junyi Du"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3312309",
          "name": "Yonatan Bisk"
        },
        {
          "authorId": "46429989",
          "name": "Nan Duan"
        }
      ]
    },
    {
      "paperId": "35b376ad9e03e5e0b930c53a48817bfb5703108d",
      "externalIds": {
        "MAG": "3094581721",
        "ACL": "2021.naacl-main.337",
        "DBLP": "conf/naacl/LiuNW21",
        "ArXiv": "2010.12771",
        "DOI": "10.18653/V1/2021.NAACL-MAIN.337",
        "CorpusId": 225070153
      },
      "url": "https://www.semanticscholar.org/paper/35b376ad9e03e5e0b930c53a48817bfb5703108d",
      "title": "On Learning Text Style Transfer with Direct Rewards",
      "abstract": "In most cases, the lack of parallel corpora makes it impossible to directly train supervised models for the text style transfer task. In this paper, we explore training algorithms that instead optimize reward functions that explicitly consider different aspects of the style-transferred outputs. In particular, we leverage semantic similarity metrics originally used for fine-tuning neural machine translation models to explicitly assess the preservation of content between system outputs and input texts. We also investigate the potential weaknesses of the existing automatic metrics and propose efficient strategies of using these metrics for training. The experimental results show that our model provides significant gains in both automatic and human evaluation over strong baselines, indicating the effectiveness of our proposed methods and training strategies.",
      "year": 2020,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.naacl-main.337.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "4262-4273"
      },
      "authors": [
        {
          "authorId": "2108176413",
          "name": "Yixin Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1771118",
          "name": "J. Wieting"
        }
      ]
    },
    {
      "paperId": "3d2ceea5dea234ae9a20f8e1c9e558735757e90e",
      "externalIds": {
        "ArXiv": "2002.11800",
        "DBLP": "conf/icassp/LiD0LLYAMNBM20",
        "MAG": "3008675085",
        "DOI": "10.1109/ICASSP40776.2020.9054362",
        "CorpusId": 211532368
      },
      "url": "https://www.semanticscholar.org/paper/3d2ceea5dea234ae9a20f8e1c9e558735757e90e",
      "title": "Universal Phone Recognition with a Multilingual Allophone System",
      "abstract": "Multilingual models can improve language processing, particularly for low resource situations, by sharing parameters across languages. Multilingual acoustic models, however, generally ignore the difference between phonemes (sounds that can support lexical contrasts in a particular language) and their corresponding phones (the sounds that are actually spoken, which are language independent). This can lead to performance degradation when combining a variety of training languages, as identically annotated phonemes can actually correspond to several different underlying phonetic realizations. In this work, we propose a joint model of both language-independent phone and language-dependent phoneme distributions. In multilingual ASR experiments over 11 languages, we find that this model improves testing performance by 2% phoneme error rate absolute in low-resource conditions. Additionally, because we are explicitly modeling language-independent phones, we can build a (nearly-)universal phone recognizer that, when combined with the PHOIBLE [1] large, manually curated database of phone inventories, can be customized into 2,000 language dependent recognizers. Experiments on two low-resourced indigenous languages, Inuktitut and Tusom, show that our recognizer achieves phone accuracy improvements of more than 17%, moving a step closer to speech recognition for all languages in the world.1",
      "year": 2020,
      "influentialCitationCount": 13,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/2002.11800",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "journal": {
        "pages": "8249-8253",
        "name": "ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
      },
      "authors": [
        {
          "authorId": "47058260",
          "name": "Xinjian Li"
        },
        {
          "authorId": "35186886",
          "name": "Siddharth Dalmia"
        },
        {
          "authorId": "3428237",
          "name": "Juncheng Billy Li"
        },
        {
          "authorId": "70108122",
          "name": "Matthew Russell Lee"
        },
        {
          "authorId": "3070462",
          "name": "Patrick Littell"
        },
        {
          "authorId": "1948799",
          "name": "Jiali Yao"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "3407646",
          "name": "David R. Mortensen"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1690706",
          "name": "A. Black"
        },
        {
          "authorId": "1740721",
          "name": "Florian Metze"
        }
      ]
    },
    {
      "paperId": "407eacc5ade80b54126c300b57b81f4b4f411487",
      "externalIds": {
        "DBLP": "journals/jair/LaubliCNSST20",
        "ArXiv": "2004.01694",
        "MAG": "3013197936",
        "DOI": "10.1613/jair.1.11371",
        "CorpusId": 214634963
      },
      "url": "https://www.semanticscholar.org/paper/407eacc5ade80b54126c300b57b81f4b4f411487",
      "title": "A Set of Recommendations for Assessing Human-Machine Parity in Language Translation",
      "abstract": "The quality of machine translation has increased remarkably over the past years, to the degree that it was found to be indistinguishable from professional human translation in a\u00a0number of empirical investigations. We reassess Hassan et al.'s 2018 investigation into Chinese to English news translation, showing that the finding of human\u2013machine parity\u00a0was owed to weaknesses in the evaluation design\u2014which is currently considered best practice in the field. We show that the professional human translations contained\u00a0significantly fewer errors, and that perceived quality in human evaluation depends on the choice of raters, the availability of linguistic context, and the creation of reference\u00a0translations. Our results call for revisiting current best practices to assess strong machine translation systems in general and human\u2013machine parity in particular, for which we\u00a0offer a set of recommendations based on our empirical findings. \n\u00a0",
      "year": 2020,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://jair.org/index.php/jair/article/download/11371/26573",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "67",
        "pages": "653-672",
        "name": "J. Artif. Intell. Res."
      },
      "authors": [
        {
          "authorId": "2812266",
          "name": "Samuel L\u00e4ubli"
        },
        {
          "authorId": "3041243",
          "name": "Sheila Castilho"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2082372",
          "name": "Rico Sennrich"
        },
        {
          "authorId": "3443246",
          "name": "Qinlan Shen"
        },
        {
          "authorId": "144514048",
          "name": "Antonio Toral"
        }
      ]
    },
    {
      "paperId": "42605c1ee030721cb38a3c225992d63297a6ace0",
      "externalIds": {
        "MAG": "3021720060",
        "DBLP": "conf/sltu/NeubigRPMCLLCGA20",
        "ArXiv": "2004.13203",
        "ACL": "2020.sltu-1.48",
        "CorpusId": 216562359
      },
      "url": "https://www.semanticscholar.org/paper/42605c1ee030721cb38a3c225992d63297a6ace0",
      "title": "A Summary of the First Workshop on Language Technology for Language Documentation and Revitalization",
      "abstract": "Despite recent advances in natural language processing and other language technology, the application of such technology to language documentation and conservation has been limited. In August 2019, a workshop was held at Carnegie Mellon University in Pittsburgh, PA, USA to attempt to bring together language community members, documentary linguists, and technologists to discuss how to bridge this gap and create prototypes of novel and practical language revitalization technologies. The workshop focused on developing technologies to aid language documentation and revitalization in four areas: 1) spoken language (speech transcription, phone to orthography decoding, text-to-speech and text-speech forced alignment), 2) dictionary extraction and management, 3) search tools for corpora, and 4) social media (language learning bots and social media analysis). This paper reports the results of this workshop, including issues discussed, and various conceived and implemented technologies for nine languages: Arapaho, Cayuga, Inuktitut, Irish Gaelic, Kidaw\u2019ida, Kwak\u2019wala, Ojibwe, San Juan Quiahije Chatino, and Seneca.",
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Sociology"
      ],
      "journal": {
        "pages": "342-351"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "6214002",
          "name": "Alexis Palmer"
        },
        {
          "authorId": "2067945941",
          "name": "Jordan MacKenzie"
        },
        {
          "authorId": "38878141",
          "name": "Hilaria Cruz"
        },
        {
          "authorId": "47058260",
          "name": "Xinjian Li"
        },
        {
          "authorId": "70108122",
          "name": "Matthew Russell Lee"
        },
        {
          "authorId": "51250894",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "1389389165",
          "name": "Luke Gessler"
        },
        {
          "authorId": "35551590",
          "name": "Steven P. Abney"
        },
        {
          "authorId": "31998283",
          "name": "Shirley Anugrah Hayati"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "21002356",
          "name": "Olga Zamaraeva"
        },
        {
          "authorId": "113057658",
          "name": "Emily Prudhommeaux"
        },
        {
          "authorId": "1660764736",
          "name": "Jennette Child"
        },
        {
          "authorId": "108454583",
          "name": "Sara Child"
        },
        {
          "authorId": "47511705",
          "name": "Rebecca Knowles"
        },
        {
          "authorId": "51500425",
          "name": "Sarah Moeller"
        },
        {
          "authorId": "2003358",
          "name": "J. Micher"
        },
        {
          "authorId": "50025465",
          "name": "Yiyuan Li"
        },
        {
          "authorId": "2074482488",
          "name": "S. Zink"
        },
        {
          "authorId": "67284811",
          "name": "Mengzhou Xia"
        },
        {
          "authorId": "145521253",
          "name": "Roshan Sharma"
        },
        {
          "authorId": "3070462",
          "name": "Patrick Littell"
        }
      ]
    },
    {
      "paperId": "4383e714f4535777ffb7b4f618d4ccede4b08bd3",
      "externalIds": {
        "MAG": "3017568157",
        "ArXiv": "2004.08031",
        "DBLP": "journals/corr/abs-2004-08031",
        "ACL": "2020.lrec-1.656",
        "CorpusId": 215716862
      },
      "url": "https://www.semanticscholar.org/paper/4383e714f4535777ffb7b4f618d4ccede4b08bd3",
      "title": "AlloVera: A Multilingual Allophone Database",
      "abstract": "We introduce a new resource, AlloVera, which provides mappings from 218 allophones to phonemes for 14 languages. Phonemes are contrastive phonological units, and allophones are their various concrete realizations, which are predictable from phonological context. While phonemic representations are language specific, phonetic representations (stated in terms of (allo)phones) are much closer to a universal (language-independent) transcription. AlloVera allows the training of speech recognition models that output phonetic transcriptions in the International Phonetic Alphabet (IPA), regardless of the input language. We show that a \u201cuniversal\u201d allophone model, Allosaurus, built with AlloVera, outperforms \u201cuniversal\u201d phonemic models and language-specific models on a speech-transcription task. We explore the implications of this technology (and related technologies) for the documentation of endangered and minority languages. We further explore other applications for which AlloVera will be suitable as it grows, including phonological typology.",
      "year": 2020,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "5329-5336"
      },
      "authors": [
        {
          "authorId": "3407646",
          "name": "David R. Mortensen"
        },
        {
          "authorId": "47058260",
          "name": "Xinjian Li"
        },
        {
          "authorId": "3070462",
          "name": "Patrick Littell"
        },
        {
          "authorId": "39720748",
          "name": "Alexis Michaud"
        },
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1690706",
          "name": "A. Black"
        },
        {
          "authorId": "1740721",
          "name": "Florian Metze"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "4bf1ea102e1eb1246929bb77c11ebbd6b6d27500",
      "externalIds": {
        "MAG": "3039602870",
        "DBLP": "journals/corr/abs-2006-16336",
        "ArXiv": "2006.16336",
        "CorpusId": 220265977
      },
      "url": "https://www.semanticscholar.org/paper/4bf1ea102e1eb1246929bb77c11ebbd6b6d27500",
      "title": "Learning Sparse Prototypes for Text Generation",
      "abstract": "Prototype-driven text generation uses non-parametric models that first choose from a library of sentence \"prototypes\" and then modify the prototype to generate the output text. While effective, these methods are inefficient at test time as a result of needing to store and index the entire training corpus. Further, existing methods often require heuristics to identify which prototypes to reference at training time. In this paper, we propose a novel generative model that automatically learns a \\emph{sparse} prototype support set that, nonetheless, achieves strong language modeling performance. This is achieved by (1) imposing a sparsity-inducing prior on the prototype selection distribution, and (2) utilizing amortized variational inference to \\emph{learn} a prototype retrieval function. In experiments, our model outperforms previous prototype-driven language models while achieving up to a 1000x memory reduction, as well as a 1000x speed-up at test time. More interestingly, we show that the learned prototypes are able to capture semantics and syntax at different granularity as we vary the sparsity of prototype selection, and that certain sentence attributes can be controlled by specifying the prototype for generation.",
      "year": 2020,
      "influentialCitationCount": 3,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2006.16336",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2109932032",
          "name": "Junxian He"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "50851e9e16b52e14c422b6e937cfd3ed063b6998",
      "externalIds": {
        "MAG": "3092973241",
        "DBLP": "conf/naacl/HuJFSN21",
        "ACL": "2021.naacl-main.284",
        "ArXiv": "2010.07972",
        "DOI": "10.18653/V1/2021.NAACL-MAIN.284",
        "CorpusId": 223953350
      },
      "url": "https://www.semanticscholar.org/paper/50851e9e16b52e14c422b6e937cfd3ed063b6998",
      "title": "Explicit Alignment Objectives for Multilingual Bidirectional Encoders",
      "abstract": "Pre-trained cross-lingual encoders such as mBERT (Devlin et al., 2019) and XLM-R (Conneau et al., 2020) have proven impressively effective at enabling transfer-learning of NLP systems from high-resource languages to low-resource languages. This success comes despite the fact that there is no explicit objective to align the contextual embeddings of words/sentences with similar meanings across languages together in the same space. In this paper, we present a new method for learning multilingual encoders, AMBER (Aligned Multilingual Bidirectional EncodeR). AMBER is trained on additional parallel data using two explicit alignment objectives that align the multilingual representations at different granularities. We conduct experiments on zero-shot cross-lingual transfer learning for different tasks including sequence tagging, sentence retrieval and sentence classification. Experimental results on the tasks in the XTREME benchmark (Hu et al., 2020) show that AMBER obtains gains of up to 1.1 average F1 score on sequence tagging and up to 27.3 average accuracy on retrieval over the XLM-R-large model which has 3.2x the parameters of AMBER. Our code and models are available at http://github.com/junjiehu/amber.",
      "year": 2020,
      "influentialCitationCount": 11,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.naacl-main.284.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3633-3643"
      },
      "authors": [
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "145657834",
          "name": "Melvin Johnson"
        },
        {
          "authorId": "2345617",
          "name": "Orhan Firat"
        },
        {
          "authorId": "9356387",
          "name": "Aditya Siddhant"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "5431098723db5858c4553f0259921cbbdd6492d5",
      "externalIds": {
        "MAG": "3039149172",
        "ArXiv": "2007.01788",
        "DBLP": "journals/corr/abs-2007-01788",
        "DOI": "10.18653/v1/2020.nlpcovid19-2.5",
        "CorpusId": 220347382
      },
      "url": "https://www.semanticscholar.org/paper/5431098723db5858c4553f0259921cbbdd6492d5",
      "title": "TICO-19: the Translation Initiative for Covid-19",
      "abstract": "The COVID-19 pandemic is the worst pandemic to strike the world in over a century. Crucial to stemming the tide of the SARS-CoV-2 virus is communicating to vulnerable populations the means by which they can protect themselves. To this end, the collaborators forming the Translation Initiative for COvid-19 (TICO-19) have made test and development data available to AI and MT researchers in 35 different languages in order to foster the development of tools and resources for improving access to information about COVID-19 in these languages. In addition to 9 high-resourced, \"pivot\" languages, the team is targeting 26 lesser resourced languages, in particular languages of Africa, South Asia and South-East Asia, whose populations may be the most vulnerable to the spread of the virus. The same data is translated into all of the languages represented, meaning that testing or development can be done for any pairing of languages in the set. Further, the team is converting the test and development data into translation memories (TMXs) that can be used by localizers from and to any of the languages.",
      "year": 2020,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2020.nlpcovid19-2.5.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2007.01788",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "2477823",
          "name": "A. Cattelan"
        },
        {
          "authorId": "14199369",
          "name": "Zi-Yi Dou"
        },
        {
          "authorId": "102811815",
          "name": "Marcello Federico"
        },
        {
          "authorId": "1792029109",
          "name": "C. Federman"
        },
        {
          "authorId": "2870702",
          "name": "Dmitriy Genzel"
        },
        {
          "authorId": "2061585840",
          "name": "Francisco Guzm'an"
        },
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "48342565",
          "name": "Macduff Hughes"
        },
        {
          "authorId": "49604675",
          "name": "Philipp Koehn"
        },
        {
          "authorId": "1792126083",
          "name": "Rosie Lazar"
        },
        {
          "authorId": "2055484840",
          "name": "Will Lewis"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2024268002",
          "name": "Mengmeng Niu"
        },
        {
          "authorId": "1591189507",
          "name": "A. Oktem"
        },
        {
          "authorId": "153519059",
          "name": "Eric Paquin"
        },
        {
          "authorId": "104235312",
          "name": "G. Tang"
        },
        {
          "authorId": "1792060446",
          "name": "Sylwia Tur"
        }
      ]
    },
    {
      "paperId": "57e4074c588c0e27e4c0bc89f12512ccdb900d79",
      "externalIds": {
        "MAG": "3004665584",
        "ArXiv": "2002.03912",
        "DBLP": "conf/iclr/HeWNB20",
        "CorpusId": 211069439
      },
      "url": "https://www.semanticscholar.org/paper/57e4074c588c0e27e4c0bc89f12512ccdb900d79",
      "title": "A Probabilistic Formulation of Unsupervised Text Style Transfer",
      "abstract": "We present a deep generative model for unsupervised text style transfer that unifies previously proposed non-generative techniques. Our probabilistic approach models non-parallel data from two domains as a partially observed parallel corpus. By hypothesizing a parallel latent sequence that generates each observed sequence, our model learns to transform sequences from one domain to another in a completely unsupervised fashion. In contrast with traditional generative sequence models (e.g. the HMM), our model makes few assumptions about the data it generates: it uses a recurrent language model as a prior and an encoder-decoder as a transduction distribution. While computation of marginal data likelihood is intractable in this model class, we show that amortized variational inference admits a practical surrogate. Further, by drawing connections between our variational objective and other recent unsupervised style transfer and machine translation techniques, we show how our probabilistic view can unify some known non-generative objectives such as backtranslation and adversarial loss. Finally, we demonstrate the effectiveness of our method on a wide range of unsupervised style transfer tasks, including sentiment transfer, formality transfer, word decipherment, author imitation, and related language translation. Across all style transfer tasks, our approach yields substantial gains over state-of-the-art non-generative baselines, including the state-of-the-art unsupervised machine translation techniques that our approach generalizes. Further, we conduct experiments on a standard unsupervised machine translation task and find that our unified approach matches the current state-of-the-art.",
      "year": 2020,
      "influentialCitationCount": 27,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2002.03912",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": null,
          "name": "Xinyi Wang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        }
      ]
    },
    {
      "paperId": "59653e5cfa854a17c2ffcb86f2a454f27e12c716",
      "externalIds": {
        "MAG": "3110266480",
        "ArXiv": "2011.13477",
        "DBLP": "journals/corr/abs-2011-13477",
        "CorpusId": 227209253
      },
      "url": "https://www.semanticscholar.org/paper/59653e5cfa854a17c2ffcb86f2a454f27e12c716",
      "title": "Decoding and Diversity in Machine Translation",
      "abstract": "Neural Machine Translation (NMT) systems are typically evaluated using automated metrics that assess the agreement between generated translations and ground truth candidates. To improve systems with respect to these metrics, NLP researchers employ a variety of heuristic techniques, including searching for the conditional mode (vs. sampling) and incorporating various training heuristics (e.g., label smoothing). While search strategies significantly improve BLEU score, they yield deterministic outputs that lack the diversity of human translations. Moreover, search tends to bias the distribution of translated gender pronouns. This makes human-level BLEU a misleading benchmark in that modern MT systems cannot approach human-level BLEU while simultaneously maintaining human-level translation diversity. In this paper, we characterize distributional differences between generated and real translations, examining the cost in diversity paid for the BLEU scores enjoyed by NMT. Moreover, our study implicates search as a salient source of known bias when translating gender pronouns.",
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2011.13477",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2069037355",
          "name": "Nicholas Roberts"
        },
        {
          "authorId": "25130521",
          "name": "Davis Liang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "32219137",
          "name": "Zachary Chase Lipton"
        }
      ]
    },
    {
      "paperId": "5e657bc8097c12649d027ca3c16ff7d37df1354d",
      "externalIds": {
        "DBLP": "conf/acl/WangTN20",
        "ArXiv": "2004.06748",
        "ACL": "2020.acl-main.754",
        "MAG": "3035019713",
        "DOI": "10.18653/v1/2020.acl-main.754",
        "CorpusId": 215768769
      },
      "url": "https://www.semanticscholar.org/paper/5e657bc8097c12649d027ca3c16ff7d37df1354d",
      "title": "Balancing Training for Multilingual Neural Machine Translation",
      "abstract": "When training multilingual machine translation (MT) models that can translate to/from multiple languages, we are faced with imbalanced training sets: some languages have much more training data than others. Standard practice is to up-sample less resourced languages to increase representation, and the degree of up-sampling has a large effect on the overall performance. In this paper, we propose a method that instead automatically learns how to weight training data through a data scorer that is optimized to maximize performance on all test languages. Experiments on two sets of languages under both one-to-many and many-to-one MT settings show our method not only consistently outperforms heuristic baselines in terms of average performance, but also offers flexible control over the performance of which languages are optimized.",
      "year": 2020,
      "influentialCitationCount": 17,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.acl-main.754.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2004.06748",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": null,
          "name": "Xinyi Wang"
        },
        {
          "authorId": "145317727",
          "name": "Yulia Tsvetkov"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "65c2a39f1579a947926ac5746888445ea4afdf6e",
      "externalIds": {
        "MAG": "3099862735",
        "DBLP": "journals/tacl/ZhuBN20",
        "ArXiv": "2007.15135",
        "DOI": "10.1162/tacl_a_00337",
        "CorpusId": 220870814
      },
      "url": "https://www.semanticscholar.org/paper/65c2a39f1579a947926ac5746888445ea4afdf6e",
      "title": "The Return of Lexical Dependencies: Neural Lexicalized PCFGs",
      "abstract": "Abstract In this paper we demonstrate that context free grammar (CFG) based methods for grammar induction benefit from modeling lexical dependencies. This contrasts to the most popular current methods for grammar induction, which focus on discovering either constituents or dependencies. Previous approaches to marry these two disparate syntactic formalisms (e.g., lexicalized PCFGs) have been plagued by sparsity, making them unsuitable for unsupervised grammar induction. However, in this work, we present novel neural models of lexicalized PCFGs that allow us to overcome sparsity problems and effectively induce both constituents and dependencies within a single model. Experiments demonstrate that this unified framework results in stronger results on both representations than achieved when modeling either formalism alone.1",
      "year": 2020,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00337/1923563/tacl_a_00337.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "8",
        "pages": "647-661",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "144459859",
          "name": "Hao Zhu"
        },
        {
          "authorId": "3312309",
          "name": "Yonatan Bisk"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "6c68866e6486923d2e8b999de57d450c9d4febab",
      "externalIds": {
        "MAG": "3014118885",
        "DBLP": "journals/mt/ZhangUSNN20",
        "DOI": "10.1007/s10590-020-09244-y",
        "CorpusId": 214783645
      },
      "url": "https://www.semanticscholar.org/paper/6c68866e6486923d2e8b999de57d450c9d4febab",
      "title": "Improving neural machine translation through phrase-based soft forced decoding",
      "abstract": null,
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "34",
        "pages": "21 - 39",
        "name": "Machine Translation"
      },
      "authors": [
        {
          "authorId": "2108123143",
          "name": "Jingyi Zhang"
        },
        {
          "authorId": "1802277",
          "name": "M. Utiyama"
        },
        {
          "authorId": "1609793456",
          "name": "Eiichro Sumita"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "708dcd8456426cd609c89a86344e0007c04c80bf",
      "externalIds": {
        "DBLP": "conf/emnlp/JiangAADN20",
        "MAG": "3092939616",
        "ACL": "2020.emnlp-main.479",
        "ArXiv": "2010.06189",
        "DOI": "10.18653/v1/2020.emnlp-main.479",
        "CorpusId": 222310559
      },
      "url": "https://www.semanticscholar.org/paper/708dcd8456426cd609c89a86344e0007c04c80bf",
      "title": "X-FACTR: Multilingual Factual Knowledge Retrieval from Pretrained Language Models",
      "abstract": "Language models (LMs) have proven surprisingly successful at capturing factual knowledge by completing cloze-style fill-in-the-blank questions such as \"Punta Cana is located in _.\" However, while knowledge is both written and queried in many languages, studies on LMs' factual representation ability have almost invariably been performed on English. To assess factual knowledge retrieval in LMs in different languages, we create a multilingual benchmark of cloze-style probes for 23 typologically diverse languages. To properly handle language variations, we expand probing methods from single- to multi-word entities, and develop several decoding algorithms to generate multi-token predictions. Extensive experimental results provide insights about how well (or poorly) current state-of-the-art LMs perform at this task in languages with more or fewer available resources. We further propose a code-switching-based method to improve the ability of multilingual LMs to access knowledge, and verify its effectiveness on several benchmark languages. Benchmark data and code have been released at https://x-factr.github.io.",
      "year": 2020,
      "influentialCitationCount": 21,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.emnlp-main.479.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2010.06189",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "50007145",
          "name": "J. Araki"
        },
        {
          "authorId": "47929135",
          "name": "Haibo Ding"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "714fef24b1e87fae20b1d49ab3b6a276548ae7a2",
      "externalIds": {
        "DBLP": "journals/tacl/ChaudharySAN21",
        "MAG": "3094999800",
        "ArXiv": "2011.00767",
        "ACL": "2021.tacl-1.1",
        "DOI": "10.1162/tacl_a_00350",
        "CorpusId": 226226610
      },
      "url": "https://www.semanticscholar.org/paper/714fef24b1e87fae20b1d49ab3b6a276548ae7a2",
      "title": "Reducing Confusion in Active Learning for Part-Of-Speech Tagging",
      "abstract": "Active learning (AL) uses a data selection algorithm to select useful training samples to minimize annotation cost. This is now an essential tool for building low-resource syntactic analyzers such as part-of-speech (POS) taggers. Existing AL heuristics are generally designed on the principle of selecting uncertain yet representative training instances, where annotating these instances may reduce a large number of errors. However, in an empirical study across six typologically diverse languages (German, Swedish, Galician, North Sami, Persian, and Ukrainian), we found the surprising result that even in an oracle scenario where we know the true uncertainty of predictions, these current heuristics are far from optimal. Based on this analysis, we pose the problem of AL as selecting instances that maximally reduce the confusion between particular pairs of output tags. Extensive experimentation on the aforementioned languages shows that our proposed AL strategy outperforms other AL strategies by a significant margin. We also present auxiliary results demonstrating the importance of proper calibration of models, which we ensure through cross-view training, and analysis demonstrating how our proposed strategy selects examples that more closely follow the oracle data distribution. The code is publicly released here.1",
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00350/1924263/tacl_a_00350.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "9",
        "pages": "1-16",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "51250894",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "38599655",
          "name": "Zaid A. W. Sheikh"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "7618c65685c98fa88526555ae3f62cd5645066ad",
      "externalIds": {
        "MAG": "3033919532",
        "DBLP": "conf/akbc/JiangAYZXYN20",
        "DOI": "10.24432/C5MS3J",
        "CorpusId": 211268475
      },
      "url": "https://www.semanticscholar.org/paper/7618c65685c98fa88526555ae3f62cd5645066ad",
      "title": "Learning Relation Entailment with Structured and Textual Information",
      "abstract": "Relations among words and entities are important for semantic understanding of text, but previous work has largely not considered relations between relations, or meta-relations. In this paper, we specifically examine relation entailment, where the existence of one relation can entail the existence of another relation. Relation entailment allows us to construct relation hierarchies, enabling applications in representation learning, question answering, relation extraction, and summarization. To this end, we formally define the new task of predicting relation entailment and construct a dataset by expanding the existing Wikidata relation hierarchy without expensive human intervention. We propose several methods that incorporate both structured and textual information to represent relations for this task. Experiments and analysis demonstrate that this task is challenging, and we provide insights into task characteristics that may form a basis for future work. The dataset and code have been released at https://github.com/jzbjyb/RelEnt.",
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "50007145",
          "name": "J. Araki"
        },
        {
          "authorId": "15121583",
          "name": "Donghan Yu"
        },
        {
          "authorId": "46752970",
          "name": "Ruohong Zhang"
        },
        {
          "authorId": "1485330793",
          "name": "Wei Xu"
        },
        {
          "authorId": "35729970",
          "name": "Yiming Yang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "77910e51a40d17157fc798325d06edfa6cff18d6",
      "externalIds": {
        "DBLP": "conf/acl/XuJYVN20",
        "MAG": "3017094847",
        "ACL": "2020.acl-main.538",
        "ArXiv": "2004.09015",
        "DOI": "10.18653/v1/2020.acl-main.538",
        "CorpusId": 215828318
      },
      "url": "https://www.semanticscholar.org/paper/77910e51a40d17157fc798325d06edfa6cff18d6",
      "title": "Incorporating External Knowledge through Pre-training for Natural Language to Code Generation",
      "abstract": "Open-domain code generation aims to generate code in a general-purpose programming language (such as Python) from natural language (NL) intents. Motivated by the intuition that developers usually retrieve resources on the web when writing code, we explore the effectiveness of incorporating two varieties of external knowledge into NL-to-code generation: automatically mined NL-code pairs from the online programming QA forum StackOverflow and programming language API documentation. Our evaluations show that combining the two sources with data augmentation and retrieval-based data re-sampling improves the current state-of-the-art by up to 2.2% absolute BLEU score on the code generation testbed CoNaLa. The code and resources are available at https://github.com/neulab/external-knowledge-codegen.",
      "year": 2020,
      "influentialCitationCount": 8,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.acl-main.538.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2004.09015",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "2434621",
          "name": "Bogdan Vasilescu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "80257b7d02ad4d6a762ebc0d7f1560e0ef182354",
      "externalIds": {
        "ArXiv": "2004.14257",
        "ACL": "2020.acl-main.169",
        "DBLP": "journals/corr/abs-2004-14257",
        "MAG": "3021491028",
        "DOI": "10.18653/v1/2020.acl-main.169",
        "CorpusId": 215811473
      },
      "url": "https://www.semanticscholar.org/paper/80257b7d02ad4d6a762ebc0d7f1560e0ef182354",
      "title": "Politeness Transfer: A Tag and Generate Approach",
      "abstract": "This paper introduces a new task of politeness transfer which involves converting non-polite sentences to polite sentences while preserving the meaning. We also provide a dataset of more than 1.39 instances automatically labeled for politeness to encourage benchmark evaluations on this new task. We design a tag and generate pipeline that identifies stylistic attributes and subsequently generates a sentence in the target style while preserving most of the source content. For politeness as well as five other transfer tasks, our model outperforms the state-of-the-art methods on automatic metrics for content preservation, with a comparable or better performance on style transfer accuracy. Additionally, our model surpasses existing methods on human evaluations for grammaticality, meaning preservation and transfer accuracy across all the six style transfer tasks. The data and code is located at https://github.com/tag-and-generate.",
      "year": 2020,
      "influentialCitationCount": 17,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.acl-main.169.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1869-1881"
      },
      "authors": [
        {
          "authorId": "21626987",
          "name": "Aman Madaan"
        },
        {
          "authorId": "80366270",
          "name": "Amrith Rajagopal Setlur"
        },
        {
          "authorId": "46719088",
          "name": "Tanmay Parekh"
        },
        {
          "authorId": "1719347",
          "name": "B. P\u00f3czos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "35729970",
          "name": "Yiming Yang"
        },
        {
          "authorId": "145124475",
          "name": "R. Salakhutdinov"
        },
        {
          "authorId": "1690706",
          "name": "A. Black"
        },
        {
          "authorId": "9358910",
          "name": "Shrimai Prabhumoye"
        }
      ]
    },
    {
      "paperId": "98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de",
      "externalIds": {
        "MAG": "3034263272",
        "ArXiv": "2005.01866",
        "ACL": "2020.acl-main.722",
        "DBLP": "journals/corr/abs-2005-01866",
        "DOI": "10.18653/v1/2020.acl-main.722",
        "CorpusId": 218502548
      },
      "url": "https://www.semanticscholar.org/paper/98caf4eb79208cf4bbfe20bde37bc1b6ded6d6de",
      "title": "Soft Gazetteers for Low-Resource Named Entity Recognition",
      "abstract": "Traditional named entity recognition models use gazetteers (lists of entities) as features to improve performance. Although modern neural network models do not require such hand-crafted features for strong performance, recent work has demonstrated their utility for named entity recognition on English data. However, designing such features for low-resource languages is challenging, because exhaustive entity gazetteers do not exist in these languages. To address this problem, we propose a method of \u201csoft gazetteers\u201d that incorporates ubiquitously available information from English knowledge bases, such as Wikipedia, into neural named entity recognition models through cross-lingual entity linking. Our experiments on four low-resource languages show an average improvement of 4 points in F1 score.",
      "year": 2020,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.acl-main.722.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2005.01866",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143712374",
          "name": "J. Carbonell"
        }
      ]
    },
    {
      "paperId": "a064010cf6fe594b2506a8fecd16dc0040211daa",
      "externalIds": {
        "MAG": "3092061379",
        "DBLP": "conf/emnlp/GaoWN20",
        "ACL": "2020.findings-emnlp.319",
        "ArXiv": "2010.01667",
        "DOI": "10.18653/v1/2020.findings-emnlp.319",
        "CorpusId": 222133308
      },
      "url": "https://www.semanticscholar.org/paper/a064010cf6fe594b2506a8fecd16dc0040211daa",
      "title": "Improving Target-side Lexical Transfer in Multilingual Neural Machine Translation",
      "abstract": "To improve the performance of Neural Machine Translation (NMT) for low-resource languages (LRL), one effective strategy is to leverage parallel data from a related high-resource language (HRL). However, multilingual data has been found more beneficial for NMT models that translate from the LRL to a target language than the ones that translate into the LRLs. In this paper, we aim to improve the effectiveness of multilingual transfer for NMT models that translate into the LRL, by designing a better decoder word embedding. Extending upon a general-purpose multilingual encoding method Soft Decoupled Encoding (Wang et al., 2019), we propose DecSDE, an efficient character n-gram based embedding specifically designed for the NMT decoder. Our experiments show that DecSDE leads to consistent gains of up to 1.8 BLEU on translation from English to four different languages.",
      "year": 2020,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.findings-emnlp.319.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3560-3566"
      },
      "authors": [
        {
          "authorId": "49715441",
          "name": "Luyu Gao"
        },
        {
          "authorId": null,
          "name": "Xinyi Wang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "a5b1d1cab073cb746a990b37d42dc7b67763f881",
      "externalIds": {
        "MAG": "3025624935",
        "ArXiv": "2005.08314",
        "DBLP": "journals/corr/abs-2005-08314",
        "ACL": "2020.acl-main.745",
        "DOI": "10.18653/v1/2020.acl-main.745",
        "CorpusId": 218674345
      },
      "url": "https://www.semanticscholar.org/paper/a5b1d1cab073cb746a990b37d42dc7b67763f881",
      "title": "TaBERT: Pretraining for Joint Understanding of Textual and Tabular Data",
      "abstract": "Recent years have witnessed the burgeoning of pretrained language models (LMs) for text-based natural language (NL) understanding tasks. Such models are typically trained on free-form NL text, hence may not be suitable for tasks like semantic parsing over structured data, which require reasoning over both free-form NL questions and structured tabular data (e.g., database tables). In this paper we present TaBERT, a pretrained LM that jointly learns representations for NL sentences and (semi-)structured tables. TaBERT is trained on a large corpus of 26 million tables and their English contexts. In experiments, neural semantic parsers using TaBERT as feature representation layers achieve new best results on the challenging weakly-supervised semantic parsing benchmark WikiTableQuestions, while performing competitively on the text-to-SQL dataset Spider.",
      "year": 2020,
      "influentialCitationCount": 71,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.acl-main.745.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2005.08314",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144105277",
          "name": "Wen-tau Yih"
        },
        {
          "authorId": "48662861",
          "name": "Sebastian Riedel"
        }
      ]
    },
    {
      "paperId": "a9e6222e71dd101d444b7192b3a0636c71edb0a4",
      "externalIds": {
        "DBLP": "journals/corr/abs-2002-10640",
        "ArXiv": "2002.10640",
        "MAG": "2995636882",
        "CorpusId": 211296452
      },
      "url": "https://www.semanticscholar.org/paper/a9e6222e71dd101d444b7192b3a0636c71edb0a4",
      "title": "Differentiable Reasoning over a Virtual Knowledge Base",
      "abstract": "We consider the task of answering complex multi-hop questions using a corpus as a virtual knowledge base (KB). In particular, we describe a neural module, DrKIT, that traverses textual data like a virtual KB, softly following paths of relations between mentions of entities in the corpus. At each step the operation uses a combination of sparse-matrix TFIDF indices and maximum inner product search (MIPS) on a special index of contextual representations. This module is differentiable, so the full system can be trained completely end-to-end using gradient based methods, starting from natural language inputs. We also describe a pretraining scheme for the index mention encoder by generating hard negative examples using existing knowledge bases. We show that DrKIT improves accuracy by 9 points on 3-hop questions in the MetaQA dataset, cutting the gap between text-based and KB-based state-of-the-art by 70%. DrKIT is also very efficient, processing upto 10x more queries per second than existing state-of-the-art QA systems.",
      "year": 2020,
      "influentialCitationCount": 9,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2002.10640",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "34994191",
          "name": "Bhuwan Dhingra"
        },
        {
          "authorId": "1771307",
          "name": "M. Zaheer"
        },
        {
          "authorId": "143820870",
          "name": "Vidhisha Balachandran"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145124475",
          "name": "R. Salakhutdinov"
        },
        {
          "authorId": "50056360",
          "name": "William W. Cohen"
        }
      ]
    },
    {
      "paperId": "aa2428e1c4ea6d6bb347cfa59beead8736e19c46",
      "externalIds": {
        "MAG": "3104349620",
        "CorpusId": 229267001
      },
      "url": "https://www.semanticscholar.org/paper/aa2428e1c4ea6d6bb347cfa59beead8736e19c46",
      "title": "DIFFERENTIABLE MULTI-HOP REASONING OVER A VIRTUAL KNOWLEDGE BASE",
      "abstract": null,
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "34994191",
          "name": "Bhuwan Dhingra"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1771307",
          "name": "M. Zaheer"
        },
        {
          "authorId": "145124475",
          "name": "R. Salakhutdinov"
        },
        {
          "authorId": "143820870",
          "name": "Vidhisha Balachandran"
        },
        {
          "authorId": "50056360",
          "name": "William W. Cohen"
        }
      ]
    },
    {
      "paperId": "addd2d86d19c1e7c8854e827fb2656a50c250440",
      "externalIds": {
        "MAG": "3099222060",
        "ArXiv": "2011.07832",
        "DBLP": "journals/tacl/HayashiBWANN21",
        "DOI": "10.1162/tacl_a_00362",
        "CorpusId": 226965071
      },
      "url": "https://www.semanticscholar.org/paper/addd2d86d19c1e7c8854e827fb2656a50c250440",
      "title": "WikiAsp: A Dataset for Multi-domain Aspect-based Summarization",
      "abstract": "Abstract Aspect-based summarization is the task of generating focused summaries based on specific points of interest. Such summaries aid efficient analysis of text, such as quickly understanding reviews or opinions from different angles. However, due to large differences in the type of aspects for different domains (e.g., sentiment, product features), the development of previous models has tended to be domain-specific. In this paper, we propose WikiAsp,1 a large-scale dataset for multi-domain aspect- based summarization that attempts to spur research in the direction of open-domain aspect-based summarization. Specifically, we build the dataset using Wikipedia articles from 20 different domains, using the section titles and boundaries of each article as a proxy for aspect annotation. We propose several straightforward baseline models for this task and conduct experiments on the dataset. Results highlight key challenges that existing summarization models face in this setting, such as proper pronoun handling of quoted sources and consistent explanation of time-sensitive events.",
      "year": 2020,
      "influentialCitationCount": 8,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00362/1924027/tacl_a_00362.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "9",
        "pages": "211-225",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "50376014",
          "name": "Hiroaki Hayashi"
        },
        {
          "authorId": "2025121134",
          "name": "Prashant Budania"
        },
        {
          "authorId": null,
          "name": "Peng Wang"
        },
        {
          "authorId": "2025130678",
          "name": "Chris Ackerson"
        },
        {
          "authorId": "2025043981",
          "name": "Raj Neervannan"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "b79dcc5304e557ce200b161d2a884c0ff77f34ec",
      "externalIds": {
        "ACL": "2020.emnlp-demos.21",
        "DBLP": "journals/corr/abs-2010-11085",
        "MAG": "3103161287",
        "ArXiv": "2010.11085",
        "DOI": "10.18653/v1/2020.emnlp-demos.21",
        "CorpusId": 224814398
      },
      "url": "https://www.semanticscholar.org/paper/b79dcc5304e557ce200b161d2a884c0ff77f34ec",
      "title": "NeuSpell: A Neural Spelling Correction Toolkit",
      "abstract": "We introduce NeuSpell, an open-source toolkit for spelling correction in English. Our toolkit comprises ten different models, and benchmarks them on naturally occurring misspellings from multiple sources. We find that many systems do not adequately leverage the context around the misspelt token. To remedy this, (i) we train neural models using spelling errors in context, synthetically constructed by reverse engineering isolated misspellings; and (ii) use richer representations of the context. By training on our synthetic examples, correction rates improve by 9% (absolute) compared to the case when models are trained on randomly sampled character perturbations. Using richer contextual representations boosts the correction rate by another 3%. Our toolkit enables practitioners to use our proposed and existing spelling correction systems, both via a simple unified command line, as well as a web interface. Among many potential applications, we demonstrate the utility of our spell-checkers in combating adversarial misspellings. The toolkit can be accessed at neuspell.github.io.",
      "year": 2020,
      "influentialCitationCount": 8,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.emnlp-demos.21.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2010.11085",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "31602573",
          "name": "Sai Muralidhar Jayanthi"
        },
        {
          "authorId": "7880098",
          "name": "Danish Pruthi"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "ba4a34680e09e77984624c95f5245d91b54373f6",
      "externalIds": {
        "MAG": "3035579820",
        "DBLP": "conf/icml/HuRSNFJ20",
        "ArXiv": "2003.11080",
        "CorpusId": 214641214
      },
      "url": "https://www.semanticscholar.org/paper/ba4a34680e09e77984624c95f5245d91b54373f6",
      "title": "XTREME: A Massively Multilingual Multi-task Benchmark for Evaluating Cross-lingual Generalization",
      "abstract": "Much recent progress in applications of machine learning models to NLP has been driven by benchmarks that evaluate models across a wide variety of tasks. However, these broad-coverage benchmarks have been mostly limited to English, and despite an increasing interest in multilingual models, a benchmark that enables the comprehensive evaluation of such methods on a diverse range of languages and tasks is still missing. To this end, we introduce the Cross-lingual TRansfer Evaluation of Multilingual Encoders XTREME benchmark, a multi-task benchmark for evaluating the cross-lingual generalization capabilities of multilingual representations across 40 languages and 9 tasks. We demonstrate that while models tested on English reach human performance on many tasks, there is still a sizable gap in the performance of cross-lingually transferred models, particularly on syntactic and sentence retrieval tasks. There is also a wide spread of results across languages. We release the benchmark to encourage research on cross-lingual learning methods that transfer linguistic knowledge across a diverse and representative set of languages and tasks.",
      "year": 2020,
      "influentialCitationCount": 167,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2003.11080",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "2884561",
          "name": "Sebastian Ruder"
        },
        {
          "authorId": "9356387",
          "name": "Aditya Siddhant"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2345617",
          "name": "Orhan Firat"
        },
        {
          "authorId": "145657834",
          "name": "Melvin Johnson"
        }
      ]
    },
    {
      "paperId": "c204d40384d39c59cd7249bde4cd8615972acaac",
      "externalIds": {
        "MAG": "3119067070",
        "DBLP": "conf/wmt/SpeciaLPCGNDBKS20",
        "ACL": "2020.wmt-1.4",
        "CorpusId": 229365777
      },
      "url": "https://www.semanticscholar.org/paper/c204d40384d39c59cd7249bde4cd8615972acaac",
      "title": "Findings of the WMT 2020 Shared Task on Machine Translation Robustness",
      "abstract": "We report the findings of the second edition of the shared task on improving robustness in Machine Translation (MT). The task aims to test current machine translation systems in their ability to handle challenges facing MT models to be deployed in the real world, including domain diversity and non-standard texts common in user generated content, especially in social media. We cover two language pairs \u2013 English-German and English-Japanese and provide test sets in zero-shot and few-shot variants. Participating systems are evaluated both automatically and manually, with an additional human evaluation for \u201dcatastrophic errors\u201d. We received 59 submissions by 11 participating teams from a variety of types of institutions.",
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "76-91"
      },
      "authors": [
        {
          "authorId": "1702974",
          "name": "Lucia Specia"
        },
        {
          "authorId": "2109641161",
          "name": "Zhenhao Li"
        },
        {
          "authorId": "145503806",
          "name": "J. Pino"
        },
        {
          "authorId": "113810201",
          "name": "Vishrav Chaudhary"
        },
        {
          "authorId": "144204682",
          "name": "Francisco Guzm\u00e1n"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145938140",
          "name": "Nadir Durrani"
        },
        {
          "authorId": "2083259",
          "name": "Yonatan Belinkov"
        },
        {
          "authorId": "1755162",
          "name": "Philipp Koehn"
        },
        {
          "authorId": "145775792",
          "name": "Hassan Sajjad"
        },
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "2116235416",
          "name": "Xian Li"
        }
      ]
    },
    {
      "paperId": "c8171eaa3a3aac78c3b37351412101bc06e5f359",
      "externalIds": {
        "MAG": "3018870313",
        "DBLP": "journals/corr/abs-2004-11954",
        "ArXiv": "2004.11954",
        "CorpusId": 216552919
      },
      "url": "https://www.semanticscholar.org/paper/c8171eaa3a3aac78c3b37351412101bc06e5f359",
      "title": "Practical Comparable Data Collection for Low-Resource Languages via Images",
      "abstract": "We propose a method of curating high-quality comparable training data for low-resource languages with monolingual annotators. Our method involves using a carefully selected set of images as a pivot between the source and target languages by getting captions for such images in both languages independently. Human evaluations on the English-Hindi comparable corpora created with our method show that 81.1% of the pairs are acceptable translations, and only 2.47% of the pairs are not translations at all. We further establish the potential of the dataset collected through our approach by experimenting on two downstream tasks - machine translation and dictionary extraction. All code and data are available at this https URL.",
      "year": 2020,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2004.11954",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "21626987",
          "name": "Aman Madaan"
        },
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "35729970",
          "name": "Yiming Yang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "c968e8dc442102b38b134b1afadc7cc78fc5b5fb",
      "externalIds": {
        "MAG": "3104329353",
        "ArXiv": "2011.06854",
        "ACL": "2020.emnlp-main.489",
        "DBLP": "journals/corr/abs-2011-06854",
        "DOI": "10.18653/v1/2020.emnlp-main.489",
        "CorpusId": 226262306
      },
      "url": "https://www.semanticscholar.org/paper/c968e8dc442102b38b134b1afadc7cc78fc5b5fb",
      "title": "Interpretable Multi-dataset Evaluation for Named Entity Recognition",
      "abstract": "With the proliferation of models for natural language processing tasks, it is even harder to understand the differences between models and their relative merits. Simply looking at differences between holistic metrics such as accuracy, BLEU, or F1 does not tell us why or how particular methods perform differently and how diverse datasets influence the model design choices. In this paper, we present a general methodology for interpretable evaluation for the named entity recognition (NER) task. The proposed evaluation method enables us to interpret the differences in models and datasets, as well as the interplay between them, identifying the strengths and weaknesses of current systems. By making our analysis tool available, we make it easy for future researchers to run similar analyses and drive progress in this area: this https URL.",
      "year": 2020,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.emnlp-main.489.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2011.06854",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "41037252",
          "name": "Jinlan Fu"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "d38b686b8b68d0b91b294fd8a55ac7dea191706f",
      "externalIds": {
        "MAG": "3170083118",
        "ACL": "2021.naacl-main.384",
        "DBLP": "conf/naacl/DouLHJN21",
        "ArXiv": "2010.08014",
        "DOI": "10.18653/V1/2021.NAACL-MAIN.384",
        "CorpusId": 223953416
      },
      "url": "https://www.semanticscholar.org/paper/d38b686b8b68d0b91b294fd8a55ac7dea191706f",
      "title": "GSum: A General Framework for Guided Neural Abstractive Summarization",
      "abstract": "Neural abstractive summarization models are flexible and can produce coherent summaries, but they are sometimes unfaithful and can be difficult to control. While previous studies attempt to provide different types of guidance to control the output and increase faithfulness, it is not clear how these strategies compare and contrast to each other. In this paper, we propose a general and extensible guided summarization framework (GSum) that can effectively take different kinds of external guidance as input, and we perform experiments across several different varieties. Experiments demonstrate that this model is effective, achieving state-of-the-art performance according to ROUGE on 4 popular summarization datasets when using highlighted sentences as guidance. In addition, we show that our guided model can generate more faithful summaries and demonstrate how different types of guidance generate qualitatively different summaries, lending a degree of controllability to the learned models.",
      "year": 2020,
      "influentialCitationCount": 29,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/2021.naacl-main.384.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2010.08014",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "14199369",
          "name": "Zi-Yi Dou"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "50376014",
          "name": "Hiroaki Hayashi"
        },
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71",
      "externalIds": {
        "MAG": "3104933204",
        "DBLP": "journals/corr/abs-2011-05402",
        "ArXiv": "2011.05402",
        "ACL": "2020.emnlp-main.478",
        "DOI": "10.18653/v1/2020.emnlp-main.478",
        "CorpusId": 226262386
      },
      "url": "https://www.semanticscholar.org/paper/da9ec5053c8ad8854bdd2ddc3f9c3d82a4114d71",
      "title": "OCR Post-Correction for Endangered Language Texts",
      "abstract": "There is little to no data available to build natural language processing models for most endangered languages. However, textual data in these languages often exists in formats that are not machine-readable, such as paper books and scanned images. In this work, we address the task of extracting text from these resources. We create a benchmark dataset of transcriptions for scanned books in three critically endangered languages and present a systematic analysis of how general-purpose OCR tools are not robust to the data-scarce setting of endangered languages. We develop an OCR post-correction method tailored to ease training in this data-scarce setting, reducing the recognition error rate by 34% on average across the three languages.",
      "year": 2020,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.emnlp-main.478.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2011.05402",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "de7d0c87794c3de6f8ab2c753ecc398c18c26631",
      "externalIds": {
        "MAG": "3098550331",
        "DBLP": "conf/emnlp/ChaudharyAPMSTN20",
        "ArXiv": "2010.01160",
        "ACL": "2020.emnlp-main.422",
        "DOI": "10.18653/v1/2020.emnlp-main.422",
        "CorpusId": 222134004
      },
      "url": "https://www.semanticscholar.org/paper/de7d0c87794c3de6f8ab2c753ecc398c18c26631",
      "title": "Automatic Extraction of Rules Governing Morphological Agreement",
      "abstract": "Creating a descriptive grammar of a language is an indispensable step for language documentation and preservation. However, at the same time it is a tedious, time-consuming task. In this paper, we take steps towards automating this process by devising an automated framework for extracting a first-pass grammatical specification from raw text in a concise, human- and machine-readable format. We focus on extracting rules describing agreement, a morphosyntactic phenomenon at the core of the grammars of many of the world's languages. We apply our framework to all languages included in the Universal Dependencies project, with promising results. Using cross-lingual transfer, even with no expert annotations in the language of interest, our framework extracts a grammatical specification which is nearly equivalent to those created with large amounts of gold-standard annotated data. We confirm this finding with human expert evaluations of the rules that our framework produces, which have an average accuracy of 78%. We release an interface demonstrating the extracted rules at this https URL.",
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.emnlp-main.422.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "5212-5236"
      },
      "authors": [
        {
          "authorId": "51250894",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "51132476",
          "name": "Adithya Pratapa"
        },
        {
          "authorId": "3407646",
          "name": "David R. Mortensen"
        },
        {
          "authorId": "38599655",
          "name": "Zaid A. W. Sheikh"
        },
        {
          "authorId": "145317727",
          "name": "Yulia Tsvetkov"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "deedb9b61a01d686b28e6034770fccc142e77fab",
      "externalIds": {
        "DBLP": "conf/acl/XiaAXYN20",
        "MAG": "3020942864",
        "ACL": "2020.acl-main.764",
        "ArXiv": "2005.00870",
        "DOI": "10.18653/v1/2020.acl-main.764",
        "CorpusId": 218487089
      },
      "url": "https://www.semanticscholar.org/paper/deedb9b61a01d686b28e6034770fccc142e77fab",
      "title": "Predicting Performance for Natural Language Processing Tasks",
      "abstract": "Given the complexity of combinations of tasks, languages, and domains in natural language processing (NLP) research, it is computationally prohibitive to exhaustively test newly proposed models on each possible experimental setting. In this work, we attempt to explore the possibility of gaining plausible judgments of how well an NLP model can perform under an experimental setting, without actually training or testing the model. To do so, we build regression models to predict the evaluation score of an NLP experiment given the experimental settings as input. Experimenting on~9 different NLP tasks, we find that our predictors can produce meaningful predictions over unseen languages and different modeling architectures, outperforming reasonable baselines as well as human experts. %we represent experimental settings using an array of features. Going further, we outline how our predictor can be used to find a small subset of representative experiments that should be run in order to obtain plausible predictions for all other experimental settings.",
      "year": 2020,
      "influentialCitationCount": 5,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.acl-main.764.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/2005.00870",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "67284811",
          "name": "Mengzhou Xia"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "8233965",
          "name": "Ruochen Xu"
        },
        {
          "authorId": "35729970",
          "name": "Yiming Yang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "df29486c04eafd004f2f0816e84c798783802cdf",
      "externalIds": {
        "MAG": "3037582736",
        "DBLP": "conf/sigmorphon/MurikinatiAN20",
        "ACL": "2020.sigmorphon-1.22",
        "DOI": "10.18653/v1/2020.sigmorphon-1.22",
        "CorpusId": 220282266
      },
      "url": "https://www.semanticscholar.org/paper/df29486c04eafd004f2f0816e84c798783802cdf",
      "title": "Transliteration for Cross-Lingual Morphological Inflection",
      "abstract": "Cross-lingual transfer between typologically related languages has been proven successful for the task of morphological inflection. However, if the languages do not share the same script, current methods yield more modest improvements. We explore the use of transliteration between related languages, as well as grapheme-to-phoneme conversion, as data preprocessing methods in order to alleviate this issue. We experimented with several diverse language pairs, finding that in most cases transliterating the transfer language data into the target one leads to accuracy improvements, even up to 9 percentage points. Converting both languages into a shared space like the International Phonetic Alphabet or the Latin alphabet is also beneficial, leading to improvements of up to 16 percentage points.",
      "year": 2020,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.sigmorphon-1.22.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "189-197"
      },
      "authors": [
        {
          "authorId": "1785063843",
          "name": "Nikitha Murikinati"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "e2d770b9ab691753a7ec1eb439185303118c8455",
      "externalIds": {
        "DBLP": "conf/eamt/MartinsGDMN20",
        "ACL": "2020.eamt-1.68",
        "MAG": "3082573155",
        "CorpusId": 221097945
      },
      "url": "https://www.semanticscholar.org/paper/e2d770b9ab691753a7ec1eb439185303118c8455",
      "title": "Project MAIA: Multilingual AI Agent Assistant",
      "abstract": "This paper presents the Multilingual Artificial Intelligence Agent Assistant (MAIA), a project led by Unbabel with the collaboration of CMU, INESC-ID and IT Lisbon. MAIA will employ cutting-edge machine learning and natural language processing technologies to build multilingual AI agent assistants, eliminating language barriers. MAIA\u2019s translation layer will empower human agents to provide customer support in real-time, in any language, with human quality.",
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "495-496"
      },
      "authors": [
        {
          "authorId": "1400227478",
          "name": "Andr\u00e9 F. T. Martins"
        },
        {
          "authorId": "143836741",
          "name": "Jo\u00e3o Gra\u00e7a"
        },
        {
          "authorId": "1812371",
          "name": "P. Dimas"
        },
        {
          "authorId": "2504458",
          "name": "Helena Moniz"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "e58edbeb41f3d2d24832e6e3abb94baac754e3f7",
      "externalIds": {
        "DBLP": "journals/corr/abs-2010-07100",
        "ArXiv": "2010.07100",
        "MAG": "3101017384",
        "ACL": "2020.emnlp-main.751",
        "DOI": "10.18653/v1/2020.emnlp-main.751",
        "CorpusId": 222341867
      },
      "url": "https://www.semanticscholar.org/paper/e58edbeb41f3d2d24832e6e3abb94baac754e3f7",
      "title": "Re-evaluating Evaluation in Text Summarization",
      "abstract": "Automated evaluation metrics as a stand-in for manual evaluation are an essential part of the development of text-generation tasks such as text summarization. However, while the field has progressed, our standard metrics have not -- for nearly 20 years ROUGE has been the standard evaluation in most summarization papers. In this paper, we make an attempt to re-evaluate the evaluation method for text summarization: assessing the reliability of automatic metrics using top-scoring system outputs, both abstractive and extractive, on recently popular datasets for both system-level and summary-level evaluation settings. We find that conclusions about evaluation metrics on older datasets do not necessarily hold on modern datasets and systems.",
      "year": 2020,
      "influentialCitationCount": 24,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.emnlp-main.751.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "9347-9359"
      },
      "authors": [
        {
          "authorId": "51444280",
          "name": "Manik Bhandari"
        },
        {
          "authorId": "2117172",
          "name": "Pranav Narayan Gour"
        },
        {
          "authorId": "1995263463",
          "name": "A. Ashfaq"
        },
        {
          "authorId": "144118452",
          "name": "Pengfei Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "ec6499842d3e51b7dda94f5d0620d6df5c1a1b6d",
      "externalIds": {
        "MAG": "3005578234",
        "DBLP": "journals/taslp/ScharenborgOPAC20",
        "DOI": "10.1109/TASLP.2020.2973896",
        "CorpusId": 212681969
      },
      "url": "https://www.semanticscholar.org/paper/ec6499842d3e51b7dda94f5d0620d6df5c1a1b6d",
      "title": "Speech Technology for Unwritten Languages",
      "abstract": "Speech technology plays an important role in our everyday life. Among others, speech is used for human-computer interaction, for instance for information retrieval and on-line shopping. In the case of an unwritten language, however, speech technology is unfortunately difficult to create, because it cannot be created by the standard combination of pre-trained speech-to-text and text-to-speech subsystems. The research presented in this article takes the first steps towards speech technology for unwritten languages. Specifically, the aim of this work was 1) to learn speech-to-meaning representations without using text as an intermediate representation, and 2) to test the sufficiency of the learned representations to regenerate speech or translated text, or to retrieve images that depict the meaning of an utterance in an unwritten language. The results suggest that building systems that go directly from speech-to-meaning and from meaning-to-speech, bypassing the need for text, is possible.",
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "28",
        "pages": "964-975",
        "name": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
      },
      "authors": [
        {
          "authorId": "1700735",
          "name": "O. Scharenborg"
        },
        {
          "authorId": "2167829",
          "name": "Lucas Ondel"
        },
        {
          "authorId": "26400211",
          "name": "Shruti Palaskar"
        },
        {
          "authorId": "144332819",
          "name": "Philip Arthur"
        },
        {
          "authorId": "35426982",
          "name": "Francesco Ciannella"
        },
        {
          "authorId": "2054401198",
          "name": "Mingxing Du"
        },
        {
          "authorId": "40651102",
          "name": "Elin Larsen"
        },
        {
          "authorId": "35833790",
          "name": "Danny Merkx"
        },
        {
          "authorId": "40425637",
          "name": "Rachid Riad"
        },
        {
          "authorId": "2109120538",
          "name": "Liming Wang"
        },
        {
          "authorId": "2202008",
          "name": "Emmanuel Dupoux"
        },
        {
          "authorId": "143823463",
          "name": "L. Besacier"
        },
        {
          "authorId": "1690706",
          "name": "A. Black"
        },
        {
          "authorId": "1399115926",
          "name": "M. Hasegawa-Johnson"
        },
        {
          "authorId": "1740721",
          "name": "Florian Metze"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "102181710",
          "name": "S. Stueker"
        },
        {
          "authorId": "2058749172",
          "name": "Pierre Godard"
        },
        {
          "authorId": "2116238031",
          "name": "Markus Mueller"
        }
      ]
    },
    {
      "paperId": "f6d6c4dd0115386c234a0b027dd38f7aa9d9df2f",
      "externalIds": {
        "MAG": "3116628494",
        "DBLP": "conf/coling/AnastasopoulosC20",
        "ACL": "2020.coling-tutorials.7",
        "DOI": "10.18653/V1/2020.COLING-TUTORIALS.7",
        "CorpusId": 227231085
      },
      "url": "https://www.semanticscholar.org/paper/f6d6c4dd0115386c234a0b027dd38f7aa9d9df2f",
      "title": "Endangered Languages meet Modern NLP",
      "abstract": "This tutorial will focus on NLP for endangered languages documentation and revitalization. First, we will acquaint the attendees with the process and the challenges of language documentation, showing how the needs of the language communities and the documentary linguists map to specific NLP tasks. We will then present the state-of-the-art in NLP applied in this particularly challenging setting (extremely low-resource datasets, noisy transcriptions, limited annotations, non-standard orthographies). In doing so, we will also analyze the challenges of working in this domain and expand on both the capabilities and the limitations of current NLP approaches. Our ultimate goal is to motivate more NLP practitioners to work towards this very important direction, and also provide them with the tools and understanding of the limitations/challenges, both of which are needed in order to have an impact.",
      "year": 2020,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "39-45"
      },
      "authors": [
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "2052347303",
          "name": "Christopher Cox"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "38878141",
          "name": "Hilaria Cruz"
        }
      ]
    },
    {
      "paperId": "005879e6587eb6e05f56c20d345f784ee84a44c4",
      "externalIds": {
        "DBLP": "conf/conll/MatthewsND19",
        "ACL": "K19-1022",
        "MAG": "2984511769",
        "DOI": "10.18653/v1/K19-1022",
        "CorpusId": 208262691
      },
      "url": "https://www.semanticscholar.org/paper/005879e6587eb6e05f56c20d345f784ee84a44c4",
      "title": "Comparing Top-Down and Bottom-Up Neural Generative Dependency Models",
      "abstract": "Recurrent neural network grammars generate sentences using phrase-structure syntax and perform very well on both parsing and language modeling. To explore whether generative dependency models are similarly effective, we propose two new generative models of dependency syntax. Both models use recurrent neural nets to avoid making explicit independence assumptions, but they differ in the order used to construct the trees: one builds the tree bottom-up and the other top-down, which profoundly changes the estimation problem faced by the learner. We evaluate the two models on three typologically different languages: English, Arabic, and Japanese. While both generative models improve parsing performance over a discriminative baseline, they are significantly less effective than non-syntactic LSTM language models. Surprisingly, little difference between the construction orders is observed for either parsing or language modeling.",
      "year": 2019,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/K19-1022.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "227-237"
      },
      "authors": [
        {
          "authorId": "144633696",
          "name": "Austin Matthews"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1745899",
          "name": "Chris Dyer"
        }
      ]
    },
    {
      "paperId": "02d98ca8f4ecd1a2b885d6867f4c1407ae8d1007",
      "externalIds": {
        "DBLP": "conf/aaai/NiYN20",
        "MAG": "2990928202",
        "ArXiv": "1911.12986",
        "DOI": "10.1609/AAAI.V34I05.6375",
        "CorpusId": 208512644
      },
      "url": "https://www.semanticscholar.org/paper/02d98ca8f4ecd1a2b885d6867f4c1407ae8d1007",
      "title": "Merging Weak and Active Supervision for Semantic Parsing",
      "abstract": "A semantic parser maps natural language commands (NLs) from the users to executable meaning representations (MRs), which are later executed in certain environment to obtain user-desired results. The fully-supervised training of such parser requires NL/MR pairs, annotated by domain experts, which makes them expensive to collect. However, weakly-supervised semantic parsers are learnt only from pairs of NL and expected execution results, leaving the MRs latent. While weak supervision is cheaper to acquire, learning from this input poses difficulties. It demands that parsers search a large space with a very weak learning signal and it is hard to avoid spurious MRs that achieve the correct answer in the wrong way. These factors lead to a performance gap between parsers trained in weakly- and fully-supervised setting. To bridge this gap, we examine the intersection between weak supervision and active learning, which allows the learner to actively select examples and query for manual annotations as extra supervision to improve the model trained under weak supervision. We study different active learning heuristics for selecting examples to query, and various forms of extra supervision for such queries. We evaluate the effectiveness of our method on two different datasets. Experiments on the WikiSQL show that by annotating only 1.8% of examples, we improve over a state-of-the-art weakly-supervised baseline by 6.4%, achieving an accuracy of 79.0%, which is only 1.3% away from the model trained with full supervision. Experiments on WikiTableQuestions with human annotators show that our method can improve the performance with only 100 active queries, especially for weakly-supervised parsers learnt from a cold start. 1",
      "year": 2019,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6375/6231",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1911.12986",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "33981736",
          "name": "Ansong Ni"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "03006aefccdd0c5c6736ab11ed574d02ba1cc086",
      "externalIds": {
        "DBLP": "journals/corr/abs-1909-00040",
        "ArXiv": "1909.00040",
        "ACL": "D19-1143",
        "MAG": "2971459527",
        "DOI": "10.18653/v1/D19-1143",
        "CorpusId": 202540346
      },
      "url": "https://www.semanticscholar.org/paper/03006aefccdd0c5c6736ab11ed574d02ba1cc086",
      "title": "Handling Syntactic Divergence in Low-resource Machine Translation",
      "abstract": "Despite impressive empirical successes of neural machine translation (NMT) on standard benchmarks, limited parallel data impedes the application of NMT models to many language pairs. Data augmentation methods such as back-translation make it possible to use monolingual data to help alleviate these issues, but back-translation itself fails in extreme low-resource scenarios, especially for syntactically divergent languages. In this paper, we propose a simple yet effective solution, whereby target-language sentences are re-ordered to match the order of the source and used as an additional source of training-time supervision. Experiments with simulated low-resource Japanese-to-English, and real low-resource Uyghur-to-English scenarios find significant improvements over other semi-supervised alternatives.",
      "year": 2019,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D19-1143.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1388-1394"
      },
      "authors": [
        {
          "authorId": "2384711",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "2378954",
          "name": "Xuezhe Ma"
        },
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "06f4de06fc37576e1e381cd76e375d57852047b9",
      "externalIds": {
        "MAG": "2970798168",
        "DBLP": "conf/wmt/ZhouZZAN19",
        "ACL": "W19-5368",
        "DOI": "10.18653/v1/W19-5368",
        "CorpusId": 201740743
      },
      "url": "https://www.semanticscholar.org/paper/06f4de06fc37576e1e381cd76e375d57852047b9",
      "title": "Improving Robustness of Neural Machine Translation with Multi-task Learning",
      "abstract": "While neural machine translation (NMT) achieves remarkable performance on clean, in-domain text, performance is known to degrade drastically when facing text which is full of typos, grammatical errors and other varieties of noise. In this work, we propose a multi-task learning algorithm for transformer-based MT systems that is more resilient to this noise. We describe our submission to the WMT 2019 Robustness shared task based on this method. Our model achieves a BLEU score of 32.8 on the shared task French to English dataset, which is 7.1 BLEU points higher than the baseline vanilla transformer trained with clean text.",
      "year": 2019,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W19-5368.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "565-571"
      },
      "authors": [
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "3510996",
          "name": "Xiangkai Zeng"
        },
        {
          "authorId": "2145474273",
          "name": "Yingqi Zhou"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "0be998fffc5f44496042f7757fb2ffa8924e54cd",
      "externalIds": {
        "DBLP": "journals/corr/abs-1911-10088",
        "ArXiv": "1911.10088",
        "MAG": "2989650185",
        "CorpusId": 208248381
      },
      "url": "https://www.semanticscholar.org/paper/0be998fffc5f44496042f7757fb2ffa8924e54cd",
      "title": "Optimizing Data Usage via Differentiable Rewards",
      "abstract": "To acquire a new skill, humans learn better and faster if a tutor, based on their current knowledge level, informs them of how much attention they should pay to particular content or practice problems. Similarly, a machine learning model could potentially be trained better with a scorer that \"adapts\" to its current learning state and estimates the importance of each training data instance. Training such an adaptive scorer efficiently is a challenging problem; in order to precisely quantify the effect of a data instance at a given time during the training, it is typically necessary to first complete the entire training process. To efficiently optimize data usage, we propose a reinforcement learning approach called Differentiable Data Selection (DDS). In DDS, we formulate a scorer network as a learnable function of the training data, which can be efficiently updated along with the main model being trained. Specifically, DDS updates the scorer with an intuitive reward signal: it should up-weigh the data that has a similar gradient with a dev set upon which we would finally like to perform well. Without significant computing overhead, DDS delivers strong and consistent improvements over several strong baselines on two very different tasks of machine translation and image classification.",
      "year": 2019,
      "influentialCitationCount": 7,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "volume": "abs/1911.10088",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": null,
          "name": "Xinyi Wang"
        },
        {
          "authorId": "121562149",
          "name": "Hieu Pham"
        },
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143712374",
          "name": "J. Carbonell"
        }
      ]
    },
    {
      "paperId": "0c89b1ec80de46222ed7efc6261c03e52a1e2c54",
      "externalIds": {
        "MAG": "2946748737",
        "DBLP": "conf/naacl/IshiwatariH0NST19",
        "ACL": "N19-1350",
        "DOI": "10.18653/v1/N19-1350",
        "CorpusId": 174799844
      },
      "url": "https://www.semanticscholar.org/paper/0c89b1ec80de46222ed7efc6261c03e52a1e2c54",
      "title": "Learning to Describe Unknown Phrases with Local and Global Contexts",
      "abstract": "When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.",
      "year": 2019,
      "influentialCitationCount": 13,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3467-3476"
      },
      "authors": [
        {
          "authorId": "46724659",
          "name": "Shonosuke Ishiwatari"
        },
        {
          "authorId": "50376014",
          "name": "Hiroaki Hayashi"
        },
        {
          "authorId": "34849332",
          "name": "Naoki Yoshinaga"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "8846453",
          "name": "Shoetsu Sato"
        },
        {
          "authorId": "2361778",
          "name": "Masashi Toyoda"
        },
        {
          "authorId": "1716799",
          "name": "M. Kitsuregawa"
        }
      ]
    },
    {
      "paperId": "159078136930f3963e01d694faa1b6b51f93c7ec",
      "externalIds": {
        "MAG": "2952040239",
        "DBLP": "conf/iclr/HeSNB19",
        "ArXiv": "1901.05534",
        "CorpusId": 58014132
      },
      "url": "https://www.semanticscholar.org/paper/159078136930f3963e01d694faa1b6b51f93c7ec",
      "title": "Lagging Inference Networks and Posterior Collapse in Variational Autoencoders",
      "abstract": "The variational autoencoder (VAE) is a popular combination of deep latent variable model and accompanying variational learning technique. By using a neural inference network to approximate the model's posterior on latent variables, VAEs efficiently parameterize a lower bound on marginal data likelihood that can be optimized directly via gradient methods. In practice, however, VAE training often results in a degenerate local optimum known as \"posterior collapse\" where the model learns to ignore the latent variable and the approximate posterior mimics the prior. In this paper, we investigate posterior collapse from the perspective of training dynamics. We find that during the initial stages of training the inference network fails to approximate the model's true posterior, which is a moving target. As a result, the model is encouraged to ignore the latent encoding and posterior collapse occurs. Based on this observation, we propose an extremely simple modification to VAE training to reduce inference lag: depending on the model's current mutual information between latent variable and observation, we aggressively optimize the inference network before performing each model update. Despite introducing neither new model components nor significant complexity over basic VAE, our approach is able to avoid the problem of collapse that has plagued a large amount of previous work. Empirically, our approach outperforms strong autoregressive baselines on text and image benchmarks in terms of held-out likelihood, and is competitive with more complex techniques for avoiding collapse while being substantially faster.",
      "year": 2019,
      "influentialCitationCount": 60,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "volume": "abs/1901.05534",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "2064240141",
          "name": "Daniel M. Spokoyny"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        }
      ]
    },
    {
      "paperId": "1941f5b053ccc80fa44980d38ac074145591b4ec",
      "externalIds": {
        "ArXiv": "1911.03895",
        "DBLP": "conf/emnlp/WietingNB20",
        "ACL": "2020.emnlp-main.122",
        "MAG": "3103680885",
        "DOI": "10.18653/v1/2020.emnlp-main.122",
        "CorpusId": 207853396
      },
      "url": "https://www.semanticscholar.org/paper/1941f5b053ccc80fa44980d38ac074145591b4ec",
      "title": "A Bilingual Generative Transformer for Semantic Sentence Embedding",
      "abstract": "Semantic sentence embedding models encode natural language sentences into vectors, such that closeness in embedding space indicates closeness in the semantics between the sentences. Bilingual data offers a useful signal for learning such embeddings: properties shared by both sentences in a translation pair are likely semantic, while divergent properties are likely stylistic or language-specific. We propose a deep latent variable model that attempts to perform source separation on parallel sentences, isolating what they have in common in a latent semantic vector, and explaining what is left over with language-specific latent vectors. Our proposed approach differs from past work on semantic sentence encoding in two ways. First, by using a variational probabilistic framework, we introduce priors that encourage source separation, and can use our model's posterior to predict sentence embeddings for monolingual data at test time. Second, we use high-capacity transformers as both data generating distributions and inference networks -- contrasting with most past work on sentence embeddings. In experiments, our approach substantially outperforms the state-of-the-art on a standard suite of unsupervised semantic similarity evaluations. Further, we demonstrate that our approach yields the largest gains on more difficult subsets of these evaluations where simple word overlap is not a good indicator of similarity.",
      "year": 2019,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.emnlp-main.122.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1581-1594"
      },
      "authors": [
        {
          "authorId": "1771118",
          "name": "J. Wieting"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        }
      ]
    },
    {
      "paperId": "1e5b826ddf0754f6e93234ba1260bd939c255e7f",
      "externalIds": {
        "DBLP": "conf/iclr/ZhouGN20",
        "MAG": "2995999067",
        "ArXiv": "1911.02727",
        "CorpusId": 207847275
      },
      "url": "https://www.semanticscholar.org/paper/1e5b826ddf0754f6e93234ba1260bd939c255e7f",
      "title": "Understanding Knowledge Distillation in Non-autoregressive Machine Translation",
      "abstract": "Non-autoregressive machine translation (NAT) systems predict a sequence of output tokens in parallel, achieving substantial improvements in generation speed compared to autoregressive models. Existing NAT models usually rely on the technique of knowledge distillation, which creates the training data from a pretrained autoregressive model for better performance. Knowledge distillation is empirically useful, leading to large gains in accuracy for NAT models, but the reason for this success has, as of yet, been unclear. In this paper, we first design systematic experiments to investigate why knowledge distillation is crucial to NAT training. We find that knowledge distillation can reduce the complexity of data sets and help NAT to model the variations in the output data. Furthermore, a strong correlation is observed between the capacity of an NAT model and the optimal complexity of the distilled data for the best translation quality. Based on these findings, we further propose several approaches that can alter the complexity of data sets to improve the performance of NAT models. We achieve the state-of-the-art performance for the NAT-based models, and close the gap with the autoregressive baseline on WMT14 En-De benchmark.",
      "year": 2019,
      "influentialCitationCount": 26,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1911.02727",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2384711",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3016273",
          "name": "Jiatao Gu"
        }
      ]
    },
    {
      "paperId": "1ea337ac24503d9da8dd9bbf98aac0bfd5920834",
      "externalIds": {
        "CorpusId": 145028106
      },
      "url": "https://www.semanticscholar.org/paper/1ea337ac24503d9da8dd9bbf98aac0bfd5920834",
      "title": "Title A monotonic statistical machine translation approach tospeaking style transformation",
      "abstract": "This paper presents a method for automatically transforming faithful transcripts or ASR results into clean transcripts for human consumption using a framework we label speaking style transformation (SST). We perform a detailed analysis of the types of corrections performed by human stenographers when creating clean transcripts, and propose a model that is able to handle the majority of the most common corrections. In particular, the proposed model uses a framework of monotonic statistical machine translation to perform not only the deletion of disfluencies and insertion of punctuation, but also correction of colloquial expressions, insertions of omitted words, and other transformations. We provide a detailed description of the model implementation in the weighted finite state transducer (WFST) framework. An evaluation of the proposed model on both faithful transcripts and speech recognition results of parliamentary and lecture speech demonstrates the effectiveness of the proposed model in performing the wide variety of corrections necessary for creating clean transcripts.",
      "year": 2019,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2339463",
          "name": "Yuya Akita"
        },
        {
          "authorId": "144873535",
          "name": "Shinsuke Mori"
        },
        {
          "authorId": "1717105",
          "name": "Tatsuya Kawahara"
        }
      ]
    },
    {
      "paperId": "248824ec5d9b4ddf0c36cdc51b6b57af6e881328",
      "externalIds": {
        "DBLP": "journals/corr/abs-1905-12688",
        "ArXiv": "1905.12688",
        "ACL": "P19-1301",
        "MAG": "2947992883",
        "DOI": "10.18653/v1/P19-1301",
        "CorpusId": 170078772
      },
      "url": "https://www.semanticscholar.org/paper/248824ec5d9b4ddf0c36cdc51b6b57af6e881328",
      "title": "Choosing Transfer Languages for Cross-Lingual Learning",
      "abstract": "Cross-lingual transfer, where a high-resource transfer language is used to improve the accuracy of a low-resource task language, is now an invaluable tool for improving performance of natural language processing (NLP) on low-resource languages. However, given a particular task language, it is not clear which language to transfer from, and the standard strategy is to select languages based on ad hoc criteria, usually the intuition of the experimenter. Since a large number of features contribute to the success of cross-lingual transfer (including phylogenetic similarity, typological properties, lexical overlap, or size of available data), even the most enlightened experimenter rarely considers all these factors for the particular task at hand. In this paper, we consider this task of automatically selecting optimal transfer languages as a ranking problem, and build models that consider the aforementioned features to perform this prediction. In experiments on representative NLP tasks, we demonstrate that our model predicts good transfer languages much better than ad hoc baselines considering single features in isolation, and glean insights on what features are most informative for each different NLP tasks, which may inform future ad hoc selection even without use of our method.",
      "year": 2019,
      "influentialCitationCount": 17,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1301.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1905.12688",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "46395826",
          "name": "Yu-Hsiang Lin"
        },
        {
          "authorId": "150054164",
          "name": "Chian-Yu Chen"
        },
        {
          "authorId": "2119693533",
          "name": "Jean Lee"
        },
        {
          "authorId": "48458251",
          "name": "Zirui Li"
        },
        {
          "authorId": "2108307596",
          "name": "Yuyan Zhang"
        },
        {
          "authorId": "67284811",
          "name": "Mengzhou Xia"
        },
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "1929423",
          "name": "Zhisong Zhang"
        },
        {
          "authorId": "2378954",
          "name": "Xuezhe Ma"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "3070462",
          "name": "Patrick Littell"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "3050735eb35af3527276aa1952f79eb2483df3f0",
      "externalIds": {
        "MAG": "2979951242",
        "ACL": "W19-5909",
        "DBLP": "conf/sigdial/ParanjapeN19",
        "DOI": "10.18653/v1/W19-5909",
        "CorpusId": 204853675
      },
      "url": "https://www.semanticscholar.org/paper/3050735eb35af3527276aa1952f79eb2483df3f0",
      "title": "Contextualized Representations for Low-resource Utterance Tagging",
      "abstract": "Utterance-level analysis of the speaker\u2019s intentions and emotions is a core task in conversational understanding. Depending on the end objective of the conversational understanding task, different categorical dialog-act or affect labels are expertly designed to cover specific aspects of the speakers\u2019 intentions or emotions respectively. Accurately annotating with these labels requires a high level of human expertise, and thus applying this process to a large conversation corpus or new domains is prohibitively expensive. The resulting paucity of data limits the use of sophisticated neural models. In this paper, we tackle these limitations by performing unsupervised training of utterance representations from a large corpus of spontaneous dialogue data. Models initialized with these representations achieve competitive performance on utterance-level dialogue-act recognition and emotion classification, especially in low-resource settings encountered when analyzing conversations in new domains.",
      "year": 2019,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W19-5909.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "68-74"
      },
      "authors": [
        {
          "authorId": "8005713",
          "name": "Bhargavi Paranjape"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "309b2c75dcdafea19a053876e56cef9747d428fb",
      "externalIds": {
        "MAG": "2952079278",
        "ACL": "P19-1115",
        "ArXiv": "1906.01617",
        "DBLP": "conf/acl/SperberNPW19",
        "DOI": "10.18653/v1/P19-1115",
        "CorpusId": 174797795
      },
      "url": "https://www.semanticscholar.org/paper/309b2c75dcdafea19a053876e56cef9747d428fb",
      "title": "Self-Attentional Models for Lattice Inputs",
      "abstract": "Lattices are an efficient and effective method to encode ambiguity of upstream systems in natural language processing tasks, for example to compactly capture multiple speech recognition hypotheses, or to represent multiple linguistic analyses. Previous work has extended recurrent neural networks to model lattice inputs and achieved improvements in various tasks, but these models suffer from very slow computation speeds. This paper extends the recently proposed paradigm of self-attention to handle lattice inputs. Self-attention is a sequence modeling technique that relates inputs to one another by computing pairwise similarities and has gained popularity for both its strong results and its computational efficiency. To extend such models to handle lattices, we introduce probabilistic reachability masks that incorporate lattice structure into the model and support lattice scores if available. We also propose a method for adapting positional embeddings to lattice structures. We apply the proposed model to a speech translation task and find that it outperforms all examined baselines while being much faster to compute than previous neural lattice models during both training and inference.",
      "year": 2019,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1115.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1185-1197"
      },
      "authors": [
        {
          "authorId": "3011998",
          "name": "Matthias Sperber"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "50214018",
          "name": "Ngoc-Quan Pham"
        },
        {
          "authorId": "1724972",
          "name": "A. Waibel"
        }
      ]
    },
    {
      "paperId": "3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c",
      "externalIds": {
        "MAG": "2949215742",
        "DBLP": "conf/acl/YinN19",
        "ACL": "P19-1447",
        "DOI": "10.18653/v1/P19-1447",
        "CorpusId": 196206775
      },
      "url": "https://www.semanticscholar.org/paper/3b3a5a9c7352b74e8377ce3182ea646b0bed5b4c",
      "title": "Reranking for Neural Semantic Parsing",
      "abstract": "Semantic parsing considers the task of transducing natural language (NL) utterances into machine executable meaning representations (MRs). While neural network-based semantic parsers have achieved impressive improvements over previous methods, results are still far from perfect, and cursory manual inspection can easily identify obvious problems such as lack of adequacy or coherence of the generated MRs. This paper presents a simple approach to quickly iterate and improve the performance of an existing neural semantic parser by reranking an n-best list of predicted MRs, using features that are designed to fix observed problems with baseline models. We implement our reranker in a competitive neural semantic parser and test on four semantic parsing (GEO, ATIS) and Python code generation (Django, CoNaLa) tasks, improving the strong baseline parser by up to 5.7% absolute in BLEU (CoNaLa) and 2.9% in accuracy (Django), outperforming the best published neural parser results on all four datasets.",
      "year": 2019,
      "influentialCitationCount": 9,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1447.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "4553-4559"
      },
      "authors": [
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "4375cccdfaf2ce3d013e4129d39f7801ef8a468e",
      "externalIds": {
        "ACL": "Y18-3001",
        "DBLP": "conf/aclwat/NakazawaDHDDMGP19",
        "MAG": "2982924472",
        "DOI": "10.18653/v1/d19-5201",
        "CorpusId": 44670650
      },
      "url": "https://www.semanticscholar.org/paper/4375cccdfaf2ce3d013e4129d39f7801ef8a468e",
      "title": "Overview of the 5th Workshop on Asian Translation",
      "abstract": "This paper presents the results of the shared tasks from the 6th workshop on Asian translation (WAT2019) including Ja\u2194En, Ja\u2194Zh scientific paper translation subtasks, Ja\u2194En, Ja\u2194Ko, Ja\u2194En patent translation subtasks, Hi\u2194En, My\u2194En, Km\u2194En, Ta\u2194En mixed domain subtasks and Ru\u2194Ja news commentary translation task. For the WAT2019, 25 teams participated in the shared tasks. We also received 10 research paper submissions out of which 61 were accepted. About 400 translation results were submitted to the automatic evaluation server, and selected submis- sions were manually evaluated.",
      "year": 2019,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-35"
      },
      "authors": [
        {
          "authorId": "40948140",
          "name": "Toshiaki Nakazawa"
        },
        {
          "authorId": "2011045",
          "name": "S. Higashiyama"
        },
        {
          "authorId": "2126835",
          "name": "Chenchen Ding"
        },
        {
          "authorId": "40315879",
          "name": "Hideya Mino"
        },
        {
          "authorId": "2205832",
          "name": "Isao Goto"
        },
        {
          "authorId": "1754386",
          "name": "H. Kazawa"
        },
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1795664",
          "name": "S. Kurohashi"
        }
      ]
    },
    {
      "paperId": "4f7b108830de2e7964b6e1a89bf1c2da60140a34",
      "externalIds": {
        "MAG": "2970134931",
        "DBLP": "journals/corr/abs-1909-00868",
        "ArXiv": "1909.00868",
        "ACL": "D19-1370",
        "DOI": "10.18653/v1/D19-1370",
        "CorpusId": 202541147
      },
      "url": "https://www.semanticscholar.org/paper/4f7b108830de2e7964b6e1a89bf1c2da60140a34",
      "title": "A Surprisingly Effective Fix for Deep Latent Variable Modeling of Text",
      "abstract": "When trained effectively, the Variational Autoencoder (VAE) is both a powerful language model and an effective representation learning framework. In practice, however, VAEs are trained with the evidence lower bound (ELBO) as a surrogate objective to the intractable marginal data likelihood. This approach to training yields unstable results, frequently leading to a disastrous local optimum known as posterior collapse. In this paper, we investigate a simple fix for posterior collapse which yields surprisingly effective results. The combination of two known heuristics, previously considered only in isolation, substantially improves held-out likelihood, reconstruction, and latent representation learning when compared with previous state-of-the-art methods. More interestingly, while our experiments demonstrate superiority on these principle evaluations, our method obtains a worse ELBO. We use these results to argue that the typical surrogate objective for VAEs may not be sufficient or necessarily appropriate for balancing the goals of representation learning and data distribution modeling.",
      "year": 2019,
      "influentialCitationCount": 17,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D19-1370.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "pages": "3601-3612"
      },
      "authors": [
        {
          "authorId": "2905344",
          "name": "Bohan Li"
        },
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        },
        {
          "authorId": "35729970",
          "name": "Yiming Yang"
        }
      ]
    },
    {
      "paperId": "56eb615c8fff0314f14e0b830afb91e350c16a89",
      "externalIds": {
        "DBLP": "journals/corr/abs-1909-06694",
        "ACL": "P19-1427",
        "ArXiv": "1909.06694",
        "MAG": "2973037648",
        "DOI": "10.18653/v1/P19-1427",
        "CorpusId": 196213784
      },
      "url": "https://www.semanticscholar.org/paper/56eb615c8fff0314f14e0b830afb91e350c16a89",
      "title": "Beyond BLEU:Training Neural Machine Translation with Semantic Similarity",
      "abstract": "While most neural machine translation (NMT)systems are still trained using maximum likelihood estimation, recent work has demonstrated that optimizing systems to directly improve evaluation metrics such as BLEU can significantly improve final translation accuracy. However, training with BLEU has some limitations: it doesn\u2019t assign partial credit, it has a limited range of output values, and it can penalize semantically correct hypotheses if they differ lexically from the reference. In this paper, we introduce an alternative reward function for optimizing NMT systems that is based on recent work in semantic similarity. We evaluate on four disparate languages trans-lated to English, and find that training with our proposed metric results in better translations as evaluated by BLEU, semantic similarity, and human evaluation, and also that the optimization procedure converges faster. Analysis suggests that this is because the proposed metric is more conducive to optimization, assigning partial credit and providing more diversity in scores than BLEU",
      "year": 2019,
      "influentialCitationCount": 8,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1427.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "4344-4355"
      },
      "authors": [
        {
          "authorId": "1771118",
          "name": "J. Wieting"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        },
        {
          "authorId": "1700980",
          "name": "Kevin Gimpel"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "57e7be6b404abfd7a56a73c0ff9bccc5b27ad7ae",
      "externalIds": {
        "ACL": "D19-1147",
        "DBLP": "journals/corr/abs-1908-10430",
        "MAG": "2970409072",
        "ArXiv": "1908.10430",
        "DOI": "10.18653/v1/D19-1147",
        "CorpusId": 201657196
      },
      "url": "https://www.semanticscholar.org/paper/57e7be6b404abfd7a56a73c0ff9bccc5b27ad7ae",
      "title": "Unsupervised Domain Adaptation for Neural Machine Translation with Domain-Aware Feature Embeddings",
      "abstract": "The recent success of neural machine translation models relies on the availability of high quality, in-domain data. Domain adaptation is required when domain-specific data is scarce or nonexistent. Previous unsupervised domain adaptation strategies include training the model with in-domain copied monolingual or back-translated data. However, these methods use generic representations for text regardless of domain shift, which makes it infeasible for translation models to control outputs conditional on a specific domain. In this work, we propose an approach that adapts models with domain-aware feature embeddings, which are learned via an auxiliary language modeling task. Our approach allows the model to assign domain-specific representations to words and output sentences in the desired domain. Our empirical results demonstrate the effectiveness of the proposed strategy, achieving consistent improvements in multiple experimental settings. In addition, we show that combining our method with back translation can further improve the performance of the model.",
      "year": 2019,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D19-1147.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1908.10430",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "14199369",
          "name": "Zi-Yi Dou"
        },
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "5884948777dfc003ba49e1513420830616281839",
      "externalIds": {
        "MAG": "2994885767",
        "ArXiv": "1910.04708",
        "DBLP": "conf/iclr/WangXXYNC20",
        "CorpusId": 204008396
      },
      "url": "https://www.semanticscholar.org/paper/5884948777dfc003ba49e1513420830616281839",
      "title": "Cross-lingual Alignment vs Joint Training: A Comparative Study and A Simple Unified Framework",
      "abstract": "Learning multilingual representations of text has proven a successful method for many cross-lingual transfer learning tasks. There are two main paradigms for learning such representations: (1) alignment, which maps different independently trained monolingual representations into a shared space, and (2) joint training, which directly learns unified multilingual representations using monolingual and cross-lingual objectives jointly. In this paper, we first conduct direct comparisons of representations learned using both of these methods across diverse cross-lingual tasks. Our empirical results reveal a set of pros and cons for both methods, and show that the relative performance of alignment versus joint training is task-dependent. Stemming from this analysis, we propose a simple and novel framework that combines these two previously mutually-exclusive approaches. Extensive experiments demonstrate that our proposed framework alleviates limitations of both approaches, and outperforms existing methods on the MUSE bilingual lexicon induction (BLI) benchmark. We further show that this framework can generalize to contextualized representations such as Multilingual BERT, and produces state-of-the-art results on the CoNLL cross-lingual NER benchmark.",
      "year": 2019,
      "influentialCitationCount": 10,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1910.04708",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2331539",
          "name": "Zirui Wang"
        },
        {
          "authorId": "20467900",
          "name": "Jiateng Xie"
        },
        {
          "authorId": "8233965",
          "name": "Ruochen Xu"
        },
        {
          "authorId": "35729970",
          "name": "Yiming Yang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143712374",
          "name": "J. Carbonell"
        }
      ]
    },
    {
      "paperId": "595306f993993e44e2c2f674367103f44df03d9b",
      "externalIds": {
        "ArXiv": "1906.03785",
        "ACL": "P19-1579",
        "DBLP": "journals/corr/abs-1906-03785",
        "MAG": "2951476960",
        "DOI": "10.18653/v1/P19-1579",
        "CorpusId": 182952423
      },
      "url": "https://www.semanticscholar.org/paper/595306f993993e44e2c2f674367103f44df03d9b",
      "title": "Generalized Data Augmentation for Low-Resource Translation",
      "abstract": "Low-resource language pairs with a paucity of parallel data pose challenges for machine translation in terms of both adequacy and fluency. Data augmentation utilizing a large amount of monolingual data is regarded as an effective way to alleviate the problem. In this paper, we propose a general framework of data augmentation for low-resource machine translation not only using target-side monolingual data, but also by pivoting through a related high-resource language. Specifically, we experiment with a two-step pivoting method to convert high-resource data to the low-resource language, making best use of available resources to better approximate the true distribution of the low-resource language. First, we inject low-resource words into high-resource sentences through an induced bilingual dictionary. Second, we further edit the high-resource data injected with low-resource words using a modified unsupervised machine translation framework. Extensive experiments on four low-resource datasets show that under extreme low-resource settings, our data augmentation techniques improve translation quality by up to 1.5 to 8 BLEU points compared to supervised back-translation baselines.",
      "year": 2019,
      "influentialCitationCount": 8,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1579.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1906.03785",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "67284811",
          "name": "Mengzhou Xia"
        },
        {
          "authorId": "145771502",
          "name": "X. Kong"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "5dab371fecc43904c0b785a50136d20cee43a99a",
      "externalIds": {
        "ACL": "Q19-1020",
        "MAG": "2949788262",
        "ArXiv": "1904.07209",
        "DBLP": "journals/tacl/SperberNNW19",
        "DOI": "10.1162/tacl_a_00270",
        "CorpusId": 119111907
      },
      "url": "https://www.semanticscholar.org/paper/5dab371fecc43904c0b785a50136d20cee43a99a",
      "title": "Attention-Passing Models for Robust and Data-Efficient End-to-End Speech Translation",
      "abstract": "Speech translation has traditionally been approached through cascaded models consisting of a speech recognizer trained on a corpus of transcribed speech, and a machine translation system trained on parallel texts. Several recent works have shown the feasibility of collapsing the cascade into a single, direct model that can be trained in an end-to-end fashion on a corpus of translated speech. However, experiments are inconclusive on whether the cascade or the direct model is stronger, and have only been conducted under the unrealistic assumption that both are trained on equal amounts of data, ignoring other available speech recognition and machine translation corpora. In this paper, we demonstrate that direct speech translation models require more data to perform well than cascaded models, and although they allow including auxiliary data through multi-task training, they are poor at exploiting such data, putting them at a severe disadvantage. As a remedy, we propose the use of end- to-end trainable models with two attention mechanisms, the first establishing source speech to source text alignments, the second modeling source to target text alignment. We show that such models naturally decompose into multi-task\u2013trainable recognition and translation tasks and propose an attention-passing technique that alleviates error propagation issues in a previous formulation of a model with two attention stages. Our proposed model outperforms all examined baselines and is able to exploit auxiliary training data much more effectively than direct attentional models.",
      "year": 2019,
      "influentialCitationCount": 10,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00270/1923092/tacl_a_00270.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "7",
        "pages": "313-325",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "3011998",
          "name": "Matthias Sperber"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2920247",
          "name": "J. Niehues"
        },
        {
          "authorId": "1724972",
          "name": "A. Waibel"
        }
      ]
    },
    {
      "paperId": "67b29c3fe6f110125a8892e8ed128d20b23957ea",
      "externalIds": {
        "DBLP": "journals/corr/abs-1909-13180",
        "MAG": "2977200951",
        "ArXiv": "1909.13180",
        "ACL": "D19-6127",
        "DOI": "10.18653/v1/D19-6127",
        "CorpusId": 203593524
      },
      "url": "https://www.semanticscholar.org/paper/67b29c3fe6f110125a8892e8ed128d20b23957ea",
      "title": "Towards Zero-resource Cross-lingual Entity Linking",
      "abstract": "Cross-lingual entity linking (XEL) grounds named entities in a source language to an English Knowledge Base (KB), such as Wikipedia. XEL is challenging for most languages because of limited availability of requisite resources. However, many works on XEL have been on simulated settings that actually use significant resources (e.g. source language Wikipedia, bilingual entity maps, multilingual embeddings) that are not available in truly low-resource languages. In this work, we first examine the effect of these resource assumptions and quantify how much the availability of these resource affects overall quality of existing XEL systems. We next propose three improvements to both entity candidate generation and disambiguation that make better use of the limited resources we do have in resource-scarce scenarios. With experiments on four extremely low-resource languages, we show that our model results in gains of 6-20% end-to-end linking accuracy.",
      "year": 2019,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D19-6127.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1909.13180",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2149163534",
          "name": "Shuyan Zhou"
        },
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "68ca176c7566067ae4b3311957cc4a134bfbc819",
      "externalIds": {
        "CorpusId": 232156021
      },
      "url": "https://www.semanticscholar.org/paper/68ca176c7566067ae4b3311957cc4a134bfbc819",
      "title": "Neural Cognitive Architectures for Never-Ending Learning",
      "abstract": "Allen Newell argued that the human mind functions as a single system and proposed the notion of a unified theory of cognition (UTC). Most existing work on UTCs has focused on symbolic approaches, such as the Soar architecture (Laird, 2012) and the ACT-R (Anderson et al., 2004) system. However, such approaches limit a system\u2019s ability to perceive information of arbitrary modalities, require a significant amount of human input, and are restrictive in terms of the learning mechanisms they support (supervised learning, semi-supervised learning, reinforcement learning, etc.). For this reason, researchers in machine learning have recently shifted their focus towards subsymbolic processing with methods such as deep learning. Deep learning systems have become a standard for solving prediction problems in multiple application areas including computer vision, natural language processing, and robotics. However, many real-world problems require integrating multiple, distinct modalities of information (e.g., image, audio, language, etc.) in ways that machine learning models cannot currently handle well. Moreover, most deep learning approaches are not able to utilize information learned from solving one problem to directly help in solving another. They are also not capable of never-ending learning, failing on problems that are dynamic, ever-changing, and not fixed a priori, which is true of problems in the real world due to the dynamicity of nature. In this thesis, we aim to bridge the gap between UTCs, deep learning, and never-ending learning. To that end, we propose a neural cognitive architecture (NCA) that is inspired by human cognition and that can learn to continuously solve multiple problems that can grow in number over time, across multiple distinct perception and action modalities, and from multiple noisy sources of supervision combined with self-supervision. Furthermore, its experience from learning to solve past problems can be leveraged to learn to solve future ones. The problems the proposed NCA is learning to solve are ever-evolving and can also be automatically generated by the system itself. In our NCA, reasoning is performed recursively in a subsymbolic latent space that is shared across all problems and modalities. The goal of this architecture is to take us a step closer towards general learning and intelligence. We have also designed, implemented, and plan to extend an artificial simulated world that allows us to test for all the aforementioned properties of the proposed architecture, in a controllable manner. We propose to perform multiple case studies\u2014within this simulated world and with real-world applications\u2014that will allow us to evaluate our architecture.",
      "year": 2019,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "144888672",
          "name": "Emmanouil Antonios Platanios"
        },
        {
          "authorId": "144135485",
          "name": "Tom M. Mitchell"
        },
        {
          "authorId": "2064595436",
          "name": "Eric Horvitz"
        },
        {
          "authorId": "145727186",
          "name": "R. Caruana"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "6b7f2f30840b0d72484784a15b3be670868a9f95",
      "externalIds": {
        "ACL": "P19-1311",
        "MAG": "2963768241",
        "ArXiv": "1906.02656",
        "DBLP": "conf/acl/HeZBN19",
        "DOI": "10.18653/v1/p19-1311",
        "CorpusId": 174802642
      },
      "url": "https://www.semanticscholar.org/paper/6b7f2f30840b0d72484784a15b3be670868a9f95",
      "title": "Cross-Lingual Syntactic Transfer through Unsupervised Adaptation of Invertible Projections",
      "abstract": "Cross-lingual transfer is an effective way to build syntactic analysis tools in low-resource languages. However, transfer is difficult when transferring to typologically distant languages, especially when neither annotated target data nor parallel corpora are available. In this paper, we focus on methods for cross-lingual transfer to distant languages and propose to learn a generative model with a structured prior that utilizes labeled source data and unlabeled target data jointly. The parameters of source model and target model are softly shared through a regularized log likelihood objective. An invertible projection is employed to learn a new interlingual latent embedding space that compensates for imperfect cross-lingual word embedding input. We evaluate our method on two syntactic tasks: part-of-speech (POS) tagging and dependency parsing. On the Universal Dependency Treebanks, we use English as the only source corpus and transfer to a wide range of target languages. On the 10 languages in this dataset that are distant from English, our method yields an average of 5.2% absolute improvement on POS tagging and 8.3% absolute improvement on dependency parsing over a direct transfer method using state-of-the-art discriminative models.",
      "year": 2019,
      "influentialCitationCount": 5,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1311.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1906.02656",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "1929423",
          "name": "Zhisong Zhang"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "712cd873d7370db280f4ceaaf000dc49f76b59fe",
      "externalIds": {
        "DBLP": "journals/corr/abs-1903-06620",
        "ArXiv": "1903.06620",
        "ACL": "N19-1314",
        "MAG": "2950352427",
        "DOI": "10.18653/v1/N19-1314",
        "CorpusId": 80628357
      },
      "url": "https://www.semanticscholar.org/paper/712cd873d7370db280f4ceaaf000dc49f76b59fe",
      "title": "On Evaluation of Adversarial Perturbations for Sequence-to-Sequence Models",
      "abstract": "Adversarial examples \u2014 perturbations to the input of a model that elicit large changes in the output \u2014 have been shown to be an effective way of assessing the robustness of sequence-to-sequence (seq2seq) models. However, these perturbations only indicate weaknesses in the model if they do not change the input so significantly that it legitimately results in changes in the expected output. This fact has largely been ignored in the evaluations of the growing body of related literature. Using the example of untargeted attacks on machine translation (MT), we propose a new evaluation framework for adversarial attacks on seq2seq models that takes the semantic equivalence of the pre- and post-perturbation input into account. Using this framework, we demonstrate that existing methods may not preserve meaning in general, breaking the aforementioned assumption that source side perturbations should not result in changes in the expected output. We further use this framework to demonstrate that adding additional constraints on attacks allows for adversarial perturbations that are more meaning-preserving, but nonetheless largely change the output sequence. Finally, we show that performing untargeted adversarial training with meaning-preserving attacks is beneficial to the model in terms of adversarial robustness, without hurting test performance. A toolkit implementing our evaluation framework is released at https://github.com/pmichel31415/teapot-nlp.",
      "year": 2019,
      "influentialCitationCount": 17,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1903.06620",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3103-3114"
      },
      "authors": [
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "2116235416",
          "name": "Xian Li"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145503806",
          "name": "J. Pino"
        }
      ]
    },
    {
      "paperId": "7225c2a42990f850f692f8d82e7f3bfaf312145c",
      "externalIds": {
        "MAG": "2953199126",
        "ArXiv": "1903.09848",
        "ACL": "N19-1119",
        "DBLP": "conf/naacl/PlataniosSNPM19",
        "DOI": "10.18653/v1/N19-1119",
        "CorpusId": 85498775
      },
      "url": "https://www.semanticscholar.org/paper/7225c2a42990f850f692f8d82e7f3bfaf312145c",
      "title": "Competence-based Curriculum Learning for Neural Machine Translation",
      "abstract": "Current state-of-the-art NMT systems use large neural networks that are not only slow to train, but also often require many heuristics and optimization tricks, such as specialized learning rate schedules and large batch sizes. This is undesirable as it requires extensive hyperparameter tuning. In this paper, we propose a curriculum learning framework for NMT that reduces training time, reduces the need for specialized heuristics or large batch sizes, and results in overall better performance. Our framework consists of a principled way of deciding which training samples are shown to the model at different times during training, based on the estimated difficulty of a sample and the current competence of the model. Filtering training samples in this manner prevents the model from getting stuck in bad local optima, making it converge faster and reach a better solution than the common approach of uniformly sampling training examples. Furthermore, the proposed method can be easily applied to existing NMT models by simply modifying their input data pipelines. We show that our framework can help improve the training time and the performance of both recurrent neural network models and Transformers, achieving up to a 70% decrease in training time, while at the same time obtaining accuracy improvements of up to 2.2 BLEU.",
      "year": 2019,
      "influentialCitationCount": 47,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1903.09848",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "pages": "1162-1172"
      },
      "authors": [
        {
          "authorId": "144888672",
          "name": "Emmanouil Antonios Platanios"
        },
        {
          "authorId": "3397269",
          "name": "Otilia Stretcu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1719347",
          "name": "B. P\u00f3czos"
        },
        {
          "authorId": "40975594",
          "name": "Tom Michael Mitchell"
        }
      ]
    },
    {
      "paperId": "72c9663494827b2e87ad5a65a6ff7e769eb15a57",
      "externalIds": {
        "MAG": "2998106530",
        "DBLP": "conf/aaai/HuCGLGN20",
        "ArXiv": "1909.05316",
        "DOI": "10.1609/AAAI.V34I05.6305",
        "CorpusId": 202565931
      },
      "url": "https://www.semanticscholar.org/paper/72c9663494827b2e87ad5a65a6ff7e769eb15a57",
      "title": "What Makes A Good Story? Designing Composite Rewards for Visual Storytelling",
      "abstract": "Previous storytelling approaches mostly focused on optimizing traditional metrics such as BLEU, ROUGE and CIDEr. In this paper, we re-examine this problem from a different angle, by looking deep into what defines a natural and topically-coherent story. To this end, we propose three assessment criteria: relevance, coherence and expressiveness, which we observe through empirical analysis could constitute a \u201chigh-quality\u201d story to the human eye. We further propose a reinforcement learning framework, ReCo-RL, with reward functions designed to capture the essence of these quality criteria. Experiments on the Visual Storytelling Dataset (VIST) with both automatic and human evaluation demonstrate that our ReCo-RL model achieves better performance than state-of-the-art baselines on both traditional metrics and the proposed new criteria.",
      "year": 2019,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6305/6161",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1909.05316",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "145215470",
          "name": "Yu Cheng"
        },
        {
          "authorId": "144702900",
          "name": "Zhe Gan"
        },
        {
          "authorId": "46700348",
          "name": "Jingjing Liu"
        },
        {
          "authorId": "1800422",
          "name": "Jianfeng Gao"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "75fe6c3ffdea2608794b4f21119c5a4dec07663a",
      "externalIds": {
        "DBLP": "conf/emnlp/MaZLNH19",
        "MAG": "2972034672",
        "ACL": "D19-1437",
        "ArXiv": "1909.02480",
        "DOI": "10.18653/v1/D19-1437",
        "CorpusId": 202539063
      },
      "url": "https://www.semanticscholar.org/paper/75fe6c3ffdea2608794b4f21119c5a4dec07663a",
      "title": "FlowSeq: Non-Autoregressive Conditional Sequence Generation with Generative Flow",
      "abstract": "Most sequence-to-sequence (seq2seq) models are autoregressive; they generate each token by conditioning on previously generated tokens. In contrast, non-autoregressive seq2seq models generate all tokens in one pass, which leads to increased efficiency through parallel processing on hardware such as GPUs. However, directly modeling the joint distribution of all tokens simultaneously is challenging, and even with increasingly complex model structures accuracy lags significantly behind autoregressive models. In this paper, we propose a simple, efficient, and effective model for non-autoregressive sequence generation using latent variable models. Specifically, we turn to generative flow, an elegant technique to model complex distributions using neural networks, and design several layers of flow tailored for modeling the conditional density of sequential latent variables. We evaluate this model on three neural machine translation (NMT) benchmark datasets, achieving comparable performance with state-of-the-art non-autoregressive NMT models and almost constant decoding time w.r.t the sequence length.",
      "year": 2019,
      "influentialCitationCount": 23,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D19-1437.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1909.02480",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2378954",
          "name": "Xuezhe Ma"
        },
        {
          "authorId": "2384711",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "2116235416",
          "name": "Xian Li"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144547315",
          "name": "E. Hovy"
        }
      ]
    },
    {
      "paperId": "7f1a6c67d03de88b898271d52dd2e51907d5b615",
      "externalIds": {
        "ACL": "2020.acl-main.192",
        "DBLP": "journals/corr/abs-1911-03822",
        "MAG": "3034694091",
        "ArXiv": "1911.03822",
        "DOI": "10.18653/v1/2020.acl-main.192",
        "CorpusId": 207853026
      },
      "url": "https://www.semanticscholar.org/paper/7f1a6c67d03de88b898271d52dd2e51907d5b615",
      "title": "Generalizing Natural Language Analysis through Span-relation Representations",
      "abstract": "Natural language processing covers a wide variety of tasks predicting syntax, semantics, and information content, and usually each type of output is generated with specially designed architectures. In this paper, we provide the simple insight that a great variety of tasks can be represented in a single unified format consisting of labeling spans and relations between spans, thus a single task-independent model can be used across different tasks. We perform extensive experiments to test this insight on 10 disparate tasks spanning dependency parsing (syntax), semantic role labeling (semantics), relation extraction (information content), aspect based sentiment analysis (sentiment), and many others, achieving performance comparable to state-of-the-art specialized models. We further demonstrate benefits of multi-task learning, and also show that the proposed method makes it easy to analyze differences and similarities in how the model handles different tasks. Finally, we convert these datasets into a unified format to build a benchmark, which provides a holistic testbed for evaluating future models for generalized natural language analysis.",
      "year": 2019,
      "influentialCitationCount": 9,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.acl-main.192.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2120-2133"
      },
      "authors": [
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "40515617",
          "name": "W. Xu"
        },
        {
          "authorId": "50007145",
          "name": "J. Araki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "8147a495b9a933742f06458244f7c5df00767c4e",
      "externalIds": {
        "ArXiv": "1905.13413",
        "ACL": "P19-1523",
        "DBLP": "journals/corr/abs-1905-13413",
        "MAG": "2952695078",
        "DOI": "10.18653/v1/P19-1523",
        "CorpusId": 173188796
      },
      "url": "https://www.semanticscholar.org/paper/8147a495b9a933742f06458244f7c5df00767c4e",
      "title": "Improving Open Information Extraction via Iterative Rank-Aware Learning",
      "abstract": "Open information extraction (IE) is the task of extracting open-domain assertions from natural language sentences. A key step in open IE is confidence modeling, ranking the extractions based on their estimated quality to adjust precision and recall of extracted assertions. We found that the extraction likelihood, a confidence measure used by current supervised open IE systems, is not well calibrated when comparing the quality of assertions extracted from different sentences. We propose an additional binary classification loss to calibrate the likelihood to make it more globally comparable, and an iterative learning process, where extractions generated by the open IE model are incrementally included as training samples to help the model learn from trial and error. Experiments on OIE2016 demonstrate the effectiveness of our method. Code and data are available at https://github.com/jzbjyb/oie_rank.",
      "year": 2019,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1523.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "5295-5300"
      },
      "authors": [
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85",
      "externalIds": {
        "MAG": "3106954555",
        "ArXiv": "1911.12543",
        "DBLP": "journals/tacl/JiangXAN20",
        "DOI": "10.1162/tacl_a_00324",
        "CorpusId": 208513249
      },
      "url": "https://www.semanticscholar.org/paper/81dd3faf762ad8f084ab1d7b8fc9e77e9e160f85",
      "title": "How Can We Know What Language Models Know?",
      "abstract": "Abstract Recent work has presented intriguing results examining the knowledge contained in language models (LMs) by having the LM fill in the blanks of prompts such as \u201cObama is a __ by profession\u201d. These prompts are usually manually created, and quite possibly sub-optimal; another prompt such as \u201cObama worked as a __ \u201d may result in more accurately predicting the correct profession. Because of this, given an inappropriate prompt, we might fail to retrieve facts that the LM does know, and thus any given prompt only provides a lower bound estimate of the knowledge contained in an LM. In this paper, we attempt to more accurately estimate the knowledge contained in LMs by automatically discovering better prompts to use in this querying process. Specifically, we propose mining-based and paraphrasing-based methods to automatically generate high-quality and diverse prompts, as well as ensemble methods to combine answers from different prompts. Extensive experiments on the LAMA benchmark for extracting relational knowledge from LMs demonstrate that our methods can improve accuracy from 31.1% to 39.6%, providing a tighter lower bound on what LMs know. We have released the code and the resulting LM Prompt And Query Archive (LPAQA) at https://github.com/jzbjyb/LPAQA.",
      "year": 2019,
      "influentialCitationCount": 44,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://direct.mit.edu/tacl/article-pdf/doi/10.1162/tacl_a_00324/1923867/tacl_a_00324.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "8",
        "pages": "423-438",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "2669515",
          "name": "Zhengbao Jiang"
        },
        {
          "authorId": "40027632",
          "name": "Frank F. Xu"
        },
        {
          "authorId": "50007145",
          "name": "J. Araki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "86d84c1c9b0a500f930696ab27c83a4b30477560",
      "externalIds": {
        "ArXiv": "1909.13872",
        "DBLP": "journals/corr/abs-1909-13872",
        "MAG": "2976163105",
        "ACL": "P19-1453",
        "DOI": "10.18653/v1/P19-1453",
        "CorpusId": 196187686
      },
      "url": "https://www.semanticscholar.org/paper/86d84c1c9b0a500f930696ab27c83a4b30477560",
      "title": "Simple and Effective Paraphrastic Similarity from Parallel Translations",
      "abstract": "We present a model and methodology for learning paraphrastic sentence embeddings directly from bitext, removing the time-consuming intermediate step of creating para-phrase corpora. Further, we show that the resulting model can be applied to cross lingual tasks where it both outperforms and is orders of magnitude faster than more complex state-of-the-art baselines.",
      "year": 2019,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1453.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "4602-4608"
      },
      "authors": [
        {
          "authorId": "1771118",
          "name": "J. Wieting"
        },
        {
          "authorId": "1700980",
          "name": "Kevin Gimpel"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        }
      ]
    },
    {
      "paperId": "88051a6dce3b67541d8096647da2f6d31daa9e9a",
      "externalIds": {
        "MAG": "2997012196",
        "ArXiv": "1908.07690",
        "DBLP": "conf/aaai/HayashiHXN20",
        "DOI": "10.1609/AAAI.V34I05.6298",
        "CorpusId": 201125279
      },
      "url": "https://www.semanticscholar.org/paper/88051a6dce3b67541d8096647da2f6d31daa9e9a",
      "title": "Latent Relation Language Models",
      "abstract": "In this paper, we propose Latent Relation Language Models (LRLMs), a class of language models that parameterizes the joint distribution over the words in a document and the entities that occur therein via knowledge graph relations. This model has a number of attractive properties: it not only improves language modeling performance, but is also able to annotate the posterior probability of entity spans for a given text through relations. Experiments demonstrate empirical improvements over both word-based language models and a previous approach that incorporates knowledge graph information. Qualitative analysis further demonstrates the proposed model's ability to learn to predict appropriate relations in context. 1",
      "year": 2019,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/6298/6154",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1908.07690",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "50376014",
          "name": "Hiroaki Hayashi"
        },
        {
          "authorId": "46210480",
          "name": "Zecong Hu"
        },
        {
          "authorId": "144628574",
          "name": "Chenyan Xiong"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "9237d6efc603465765e80eb5ca1268c2bd7b5c24",
      "externalIds": {
        "ACL": "N19-4007",
        "DBLP": "journals/corr/abs-1903-07926",
        "MAG": "2952573656",
        "ArXiv": "1903.07926",
        "DOI": "10.18653/v1/N19-4007",
        "CorpusId": 83458807
      },
      "url": "https://www.semanticscholar.org/paper/9237d6efc603465765e80eb5ca1268c2bd7b5c24",
      "title": "compare-mt: A Tool for Holistic Comparison of Language Generation Systems",
      "abstract": "In this paper, we describe compare-mt, a tool for holistic analysis and comparison of the results of systems for language generation tasks such as machine translation. The main goal of the tool is to give the user a high-level and coherent view of the salient differences between systems that can then be used to guide further analysis or system improvement. It implements a number of tools to do so, such as analysis of accuracy of generation of particular types of words, bucketed histograms of sentence accuracies or counts based on salient characteristics, and extraction of characteristic n-grams for each system. It also has a number of advanced features such as use of linguistic labels, source side data, or comparison of log likelihoods for probabilistic models, and also aims to be easily extensible by users to new types of analysis. compare-mt is a pure-Python open source package, that has already proven useful to generate analyses that have been used in our published papers. Demo Video: https://youtu.be/NyJEQT7t2CA",
      "year": 2019,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1903.07926",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "35-41"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "14199369",
          "name": "Zi-Yi Dou"
        },
        {
          "authorId": "145919378",
          "name": "Junjie Hu"
        },
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "7880098",
          "name": "Danish Pruthi"
        },
        {
          "authorId": "3277494",
          "name": "Xinyi Wang"
        }
      ]
    },
    {
      "paperId": "970383c0a41d7ae1ec4b8abaa3033778203377b9",
      "externalIds": {
        "MAG": "2966118729",
        "DBLP": "journals/corr/abs-1908-02914",
        "ArXiv": "1908.02914",
        "DOI": "10.21437/interspeech.2019-3154",
        "CorpusId": 199501690
      },
      "url": "https://www.semanticscholar.org/paper/970383c0a41d7ae1ec4b8abaa3033778203377b9",
      "title": "Mitigating Noisy Inputs for Question Answering",
      "abstract": "Natural language processing systems are often downstream of unreliable inputs: machine translation, optical character recognition, or speech recognition. For instance, virtual assistants can only answer your questions after understanding your speech. We investigate and mitigate the effects of noise from Automatic Speech Recognition systems on two factoid Question Answering (QA) tasks. Integrating confidences into the model and forced decoding of unknown words are empirically shown to improve the accuracy of downstream neural QA systems. We create and train models on a synthetic corpus of over 500,000 noisy sentences and evaluate on two human corpora from Quizbowl and Jeopardy! competitions.",
      "year": 2019,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1908.02914",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1908.02914",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "21317593",
          "name": "Denis Peskov"
        },
        {
          "authorId": "40080808",
          "name": "Joe Barrow"
        },
        {
          "authorId": "145009056",
          "name": "Pedro Rodriguez"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1389036863",
          "name": "Jordan L. Boyd-Graber"
        }
      ]
    },
    {
      "paperId": "a0cbaf59f563580f68523ab6839a436e38b6db18",
      "externalIds": {
        "ACL": "P19-1583",
        "DBLP": "conf/acl/WangN19",
        "MAG": "2951065878",
        "ArXiv": "1905.08212",
        "DOI": "10.18653/v1/P19-1583",
        "CorpusId": 159041276
      },
      "url": "https://www.semanticscholar.org/paper/a0cbaf59f563580f68523ab6839a436e38b6db18",
      "title": "Target Conditioned Sampling: Optimizing Data Selection for Multilingual Neural Machine Translation",
      "abstract": "To improve low-resource Neural Machine Translation (NMT) with multilingual corpus, training on the most related high-resource language only is generally more effective than us- ing all data available (Neubig and Hu, 2018). However, it remains a question whether a smart data selection strategy can further improve low-resource NMT with data from other auxiliary languages. In this paper, we seek to construct a sampling distribution over all multilingual data, so that it minimizes the training loss of the low-resource language. Based on this formulation, we propose and efficient algorithm, (TCS), which first samples a target sentence, and then conditionally samples its source sentence. Experiments show TCS brings significant gains of up to 2 BLEU improvements on three of four languages we test, with minimal training overhead.",
      "year": 2019,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1583.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "5823-5828"
      },
      "authors": [
        {
          "authorId": "3277494",
          "name": "Xinyi Wang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "a1da1d600acd506b80c8870d293a756c70791683",
      "externalIds": {
        "CorpusId": 199500777
      },
      "url": "https://www.semanticscholar.org/paper/a1da1d600acd506b80c8870d293a756c70791683",
      "title": "Source ! Target Incorrect Predicted aunt ! \u0442\u0435\u0442\u044f \u0431\u0430\u0431\u0443\u0448\u043a\u0430 ( Grandmother ) uruguay ! \u0443\u0440\u0443\u0433\u0432\u0430\u044f \u0430\u0440\u0433\u0435\u043d\u0442\u0438\u043d\u044b ( Argentina ) regiments ! \u043f\u043e\u043b\u043a\u043e\u0432 \u043a\u0430\u0432\u0430\u043b\u0435\u0440\u0438\u0439\u0441\u043a\u0438\u0435",
      "abstract": "Recent work on bilingual lexicon induction (BLI) has frequently depended either on aligned bilingual lexicons or on distribution matching, often with an assumption about the isometry of the two spaces. We propose a technique to quantitatively estimate this assumption of the isometry between two embedding spaces and empirically show that this assumption weakens as the languages in question become increasingly etymologically distant. We then propose Bilingual Lexicon Induction with Semi-Supervision (BLISS) \u2014 a semi-supervised approach that relaxes the isometric assumption while leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don\u2019t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.\u21e4",
      "year": 2019,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "27419446",
          "name": "Barun Patra"
        },
        {
          "authorId": "22272110",
          "name": "Joel Ruben Antony Moniz"
        },
        {
          "authorId": "31264049",
          "name": "Sarthak Garg"
        },
        {
          "authorId": null,
          "name": "Matthew R. Gormley"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "a5690b0a514a7cbc913871e41e54c9ad4f6362db",
      "externalIds": {
        "ArXiv": "1906.11943",
        "MAG": "2970558573",
        "DBLP": "conf/wmt/LiMABDFKNPS19",
        "ACL": "W19-5303",
        "DOI": "10.18653/v1/W19-5303",
        "CorpusId": 195750845
      },
      "url": "https://www.semanticscholar.org/paper/a5690b0a514a7cbc913871e41e54c9ad4f6362db",
      "title": "Findings of the First Shared Task on Machine Translation Robustness",
      "abstract": "We share the findings of the first shared task on improving robustness of Machine Translation (MT). The task provides a testbed representing challenges facing MT models deployed in the real world, and facilitates new approaches to improve models\u2019 robustness to noisy input and domain mismatch. We focus on two language pairs (English-French and English-Japanese), and the submitted systems are evaluated on a blind test set consisting of noisy comments on Reddit and professionally sourced translations. As a new task, we received 23 submissions by 11 participating teams from universities, companies, national labs, etc. All submitted systems achieved large improvements over baselines, with the best improvement having +22.33 BLEU. We evaluated submissions by both human judgment and automatic evaluation (BLEU), which shows high correlations (Pearson\u2019s r = 0.94 and 0.95). Furthermore, we conducted a qualitative analysis of the submitted systems using compare-mt, which revealed their salient differences in handling challenges in this task. Such analysis provides additional insights when there is occasional disagreement between human judgment and BLEU, e.g. systems better at producing colloquial expressions received higher score from human judgment.",
      "year": 2019,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W19-5303.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "91-102"
      },
      "authors": [
        {
          "authorId": "2116235416",
          "name": "Xian Li"
        },
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "2083259",
          "name": "Yonatan Belinkov"
        },
        {
          "authorId": "145938140",
          "name": "Nadir Durrani"
        },
        {
          "authorId": "1755162",
          "name": "Philipp Koehn"
        },
        {
          "authorId": "49604675",
          "name": "Philipp Koehn"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145503806",
          "name": "J. Pino"
        },
        {
          "authorId": "145775792",
          "name": "Hassan Sajjad"
        }
      ]
    },
    {
      "paperId": "b03c7ff961822183bab66b2e594415e585d3fd09",
      "externalIds": {
        "ArXiv": "1905.10650",
        "MAG": "2945767825",
        "DBLP": "conf/nips/MichelLN19",
        "CorpusId": 166227946
      },
      "url": "https://www.semanticscholar.org/paper/b03c7ff961822183bab66b2e594415e585d3fd09",
      "title": "Are Sixteen Heads Really Better than One?",
      "abstract": "Attention is a powerful and ubiquitous mechanism for allowing neural models to focus on particular salient pieces of information by taking their weighted average when making predictions. In particular, multi-headed attention is a driving force behind many recent state-of-the-art NLP models such as Transformer-based MT models and BERT. These models apply multiple attention mechanisms in parallel, with each attention \"head\" potentially focusing on different parts of the input, which makes it possible to express sophisticated functions beyond the simple weighted average. In this paper we make the surprising observation that even if models have been trained using multiple heads, in practice, a large percentage of attention heads can be removed at test time without significantly impacting performance. In fact, some layers can even be reduced to a single head. We further examine greedy algorithms for pruning down models, and the potential speed, memory efficiency, and accuracy improvements obtainable therefrom. Finally, we analyze the results with respect to which parts of the model are more reliant on having multiple heads, and provide precursory evidence that training dynamics play a role in the gains provided by multi-head attention.",
      "year": 2019,
      "influentialCitationCount": 82,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "14014-14024"
      },
      "authors": [
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "39455775",
          "name": "Omer Levy"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "b990517fbbf4499861d7aa00407b0422874ab990",
      "externalIds": {
        "ArXiv": "1904.00930",
        "DBLP": "journals/corr/abs-1904-00930",
        "MAG": "2933788108",
        "ACL": "N19-1010",
        "DOI": "10.18653/v1/N19-1010",
        "CorpusId": 90258034
      },
      "url": "https://www.semanticscholar.org/paper/b990517fbbf4499861d7aa00407b0422874ab990",
      "title": "Lost in Interpretation: Predicting Untranslated Terminology in Simultaneous Interpretation",
      "abstract": "Simultaneous interpretation, the translation of speech from one language to another in real-time, is an inherently difficult and strenuous task. One of the greatest challenges faced by interpreters is the accurate translation of difficult terminology like proper names, numbers, or other entities. Intelligent computer-assisted interpreting (CAI) tools that could analyze the spoken word and detect terms likely to be untranslated by an interpreter could reduce translation error and improve interpreter performance. In this paper, we propose a task of predicting which terminology simultaneous interpreters will leave untranslated, and examine methods that perform this task using supervised sequence taggers. We describe a number of task-specific features explicitly designed to indicate when an interpreter may struggle with translating a word. Experimental results on a newly-annotated version of the NAIST Simultaneous Translation Corpus (Shimizu et al., 2014) indicate the promise of our proposed method.",
      "year": 2019,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "109-118"
      },
      "authors": [
        {
          "authorId": "46623434",
          "name": "Nikolai Vogler"
        },
        {
          "authorId": "40163298",
          "name": "Craig Alan Stewart"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "bdbf635476477eec5be5a292b494e20b8902cc35",
      "externalIds": {
        "DBLP": "journals/corr/abs-1902-09508",
        "MAG": "2916835973",
        "ACL": "N19-1190",
        "ArXiv": "1902.09508",
        "DOI": "10.18653/v1/N19-1190",
        "CorpusId": 67856759
      },
      "url": "https://www.semanticscholar.org/paper/bdbf635476477eec5be5a292b494e20b8902cc35",
      "title": "Improving Robustness of Machine Translation with Synthetic Noise",
      "abstract": "Modern Machine Translation (MT) systems perform remarkably well on clean, in-domain text. However most of the human generated text, particularly in the realm of social media, is full of typos, slang, dialect, idiolect and other noise which can have a disastrous impact on the accuracy of MT. In this paper we propose methods to enhance the robustness of MT systems by emulating naturally occurring noise in otherwise clean data. Synthesizing noise in this manner we are ultimately able to make a vanilla MT system more resilient to naturally occurring noise, partially mitigating loss in accuracy resulting therefrom.",
      "year": 2019,
      "influentialCitationCount": 7,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1902.09508",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1916-1920"
      },
      "authors": [
        {
          "authorId": "2066205749",
          "name": "Vaibhav"
        },
        {
          "authorId": "10220743",
          "name": "Sumeet Singh"
        },
        {
          "authorId": "145547332",
          "name": "Craig Stewart"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "be312e930f6739a709e60547aa0dfb9c3dc44497",
      "externalIds": {
        "MAG": "2963831310",
        "DBLP": "journals/corr/abs-1902-03499",
        "ArXiv": "1902.03499",
        "CorpusId": 60440615
      },
      "url": "https://www.semanticscholar.org/paper/be312e930f6739a709e60547aa0dfb9c3dc44497",
      "title": "Multilingual Neural Machine Translation With Soft Decoupled Encoding",
      "abstract": "Multilingual training of neural machine translation (NMT) systems has led to impressive accuracy improvements on low-resource languages. However, there are still significant challenges in efficiently learning word representations in the face of paucity of data. In this paper, we propose Soft Decoupled Encoding (SDE), a multilingual lexicon encoding framework specifically designed to share lexical-level information intelligently without requiring heuristic preprocessing such as pre-segmenting the data. SDE represents a word by its spelling through a character encoding, and its semantic meaning through a latent embedding space shared by all languages. Experiments on a standard dataset of four low-resource languages show consistent improvements over strong multilingual NMT baselines, with gains of up to 2 BLEU on one of the tested languages, achieving the new state-of-the-art on all four language pairs.",
      "year": 2019,
      "influentialCitationCount": 7,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1902.03499",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "3277494",
          "name": "Xinyi Wang"
        },
        {
          "authorId": "143950636",
          "name": "Hieu Pham"
        },
        {
          "authorId": "144332819",
          "name": "Philip Arthur"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "c1a4c5380d90dc77064de6003cfb9611ad218600",
      "externalIds": {
        "DBLP": "journals/corr/abs-1901-07129",
        "ArXiv": "1901.07129",
        "MAG": "2912878122",
        "CorpusId": 58981886
      },
      "url": "https://www.semanticscholar.org/paper/c1a4c5380d90dc77064de6003cfb9611ad218600",
      "title": "An Adversarial Approach to High-Quality, Sentiment-Controlled Neural Dialogue Generation",
      "abstract": "In this work, we propose a method for neural dialogue response generation that allows not only generating semantically reasonable responses according to the dialogue history, but also explicitly controlling the sentiment of the response via sentiment labels. Our proposed model is based on the paradigm of conditional adversarial learning; the training of a sentiment-controlled dialogue generator is assisted by an adversarial discriminator which assesses the fluency and feasibility of the response generating from the dialogue history and a given sentiment label. Because of the flexibility of our framework, the generator could be a standard sequence-to-sequence (SEQ2SEQ) model or a more complicated one such as a conditional variational autoencoder-based SEQ2SEQ model. Experimental results using automatic and human evaluation both demonstrate that our proposed framework is able to generate both semantically reasonable and sentiment-controlled dialogue responses.",
      "year": 2019,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1901.07129",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "145771502",
          "name": "X. Kong"
        },
        {
          "authorId": "2905344",
          "name": "Bohan Li"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144547315",
          "name": "E. Hovy"
        },
        {
          "authorId": "35729970",
          "name": "Yiming Yang"
        }
      ]
    },
    {
      "paperId": "cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a",
      "externalIds": {
        "ArXiv": "1909.07913",
        "MAG": "3035563045",
        "DBLP": "conf/acl/PruthiGDNL20",
        "ACL": "2020.acl-main.432",
        "DOI": "10.18653/v1/2020.acl-main.432",
        "CorpusId": 202583616
      },
      "url": "https://www.semanticscholar.org/paper/cf2fcb73e2effff29ceb5a5b89bbca34d2d27c1a",
      "title": "Learning to Deceive with Attention-Based Explanations",
      "abstract": "Attention mechanisms are ubiquitous components in neural architectures applied to natural language processing. In addition to yielding gains in predictive accuracy, attention weights are often claimed to confer interpretability, purportedly useful both for providing insights to practitioners and for explaining why a model makes its decisions to stakeholders. We call the latter use of attention mechanisms into question by demonstrating a simple method for training models to produce deceptive attention masks. Our method diminishes the total weight assigned to designated impermissible tokens, even when the models can be shown to nevertheless rely on these features to drive predictions. Across multiple models and tasks, our approach manipulates attention weights while paying surprisingly little cost in accuracy. Through a human study, we show that our manipulated attention-based explanations deceive people into thinking that predictions from a model biased against gender minorities do not rely on the gender. Consequently, our results cast doubt on attention\u2019s reliability as a tool for auditing algorithms in the context of fairness and accountability.",
      "year": 2019,
      "influentialCitationCount": 11,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.acl-main.432.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "4782-4793"
      },
      "authors": [
        {
          "authorId": "7880098",
          "name": "Danish Pruthi"
        },
        {
          "authorId": "46722266",
          "name": "Mansi Gupta"
        },
        {
          "authorId": "34994191",
          "name": "Bhuwan Dhingra"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "32219137",
          "name": "Zachary Chase Lipton"
        }
      ]
    },
    {
      "paperId": "d530a007ae0493ef6a8167c25bd007104623c504",
      "externalIds": {
        "MAG": "2973871154",
        "DBLP": "journals/corr/abs-1909-09029",
        "ArXiv": "1909.09029",
        "DOI": "10.1109/ASE.2019.00064",
        "CorpusId": 202676778
      },
      "url": "https://www.semanticscholar.org/paper/d530a007ae0493ef6a8167c25bd007104623c504",
      "title": "DIRE: A Neural Approach to Decompiled Identifier Naming",
      "abstract": "The decompiler is one of the most common tools for examining binaries without corresponding source code. It transforms binaries into high-level code, reversing the compilation process. Decompilers can reconstruct much of the information that is lost during the compilation process (e.g., structure and type information). Unfortunately, they do not reconstruct semantically meaningful variable names, which are known to increase code understandability. We propose the Decompiled Identifier Renaming Engine (DIRE), a novel probabilistic technique for variable name recovery that uses both lexical and structural information recovered by the decompiler. We also present a technique for generating corpora suitable for training and evaluating models of decompiled code renaming, which we use to create a corpus of 164,632 unique x86-64 binaries generated from C projects mined from GitHub. Our results show that on this corpus DIRE can predict variable names identical to the names in the original source code up to 74.3% of the time.",
      "year": 2019,
      "influentialCitationCount": 9,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1909.09029",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "628-639",
        "name": "2019 34th IEEE/ACM International Conference on Automated Software Engineering (ASE)"
      },
      "authors": [
        {
          "authorId": "51119916",
          "name": "Jeremy Lacomis"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "32804915",
          "name": "Edward J. Schwartz"
        },
        {
          "authorId": "3216345",
          "name": "Miltiadis Allamanis"
        },
        {
          "authorId": "2957803",
          "name": "Claire Le Goues"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2434621",
          "name": "Bogdan Vasilescu"
        }
      ]
    },
    {
      "paperId": "db1dafd0c356491cbbf53338b9984de324e7239c",
      "externalIds": {
        "DBLP": "conf/acl/PatraMGGN19",
        "ACL": "P19-1018",
        "MAG": "2969145415",
        "ArXiv": "1908.06625",
        "DOI": "10.18653/v1/P19-1018",
        "CorpusId": 196176630
      },
      "url": "https://www.semanticscholar.org/paper/db1dafd0c356491cbbf53338b9984de324e7239c",
      "title": "Bilingual Lexicon Induction with Semi-supervision in Non-Isometric Embedding Spaces",
      "abstract": "Recent work on bilingual lexicon induction (BLI) has frequently depended either on aligned bilingual lexicons or on distribution matching, often with an assumption about the isometry of the two spaces. We propose a technique to quantitatively estimate this assumption of the isometry between two embedding spaces and empirically show that this assumption weakens as the languages in question become increasingly etymologically distant. We then propose Bilingual Lexicon Induction with Semi-Supervision (BLISS) \u2014 a semi-supervised approach that relaxes the isometric assumption while leveraging both limited aligned bilingual lexicons and a larger set of unaligned word embeddings, as well as a novel hubness filtering technique. Our proposed method obtains state of the art results on 15 of 18 language pairs on the MUSE dataset, and does particularly well when the embedding spaces don\u2019t appear to be isometric. In addition, we also show that adding supervision stabilizes the learning procedure, and is effective even with minimal supervision.",
      "year": 2019,
      "influentialCitationCount": 17,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1018.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "184-193"
      },
      "authors": [
        {
          "authorId": "27419446",
          "name": "Barun Patra"
        },
        {
          "authorId": "22272110",
          "name": "Joel Ruben Antony Moniz"
        },
        {
          "authorId": "31264049",
          "name": "Sarthak Garg"
        },
        {
          "authorId": "1762110",
          "name": "Matthew R. Gormley"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "df4e3aa275b8f81e22a5332ab550805083094dae",
      "externalIds": {
        "ACL": "D19-5601",
        "DBLP": "conf/emnlp/HayashiOBKFLNS19",
        "ArXiv": "1910.13299",
        "MAG": "2987188351",
        "DOI": "10.18653/v1/D19-5601",
        "CorpusId": 204957305
      },
      "url": "https://www.semanticscholar.org/paper/df4e3aa275b8f81e22a5332ab550805083094dae",
      "title": "Findings of the Third Workshop on Neural Generation and Translation",
      "abstract": "This document describes the findings of the Third Workshop on Neural Generation and Translation, held in concert with the annual conference of the Empirical Methods in Natural Language Processing (EMNLP 2019). First, we summarize the research trends of papers presented in the proceedings. Second, we describe the results of the two shared tasks 1) efficient neural machine translation (NMT) where participants were tasked with creating NMT systems that are both accurate and efficient, and 2) document generation and translation (DGT) where participants were tasked with developing systems that generate summaries from structured data, potentially with assistance from text in another language.",
      "year": 2019,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D19-5601.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-14"
      },
      "authors": [
        {
          "authorId": "50376014",
          "name": "Hiroaki Hayashi"
        },
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "2539211",
          "name": "Alexandra Birch"
        },
        {
          "authorId": "2621022",
          "name": "Ioannis Konstas"
        },
        {
          "authorId": "2987548",
          "name": "A. Finch"
        },
        {
          "authorId": "1707242",
          "name": "Minh-Thang Luong"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1790811",
          "name": "Katsuhito Sudoh"
        }
      ]
    },
    {
      "paperId": "e29e43d9c0772d44cff53044484970599db30d5f",
      "externalIds": {
        "MAG": "2988849053",
        "DBLP": "conf/emnlp/DouWHN19",
        "ArXiv": "1910.02555",
        "ACL": "D19-5606",
        "DOI": "10.18653/v1/D19-5606",
        "CorpusId": 203837718
      },
      "url": "https://www.semanticscholar.org/paper/e29e43d9c0772d44cff53044484970599db30d5f",
      "title": "Domain Differential Adaptation for Neural Machine Translation",
      "abstract": "Neural networks are known to be data hungry and domain sensitive, but it is nearly impossible to obtain large quantities of labeled data for every domain we are interested in. This necessitates the use of domain adaptation strategies. One common strategy encourages generalization by aligning the global distribution statistics between source and target domains, but one drawback is that the statistics of different domains or tasks are inherently divergent, and smoothing over these differences can lead to sub-optimal performance. In this paper, we propose the framework of Domain Differential Adaptation (DDA), where instead of smoothing over these differences we embrace them, directly modeling the difference between domains using models in a related task. We then use these learned domain differentials to adapt models for the target task accordingly. Experimental results on domain adaptation for neural machine translation demonstrate the effectiveness of this strategy, achieving consistent improvements over other alternative adaptation strategies in multiple experimental settings.",
      "year": 2019,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D19-5606.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1910.02555",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "14199369",
          "name": "Zi-Yi Dou"
        },
        {
          "authorId": null,
          "name": "Xinyi Wang"
        },
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "ea77b71385648f5c6ea533a0e3685f0e76302eba",
      "externalIds": {
        "DBLP": "journals/corr/abs-1908-08983",
        "ACL": "D19-1520",
        "ArXiv": "1908.08983",
        "MAG": "2969861850",
        "DOI": "10.18653/v1/D19-1520",
        "CorpusId": 201666359
      },
      "url": "https://www.semanticscholar.org/paper/ea77b71385648f5c6ea533a0e3685f0e76302eba",
      "title": "A Little Annotation does a Lot of Good: A Study in Bootstrapping Low-resource Named Entity Recognizers",
      "abstract": "Most state-of-the-art models for named entity recognition (NER) rely on the availability of large amounts of labeled data, making them challenging to extend to new, lower-resourced languages. However, there are now many proposed solutions to this problem involving either cross-lingual transfer learning, which learns from other highly resourced languages, or active learning, which efficiently selects effective training data based on model predictions. In this paper, we ask the question: given this recent progress, and some amount of human annotation, what is the most effective method for efficiently creating high-quality entity recognizers in under-resourced languages? Based on extensive experimentation using both simulated and real human annotation, we settle on a recipe of starting with a cross-lingual transferred model, then performing targeted annotation of only uncertain entity spans in the target language, minimizing annotator effort. Results demonstrate that cross-lingual transfer is a powerful tool when very little data can be annotated, but an entity-targeted annotation strategy can achieve competitive accuracy quickly, with just one-tenth of training data.",
      "year": 2019,
      "influentialCitationCount": 8,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D19-1520.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "5163-5173"
      },
      "authors": [
        {
          "authorId": "51250894",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "20467900",
          "name": "Jiateng Xie"
        },
        {
          "authorId": "38599655",
          "name": "Zaid A. W. Sheikh"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143712374",
          "name": "J. Carbonell"
        }
      ]
    },
    {
      "paperId": "ece62ada00cef99d9fc7a60e7d4b773f6d87c8f9",
      "externalIds": {
        "ArXiv": "1902.08899",
        "DBLP": "journals/corr/abs-1902-08899",
        "MAG": "2916049236",
        "CorpusId": 67855908
      },
      "url": "https://www.semanticscholar.org/paper/ece62ada00cef99d9fc7a60e7d4b773f6d87c8f9",
      "title": "The ARIEL-CMU Systems for LoReHLT18",
      "abstract": "This paper describes the ARIEL-CMU submissions to the Low Resource Human Language Technologies (LoReHLT) 2018 evaluations for the tasks Machine Translation (MT), Entity Discovery and Linking (EDL), and detection of Situation Frames in Text and Speech (SF Text and Speech).",
      "year": 2019,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1902.08899",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "51250894",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "35186886",
          "name": "Siddharth Dalmia"
        },
        {
          "authorId": "145919378",
          "name": "Junjie Hu"
        },
        {
          "authorId": "47058260",
          "name": "Xinjian Li"
        },
        {
          "authorId": "144633696",
          "name": "Austin Matthews"
        },
        {
          "authorId": "3456073",
          "name": "Aldrian Obaja Muis"
        },
        {
          "authorId": "145671279",
          "name": "Naoki Otani"
        },
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "38599655",
          "name": "Zaid A. W. Sheikh"
        },
        {
          "authorId": "47963068",
          "name": "Nidhi Vyas"
        },
        {
          "authorId": null,
          "name": "Xinyi Wang"
        },
        {
          "authorId": "20467900",
          "name": "Jiateng Xie"
        },
        {
          "authorId": "8233965",
          "name": "Ruochen Xu"
        },
        {
          "authorId": "2384711",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "25707161",
          "name": "Peter J. Jansen"
        },
        {
          "authorId": "35729970",
          "name": "Yiming Yang"
        },
        {
          "authorId": "1686960",
          "name": "Lori S. Levin"
        },
        {
          "authorId": "1740721",
          "name": "Florian Metze"
        },
        {
          "authorId": "1706595",
          "name": "T. Mitamura"
        },
        {
          "authorId": "3407646",
          "name": "David R. Mortensen"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144547315",
          "name": "E. Hovy"
        },
        {
          "authorId": "1690706",
          "name": "A. Black"
        },
        {
          "authorId": "143712374",
          "name": "J. Carbonell"
        },
        {
          "authorId": "2429228",
          "name": "Graham Horwood"
        },
        {
          "authorId": "41123680",
          "name": "Shabnam Tafreshi"
        },
        {
          "authorId": "1700007",
          "name": "Mona T. Diab"
        },
        {
          "authorId": "2793610",
          "name": "Efsun Sarioglu Kayi"
        },
        {
          "authorId": "2881964",
          "name": "N. Farra"
        },
        {
          "authorId": "145590324",
          "name": "K. McKeown"
        }
      ]
    },
    {
      "paperId": "f249e3a7d4f7f964e9a4ca6e633ac31410a91dd8",
      "externalIds": {
        "ACL": "D19-1091",
        "ArXiv": "1908.05838",
        "MAG": "2968525655",
        "DBLP": "conf/emnlp/AnastasopoulosN19",
        "DOI": "10.18653/v1/D19-1091",
        "CorpusId": 201058388
      },
      "url": "https://www.semanticscholar.org/paper/f249e3a7d4f7f964e9a4ca6e633ac31410a91dd8",
      "title": "Pushing the Limits of Low-Resource Morphological Inflection",
      "abstract": "Recent years have seen exceptional strides in the task of automatic morphological inflection generation. However, for a long tail of languages the necessary resources are hard to come by, and state-of-the-art neural methods that work well under higher resource settings perform poorly in the face of a paucity of data. In response, we propose a battery of improvements that greatly improve performance under such low-resource conditions. First, we present a novel two-step attention architecture for the inflection decoder. In addition, we investigate the effects of cross-lingual transfer from single and multiple languages, as well as monolingual data hallucination. The macro-averaged accuracy of our models outperforms the state-of-the-art by 15 percentage points. Also, we identify the crucial factors for success with cross-lingual transfer for morphological inflection: typological similarity and a common representation across languages.",
      "year": 2019,
      "influentialCitationCount": 10,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D19-1091.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "984-996"
      },
      "authors": [
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "f26f17ec49f2593bcc926051394871480a80c0c2",
      "externalIds": {
        "MAG": "2964259010",
        "ACL": "N19-1161",
        "DBLP": "journals/corr/abs-1904-02343",
        "ArXiv": "1904.02343",
        "DOI": "10.18653/v1/N19-1161",
        "CorpusId": 102354931
      },
      "url": "https://www.semanticscholar.org/paper/f26f17ec49f2593bcc926051394871480a80c0c2",
      "title": "Density Matching for Bilingual Word Embedding",
      "abstract": "Recent approaches to cross-lingual word embedding have generally been based on linear transformations between the sets of embedding vectors in the two languages. In this paper, we propose an approach that instead expresses the two monolingual embedding spaces as probability densities defined by a Gaussian mixture model, and matches the two densities using a method called normalizing flow. The method requires no explicit supervision, and can be learned with only a seed dictionary of words that have identical strings. We argue that this formulation has several intuitively attractive properties, particularly with the respect to improving robustness and generalization to mappings between difficult language pairs or word pairs. On a benchmark data set of bilingual lexicon induction and cross-lingual word similarity, our approach can achieve competitive or superior performance compared to state-of-the-art published results, with particularly strong results being found on etymologically distant and/or morphologically rich languages.",
      "year": 2019,
      "influentialCitationCount": 7,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1904.02343",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1588-1598"
      },
      "authors": [
        {
          "authorId": "2384711",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "2378954",
          "name": "Xuezhe Ma"
        },
        {
          "authorId": "144629034",
          "name": "Di Wang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "fc5d79301a0876201c95954a764ec374b8eb236e",
      "externalIds": {
        "MAG": "2947177396",
        "ArXiv": "1906.00376",
        "ACL": "P19-1286",
        "DBLP": "conf/acl/HuXNC19",
        "DOI": "10.18653/v1/P19-1286",
        "CorpusId": 173991071
      },
      "url": "https://www.semanticscholar.org/paper/fc5d79301a0876201c95954a764ec374b8eb236e",
      "title": "Domain Adaptation of Neural Machine Translation by Lexicon Induction",
      "abstract": "It has been previously noted that neural machine translation (NMT) is very sensitive to domain shift. In this paper, we argue that this is a dual effect of the highly lexicalized nature of NMT, resulting in failure for sentences with large numbers of unknown words, and lack of supervision for domain-specific words. To remedy this problem, we propose an unsupervised adaptation method which fine-tunes a pre-trained out-of-domain NMT model using a pseudo-in-domain corpus. Specifically, we perform lexicon induction to extract an in-domain lexicon, and construct a pseudo-parallel in-domain corpus by performing word-for-word back-translation of monolingual in-domain target sentences. In five domains over twenty pairwise adaptation settings and two model architectures, our method achieves consistent improvements without using any in-domain parallel sentences, improving up to 14 BLEU over unadapted models, and up to 2 BLEU over strong back-translation baselines.",
      "year": 2019,
      "influentialCitationCount": 9,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P19-1286.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1906.00376",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "145919378",
          "name": "Junjie Hu"
        },
        {
          "authorId": "67284811",
          "name": "Mengzhou Xia"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143712374",
          "name": "J. Carbonell"
        }
      ]
    },
    {
      "paperId": "fc78af26fd7644867af1abb8fbf2c37b47ad8257",
      "externalIds": {
        "DBLP": "conf/acl/AnastasopoulosN20",
        "MAG": "2983273582",
        "ArXiv": "1911.03058",
        "ACL": "2020.acl-main.766",
        "DOI": "10.18653/v1/2020.acl-main.766",
        "CorpusId": 207847595
      },
      "url": "https://www.semanticscholar.org/paper/fc78af26fd7644867af1abb8fbf2c37b47ad8257",
      "title": "Should All Cross-Lingual Embeddings Speak English?",
      "abstract": "Most of recent work in cross-lingual word embeddings is severely Anglocentric. The vast majority of lexicon induction evaluation dictionaries are between English and another language, and the English embedding space is selected by default as the hub when learning in a multilingual setting. With this work, however, we challenge these practices. First, we show that the choice of hub language can significantly impact downstream lexicon induction zero-shot POS tagging performance. Second, we both expand a standard English-centered evaluation dictionary collection to include all language pairs using triangulation, and create new dictionaries for under-represented languages. Evaluating established methods over all these language pairs sheds light into their suitability for aligning embeddings from distant languages and presents new challenges for the field. Finally, in our analysis we identify general guidelines for strong cross-lingual embedding baselines, that extend to language pairs that do not include English.",
      "year": 2019,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/2020.acl-main.766.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1911.03058",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "00799dceb9e7209bb9d71b38fa5b49483e886978",
      "externalIds": {
        "MAG": "2886439381",
        "DBLP": "conf/usenix/XuZN0KDHYX18",
        "CorpusId": 51876001
      },
      "url": "https://www.semanticscholar.org/paper/00799dceb9e7209bb9d71b38fa5b49483e886978",
      "title": "Cavs: An Efficient Runtime System for Dynamic Neural Networks",
      "abstract": "Recent deep learning (DL) models are moving more and more to dynamic neural network (NN) architectures, where the NN structure changes for every data sample. However, existing DL programming models are inef\ufb01cient in handling dynamic network architectures because of: (1) substantial overhead caused by repeating data\ufb02ow graph construction and processing every example; (2) dif\ufb01culties in batched execution of multiple samples; (3) inability to incorporate graph optimization techniques such as those used in static graphs. In this paper, we present \u201cCavs\u201d, a runtime system that overcomes these bottlenecks and achieves ef\ufb01cient training and inference of dynamic NNs. Cavs represents a dynamic NN as a static vertex function F and a dynamic instance-speci\ufb01c graph G . It avoids the overhead of repeated graph construction by only declaring and constructing F once, and allows for the use of static graph optimization techniques on pre-de\ufb01ned operations in F . Cavs performs training and inference by scheduling the execution of F following the dependencies in G , hence naturally exposing batched execution opportunities over different samples. Experiments comparing Cavs to state-of-the-art frameworks for dynamic NNs (TensorFlow Fold, PyTorch and DyNet) demonstrate the ef\ufb01cacy of our approach: Cavs achieves a near one order of magnitude speedup on training of dynamic NN architectures, and ablations verify the effectiveness of our proposed design and optimizations.",
      "year": 2018,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "937-950"
      },
      "authors": [
        {
          "authorId": "1704538",
          "name": "Shizhen Xu"
        },
        {
          "authorId": "1682058",
          "name": "H. Zhang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143716171",
          "name": "Wei Dai"
        },
        {
          "authorId": "2152674455",
          "name": "Jin Kyu Kim"
        },
        {
          "authorId": "145114723",
          "name": "Zhijie Deng"
        },
        {
          "authorId": "1707357",
          "name": "Qirong Ho"
        },
        {
          "authorId": "145789924",
          "name": "Guangwen Yang"
        },
        {
          "authorId": "143977260",
          "name": "E. Xing"
        }
      ]
    },
    {
      "paperId": "032e660447156a045ad6cf50272bca46246f4645",
      "externalIds": {
        "MAG": "2798878995",
        "DBLP": "conf/acl/MichelN18",
        "ArXiv": "1805.01817",
        "ACL": "P18-2050",
        "DOI": "10.18653/v1/P18-2050",
        "CorpusId": 19247366
      },
      "url": "https://www.semanticscholar.org/paper/032e660447156a045ad6cf50272bca46246f4645",
      "title": "Extreme Adaptation for Personalized Neural Machine Translation",
      "abstract": "Every person speaks or writes their own flavor of their native language, influenced by a number of factors: the content they tend to talk about, their gender, their social status, or their geographical origin. When attempting to perform Machine Translation (MT), these variations have a significant effect on how the system should perform translation, but this is not captured well by standard one-size-fits-all models. In this paper, we propose a simple and parameter-efficient adaptation technique that only requires adapting the bias of the output softmax to each particular user of the MT system, either directly or through a factored approximation. Experiments on TED talks in three languages demonstrate improvements in translation accuracy, and better reflection of speaker traits in the target text.",
      "year": 2018,
      "influentialCitationCount": 7,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P18-2050.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "312-318"
      },
      "authors": [
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "0aa0131253b832fdba27ac43f8fa78a322763191",
      "externalIds": {
        "MAG": "2795581297",
        "DBLP": "journals/mt/KanoTSNTN18",
        "DOI": "10.1007/s10590-018-9217-7",
        "CorpusId": 4663894
      },
      "url": "https://www.semanticscholar.org/paper/0aa0131253b832fdba27ac43f8fa78a322763191",
      "title": "An end-to-end model for cross-lingual transformation of paralinguistic information",
      "abstract": null,
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "32",
        "pages": "353-368",
        "name": "Machine Translation"
      },
      "authors": [
        {
          "authorId": "3218696",
          "name": "Takatomo Kano"
        },
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b",
      "externalIds": {
        "MAG": "2963831883",
        "DBLP": "journals/tacl/BuckmanN18",
        "ArXiv": "1803.05071",
        "ACL": "Q18-1036",
        "DOI": "10.1162/tacl_a_00036",
        "CorpusId": 3834706
      },
      "url": "https://www.semanticscholar.org/paper/0ca2a7465fe88f1f4912b8dd7b4b0db69a268b0b",
      "title": "Neural Lattice Language Models",
      "abstract": "In this work, we propose a new language modeling paradigm that has the ability to perform both prediction and moderation of information flow at multiple granularities: neural lattice language models. These models construct a lattice of possible paths through a sentence and marginalize across this lattice to calculate sequence probabilities or optimize parameters. This approach allows us to seamlessly incorporate linguistic intuitions \u2014 including polysemy and the existence of multiword lexical items \u2014 into our language model. Experiments on multiple language modeling tasks show that English neural lattice language models that utilize polysemous embeddings are able to improve perplexity by 9.95% relative to a word-level baseline, and that a Chinese model that handles multi-character tokens is able to improve perplexity by 20.94% relative to a character-level baseline.",
      "year": 2018,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00036",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "6",
        "pages": "529-541",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "47619311",
          "name": "Jacob Buckman"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "0ee468b9b709a2610c4b574d67218e7960350224",
      "externalIds": {
        "DBLP": "conf/emnlp/WangPDN18",
        "MAG": "2952148143",
        "ArXiv": "1808.07512",
        "ACL": "D18-1100",
        "DOI": "10.18653/v1/D18-1100",
        "CorpusId": 52078335
      },
      "url": "https://www.semanticscholar.org/paper/0ee468b9b709a2610c4b574d67218e7960350224",
      "title": "SwitchOut: an Efficient Data Augmentation Algorithm for Neural Machine Translation",
      "abstract": "In this work, we examine methods for data augmentation for text-based tasks such as neural machine translation (NMT). We formulate the design of a data augmentation policy with desirable properties as an optimization problem, and derive a generic analytic solution. This solution not only subsumes some existing augmentation schemes, but also leads to an extremely simple data augmentation strategy for NMT: randomly replacing words in both the source sentence and the target sentence with other random words from their corresponding vocabularies. We name this method SwitchOut. Experiments on three translation datasets of different scales show that SwitchOut yields consistent improvements of about 0.5 BLEU, achieving better or comparable performances to strong alternatives such as word dropout (Sennrich et al., 2016a). Code to implement this method is included in the appendix.",
      "year": 2018,
      "influentialCitationCount": 25,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D18-1100.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "856-861"
      },
      "authors": [
        {
          "authorId": "3277494",
          "name": "Xinyi Wang"
        },
        {
          "authorId": "143950636",
          "name": "Hieu Pham"
        },
        {
          "authorId": "3422912",
          "name": "Zihang Dai"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "12239e761e8c7cd05e12e18f43dba7b46dfd8ac1",
      "externalIds": {
        "MAG": "2897058237",
        "DBLP": "journals/corr/abs-1810-06826",
        "ACL": "2018.iwslt-1.7",
        "ArXiv": "1810.06826",
        "CorpusId": 53116366
      },
      "url": "https://www.semanticscholar.org/paper/12239e761e8c7cd05e12e18f43dba7b46dfd8ac1",
      "title": "Multi-Source Neural Machine Translation with Data Augmentation",
      "abstract": "Multi-source translation systems translate from multiple languages to a single target language. By using information from these multiple sources, these systems achieve large gains in accuracy. To train these systems, it is necessary to have corpora with parallel text in multiple sources and the target language. However, these corpora are rarely complete in practice due to the difficulty of providing human translations in all of the relevant languages. In this paper, we propose a data augmentation approach to fill such incomplete parts using multi-source neural machine translation (NMT). In our experiments, results varied over different language combinations but significant gains were observed when using a source language similar to the target language.",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1810.06826",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1490936400",
          "name": "Yuta Nishimura"
        },
        {
          "authorId": "1790811",
          "name": "Katsuhito Sudoh"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "175b58fe7e49bb5c0c771b73f8834bcff21b59c7",
      "externalIds": {
        "ArXiv": "1806.00692",
        "DBLP": "conf/coling/NaikRSRN18",
        "ACL": "C18-1198",
        "MAG": "2805083708",
        "CorpusId": 46932607
      },
      "url": "https://www.semanticscholar.org/paper/175b58fe7e49bb5c0c771b73f8834bcff21b59c7",
      "title": "Stress Test Evaluation for Natural Language Inference",
      "abstract": "Natural language inference (NLI) is the task of determining if a natural language hypothesis can be inferred from a given premise in a justifiable manner. NLI was proposed as a benchmark task for natural language understanding. Existing models perform well at standard datasets for NLI, achieving impressive results across different genres of text. However, the extent to which these models understand the semantic content of sentences is unclear. In this work, we propose an evaluation methodology consisting of automatically constructed \u201cstress tests\u201d that allow us to examine whether systems have the ability to make real inferential decisions. Our evaluation of six sentence-encoder models on these stress tests reveals strengths and weaknesses of these models with respect to challenging linguistic phenomena, and suggests important directions for future work in this area.",
      "year": 2018,
      "influentialCitationCount": 47,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1806.00692",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "23175870",
          "name": "Aakanksha Naik"
        },
        {
          "authorId": "3023068",
          "name": "Abhilasha Ravichander"
        },
        {
          "authorId": "2464164",
          "name": "N. Sadeh"
        },
        {
          "authorId": "35959897",
          "name": "C. Ros\u00e9"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "18268bdfc8a6e0a51f373bc4acf65c8b9a7bd6a0",
      "externalIds": {
        "MAG": "2808023569",
        "DOI": "10.5715/JNLP.25.167",
        "CorpusId": 67367994
      },
      "url": "https://www.semanticscholar.org/paper/18268bdfc8a6e0a51f373bc4acf65c8b9a7bd6a0",
      "title": "Neural Machine Translation Models using Binarized Prediction and Error Correction",
      "abstract": "\u672c\u8ad6\u6587\u3067\u306f,\u30cb\u30e5\u30fc\u30e9\u30eb\u7ffb\u8a33\u30e2\u30c7\u30eb\u3067\u554f\u984c\u3068\u306a\u308b\u51fa\u529b\u5c64\u306e\u6642\u9593\u30fb\u7a7a\u9593\u8a08\u7b97\u91cf\u3092,\u4e8c \u5024\u7b26\u53f7\u3092\u7528\u3044\u305f\u4e88\u6e2c\u6cd5\u306b\u3088\u308a\u5927\u5e45\u306b\u524a\u6e1b\u3059\u308b\u624b\u6cd5\u3092\u63d0\u6848\u3059\u308b.\u63d0\u6848\u624b\u6cd5\u3067\u306f\u5f93\u6765\u306e \u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u306e\u3088\u3046\u306b\u5404\u5358\u8a9e\u306e\u30b9\u30b3\u30a2\u3092\u76f4\u63a5\u6c42\u3081\u308b\u306e\u3067\u306f\u306a\u304f,\u5404\u5358\u8a9e\u306b\u5bfe\u5fdc\u4ed8 \u3051\u3089\u308c\u305f\u30d3\u30c3\u30c8\u5217\u3092\u4e88\u6e2c\u3059\u308b\u3053\u3068\u306b\u3088\u308a,\u9593\u63a5\u7684\u306b\u51fa\u529b\u5358\u8a9e\u306e\u78ba\u7387\u3092\u6c42\u3081\u308b.\u3053\u308c \u306b\u3088\u308a,\u6700\u3082\u52b9\u7387\u7684\u306a\u5834\u5408\u3067\u5f93\u6765\u6cd5\u306e\u5bfe\u6570\u7a0b\u5ea6\u307e\u3067\u51fa\u529b\u5c64\u306e\u8a08\u7b97\u91cf\u3092\u524a\u6e1b\u53ef\u80fd\u3067\u3042 \u308b.\u3053\u306e\u3088\u3046\u306a\u30e2\u30c7\u30eb\u306f\u30bd\u30d5\u30c8\u30de\u30c3\u30af\u30b9\u3088\u308a\u3082\u63a8\u5b9a\u304c\u96e3\u3057\u304f,\u5358\u4f53\u3067\u9069\u7528\u3057\u305f\u5834\u5408 \u306b\u306f\u7ffb\u8a33\u7cbe\u5ea6\u306e\u4f4e\u4e0b\u3092\u62db\u304f.\u3053\u306e\u305f\u3081,\u672c\u7814\u7a76\u3067\u306f\u63d0\u6848\u624b\u6cd5\u306e\u6027\u80fd\u3092\u88dc\u511f\u3059\u308b\u305f\u3081 \u306b,\u5f93\u6765\u6cd5\u3068\u306e\u6df7\u5408\u30e2\u30c7\u30eb,\u304a\u3088\u3073\u4e8c\u5024\u7b26\u53f7\u306b\u5bfe\u3059\u308b\u8aa4\u308a\u8a02\u6b63\u624b\u6cd5\u306e\u9069\u7528\u3068\u3044\u3046 2 \u70b9\u306e\u6539\u826f\u3082\u63d0\u6848\u3059\u308b.\u65e5\u82f1\u30fb\u82f1\u65e5\u7ffb\u8a33\u30bf\u30b9\u30af\u3092\u7528\u3044\u305f\u8a55\u4fa1\u5b9f\u9a13\u306b\u3088\u308a,\u63d0\u6848\u6cd5\u304c\u5f93 \u6765\u6cd5\u3068\u6bd4\u8f03\u3057\u3066\u540c\u7b49\u7a0b\u5ea6\u306e BLEU\u3092\u9054\u6210\u53ef\u80fd\u3067\u3042\u308b\u3068\u3068\u3082\u306b,\u51fa\u529b\u5c64\u306b\u8981\u3059\u308b\u30e1\u30e2 \u30ea\u3092\u6570\u5341\u5206\u306e 1\u306b\u524a\u6e1b\u3057,CPU\u3067\u306e\u5b9f\u884c\u901f\u5ea6\u3092 5\u500d\u304b\u3089 10\u500d\u7a0b\u5ea6\u306b\u5411\u4e0a\u53ef\u80fd\u3067\u3042 \u308b\u3053\u3068\u3092\u793a\u3059. \u30ad\u30fc\u30ef\u30fc\u30c9:\u30cb\u30e5\u30fc\u30e9\u30eb\u7ffb\u8a33,\u4e8c\u5024\u7b26\u53f7\u4e88\u6e2c,\u8aa4\u308a\u8a02\u6b63",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "25",
        "pages": "167-199",
        "name": ""
      },
      "authors": [
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "144332819",
          "name": "Philip Arthur"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "19a6e362840d3a2d27d0fa5509eaa4d4597a2859",
      "externalIds": {
        "MAG": "2941679272",
        "CorpusId": 149724718
      },
      "url": "https://www.semanticscholar.org/paper/19a6e362840d3a2d27d0fa5509eaa4d4597a2859",
      "title": "On Meaning-Preserving Adversarial Perturbations for Sequence-to-Sequence Models",
      "abstract": null,
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2116235416",
          "name": "Xian Li"
        },
        {
          "authorId": "145503806",
          "name": "J. Pino"
        }
      ]
    },
    {
      "paperId": "1a53e7446274016f737236bdd48e3ff05d966384",
      "externalIds": {
        "MAG": "2964315653",
        "DBLP": "conf/msr/YinDCVN08",
        "ArXiv": "1805.08949",
        "DOI": "10.1145/3196398.3196408",
        "CorpusId": 43922261
      },
      "url": "https://www.semanticscholar.org/paper/1a53e7446274016f737236bdd48e3ff05d966384",
      "title": "Learning to Mine Aligned Code and Natural Language Pairs from Stack Overflow",
      "abstract": "For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models require parallel data between natural language (NL) and code with fine-grained alignments. StackOverflow (SO) is a promising source to create such a data set: the questions are diverse and most of them have corresponding answers with high-quality code snippets. However, existing heuristic methods (e.g. pairing the title of a post with the code in the accepted answer) are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a novel method to mine high-quality aligned data from SO using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a probabilistic model to capture the correlation between NL and code using neural networks. These features are fed into a classifier that determines the quality of mined NL-code pairs. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples. Further, we find that reasonable results are achieved even when training the classifier on one language and testing on another, showing promise for scaling NL-code mining to a wide variety of programming languages beyond those for which we are able to annotate data.",
      "year": 2018,
      "influentialCitationCount": 37,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1805.08949",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "476-486",
        "name": "2018 IEEE/ACM 15th International Conference on Mining Software Repositories (MSR)"
      },
      "authors": [
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "2070544979",
          "name": "Bowen Deng"
        },
        {
          "authorId": "49824718",
          "name": "Edgar Chen"
        },
        {
          "authorId": "2434621",
          "name": "Bogdan Vasilescu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "1b114486d67252ff83fc90d4a8607636045c54ce",
      "externalIds": {
        "MAG": "2809170625",
        "DBLP": "conf/icse/YinDCVN18",
        "DOI": "10.1145/3183440.3195021",
        "CorpusId": 49291907
      },
      "url": "https://www.semanticscholar.org/paper/1b114486d67252ff83fc90d4a8607636045c54ce",
      "title": "Poster: Learning to Mine Parallel Natural Language/Source Code Corpora from Stack Overflow",
      "abstract": "For tasks like code synthesis from natural language, code retrieval, and code summarization, data-driven models have shown great promise. However, creating these models requires parallel data between natural language (NL) and code with fine-grained alignments. Stack Overflow (SO) is a promising source to create such a data set but existing heuristic methods are limited both in their coverage and the correctness of the NL-code pairs obtained. In this paper, we propose a method to mine high-quality aligned data from SO by training a classifier using two sets of features: hand-crafted features considering the structure of the extracted snippets, and correspondence features obtained by training a neural network model to capture the correlation between NL and code. Experiments using Python and Java as test beds show that the proposed method greatly expands coverage and accuracy over existing mining methods, even when using only a small number of labeled examples.",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "388-389",
        "name": "2018 IEEE/ACM 40th International Conference on Software Engineering: Companion (ICSE-Companion)"
      },
      "authors": [
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "2070544979",
          "name": "Bowen Deng"
        },
        {
          "authorId": "49824718",
          "name": "Edgar Chen"
        },
        {
          "authorId": "2434621",
          "name": "Bogdan Vasilescu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "1d5d170670889bd82364fbcc594dadcb5481e9e4",
      "externalIds": {
        "MAG": "2795933031",
        "DBLP": "journals/corr/abs-1804-02559",
        "ArXiv": "1804.02559",
        "ACL": "N18-1120",
        "DOI": "10.18653/v1/N18-1120",
        "CorpusId": 4698173
      },
      "url": "https://www.semanticscholar.org/paper/1d5d170670889bd82364fbcc594dadcb5481e9e4",
      "title": "Guiding Neural Machine Translation with Retrieved Translation Pieces",
      "abstract": "One of the difficulties of neural machine translation (NMT) is the recall and appropriate translation of low-frequency words or phrases. In this paper, we propose a simple, fast, and effective method for recalling previously seen translation examples and incorporating them into the NMT decoding process. Specifically, for an input sentence, we use a search engine to retrieve sentence pairs whose source sides are similar with the input sentence, and then collect n-grams that are both in the retrieved target sentences and aligned with words that match in the source sentences, which we call \u201ctranslation pieces\u201d. We compute pseudo-probabilities for each retrieved sentence based on similarities between the input sentence and the retrieved source sentences, and use these to weight the retrieved translation pieces. Finally, an existing NMT model is used to translate the input sentence, with an additional bonus given to outputs that contain the collected translation pieces. We show our method improves NMT translation results up to 6 BLEU points on three narrow domain translation tasks where repetitiveness of the target sentences is particularly salient. It also causes little increase in the translation time, and compares favorably to another alternative retrieval-based method with respect to accuracy, speed, and simplicity of implementation.",
      "year": 2018,
      "influentialCitationCount": 20,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/N18-1120.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1804.02559",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "4279134",
          "name": "Jingyi Zhang"
        },
        {
          "authorId": "1802277",
          "name": "M. Utiyama"
        },
        {
          "authorId": "1698363",
          "name": "E. Sumita"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4",
      "externalIds": {
        "MAG": "2887920589",
        "DBLP": "journals/corr/abs-1808-04189",
        "ACL": "D18-1103",
        "ArXiv": "1808.04189",
        "DOI": "10.18653/v1/D18-1103",
        "CorpusId": 51976920
      },
      "url": "https://www.semanticscholar.org/paper/205ff5dae21ca44c15d3b7d7a9febb7d84b47bc4",
      "title": "Rapid Adaptation of Neural Machine Translation to New Languages",
      "abstract": "This paper examines the problem of adapting neural machine translation systems to new, low-resourced languages (LRLs) as effectively and rapidly as possible. We propose methods based on starting with massively multilingual \u201cseed models\u201d, which can be trained ahead-of-time, and then continuing training on data related to the LRL. We contrast a number of strategies, leading to a novel, simple, yet effective method of \u201csimilar-language regularization\u201d, where we jointly train on both a LRL of interest and a similar high-resourced language to prevent over-fitting to small LRL data. Experiments demonstrate that massively multilingual models, even without any explicit adaptation, are surprisingly effective, achieving BLEU scores of up to 15.5 with no data from the LRL, and that the proposed similar-language regularization method improves over other adaptation methods by 1.7 BLEU points average over 4 LRL settings.",
      "year": 2018,
      "influentialCitationCount": 12,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D18-1103.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1808.04189",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145919378",
          "name": "Junjie Hu"
        }
      ]
    },
    {
      "paperId": "2068825cabd94c951a0282ed731a8b8f2da1721c",
      "externalIds": {
        "MAG": "2950420506",
        "DBLP": "journals/corr/abs-1806-07832",
        "ArXiv": "1806.07832",
        "ACL": "P18-1070",
        "DOI": "10.18653/v1/P18-1070",
        "CorpusId": 49325612
      },
      "url": "https://www.semanticscholar.org/paper/2068825cabd94c951a0282ed731a8b8f2da1721c",
      "title": "StructVAE: Tree-structured Latent Variable Models for Semi-supervised Semantic Parsing",
      "abstract": "Semantic parsing is the task of transducing natural language (NL) utterances into formal meaning representations (MRs), commonly represented as tree structures. Annotating NL utterances with their corresponding MRs is expensive and time-consuming, and thus the limited availability of labeled data often becomes the bottleneck of data-driven, supervised models. We introduce StructVAE, a variational auto-encoding model for semi-supervised semantic parsing, which learns both from limited amounts of parallel data, and readily-available unlabeled NL utterances. StructVAE models latent MRs not observed in the unlabeled data as tree-structured latent variables. Experiments on semantic parsing on the ATIS domain and Python code generation show that with extra unlabeled data, StructVAE outperforms strong supervised models.",
      "year": 2018,
      "influentialCitationCount": 12,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P18-1070.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1806.07832",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "2384711",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "34cb1f081c1d1d6b3dc16a9278940a9ee85fb2e0",
      "externalIds": {
        "DBLP": "journals/corr/abs-1805-04016",
        "ACL": "P18-2105",
        "MAG": "2951634684",
        "ArXiv": "1805.04016",
        "DOI": "10.18653/v1/p18-2105",
        "CorpusId": 13706308
      },
      "url": "https://www.semanticscholar.org/paper/34cb1f081c1d1d6b3dc16a9278940a9ee85fb2e0",
      "title": "Automatic Estimation of Simultaneous Interpreter Performance",
      "abstract": "Simultaneous interpretation, translation of the spoken word in real-time, is both highly challenging and physically demanding. Methods to predict interpreter confidence and the adequacy of the interpreted message have a number of potential applications, such as in computer-assisted interpretation interfaces or pedagogical tools. We propose the task of predicting simultaneous interpreter performance by building on existing methodology for quality estimation (QE) of machine translation output. In experiments over five settings in three language pairs, we extend a QE pipeline to estimate interpreter performance (as approximated by the METEOR evaluation metric) and propose novel features reflecting interpretation strategy and evaluation measures that further improve prediction accuracy.",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P18-2105.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "662-666"
      },
      "authors": [
        {
          "authorId": "40163298",
          "name": "Craig Alan Stewart"
        },
        {
          "authorId": "46623434",
          "name": "Nikolai Vogler"
        },
        {
          "authorId": "2149221827",
          "name": "Junjie Hu"
        },
        {
          "authorId": "1389036863",
          "name": "Jordan L. Boyd-Graber"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "36b6abfb32ea56208a2858b558acbdd001c965e9",
      "externalIds": {
        "MAG": "2806697599",
        "ArXiv": "1806.02940",
        "DBLP": "conf/aclnmt/BirchFLNO18",
        "ACL": "W18-2701",
        "DOI": "10.18653/v1/W18-2701",
        "CorpusId": 47010809
      },
      "url": "https://www.semanticscholar.org/paper/36b6abfb32ea56208a2858b558acbdd001c965e9",
      "title": "Findings of the Second Workshop on Neural Machine Translation and Generation",
      "abstract": "This document describes the findings of the Second Workshop on Neural Machine Translation and Generation, held in concert with the annual conference of the Association for Computational Linguistics (ACL 2018). First, we summarize the research trends of papers presented in the proceedings, and note that there is particular interest in linguistic structure, domain adaptation, data augmentation, handling inadequate resources, and analysis of models. Second, we describe the results of the workshop\u2019s shared task on efficient neural machine translation, where participants were tasked with creating MT systems that are both accurate and efficient.",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W18-2701.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-10"
      },
      "authors": [
        {
          "authorId": "2539211",
          "name": "Alexandra Birch"
        },
        {
          "authorId": "2987548",
          "name": "A. Finch"
        },
        {
          "authorId": "1707242",
          "name": "Minh-Thang Luong"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        }
      ]
    },
    {
      "paperId": "39365d95992c8294ba32d85c69d337040ddb8e54",
      "externalIds": {
        "DBLP": "conf/emnlp/WangPYN18",
        "ACL": "D18-1509",
        "ArXiv": "1808.09374",
        "MAG": "2950462711",
        "DOI": "10.18653/v1/D18-1509",
        "CorpusId": 52112576
      },
      "url": "https://www.semanticscholar.org/paper/39365d95992c8294ba32d85c69d337040ddb8e54",
      "title": "A Tree-based Decoder for Neural Machine Translation",
      "abstract": "Recent advances in Neural Machine Translation (NMT) show that adding syntactic information to NMT systems can improve the quality of their translations. Most existing work utilizes some specific types of linguistically-inspired tree structures, like constituency and dependency parse trees. This is often done via a standard RNN decoder that operates on a linearized target tree structure. However, it is an open question of what specific linguistic formalism, if any, is the best structural representation for NMT. In this paper, we (1) propose an NMT model that can naturally generate the topology of an arbitrary tree structure on the target side, and (2) experiment with various target tree structures. Our experiments show the surprising result that our model delivers the best improvements with balanced binary trees constructed without any linguistic knowledge; this model outperforms standard seq2seq models by up to 2.1 BLEU points, and other methods for incorporating target-side syntax by up to 0.7 BLEU.",
      "year": 2018,
      "influentialCitationCount": 7,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D18-1509.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "4772-4777"
      },
      "authors": [
        {
          "authorId": "3277494",
          "name": "Xinyi Wang"
        },
        {
          "authorId": "143950636",
          "name": "Hieu Pham"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "3a4f39dbb5e06a5fc55622315797da7a97cc76f6",
      "externalIds": {
        "DBLP": "journals/corr/abs-1809-00129",
        "MAG": "2890834189",
        "ACL": "W18-6462",
        "ArXiv": "1809.00129",
        "DOI": "10.18653/v1/W18-6462",
        "CorpusId": 52154927
      },
      "url": "https://www.semanticscholar.org/paper/3a4f39dbb5e06a5fc55622315797da7a97cc76f6",
      "title": "Contextual Encoding for Translation Quality Estimation",
      "abstract": "The task of word-level quality estimation (QE) consists of taking a source sentence and machine-generated translation, and predicting which words in the output are correct and which are wrong. In this paper, propose a method to effectively encode the local and global contextual information for each target word using a three-part neural network approach. The first part uses an embedding layer to represent words and their part-of-speech tags in both languages. The second part leverages a one-dimensional convolution layer to integrate local context information for each target word. The third part applies a stack of feed-forward and recurrent neural networks to further encode the global context in the sentence before making the predictions. This model was submitted as the CMU entry to the WMT2018 shared task on QE, and achieves strong results, ranking first in three of the six tracks.",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W18-6462.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1809.00129",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "145919378",
          "name": "Junjie Hu"
        },
        {
          "authorId": "1702500",
          "name": "Wei-Cheng Chang"
        },
        {
          "authorId": "9287688",
          "name": "Yuexin Wu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "3ad287cf3b17cb109bf991731d2c0dcf8b7db2b1",
      "externalIds": {
        "ArXiv": "1805.04570",
        "DBLP": "conf/acl/NeubigGM18",
        "ACL": "P18-1247",
        "MAG": "2964085249",
        "DOI": "10.18653/v1/P18-1247",
        "CorpusId": 44181076
      },
      "url": "https://www.semanticscholar.org/paper/3ad287cf3b17cb109bf991731d2c0dcf8b7db2b1",
      "title": "Neural Factor Graph Models for Cross-lingual Morphological Tagging",
      "abstract": "Morphological analysis involves predicting the syntactic traits of a word (e.g. POS: Noun, Case: Acc, Gender: Fem). Previous work in morphological tagging improves performance for low-resource languages (LRLs) through cross-lingual training with a high-resource language (HRL) from the same family, but is limited by the strict, often false, assumption that tag sets exactly overlap between the HRL and LRL. In this paper we propose a method for cross-lingual morphological tagging that aims to improve information sharing between languages by relaxing this assumption. The proposed model uses factorial conditional random fields with neural network potentials, making it possible to (1) utilize the expressive power of neural network representations to smooth over superficial differences in the surface forms, (2) model pairwise and transitive relationships between tags, and (3) accurately generate tag sets that are unseen or rare in the training data. Experiments on four languages from the Universal Dependencies Treebank demonstrate superior tagging accuracies over existing cross-lingual approaches.",
      "year": 2018,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P18-1247.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1805.04570",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "8805254",
          "name": "Chaitanya Malaviya"
        },
        {
          "authorId": "1762110",
          "name": "Matthew R. Gormley"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "4857e0e3d720b87b4523a6435cc166bcb7ae328a",
      "externalIds": {
        "MAG": "2902214435",
        "DOI": "10.18653/v1/w18-27",
        "CorpusId": 70187612
      },
      "url": "https://www.semanticscholar.org/paper/4857e0e3d720b87b4523a6435cc166bcb7ae328a",
      "title": "Proceedings of the 2nd Workshop on Neural Machine Translation and Generation",
      "abstract": null,
      "year": 2018,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "2539211",
          "name": "Alexandra Birch"
        },
        {
          "authorId": "2987548",
          "name": "A. Finch"
        },
        {
          "authorId": "1821711",
          "name": "Thang Luong"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        }
      ]
    },
    {
      "paperId": "485b3f77b9913e151e7ca897d99497e70e7f30d1",
      "externalIds": {
        "MAG": "3001816066",
        "DBLP": "journals/mt/SaleskyRCNN20",
        "ArXiv": "1810.08641",
        "DOI": "10.1007/s10590-019-09243-8",
        "CorpusId": 53047158
      },
      "url": "https://www.semanticscholar.org/paper/485b3f77b9913e151e7ca897d99497e70e7f30d1",
      "title": "Optimizing segmentation granularity for neural machine translation",
      "abstract": null,
      "year": 2018,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1810.08641",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "34",
        "pages": "41 - 59",
        "name": "Machine Translation"
      },
      "authors": [
        {
          "authorId": "3448427",
          "name": "Elizabeth Salesky"
        },
        {
          "authorId": "80081214",
          "name": "Andrew Runge"
        },
        {
          "authorId": "80195231",
          "name": "Alex Coda"
        },
        {
          "authorId": "2920247",
          "name": "J. Niehues"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "4e1b16fd719354b0a9e92075be66c85d4b95082c",
      "externalIds": {
        "MAG": "2939768265",
        "CorpusId": 189282176
      },
      "url": "https://www.semanticscholar.org/paper/4e1b16fd719354b0a9e92075be66c85d4b95082c",
      "title": "Measuring Density and Similarity of Task Relevant Information in Neural Representations",
      "abstract": null,
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "2064506371",
          "name": "Danish Pruthi"
        },
        {
          "authorId": "2109835205",
          "name": "Mansi Gupta"
        },
        {
          "authorId": "51115915",
          "name": "Nitish Kulkarni"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144547315",
          "name": "E. Hovy"
        }
      ]
    },
    {
      "paperId": "5b1516c87818084dc5d195cc274e1ee8923210d2",
      "externalIds": {
        "ACL": "D18-1034",
        "ArXiv": "1808.09861",
        "DBLP": "conf/emnlp/XieYNSC18",
        "MAG": "2889191148",
        "DOI": "10.18653/v1/D18-1034",
        "CorpusId": 52117484
      },
      "url": "https://www.semanticscholar.org/paper/5b1516c87818084dc5d195cc274e1ee8923210d2",
      "title": "Neural Cross-Lingual Named Entity Recognition with Minimal Resources",
      "abstract": "For languages with no annotated resources, unsupervised transfer of natural language processing models such as named-entity recognition (NER) from resource-rich languages would be an appealing capability. However, differences in words and word order across languages make it a challenging problem. To improve mapping of lexical items across languages, we propose a method that finds translations based on bilingual word embeddings. To improve robustness to word order differences, we propose to use self-attention, which allows for a degree of flexibility with respect to word order. We demonstrate that these methods achieve state-of-the-art or competitive NER performance on commonly tested languages under a cross-lingual setting, with much lower resource requirements than past approaches. We also evaluate the challenges of applying these methods to Uyghur, a low-resource language.",
      "year": 2018,
      "influentialCitationCount": 18,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D18-1034.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1808.09861",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "20467900",
          "name": "Jiateng Xie"
        },
        {
          "authorId": "2109512754",
          "name": "Zhilin Yang"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144365875",
          "name": "Noah A. Smith"
        },
        {
          "authorId": "143712374",
          "name": "J. Carbonell"
        }
      ]
    },
    {
      "paperId": "649c1148439a4e875dab414ba67bf8c80350af4a",
      "externalIds": {
        "MAG": "2890867094",
        "ArXiv": "1810.02720",
        "DBLP": "conf/emnlp/YinN18",
        "ACL": "D18-2002",
        "DOI": "10.18653/v1/D18-2002",
        "CorpusId": 52926380
      },
      "url": "https://www.semanticscholar.org/paper/649c1148439a4e875dab414ba67bf8c80350af4a",
      "title": "TRANX: A Transition-based Neural Abstract Syntax Parser for Semantic Parsing and Code Generation",
      "abstract": "We present TRANX, a transition-based neural semantic parser that maps natural language (NL) utterances into formal meaning representations (MRs). TRANX uses a transition system based on the abstract syntax description language for the target MR, which gives it two major advantages: (1) it is highly accurate, using information from the syntax of the target MR to constrain the output space and model the information flow, and (2) it is highly generalizable, and can easily be applied to new types of MR by just writing a new abstract syntax description corresponding to the allowable structures in the MR. Experiments on four different semantic parsing and code generation tasks show that our system is generalizable, extensible, and effective, registering strong results compared to existing neural semantic parsers.",
      "year": 2018,
      "influentialCitationCount": 27,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D18-2002.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "7-12"
      },
      "authors": [
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "6c82727731955a2332a0cc38ec56b35a971061eb",
      "externalIds": {
        "ACL": "W18-1818",
        "DBLP": "journals/corr/abs-1803-00188",
        "MAG": "2963260927",
        "ArXiv": "1803.00188",
        "CorpusId": 3628568
      },
      "url": "https://www.semanticscholar.org/paper/6c82727731955a2332a0cc38ec56b35a971061eb",
      "title": "XNMT: The eXtensible Neural Machine Translation Toolkit",
      "abstract": "This paper describes XNMT, the eXtensible Neural Machine Translation toolkit. XNMT distin- guishes itself from other open-source NMT toolkits by its focus on modular code design, with the purpose of enabling fast iteration in research and replicable, reliable results. In this paper we describe the design of XNMT and its experiment configuration system, and demonstrate its utility on the tasks of machine translation, speech recognition, and multi-tasked machine translation/parsing. XNMT is available open-source at this https URL",
      "year": 2018,
      "influentialCitationCount": 8,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1803.00188",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3011998",
          "name": "Matthias Sperber"
        },
        {
          "authorId": "3277494",
          "name": "Xinyi Wang"
        },
        {
          "authorId": "40895015",
          "name": "Matthieu Felix"
        },
        {
          "authorId": "144633696",
          "name": "Austin Matthews"
        },
        {
          "authorId": "51177454",
          "name": "Sarguna Padmanabhan"
        },
        {
          "authorId": "145270896",
          "name": "Ye Qi"
        },
        {
          "authorId": "39670454",
          "name": "Devendra Singh Sachan"
        },
        {
          "authorId": "144332819",
          "name": "Philip Arthur"
        },
        {
          "authorId": "2058749172",
          "name": "Pierre Godard"
        },
        {
          "authorId": "145430120",
          "name": "John Hewitt"
        },
        {
          "authorId": "40425637",
          "name": "Rachid Riad"
        },
        {
          "authorId": "2109120538",
          "name": "Liming Wang"
        }
      ]
    },
    {
      "paperId": "771c1cb5fb161231e9aaa0a189caba672256a880",
      "externalIds": {
        "MAG": "2804639187",
        "ACL": "N18-1130",
        "DBLP": "conf/naacl/MatthewsND18",
        "DOI": "10.18653/v1/N18-1130",
        "CorpusId": 44119185
      },
      "url": "https://www.semanticscholar.org/paper/771c1cb5fb161231e9aaa0a189caba672256a880",
      "title": "Using Morphological Knowledge in Open-Vocabulary Neural Language Models",
      "abstract": "Languages with productive morphology pose problems for language models that generate words from a fixed vocabulary. Although character-based models allow any possible word type to be generated, they are linguistically na\u00efve: they must discover that words exist and are delimited by spaces\u2014basic linguistic facts that are built in to the structure of word-based models. We introduce an open-vocabulary language model that incorporates more sophisticated linguistic knowledge by predicting words using a mixture of three generative processes: (1) by generating words as a sequence of characters, (2) by directly generating full word forms, and (3) by generating words as a sequence of morphemes that are combined using a hand-written morphological analyzer. Experiments on Finnish, Turkish, and Russian show that our model outperforms character sequence models and other strong baselines on intrinsic and extrinsic measures. Furthermore, we show that our model learns to exploit morphological knowledge encoded in the analyzer, and, as a byproduct, it can perform effective unsupervised morphological disambiguation.",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/N18-1130.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1435-1445"
      },
      "authors": [
        {
          "authorId": "144633696",
          "name": "Austin Matthews"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1745899",
          "name": "Chris Dyer"
        }
      ]
    },
    {
      "paperId": "7b29f45df975ed1e4c3864b6ab4483f11086aa76",
      "externalIds": {
        "MAG": "2798081680",
        "DBLP": "conf/naacl/QiSFPN18",
        "ArXiv": "1804.06323",
        "ACL": "N18-2084",
        "DOI": "10.18653/v1/N18-2084",
        "CorpusId": 4929974
      },
      "url": "https://www.semanticscholar.org/paper/7b29f45df975ed1e4c3864b6ab4483f11086aa76",
      "title": "When and Why Are Pre-Trained Word Embeddings Useful for Neural Machine Translation?",
      "abstract": "The performance of Neural Machine Translation (NMT) systems often suffers in low-resource scenarios where sufficiently large-scale parallel corpora cannot be obtained. Pre-trained word embeddings have proven to be invaluable for improving performance in natural language analysis tasks, which often suffer from paucity of data. However, their utility for NMT has not been extensively explored. In this work, we perform five sets of experiments that analyze when we can expect pre-trained word embeddings to help in NMT tasks. We show that such embeddings can be surprisingly effective in some cases \u2013 providing gains of up to 20 BLEU points in the most favorable setting.",
      "year": 2018,
      "influentialCitationCount": 35,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/N18-2084.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1804.06323",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "145270896",
          "name": "Ye Qi"
        },
        {
          "authorId": "39670454",
          "name": "Devendra Singh Sachan"
        },
        {
          "authorId": "40895015",
          "name": "Matthieu Felix"
        },
        {
          "authorId": "51177454",
          "name": "Sarguna Padmanabhan"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "7cdee781985dce1f6f9a373254244df2b79f2843",
      "externalIds": {
        "MAG": "2889756799",
        "DBLP": "conf/wmt/SachanN18",
        "ArXiv": "1809.00252",
        "ACL": "W18-6327",
        "DOI": "10.18653/v1/W18-6327",
        "CorpusId": 52154258
      },
      "url": "https://www.semanticscholar.org/paper/7cdee781985dce1f6f9a373254244df2b79f2843",
      "title": "Parameter Sharing Methods for Multilingual Self-Attentional Translation Models",
      "abstract": "In multilingual neural machine translation, it has been shown that sharing a single translation model between multiple languages can achieve competitive performance, sometimes even leading to performance gains over bilingually trained models. However, these improvements are not uniform; often multilingual parameter sharing results in a decrease in accuracy due to translation models not being able to accommodate different languages in their limited parameter space. In this work, we examine parameter sharing techniques that strike a happy medium between full sharing and individual training, specifically focusing on the self-attentional Transformer model. We find that the full parameter sharing approach leads to increases in BLEU scores mainly when the target languages are from a similar language family. However, even in the case where target languages are from different families where full parameter sharing leads to a noticeable drop in BLEU scores, our proposed methods for partial sharing of parameters can lead to substantial improvements in translation accuracy.",
      "year": 2018,
      "influentialCitationCount": 8,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W18-6327.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "261-271"
      },
      "authors": [
        {
          "authorId": "39670454",
          "name": "Devendra Singh Sachan"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "9d3e33875ec39001e72313fb919f66242ee97880",
      "externalIds": {
        "MAG": "2964115348",
        "DBLP": "journals/corr/abs-1802-05092",
        "ArXiv": "1802.05092",
        "DOI": "10.1109/ICASSP.2018.8461761",
        "CorpusId": 19576196
      },
      "url": "https://www.semanticscholar.org/paper/9d3e33875ec39001e72313fb919f66242ee97880",
      "title": "Linguistic Unit Discovery from Multi-Modal Inputs in Unwritten Languages: Summary of the \u201cSpeaking Rosetta\u201d JSALT 2017 Workshop",
      "abstract": "We summarize the accomplishments of a multi-disciplinary workshop exploring the computational and scientific issues surrounding the discovery of linguistic units (subwords and words) in a language without orthography. We study the replacement of orthographic transcriptions by images and/or translated text in a well-resourced language to help unsupervised discovery from raw speech.",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1802.05092",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "4979-4983",
        "name": "2018 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
      },
      "authors": [
        {
          "authorId": "1700735",
          "name": "O. Scharenborg"
        },
        {
          "authorId": "143823463",
          "name": "L. Besacier"
        },
        {
          "authorId": "1690706",
          "name": "A. Black"
        },
        {
          "authorId": "1399115926",
          "name": "M. Hasegawa-Johnson"
        },
        {
          "authorId": "1740721",
          "name": "Florian Metze"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "11126660",
          "name": "Sebastian St\u00fcker"
        },
        {
          "authorId": "2058749172",
          "name": "Pierre Godard"
        },
        {
          "authorId": "48588187",
          "name": "Markus M\u00fcller"
        },
        {
          "authorId": "2167829",
          "name": "Lucas Ondel"
        },
        {
          "authorId": "26400211",
          "name": "Shruti Palaskar"
        },
        {
          "authorId": "144332819",
          "name": "Philip Arthur"
        },
        {
          "authorId": "35426982",
          "name": "Francesco Ciannella"
        },
        {
          "authorId": "2054401198",
          "name": "Mingxing Du"
        },
        {
          "authorId": "40651102",
          "name": "Elin Larsen"
        },
        {
          "authorId": "35833790",
          "name": "Danny Merkx"
        },
        {
          "authorId": "40425637",
          "name": "Rachid Riad"
        },
        {
          "authorId": "2109120538",
          "name": "Liming Wang"
        },
        {
          "authorId": "2202008",
          "name": "Emmanuel Dupoux"
        }
      ]
    },
    {
      "paperId": "a03675379685d88c727bc985a323cc71d06f2514",
      "externalIds": {
        "MAG": "2949907253",
        "DBLP": "conf/emnlp/HeNB18",
        "ArXiv": "1808.09111",
        "ACL": "D18-1160",
        "DOI": "10.18653/v1/D18-1160",
        "CorpusId": 52113103
      },
      "url": "https://www.semanticscholar.org/paper/a03675379685d88c727bc985a323cc71d06f2514",
      "title": "Unsupervised Learning of Syntactic Structure with Invertible Neural Projections",
      "abstract": "Unsupervised learning of syntactic structure is typically performed using generative models with discrete latent variables and multinomial parameters. In most cases, these models have not leveraged continuous word representations. In this work, we propose a novel generative model that jointly learns discrete syntactic structure and continuous word representations in an unsupervised fashion by cascading an invertible neural network with a structured generative prior. We show that the invertibility condition allows for efficient exact inference and marginal likelihood computation in our model so long as the prior is well-behaved. In experiments we instantiate our approach with both Markov and tree-structured priors, evaluating on two tasks: part-of-speech (POS) induction, and unsupervised dependency parsing without gold POS annotation. On the Penn Treebank, our Markov-structured model surpasses state-of-the-art results on POS induction. Similarly, we find that our tree-structured model achieves state-of-the-art performance on unsupervised dependency parsing for the difficult training condition where neither gold POS annotation nor punctuation-based constraints are available.",
      "year": 2018,
      "influentialCitationCount": 10,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D18-1160.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1292-1302"
      },
      "authors": [
        {
          "authorId": "6215698",
          "name": "Junxian He"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        }
      ]
    },
    {
      "paperId": "a2ce385fc8d5068e8c87ebe4699c8f9b295cad5e",
      "externalIds": {
        "MAG": "2888831408",
        "ArXiv": "1808.09500",
        "DBLP": "conf/emnlp/ChaudharyZLNMC18",
        "ACL": "D18-1366",
        "DOI": "10.18653/v1/D18-1366",
        "CorpusId": 52124918
      },
      "url": "https://www.semanticscholar.org/paper/a2ce385fc8d5068e8c87ebe4699c8f9b295cad5e",
      "title": "Adapting Word Embeddings to New Languages with Morphological and Phonological Subword Representations",
      "abstract": "Much work in Natural Language Processing (NLP) has been for resource-rich languages, making generalization to new, less-resourced languages challenging. We present two approaches for improving generalization to low-resourced languages by adapting continuous word representations using linguistically motivated subword units: phonemes, morphemes and graphemes. Our method requires neither parallel corpora nor bilingual dictionaries and provides a significant gain in performance over previous methods relying on these resources. We demonstrate the effectiveness of our approaches on Named Entity Recognition for four languages, namely Uyghur, Turkish, Bengali and Hindi, of which Uyghur and Bengali are low resource languages, and also perform experiments on Machine Translation. Exploiting subwords with transfer learning gives us a boost of +15.2 NER F1 for Uyghur and +9.7 F1 for Bengali. We also show improvements in the monolingual setting where we achieve (avg.) +3 F1 and (avg.) +1.35 BLEU.",
      "year": 2018,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D18-1366.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1808.09500",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "51250894",
          "name": "Aditi Chaudhary"
        },
        {
          "authorId": "2384711",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "1686960",
          "name": "Lori S. Levin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3407646",
          "name": "David R. Mortensen"
        },
        {
          "authorId": "143712374",
          "name": "J. Carbonell"
        }
      ]
    },
    {
      "paperId": "a8a863e85a95919773868204d672f1260e0058ce",
      "externalIds": {
        "ArXiv": "1808.08493",
        "ACL": "D18-1039",
        "DBLP": "journals/corr/abs-1808-08493",
        "MAG": "2950135462",
        "DOI": "10.18653/v1/D18-1039",
        "CorpusId": 52100117
      },
      "url": "https://www.semanticscholar.org/paper/a8a863e85a95919773868204d672f1260e0058ce",
      "title": "Contextual Parameter Generation for Universal Neural Machine Translation",
      "abstract": "We propose a simple modification to existing neural machine translation (NMT) models that enables using a single universal model to translate between multiple languages while allowing for language specific parameterization, and that can also be used for domain adaptation. Our approach requires no changes to the model architecture of a standard NMT system, but instead introduces a new component, the contextual parameter generator (CPG), that generates the parameters of the system (e.g., weights in a neural network). This parameter generator accepts source and target language embeddings as input, and generates the parameters for the encoder and the decoder, respectively. The rest of the model remains unchanged and is shared across all languages. We show how this simple modification enables the system to use monolingual data for training and also perform zero-shot translation. We further show it is able to surpass state-of-the-art performance for both the IWSLT-15 and IWSLT-17 datasets and that the learned language embeddings are able to uncover interesting relationships between languages.",
      "year": 2018,
      "influentialCitationCount": 14,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D18-1039.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "pages": "425-435"
      },
      "authors": [
        {
          "authorId": "144888672",
          "name": "Emmanouil Antonios Platanios"
        },
        {
          "authorId": "2790926",
          "name": "Mrinmaya Sachan"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "40975594",
          "name": "Tom Michael Mitchell"
        }
      ]
    },
    {
      "paperId": "b2c3d660aaefb80085fe72c80ce81c5fa71980e9",
      "externalIds": {
        "MAG": "2921073793",
        "DOI": "10.5715/JNLP.25.599",
        "CorpusId": 88494851
      },
      "url": "https://www.semanticscholar.org/paper/b2c3d660aaefb80085fe72c80ce81c5fa71980e9",
      "title": "Syntactic Matching Methods in Pivot Translation",
      "abstract": "The pivot translation is useful method for translating between languages that contain little or no parallel data by utilizing equivalents in an intermediate language such as English. Commonly, phrase-based or tree-based pivot translation methods merge source pivot and pivot target translation models into a source target model. This tactic is known as triangulation. However, the combination is based on the surface forms of constituent words, and it often produces incorrect source target phrase pairs because of interlingual di erences and semantic ambiguities in the pivot language. The translation accuracy is thus degraded. This paper proposes a triangulation approach that utilizes syntactic subtrees in the pivot language to avoid incorrect phrase combinations by distinguishing pivot language words by their syntactic roles. The results of the experiments conducted on the United Nations Parallel Corpus demonstrate that the proposed method is superior to other pivot translation approaches in all tested combinations of languages.",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "name": "Journal of Natural Language Processing"
      },
      "authors": [
        {
          "authorId": "31504732",
          "name": "Akiva Miura"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1790811",
          "name": "Katsuhito Sudoh"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "b81acc013c42796a5eea0fc20cfb04846da3a589",
      "externalIds": {
        "ArXiv": "1812.05272",
        "DBLP": "journals/corr/abs-1812-05272",
        "MAG": "2904840750",
        "DOI": "10.33011/computel.v2i.437",
        "CorpusId": 54999846
      },
      "url": "https://www.semanticscholar.org/paper/b81acc013c42796a5eea0fc20cfb04846da3a589",
      "title": "Towards a General-Purpose Linguistic Annotation Backend",
      "abstract": "Language documentation is inherently a time-intensive process; transcription, glossing, and corpus management consume a significant portion of documentary linguists\u2019 work. Advances in natural language processing can help to accelerate this work, using the linguists\u2019 past decisions as training material, but questions remain about how to prioritize human involvement. In this extended abstract, we describe the beginnings of a new project that will attempt to ease this language documentation process through the use of natural language processing (NLP) technology. It is based on (1) methods to adapt NLP tools to new languages, based on recent advances in massively multilingual neural networks, and (2) backend APIs and interfaces that allow linguists to upload their data (\u00a72). We then describe our current progress on two fronts: automatic phoneme transcription, and glossing (\u00a73). Finally, we briefly describe our future directions (\u00a74).",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://journals.colorado.edu/index.php/computel/article/download/437/419",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1812.05272",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3070462",
          "name": "Patrick Littell"
        },
        {
          "authorId": "150054164",
          "name": "Chian-Yu Chen"
        },
        {
          "authorId": "2119693533",
          "name": "Jean Lee"
        },
        {
          "authorId": "48458251",
          "name": "Zirui Li"
        },
        {
          "authorId": "46395826",
          "name": "Yu-Hsiang Lin"
        },
        {
          "authorId": "2108307596",
          "name": "Yuyan Zhang"
        }
      ]
    },
    {
      "paperId": "b9913ddf94245c864509f0b94847bdbe77899b46",
      "externalIds": {
        "ACL": "L18-1530",
        "DBLP": "conf/lrec/AdamsCNCBM18",
        "MAG": "2787760419",
        "CorpusId": 199433164
      },
      "url": "https://www.semanticscholar.org/paper/b9913ddf94245c864509f0b94847bdbe77899b46",
      "title": "Evaluation Phonemic Transcription of Low-Resource Tonal Languages for Language Documentation",
      "abstract": "Transcribing speech is an important part of language documentation, yet speech recognition technology has not been widely harnessed to aid linguists. We explore the use of a neural network architecture with the connectionist temporal classification loss function for phonemic and tonal transcription in a language documentation setting. In this framework, we explore jointly modelling phonemes and tones versus modelling them separately, and assess the importance of pitch information versus phonemic context for tonal prediction. Experiments on two tonal languages, Yongning Na and Eastern Chatino, show the changes in recognition performance as training data is scaled from 10 minutes up to 50 minutes for Chatino, and up to 224 minutes for Na. We discuss the findings from incorporating this technology into the linguistic workflow for documenting Yongning Na, which show the method's promise in improving efficiency, minimizing typographical errors, and maintaining the transcription's faithfulness to the acoustic signal, while highlighting phonetic and phonemic facts for linguistic consideration.",
      "year": 2018,
      "influentialCitationCount": 8,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "pages": "3356-3365",
        "name": ""
      },
      "authors": [
        {
          "authorId": "38535429",
          "name": "Oliver Adams"
        },
        {
          "authorId": "143620680",
          "name": "Trevor Cohn"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "38878141",
          "name": "Hilaria Cruz"
        },
        {
          "authorId": "2066194290",
          "name": "Steven Bird"
        },
        {
          "authorId": "39720748",
          "name": "Alexis Michaud"
        }
      ]
    },
    {
      "paperId": "c43d9d868f5288738cd625d365f0b3a5c18d4a20",
      "externalIds": {
        "MAG": "2767114424",
        "DOI": "10.1007/978-981-10-6199-8_11",
        "CorpusId": 64830852
      },
      "url": "https://www.semanticscholar.org/paper/c43d9d868f5288738cd625d365f0b3a5c18d4a20",
      "title": "The NAIST Simultaneous Translation Corpus",
      "abstract": null,
      "year": 2018,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "pages": "205-215",
        "name": ""
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2114102929",
          "name": "Hiroaki Shimizu"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        }
      ]
    },
    {
      "paperId": "c52ac453e154953abdb06fc041023e327ea609a4",
      "externalIds": {
        "DBLP": "journals/corr/abs-1803-09519",
        "ArXiv": "1803.09519",
        "MAG": "2964089206",
        "DOI": "10.21437/Interspeech.2018-1910",
        "CorpusId": 4427800
      },
      "url": "https://www.semanticscholar.org/paper/c52ac453e154953abdb06fc041023e327ea609a4",
      "title": "Self-Attentional Acoustic Models",
      "abstract": "Self-attention is a method of encoding sequences of vectors by relating these vectors to each-other based on pairwise similarities. These models have recently shown promising results for modeling discrete sequences, but they are non-trivial to apply to acoustic modeling due to computational and modeling issues. In this paper, we apply self-attention to acoustic modeling, proposing several improvements to mitigate these issues: First, self-attention memory grows quadratically in the sequence length, which we address through a downsampling technique. Second, we find that previous approaches to incorporate position information into the model are unsuitable and explore other representations and hybrid models to this end. Third, to stress the importance of local context in the acoustic signal, we propose a Gaussian biasing approach that allows explicit control over the context range. Experiments find that our model approaches a strong baseline based on LSTMs with network-in-network connections while being much faster to compute. Besides speed, we find that interpretability is a strength of self-attentional acoustic models, and demonstrate that self-attention heads learn a linguistically plausible division of labor.",
      "year": 2018,
      "influentialCitationCount": 13,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1803.09519",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3723-3727"
      },
      "authors": [
        {
          "authorId": "3011998",
          "name": "Matthias Sperber"
        },
        {
          "authorId": "2920247",
          "name": "J. Niehues"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "11126660",
          "name": "Sebastian St\u00fcker"
        },
        {
          "authorId": "1724972",
          "name": "A. Waibel"
        }
      ]
    },
    {
      "paperId": "c612905cffc5a9aa9f0d8ac7ce1fd17f90413dab",
      "externalIds": {
        "MAG": "2962980425",
        "DBLP": "conf/naacl/JoPJSRN18",
        "ACL": "N18-1010",
        "ArXiv": "1804.00065",
        "DOI": "10.18653/v1/N18-1010",
        "CorpusId": 4551282
      },
      "url": "https://www.semanticscholar.org/paper/c612905cffc5a9aa9f0d8ac7ce1fd17f90413dab",
      "title": "Attentive Interaction Model: Modeling Changes in View in Argumentation",
      "abstract": "We present a neural architecture for modeling argumentative dialogue that explicitly models the interplay between an Opinion Holder\u2019s (OH\u2019s) reasoning and a challenger\u2019s argument, with the goal of predicting if the argument successfully changes the OH\u2019s view. The model has two components: (1) vulnerable region detection, an attention model that identifies parts of the OH\u2019s reasoning that are amenable to change, and (2) interaction encoding, which identifies the relationship between the content of the OH\u2019s reasoning and that of the challenger\u2019s argument. Based on evaluation on discussions from the Change My View forum on Reddit, the two components work together to predict an OH\u2019s change in view, outperforming several baselines. A posthoc analysis suggests that sentences picked out by the attention model are addressed more frequently by successful arguments than by unsuccessful ones.",
      "year": 2018,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/N18-1010.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "103-116"
      },
      "authors": [
        {
          "authorId": "39947629",
          "name": "Yohan Jo"
        },
        {
          "authorId": "40346865",
          "name": "Shivani Poddar"
        },
        {
          "authorId": "24273380",
          "name": "Byungsoo Jeon"
        },
        {
          "authorId": "3443246",
          "name": "Qinlan Shen"
        },
        {
          "authorId": "35959897",
          "name": "C. Ros\u00e9"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "c859416a8e5682bee3c35df29bc02e02a22de072",
      "externalIds": {
        "MAG": "2883972335",
        "CorpusId": 53065219
      },
      "url": "https://www.semanticscholar.org/paper/c859416a8e5682bee3c35df29bc02e02a22de072",
      "title": "Integrating automatic transcription into the language documentation workflow: Experiments with Na data and the Persephone toolkit",
      "abstract": "Automatic speech recognition tools have potential for facilitating language documentation, but in practice these tools remain little-used by linguists for a variety of reasons, such as that the technology is still new (and evolving rapidly), user-friendly interfaces are still under development, and case studies demonstrating the practical usefulness of automatic recognition in a low-resource setting remain few. This article reports on a success story in integrating automatic transcription into the language documentation workflow, specifically for Yongning Na, a language of Southwest China. Using PERSEPHONE, an open-source toolkit, a single-speaker speech transcription tool was trained over five hours of manually transcribed speech. The experiments found that this method can achieve a remarkably low error rate (on the order of 17%), and that automatic transcriptions were useful as a canvas for the linguist. The present report is intended for linguists with little or no knowledge of speech processing. It aims to provide insights into (i) the way the tool operates and (ii) the process of collaborating with natural language processing specialists. Practical recommendations are offered on how to anticipate the requirements of this type of technology from the early stages of data collection in the field.",
      "year": 2018,
      "influentialCitationCount": 3,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "12",
        "pages": "393-429",
        "name": "Language Documentation & Conservation"
      },
      "authors": [
        {
          "authorId": "39720748",
          "name": "Alexis Michaud"
        },
        {
          "authorId": "38535429",
          "name": "Oliver Adams"
        },
        {
          "authorId": "143620680",
          "name": "Trevor Cohn"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "46673007",
          "name": "Severine Guillaume"
        }
      ]
    },
    {
      "paperId": "cd5a9a0061de6a6841c63e60281133207b2d6763",
      "externalIds": {
        "MAG": "2899273198",
        "ArXiv": "1811.00266",
        "DBLP": "journals/corr/abs-1811-00266",
        "CorpusId": 53198776
      },
      "url": "https://www.semanticscholar.org/paper/cd5a9a0061de6a6841c63e60281133207b2d6763",
      "title": "Learning to Describe Phrases with Local and Global Contexts",
      "abstract": "When reading a text, it is common to become stuck on unfamiliar words and phrases, such as polysemous words with novel senses, rarely used idioms, internet slang, or emerging entities. If we humans cannot figure out the meaning of those expressions from the immediate local context, we consult dictionaries for definitions or search documents or the web to find other global context to help in interpretation. Can machines help us do this work? Which type of context is more important for machines to solve the problem? To answer these questions, we undertake a task of describing a given phrase in natural language based on its local and global contexts. To solve this task, we propose a neural description model that consists of two context encoders and a description decoder. In contrast to the existing methods for non-standard English explanation [Ni+ 2017] and definition generation [Noraset+ 2017; Gadetsky+ 2018], our model appropriately takes important clues from both local and global contexts. Experimental results on three existing datasets (including WordNet, Oxford and Urban Dictionaries) and a dataset newly created from Wikipedia demonstrate the effectiveness of our method over previous work.",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1811.00266",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "46724659",
          "name": "Shonosuke Ishiwatari"
        },
        {
          "authorId": "50376014",
          "name": "Hiroaki Hayashi"
        },
        {
          "authorId": "34849332",
          "name": "Naoki Yoshinaga"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2361778",
          "name": "Masashi Toyoda"
        },
        {
          "authorId": "1716799",
          "name": "M. Kitsuregawa"
        }
      ]
    },
    {
      "paperId": "ce89ee7aaeeea2c9d474707690f3ea9d948776a3",
      "externalIds": {
        "DBLP": "conf/emnlp/MichelN18",
        "MAG": "2889746679",
        "ArXiv": "1809.00388",
        "ACL": "D18-1050",
        "DOI": "10.18653/v1/D18-1050",
        "CorpusId": 52155427
      },
      "url": "https://www.semanticscholar.org/paper/ce89ee7aaeeea2c9d474707690f3ea9d948776a3",
      "title": "MTNT: A Testbed for Machine Translation of Noisy Text",
      "abstract": "Noisy or non-standard input text can cause disastrous mistranslations in most modern Machine Translation (MT) systems, and there has been growing research interest in creating noise-robust MT systems. However, as of yet there are no publicly available parallel corpora of with naturally occurring noisy inputs and translations, and thus previous work has resorted to evaluating on synthetically created datasets. In this paper, we propose a benchmark dataset for Machine Translation of Noisy Text (MTNT), consisting of noisy comments on Reddit (www.reddit.com) and professionally sourced translations. We commissioned translations of English comments into French and Japanese, as well as French and Japanese comments into English, on the order of 7k-37k sentences per language pair. We qualitatively and quantitatively examine the types of noise included in this dataset, then demonstrate that existing MT models fail badly on a number of noise-related phenomena, even after performing adaptation on a small training set of in-domain data. This indicates that this dataset can provide an attractive testbed for methods tailored to handling noisy text in MT.",
      "year": 2018,
      "influentialCitationCount": 21,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D18-1050.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "543-553"
      },
      "authors": [
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "d3e13d2514edaf74b863bfbe45a739c32a7689e1",
      "externalIds": {
        "MAG": "2889467844",
        "ACL": "D18-1111",
        "ArXiv": "1808.10025",
        "DBLP": "conf/emnlp/HayatiOAYTN18",
        "DOI": "10.18653/v1/D18-1111",
        "CorpusId": 52136345
      },
      "url": "https://www.semanticscholar.org/paper/d3e13d2514edaf74b863bfbe45a739c32a7689e1",
      "title": "Retrieval-Based Neural Code Generation",
      "abstract": "In models to generate program source code from natural language, representing this code in a tree structure has been a common approach. However, existing methods often fail to generate complex code correctly due to a lack of ability to memorize large and complex structures. We introduce RECODE, a method based on subtree retrieval that makes it possible to explicitly reference existing code examples within a neural code generation model. First, we retrieve sentences that are similar to input sentences using a dynamic-programming-based sentence similarity scoring method. Next, we extract n-grams of action sequences that build the associated abstract syntax tree. Finally, we increase the probability of actions that cause the retrieved n-gram action subtree to be in the predicted code. We show that our approach improves the performance on two code generation tasks by up to +2.6 BLEU.",
      "year": 2018,
      "influentialCitationCount": 7,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D18-1111.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1808.10025",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "31998283",
          "name": "Shirley Anugrah Hayati"
        },
        {
          "authorId": "2065951003",
          "name": "R. Olivier"
        },
        {
          "authorId": "51241363",
          "name": "Pravalika Avvaru"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "1693125",
          "name": "A. Tomasic"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "de5834305ea419c25b17f0c8d27bad6a5feb311a",
      "externalIds": {
        "MAG": "2798273548",
        "DBLP": "conf/acl/HovyNBGJ18",
        "ACL": "P18-1154",
        "DOI": "10.18653/v1/P18-1154",
        "CorpusId": 51883043
      },
      "url": "https://www.semanticscholar.org/paper/de5834305ea419c25b17f0c8d27bad6a5feb311a",
      "title": "Learning to Generate Move-by-Move Commentary for Chess Games from Large-Scale Social Forum Data",
      "abstract": "This paper examines the problem of generating natural language descriptions of chess games. We introduce a new large-scale chess commentary dataset and propose methods to generate commentary for individual moves in a chess game. The introduced dataset consists of more than 298K chess move-commentary pairs across 11K chess games. We highlight how this task poses unique research challenges in natural language generation: the data contain a large variety of styles of commentary and frequently depend on pragmatic context. We benchmark various baselines and propose an end-to-end trainable neural model which takes into account multiple pragmatic aspects of the game state that may be commented upon to describe a given chess move. Through a human study on predictions for a subset of the data which deals with direct move descriptions, we observe that outputs from our models are rated similar to ground truth commentary texts in terms of correctness and fluency.",
      "year": 2018,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P18-1154.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1661-1671"
      },
      "authors": [
        {
          "authorId": "2006291",
          "name": "Harsh Jhamtani"
        },
        {
          "authorId": "3375999",
          "name": "Varun Gangal"
        },
        {
          "authorId": "144547315",
          "name": "E. Hovy"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        }
      ]
    },
    {
      "paperId": "e862e5f9a17938f1817017b2730e10463d94fb54",
      "externalIds": {
        "MAG": "2939480828",
        "CorpusId": 146033686
      },
      "url": "https://www.semanticscholar.org/paper/e862e5f9a17938f1817017b2730e10463d94fb54",
      "title": "BLISS in Non-Isometric Embedding Spaces",
      "abstract": null,
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "27419446",
          "name": "Barun Patra"
        },
        {
          "authorId": "22272110",
          "name": "Joel Ruben Antony Moniz"
        },
        {
          "authorId": "31264049",
          "name": "Sarthak Garg"
        },
        {
          "authorId": "1762110",
          "name": "Matthew R. Gormley"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "e92de0c4ef62a84201fac284eb66c37330b5fe1c",
      "externalIds": {
        "DBLP": "journals/corr/abs-1812-07170",
        "MAG": "2905288168",
        "ArXiv": "1812.07170",
        "CorpusId": 56173576
      },
      "url": "https://www.semanticscholar.org/paper/e92de0c4ef62a84201fac284eb66c37330b5fe1c",
      "title": "Learning to Generate Corrective Patches using Neural Machine Translation",
      "abstract": "Bug fixing is generally a manually-intensive task. However, recent work has proposed the idea of automated program repair, which aims to repair (at least a subset of) bugs in different ways such as code mutation, etc. Following in the same line of work as automated bug repair, in this paper we aim to leverage past fixes to propose fixes of current/future bugs. Specifically, we propose Ratchet, a corrective patch generation system using neural machine translation. By learning corresponding pre-correction and post-correction code in past fixes with a neural sequence-to-sequence model, Ratchet is able to generate a fix code for a given bug-prone code query. We perform an empirical study with five open source projects, namely Ambari, Camel, Hadoop, Jetty and Wicket, to evaluate the effectiveness of Ratchet. Our findings show that Ratchet can generate syntactically valid statements 98.7% of the time, and achieve an F1-measure between 0.41-0.83 with respect to the actual fixes adopted in the code base. In addition, we perform a qualitative validation using 20 participants to see whether the generated statements can be helpful in correcting bugs. Our survey showed that Ratchet's output was considered to be helpful in fixing the bugs on many occasions, even if fix was not 100% correct.",
      "year": 2018,
      "influentialCitationCount": 7,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1812.07170",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "145050815",
          "name": "Hideaki Hata"
        },
        {
          "authorId": "3318024",
          "name": "Emad Shihab"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "f053137323a88eb932d590bcdfc959ee805e2520",
      "externalIds": {
        "ArXiv": "1805.01087",
        "DBLP": "journals/corr/abs-1805-01087",
        "MAG": "2949651109",
        "ACL": "P18-1130",
        "DOI": "10.18653/v1/P18-1130",
        "CorpusId": 19263554
      },
      "url": "https://www.semanticscholar.org/paper/f053137323a88eb932d590bcdfc959ee805e2520",
      "title": "Stack-Pointer Networks for Dependency Parsing",
      "abstract": "We introduce a novel architecture for dependency parsing: stack-pointer networks (StackPtr). Combining pointer networks (Vinyals et al., 2015) with an internal stack, the proposed model first reads and encodes the whole sentence, then builds the dependency tree top-down (from root-to-leaf) in a depth-first fashion. The stack tracks the status of the depth-first search and the pointer networks select one child for the word at the top of the stack at each step. The StackPtr parser benefits from the information of whole sentence and all previously derived subtree structures, and removes the left-to-right restriction in classical transition-based parsers. Yet the number of steps for building any (non-projective) parse tree is linear in the length of the sentence just as other transition-based parsers, yielding an efficient decoding algorithm with O(n^2) time complexity. We evaluate our model on 29 treebanks spanning 20 languages and different dependency annotation schemas, and achieve state-of-the-art performances on 21 of them",
      "year": 2018,
      "influentialCitationCount": 40,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P18-1130.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1805.01087",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2378954",
          "name": "Xuezhe Ma"
        },
        {
          "authorId": "46210480",
          "name": "Zecong Hu"
        },
        {
          "authorId": "48211835",
          "name": "J. Liu"
        },
        {
          "authorId": "3157053",
          "name": "Nanyun Peng"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144547315",
          "name": "E. Hovy"
        }
      ]
    },
    {
      "paperId": "f21a9d70319ca99227300349d7bcab5dee5869cd",
      "externalIds": {
        "MAG": "2953085733",
        "DBLP": "conf/aclnmt/NishimuraSNN18",
        "ACL": "W18-2711",
        "ArXiv": "1806.02525",
        "DOI": "10.1109/TASLP.2019.2959224",
        "CorpusId": 46983454
      },
      "url": "https://www.semanticscholar.org/paper/f21a9d70319ca99227300349d7bcab5dee5869cd",
      "title": "Multi-Source Neural Machine Translation with Missing Data",
      "abstract": "Multi-source translation is an approach to exploit multiple inputs (e.g. in two different languages) to increase translation accuracy. In this paper, we examine approaches for multi-source neural machine translation (NMT) using an incomplete multilingual corpus in which some translations are missing. In practice, many multilingual corpora are not complete due to the difficulty to provide translations in all of the relevant languages (for example, in TED talks, most English talks only have subtitles for a small portion of the languages that TED supports). Existing studies on multi-source translation did not explicitly handle such situations. This study focuses on the use of incomplete multilingual corpora in multi-encoder NMT and mixture of NMT experts and examines a very simple implementation where missing source translations are replaced by a special symbol . These methods allow us to use incomplete corpora both at training time and test time. In experiments with real incomplete multilingual corpora of TED Talks, the multi-source NMT with the  tokens achieved higher translation accuracies measured by BLEU than those by any one-to-one NMT systems.",
      "year": 2018,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1806.02525",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "28",
        "pages": "569-580",
        "name": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
      },
      "authors": [
        {
          "authorId": "1490936400",
          "name": "Yuta Nishimura"
        },
        {
          "authorId": "1790811",
          "name": "Katsuhito Sudoh"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "f481d6dea08e348cecd5eb23a813d47373e62a94",
      "externalIds": {
        "MAG": "2805248761",
        "DBLP": "conf/naacl/NeubigA18",
        "ACL": "N18-6001",
        "DOI": "10.18653/v1/N18-6001",
        "CorpusId": 46919506
      },
      "url": "https://www.semanticscholar.org/paper/f481d6dea08e348cecd5eb23a813d47373e62a94",
      "title": "Modelling Natural Language, Programs, and their Intersection",
      "abstract": "As computers and information grow a more integral part of our world, it is becoming more and more important for humans to be able to interact with their computers in complex ways. One way to do so is by programming, but the ability to understand and generate programming languages is a highly specialized skill. As a result, in the past several years there has been an increasing research interest in methods that focus on the intersection of programming and natural language, allowing users to use natural language to interact with computers in the complex ways that programs allow us to do. In this tutorial, we will focus on machine learning models of programs and natural language focused on making this goal a reality. First, we will discuss the similarities and differences between programming and natural language. Then we will discuss methods that have been designed to cover a variety of tasks in this field, including automatic explanation of programs in natural language (code-to-language), automatic generation of programs from natural language specifications (language-to-code), modeling the natural language elements of source code, and analysis of communication in collaborative programming communities. The tutorial will be aimed at NLP researchers and practitioners, aiming to describe the interesting opportunities that models at the intersection of natural and programming languages provide, and also how their techniques could provide benefit to the practice of software engineering as a whole.",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-3"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3216345",
          "name": "Miltiadis Allamanis"
        }
      ]
    },
    {
      "paperId": "f6224ed8b4969aee883c8c0a57444b79f681d499",
      "externalIds": {
        "MAG": "2951623192",
        "DBLP": "journals/corr/abs-1810-13337",
        "ArXiv": "1810.13337",
        "CorpusId": 53109277
      },
      "url": "https://www.semanticscholar.org/paper/f6224ed8b4969aee883c8c0a57444b79f681d499",
      "title": "Learning to Represent Edits",
      "abstract": "We introduce the problem of learning distributed representations of edits. By combining a \"neural editor\" with an \"edit encoder\", our models learn to represent the salient information of an edit and can be used to apply edits to new inputs. We experiment on natural language and source code edit data. Our evaluation yields promising results that suggest that our neural network models learn to capture the structure and semantics of edits. We hope that this interesting task and data source will inspire other researchers to work further on this problem.",
      "year": 2018,
      "influentialCitationCount": 16,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "volume": "abs/1810.13337",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3216345",
          "name": "Miltiadis Allamanis"
        },
        {
          "authorId": "2107692",
          "name": "Marc Brockschmidt"
        },
        {
          "authorId": "35058304",
          "name": "Alexander L. Gaunt"
        }
      ]
    },
    {
      "paperId": "f66a17836380c0c79c1b42a9219cf8fde6524287",
      "externalIds": {
        "CorpusId": 53956615
      },
      "url": "https://www.semanticscholar.org/paper/f66a17836380c0c79c1b42a9219cf8fde6524287",
      "title": "Answering Cloze-style Software Questions Using Stack Overflow",
      "abstract": "Modern Question Answering (QA) systems rely on both knowledge bases (KBs) and unstructured text corpora as sources for their answers. KBs, when available, generally offer more precise answers than unstructured text. However, in specialized domains such as software engineering, QA requires deep domain expertise and KBs are often lacking. In this paper we tackle such specialized QA by using both text and semi-structured knowledge, in the form of a corpus of entity-labeled documents. We propose CASE, a hybrid of an RNN language model and an entity co-occurrence model, where the entity co-occurrence model is learned from the entity-labeled corpus. On QUASAR-S, a dataset derived from Stack Overflow consisting of Cloze (fill-in-the-blank) software questions and a corpus of tagged posts, CASE shows large accuracy gains over strong baselines.",
      "year": 2018,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "39723861",
          "name": "Ezra Winston"
        },
        {
          "authorId": "34994191",
          "name": "Bhuwan Dhingra"
        },
        {
          "authorId": "2406799",
          "name": "Kathryn Mazaitis"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "50056360",
          "name": "William W. Cohen"
        }
      ]
    },
    {
      "paperId": "f7979c6690562c5f8bf700e3fd184c4d1df0a54c",
      "externalIds": {
        "MAG": "2899641520",
        "ArXiv": "1811.04154",
        "DBLP": "journals/corr/abs-1811-04154",
        "DOI": "10.1609/AAAI.V33I01.33016924",
        "CorpusId": 53285682
      },
      "url": "https://www.semanticscholar.org/paper/f7979c6690562c5f8bf700e3fd184c4d1df0a54c",
      "title": "Zero-shot Neural Transfer for Cross-lingual Entity Linking",
      "abstract": "Cross-lingual entity linking maps an entity mention in a source language to its corresponding entry in a structured knowledge base that is in a different (target) language. While previous work relies heavily on bilingual lexical resources to bridge the gap between the source and the target languages, these resources are scarce or unavailable for many low-resource languages. To address this problem, we investigate zero-shot cross-lingual entity linking, in which we assume no bilingual lexical resources are available in the source low-resource language. Specifically, we propose pivot-basedentity linking, which leverages information from a highresource \u201cpivot\u201d language to train character-level neural entity linking models that are transferred to the source lowresource language in a zero-shot manner. With experiments on 9 low-resource languages and transfer through a total of54 languages, we show that our proposed pivot-based framework improves entity linking accuracy 17% (absolute) on average over the baseline systems, for the zero-shot scenario.1 Further, we also investigate the use of language-universal phonological representations which improves average accuracy (absolute) by 36% when transferring between languages that use different scripts.",
      "year": 2018,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/4670/4548",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "6924-6931"
      },
      "authors": [
        {
          "authorId": "7391530",
          "name": "Shruti Rijhwani"
        },
        {
          "authorId": "20467900",
          "name": "Jiateng Xie"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143712374",
          "name": "J. Carbonell"
        }
      ]
    },
    {
      "paperId": "0c39c0dc296a902e4a5eb85182209f7b9e6053b0",
      "externalIds": {
        "ArXiv": "1712.04048",
        "MAG": "2772752810",
        "DBLP": "journals/corr/abs-1712-04048",
        "CorpusId": 28418968
      },
      "url": "https://www.semanticscholar.org/paper/0c39c0dc296a902e4a5eb85182209f7b9e6053b0",
      "title": "Cavs: A Vertex-centric Programming Interface for Dynamic Neural Networks",
      "abstract": "Recent deep learning (DL) models have moved beyond static network architectures to dynamic ones, handling data where the network structure changes every example, such as sequences of variable lengths, trees, and graphs. Existing dataflow-based programming models for DL---both static and dynamic declaration---either cannot readily express these dynamic models, or are inefficient due to repeated dataflow graph construction and processing, and difficulties in batched execution. We present Cavs, a vertex-centric programming interface and optimized system implementation for dynamic DL models. Cavs represents dynamic network structure as a static vertex function $\\mathcal{F}$ and a dynamic instance-specific graph $\\mathcal{G}$, and performs backpropagation by scheduling the execution of $\\mathcal{F}$ following the dependencies in $\\mathcal{G}$. Cavs bypasses expensive graph construction and preprocessing overhead, allows for the use of static graph optimization techniques on pre-defined operations in $\\mathcal{F}$, and naturally exposes batched execution opportunities over different graphs. Experiments comparing Cavs to two state-of-the-art frameworks for dynamic NNs (TensorFlow Fold and DyNet) demonstrate the efficacy of this approach: Cavs achieves a near one order of magnitude speedup on training of various dynamic NN architectures, and ablations demonstrate the contribution of our proposed batching and memory management strategies.",
      "year": 2017,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Mathematics",
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1712.04048",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1682058",
          "name": "H. Zhang"
        },
        {
          "authorId": "1704538",
          "name": "Shizhen Xu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143716171",
          "name": "Wei Dai"
        },
        {
          "authorId": "1707357",
          "name": "Qirong Ho"
        },
        {
          "authorId": "145789924",
          "name": "Guangwen Yang"
        },
        {
          "authorId": "143977260",
          "name": "E. Xing"
        }
      ]
    },
    {
      "paperId": "19a3af37df22c7c646cc99efad3af96cda6e80f0",
      "externalIds": {
        "ACL": "W17-4753",
        "MAG": "2757877530",
        "DBLP": "conf/wmt/ZhangUSNN17",
        "DOI": "10.18653/v1/W17-4753",
        "CorpusId": 26327431
      },
      "url": "https://www.semanticscholar.org/paper/19a3af37df22c7c646cc99efad3af96cda6e80f0",
      "title": "NICT-NAIST System for WMT17 Multimodal Translation Task",
      "abstract": "This paper describes the NICT-NAIST system for the WMT 2017 shared multimodal machine translation task for both language pairs, English-to-German and English-to-French. We built a hierarchical phrase-based (Hiero) translation system and trained an attentional encoder-decoder neural machine translation (NMT) model to rerank the n-best output of the Hiero system, which obtained significant gains over both the Hiero system and NMT decoding alone. We also present a multimodal NMT model that integrates the target language descriptions of images that are similar to the image described by the source sentence as additional inputs of the neural networks to help the translation of the source sentence. We give detailed analysis for the results of the multimodal NMT model. Our system obtained the first place for the English-to-French task according to human evaluation.",
      "year": 2017,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W17-4753.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "477-482"
      },
      "authors": [
        {
          "authorId": "4279134",
          "name": "Jingyi Zhang"
        },
        {
          "authorId": "1802277",
          "name": "M. Utiyama"
        },
        {
          "authorId": "1698363",
          "name": "E. Sumita"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "2b63812db40152b12925ce4a848b929fa591b858",
      "externalIds": {
        "DBLP": "journals/corr/Neubig17",
        "MAG": "2592864539",
        "ArXiv": "1703.01619",
        "CorpusId": 17964596
      },
      "url": "https://www.semanticscholar.org/paper/2b63812db40152b12925ce4a848b929fa591b858",
      "title": "Neural Machine Translation and Sequence-to-sequence Models: A Tutorial",
      "abstract": "This tutorial introduces a new and powerful set of techniques variously called \"neural machine translation\" or \"neural sequence-to-sequence models\". These techniques have been used in a number of tasks regarding the handling of human language, and can be a powerful tool in the toolbox of anyone who wants to model sequential data of some sort. The tutorial assumes that the reader knows the basics of math and programming, but does not assume any particular experience with neural networks or natural language processing. It attempts to explain the intuition behind the various methods covered, then delves into them with enough mathematical detail to understand them concretely, and culiminates with a suggestion for an implementation exercise, where readers can test that they understood the content in practice.",
      "year": 2017,
      "influentialCitationCount": 10,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "volume": "abs/1703.01619",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "2c94bc68388517aa4a2d2dfc7d35df95ce24b1a8",
      "externalIds": {
        "MAG": "2753599454",
        "CorpusId": 125729672
      },
      "url": "https://www.semanticscholar.org/paper/2c94bc68388517aa4a2d2dfc7d35df95ce24b1a8",
      "title": "Adversarial Invariant Feature Learning",
      "abstract": "Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data, leading to better generalization. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game. On three benchmark tasks, namely fair classifications that are bias-free, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved test performance.",
      "year": 2017,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Mathematics"
      ],
      "journal": {
        "volume": "",
        "pages": "585-596",
        "name": ""
      },
      "authors": [
        {
          "authorId": "1912046",
          "name": "Qizhe Xie"
        },
        {
          "authorId": "3422912",
          "name": "Zihang Dai"
        },
        {
          "authorId": "15394369",
          "name": "Yulun Du"
        },
        {
          "authorId": "144547315",
          "name": "E. Hovy"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "30109a213aa10765486c676ecfa511db227ab543",
      "externalIds": {
        "MAG": "2951649469",
        "DBLP": "journals/corr/MorishitaONYSN17",
        "ACL": "W17-3208",
        "ArXiv": "1706.05765",
        "DOI": "10.18653/v1/W17-3208",
        "CorpusId": 21460834
      },
      "url": "https://www.semanticscholar.org/paper/30109a213aa10765486c676ecfa511db227ab543",
      "title": "An Empirical Study of Mini-Batch Creation Strategies for Neural Machine Translation",
      "abstract": "Training of neural machine translation (NMT) models usually uses mini-batches for efficiency purposes. During the mini-batched training process, it is necessary to pad shorter sentences in a mini-batch to be equal in length to the longest sentence therein for efficient computation. Previous work has noted that sorting the corpus based on the sentence length before making mini-batches reduces the amount of padding and increases the processing speed. However, despite the fact that mini-batch creation is an essential step in NMT training, widely used NMT toolkits implement disparate strategies for doing so, which have not been empirically validated or compared. This work investigates mini-batch creation strategies with experiments over two different datasets. Our results suggest that the choice of a mini-batch creation strategy has a large effect on NMT training and some length-based sorting strategies do not always work well compared with simple shuffling.",
      "year": 2017,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W17-3208.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "61-68"
      },
      "authors": [
        {
          "authorId": "2731589",
          "name": "Makoto Morishita"
        },
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "1790811",
          "name": "Katsuhito Sudoh"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "31412f9b23511e212895305927d9ccddb445bcbc",
      "externalIds": {
        "MAG": "2786220077",
        "DBLP": "conf/apsipa/KuboKTNS017",
        "DOI": "10.1109/APSIPA.2017.8282283",
        "CorpusId": 22051359
      },
      "url": "https://www.semanticscholar.org/paper/31412f9b23511e212895305927d9ccddb445bcbc",
      "title": "An investigation of how to design control parameters for statistical voice timbre control",
      "abstract": "Multiple-regression Gaussian mixture models (MR-GMM) allow for control of voice timbre along several axes each described by a voice timbre expression word. To create these axes, perceptual scores corresponding to multiple voice timbre expression words are manually assigned to individual pre-stored target speakers as the voice timbre control parameters, and then acoustic basis vectors corresponding to the individual control parameters are learned. The voice timbre expression words are usually selected from various words using factor analysis so that the voice timbre control parameters are independent of each other. However, the resulting basis vectors are not often orthogonal to each other, and they practically cause difficulties in intuitively controlling the converted voice timbre. Towards the development of the MR-GMM capable of intuitively controlling converted voice timbre, we investigate how to design the voice timbre control parameters so that not only the voice timbre control parameters but also the corresponding acoustic basis vectors are independent of each other. Experimental results demonstrate that 1) a method for annotation of the voice timbre control parameters using the converted voices rather than natural voices is effective, and 2) the independences of the voice timbre control parameters and acoustic basis vectors is helpful for improving the converted voice timbre controllability of the MR-GMM.",
      "year": 2017,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1520-1523",
        "name": "2017 Asia-Pacific Signal and Information Processing Association Annual Summit and Conference (APSIPA ASC)"
      },
      "authors": [
        {
          "authorId": "35467932",
          "name": "Kazutaka Kubo"
        },
        {
          "authorId": "65851830",
          "name": "Kazuhiro Kobayashi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "480d545ac4a4ffff5b1bc291c2de613192e35d91",
      "externalIds": {
        "ArXiv": "1701.03980",
        "DBLP": "journals/corr/NeubigDGMAABCCC17",
        "MAG": "2577255746",
        "CorpusId": 2170930
      },
      "url": "https://www.semanticscholar.org/paper/480d545ac4a4ffff5b1bc291c2de613192e35d91",
      "title": "DyNet: The Dynamic Neural Network Toolkit",
      "abstract": "We describe DyNet, a toolkit for implementing neural network models based on dynamic declaration of network structure. In the static declaration strategy that is used in toolkits like Theano, CNTK, and TensorFlow, the user first defines a computation graph (a symbolic representation of the computation), and then examples are fed into an engine that executes this computation and computes its derivatives. In DyNet's dynamic declaration strategy, computation graph construction is mostly transparent, being implicitly constructed by executing procedural code that computes the network outputs, and the user is free to use different network structures for each input. Dynamic declaration thus facilitates the implementation of more complicated network architectures, and DyNet is specifically designed to allow users to implement their models in a way that is idiomatic in their preferred programming language (C++ or Python). One challenge with dynamic declaration is that because the symbolic computation graph is defined anew for every training example, its construction must have low overhead. To achieve this, DyNet has an optimized C++ backend and lightweight graph representation. Experiments show that DyNet's speeds are faster than or comparable with static declaration toolkits, and significantly faster than Chainer, another dynamic declaration toolkit. DyNet is released open-source under the Apache 2.0 license and available at this http URL.",
      "year": 2017,
      "influentialCitationCount": 37,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "volume": "abs/1701.03980",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1745899",
          "name": "Chris Dyer"
        },
        {
          "authorId": "2089067",
          "name": "Yoav Goldberg"
        },
        {
          "authorId": "144633696",
          "name": "Austin Matthews"
        },
        {
          "authorId": "145585097",
          "name": "Waleed Ammar"
        },
        {
          "authorId": "49513989",
          "name": "Antonios Anastasopoulos"
        },
        {
          "authorId": "143668305",
          "name": "Miguel Ballesteros"
        },
        {
          "authorId": "145287425",
          "name": "David Chiang"
        },
        {
          "authorId": "7197436",
          "name": "Daniel Clothiaux"
        },
        {
          "authorId": "143620680",
          "name": "Trevor Cohn"
        },
        {
          "authorId": "1800354",
          "name": "Kevin Duh"
        },
        {
          "authorId": "1779225",
          "name": "Manaal Faruqui"
        },
        {
          "authorId": "2056157475",
          "name": "Cynthia Gan"
        },
        {
          "authorId": "2758616",
          "name": "Dan Garrette"
        },
        {
          "authorId": "40608686",
          "name": "Yangfeng Ji"
        },
        {
          "authorId": "47648549",
          "name": "Lingpeng Kong"
        },
        {
          "authorId": "3376845",
          "name": "A. Kuncoro"
        },
        {
          "authorId": "48387892",
          "name": "Manish Kumar"
        },
        {
          "authorId": "8805254",
          "name": "Chaitanya Malaviya"
        },
        {
          "authorId": "144397625",
          "name": "Paul Michel"
        },
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "144422314",
          "name": "Matthew Richardson"
        },
        {
          "authorId": "2362960",
          "name": "Naomi Saphra"
        },
        {
          "authorId": "2705113",
          "name": "Swabha Swayamdipta"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        }
      ]
    },
    {
      "paperId": "4e2c41466c8246af0a563ea36fbe80c896bbab2c",
      "externalIds": {
        "DBLP": "conf/naacl/LiuLN18",
        "MAG": "2952354668",
        "ACL": "N18-1121",
        "ArXiv": "1708.06510",
        "DOI": "10.18653/v1/N18-1121",
        "CorpusId": 4410027
      },
      "url": "https://www.semanticscholar.org/paper/4e2c41466c8246af0a563ea36fbe80c896bbab2c",
      "title": "Handling Homographs in Neural Machine Translation",
      "abstract": "Homographs, words with different meanings but the same surface form, have long caused difficulty for machine translation systems, as it is difficult to select the correct translation based on the context. However, with the advent of neural machine translation (NMT) systems, which can theoretically take into account global sentential context, one may hypothesize that this problem has been alleviated. In this paper, we first provide empirical evidence that existing NMT systems in fact still have significant problems in properly translating ambiguous words. We then proceed to describe methods, inspired by the word sense disambiguation literature, that model the context of the input word with context-aware word embeddings that help to differentiate the word sense before feeding it into the encoder. Experiments on three language pairs demonstrate that such models improve the performance of NMT systems both in terms of BLEU score and in the accuracy of translating homographs.",
      "year": 2017,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/N18-1121.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1336-1345"
      },
      "authors": [
        {
          "authorId": "2155134",
          "name": "Frederick Liu"
        },
        {
          "authorId": "2115605894",
          "name": "Han Lu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "516a0faeab9ec3a68bc6e7ec13a2df235a27ab52",
      "externalIds": {
        "MAG": "2775369792",
        "ArXiv": "1712.02768",
        "DBLP": "journals/corr/abs-1712-02768",
        "CorpusId": 21773184
      },
      "url": "https://www.semanticscholar.org/paper/516a0faeab9ec3a68bc6e7ec13a2df235a27ab52",
      "title": "Convolutional Neural Networks for Medical Diagnosis from Admission Notes",
      "abstract": "$\\textbf{Objective}$ Develop an automatic diagnostic system which only uses textual admission information from Electronic Health Records (EHRs) and assist clinicians with a timely and statistically proved decision tool. The hope is that the tool can be used to reduce mis-diagnosis. \n$\\textbf{Materials and Methods}$ We use the real-world clinical notes from MIMIC-III, a freely available dataset consisting of clinical data of more than forty thousand patients who stayed in intensive care units of the Beth Israel Deaconess Medical Center between 2001 and 2012. We proposed a Convolutional Neural Network model to learn semantic features from unstructured textual input and automatically predict primary discharge diagnosis. \n$\\textbf{Results}$ The proposed model achieved an overall 96.11% accuracy and 80.48% weighted F1 score values on 10 most frequent disease classes, significantly outperforming four strong baseline models by at least 12.7% in weighted F1 score. \n$\\textbf{Discussion}$ Experimental results imply that the CNN model is suitable for supporting diagnosis decision making in the presence of complex, noisy and unstructured clinical data while at the same time using fewer layers and parameters that other traditional Deep Network models. \n$\\textbf{Conclusion}$ Our model demonstrated capability of representing complex medical meaningful features from unstructured clinical notes and prediction power for commonly misdiagnosed frequent diseases. It can use easily adopted in clinical setting to provide timely and statistically proved decision support. \n$\\textbf{Keywords}$ Convolutional neural network, text classification, discharge diagnosis prediction, admission information from EHRs.",
      "year": 2017,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1712.02768",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2118204633",
          "name": "Yuan Li"
        },
        {
          "authorId": "9758493",
          "name": "Dimitris Konomis"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "40526720",
          "name": "P. Xie"
        },
        {
          "authorId": "144397536",
          "name": "C. Cheng"
        },
        {
          "authorId": "143977260",
          "name": "E. Xing"
        }
      ]
    },
    {
      "paperId": "6bb2b856d9a9b873259ba9dc48bc450c96eb3318",
      "externalIds": {
        "MAG": "2742223267",
        "DBLP": "journals/corr/abs-1709-05227",
        "ArXiv": "1709.05227",
        "DOI": "10.1016/j.specom.2017.07.006",
        "CorpusId": 13003733
      },
      "url": "https://www.semanticscholar.org/paper/6bb2b856d9a9b873259ba9dc48bc450c96eb3318",
      "title": "Transcribing against time",
      "abstract": null,
      "year": 2017,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://arxiv.org/pdf/1709.05227",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1709.05227",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "3011998",
          "name": "Matthias Sperber"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2920247",
          "name": "J. Niehues"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        },
        {
          "authorId": "1724972",
          "name": "A. Waibel"
        }
      ]
    },
    {
      "paperId": "6cddfbed35c46937588bd9d6b846ca2855953cea",
      "externalIds": {
        "ArXiv": "1704.00559",
        "DBLP": "conf/emnlp/SperberNNW17",
        "MAG": "2605202026",
        "ACL": "D17-1145",
        "DOI": "10.18653/v1/D17-1145",
        "CorpusId": 8070939
      },
      "url": "https://www.semanticscholar.org/paper/6cddfbed35c46937588bd9d6b846ca2855953cea",
      "title": "Neural Lattice-to-Sequence Models for Uncertain Inputs",
      "abstract": "The input to a neural sequence-to-sequence model is often determined by an up-stream system, e.g. a word segmenter, part of speech tagger, or speech recognizer. These up-stream models are potentially error-prone. Representing inputs through word lattices allows making this uncertainty explicit by capturing alternative sequences and their posterior probabilities in a compact form. In this work, we extend the TreeLSTM (Tai et al., 2015) into a LatticeLSTM that is able to consume word lattices, and can be used as encoder in an attentional encoder-decoder model. We integrate lattice posterior scores into this architecture by extending the TreeLSTM\u2019s child-sum and forget gates and introducing a bias term into the attention mechanism. We experiment with speech translation lattices and report consistent improvements over baselines that translate either the 1-best hypothesis or the lattice without posterior scores.",
      "year": 2017,
      "influentialCitationCount": 6,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D17-1145.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1380-1389"
      },
      "authors": [
        {
          "authorId": "3011998",
          "name": "Matthias Sperber"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2920247",
          "name": "J. Niehues"
        },
        {
          "authorId": "1724972",
          "name": "A. Waibel"
        }
      ]
    },
    {
      "paperId": "709f0a4229e40339b595072ae9fbd3a1ae1fd93e",
      "externalIds": {
        "ArXiv": "1705.07860",
        "DBLP": "journals/corr/NeubigGD17",
        "MAG": "2619269479",
        "CorpusId": 22177804
      },
      "url": "https://www.semanticscholar.org/paper/709f0a4229e40339b595072ae9fbd3a1ae1fd93e",
      "title": "On-the-fly Operation Batching in Dynamic Computation Graphs",
      "abstract": "Dynamic neural network toolkits such as PyTorch, DyNet, and Chainer offer more flexibility for implementing models that cope with data of varying dimensions and structure, relative to toolkits that operate on statically declared computations (e.g., TensorFlow, CNTK, and Theano). However, existing toolkits - both static and dynamic - require that the developer organize the computations into the batches necessary for exploiting high-performance algorithms and hardware. This batching task is generally difficult, but it becomes a major hurdle as architectures become complex. In this paper, we present an algorithm, and its implementation in the DyNet toolkit, for automatically batching operations. Developers simply write minibatch computations as aggregations of single instance computations, and the batching algorithm seamlessly executes them, on the fly, using computationally efficient batched operations. On a variety of tasks, we obtain throughput similar to that obtained with manual batches, as well as comparable speedups over single-instance learning on architectures that are impractical to batch manually.",
      "year": 2017,
      "influentialCitationCount": 10,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "pages": "3971-3981"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2089067",
          "name": "Yoav Goldberg"
        },
        {
          "authorId": "1745899",
          "name": "Chris Dyer"
        }
      ]
    },
    {
      "paperId": "76fe5f80dd25078eefa522e59a7763bc5d5da826",
      "externalIds": {
        "MAG": "2753261376",
        "DBLP": "conf/kes/NagataTN17",
        "DOI": "10.1016/j.procs.2017.08.065",
        "CorpusId": 27557603
      },
      "url": "https://www.semanticscholar.org/paper/76fe5f80dd25078eefa522e59a7763bc5d5da826",
      "title": "Adaptive Spelling Error Correction Models for Learner English",
      "abstract": null,
      "year": 2017,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "474-483"
      },
      "authors": [
        {
          "authorId": "134661453",
          "name": "Ryo Nagata"
        },
        {
          "authorId": "36514372",
          "name": "Hiroya Takamura"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "7f817600b612aab6039dfba576ae8e8e7460d8f1",
      "externalIds": {
        "MAG": "2747414243",
        "DBLP": "conf/interspeech/ShinozakiWMN17",
        "DOI": "10.21437/Interspeech.2017-1081",
        "CorpusId": 5685480
      },
      "url": "https://www.semanticscholar.org/paper/7f817600b612aab6039dfba576ae8e8e7460d8f1",
      "title": "Semi-Supervised Learning of a Pronunciation Dictionary from Disjoint Phonemic Transcripts and Text",
      "abstract": "While the performance of automatic speech recognition systems has recently approached human levels in some tasks, the application is still limited to specific domains. This is because system development relies on extensive supervised training and expert tuning in the target domain. To solve this problem, systems must become more self-sufficient, having the ability to learn directly from speech and adapt to new tasks. One open question in this area is how to learn a pronunciation dictionary containing the appropriate vocabulary. Humans can recognize words, even ones they have never heard before, by reading text and understanding the context in which a word is used. However, this ability is missing in current speech recognition systems. In this work, we propose a new framework that automatically expands an initial pronunciation dictionary using independently sampled acoustic and textual data. While the task is very challenging and in its initial stage, we demonstrate that a model based on Bayesian learning of Dirichlet processes can acquire word pronunciations from phone transcripts and text of the WSJ data set.",
      "year": 2017,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2546-2550"
      },
      "authors": [
        {
          "authorId": "1732454",
          "name": "T. Shinozaki"
        },
        {
          "authorId": "1746678",
          "name": "Shinji Watanabe"
        },
        {
          "authorId": "2619938",
          "name": "D. Mochihashi"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "86b91922923b03c66497accfa88c638299fc8d26",
      "externalIds": {
        "ACL": "K17-2005",
        "MAG": "2741609577",
        "DBLP": "conf/conll/ZhouN17",
        "DOI": "10.18653/v1/K17-2005",
        "CorpusId": 41262789
      },
      "url": "https://www.semanticscholar.org/paper/86b91922923b03c66497accfa88c638299fc8d26",
      "title": "Morphological Inflection Generation with Multi-space Variational Encoder-Decoders",
      "abstract": "This paper describes the CMU submission to shared task 1 of SIGMORPHON 2017. The system is based on the multi-space variational encoder-decoder (MSVED) method of Zhou and Neubig (2017), which employs both continuous and discrete latent variables for the variational encoder-decoder and is trained in a semi-supervised fashion. We discuss some language-speci\ufb01c errors and present result analysis.",
      "year": 2017,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "58-65"
      },
      "authors": [
        {
          "authorId": "2384711",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "941e42ee75fc2bf07078bcfbd14bdf9ca7fe99ff",
      "externalIds": {
        "ACL": "E17-1088",
        "DBLP": "conf/eacl/CohnBNAM17",
        "MAG": "2741986357",
        "DOI": "10.18653/V1/E17-1088",
        "CorpusId": 17419868
      },
      "url": "https://www.semanticscholar.org/paper/941e42ee75fc2bf07078bcfbd14bdf9ca7fe99ff",
      "title": "Cross-Lingual Word Embeddings for Low-Resource Language Modeling",
      "abstract": "Most languages have no established writing system and minimal written records. However, textual data is essential for natural language processing, and particularly important for training language models to support speech recognition. Even in cases where text data is missing, there are some languages for which bilingual lexicons are available, since creating lexicons is a fundamental task of documentary linguistics. We investigate the use of such lexicons to improve language models when textual training data is limited to as few as a thousand sentences. The method involves learning cross-lingual word embeddings as a preliminary step in training monolingual language models. Results across a number of languages show that language models are improved by this pre-training. Application to Yongning Na, a threatened language, highlights challenges in deploying the approach in real low-resource environments.",
      "year": 2017,
      "influentialCitationCount": 5,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/E17-1088.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "937-947"
      },
      "authors": [
        {
          "authorId": "143620680",
          "name": "Trevor Cohn"
        },
        {
          "authorId": "21308992",
          "name": "Steven Bird"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "38535429",
          "name": "Oliver Adams"
        },
        {
          "authorId": "3120277",
          "name": "Adam J. Makarucha"
        }
      ]
    },
    {
      "paperId": "97943a5dee3c6e36d01a6099acb9ec360ad0ee19",
      "externalIds": {
        "DBLP": "journals/corr/GangalJNHN17",
        "MAG": "2731386152",
        "ACL": "D17-1315",
        "ArXiv": "1707.01176",
        "DOI": "10.18653/v1/D17-1315",
        "CorpusId": 20803199
      },
      "url": "https://www.semanticscholar.org/paper/97943a5dee3c6e36d01a6099acb9ec360ad0ee19",
      "title": "Charmanteau: Character Embedding Models For Portmanteau Creation",
      "abstract": "Portmanteaus are a word formation phenomenon where two words combine into a new word. We propose character-level neural sequence-to-sequence (S2S) methods for the task of portmanteau generation that are end-to-end-trainable, language independent, and do not explicitly use additional phonetic information. We propose a noisy-channel-style model, which allows for the incorporation of unsupervised word lists, improving performance over a standard source-to-target model. This model is made possible by an exhaustive candidate generation strategy specifically enabled by the features of the portmanteau task. Experiments find our approach superior to a state-of-the-art FST-based baseline with respect to ground truth accuracy and human evaluation.",
      "year": 2017,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D17-1315.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1707.01176",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "3375999",
          "name": "Varun Gangal"
        },
        {
          "authorId": "2006291",
          "name": "Harsh Jhamtani"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144547315",
          "name": "E. Hovy"
        },
        {
          "authorId": "144287919",
          "name": "Eric Nyberg"
        }
      ]
    },
    {
      "paperId": "9895531c6dc3854f082de1a1ec651a9e179bbd07",
      "externalIds": {
        "ACL": "U17-1006",
        "DBLP": "conf/acl-alta/AdamsCNM17",
        "MAG": "2773798130",
        "CorpusId": 29077435
      },
      "url": "https://www.semanticscholar.org/paper/9895531c6dc3854f082de1a1ec651a9e179bbd07",
      "title": "Phonemic Transcription of Low-Resource Tonal Languages",
      "abstract": "Transcription of speech is an important part of language documentation, and yet speech recognition technology has not been widely harnessed to aid linguists. We explore the use of a neural network architecture with the connectionist temporal classification loss function for phonemic and tonal transcription in a language documentation setting. In this framework, we explore jointly modelling phonemes and tones versus modelling them separately, and assess the importance of pitch information versus phonemic context for tonal prediction. Experiments on two tonal languages, Yongning Na and Eastern Chatino, show the changes in recognition performance as training data is scaled from 10 minutes to 150 minutes. We discuss the findings from incorporating this technology into the linguistic workflow for documenting Yongning Na, which show the method's promise in improving efficiency, minimizing typographical errors, and maintaining the transcription's faithfulness to the acoustic signal, while highlighting phonetic and phonemic facts for linguistic consideration.",
      "year": 2017,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "53-60"
      },
      "authors": [
        {
          "authorId": "38535429",
          "name": "Oliver Adams"
        },
        {
          "authorId": "143620680",
          "name": "Trevor Cohn"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "39720748",
          "name": "Alexis Michaud"
        }
      ]
    },
    {
      "paperId": "9a334566b79bc6c6906e2b5285d5ea50b9b99479",
      "externalIds": {
        "MAG": "2950550269",
        "DBLP": "journals/corr/XieDDHN17",
        "ArXiv": "1705.11122",
        "CorpusId": 34807644
      },
      "url": "https://www.semanticscholar.org/paper/9a334566b79bc6c6906e2b5285d5ea50b9b99479",
      "title": "Controllable Invariance through Adversarial Feature Learning",
      "abstract": "Learning meaningful representations that maintain the content necessary for a particular task while filtering away detrimental variations is a problem of great interest in machine learning. In this paper, we tackle the problem of learning representations invariant to a specific factor or trait of data. The representation learning process is formulated as an adversarial minimax game. We analyze the optimal equilibrium of such a game and find that it amounts to maximizing the uncertainty of inferring the detrimental factor given the representation while maximizing the certainty of making task-specific predictions. On three benchmark tasks, namely fair and bias-free classification, language-independent generation, and lighting-independent image classification, we show that the proposed framework induces an invariant representation, and leads to better generalization evidenced by the improved performance.",
      "year": 2017,
      "influentialCitationCount": 39,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "pages": "585-596"
      },
      "authors": [
        {
          "authorId": "1912046",
          "name": "Qizhe Xie"
        },
        {
          "authorId": "3422912",
          "name": "Zihang Dai"
        },
        {
          "authorId": "15394369",
          "name": "Yulun Du"
        },
        {
          "authorId": "144547315",
          "name": "E. Hovy"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "a1999b7fe0bcdd1594e86f9906d68d636201636b",
      "externalIds": {
        "ArXiv": "1704.01691",
        "ACL": "P17-1029",
        "DBLP": "conf/acl/ZhouN17",
        "MAG": "2606657156",
        "DOI": "10.18653/v1/P17-1029",
        "CorpusId": 13875246
      },
      "url": "https://www.semanticscholar.org/paper/a1999b7fe0bcdd1594e86f9906d68d636201636b",
      "title": "Multi-space Variational Encoder-Decoders for Semi-supervised Labeled Sequence Transduction",
      "abstract": "Labeled sequence transduction is a task of transforming one sequence into another sequence that satisfies desiderata specified by a set of labels. In this paper we propose multi-space variational encoder-decoders, a new model for labeled sequence transduction with semi-supervised learning. The generative model can use neural networks to handle both discrete and continuous latent variables to exploit various features of data. Experiments show that our model provides not only a powerful supervised framework but also can effectively take advantage of the unlabeled data. On the SIGMORPHON morphological inflection benchmark, our model outperforms single-model state-of-art results by a large margin for the majority of languages.",
      "year": 2017,
      "influentialCitationCount": 3,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P17-1029.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1704.01691",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2384711",
          "name": "Chunting Zhou"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "b131cf78363993e4126b2562a156bd9d046c8bc4",
      "externalIds": {
        "MAG": "2759274242",
        "DBLP": "conf/wmt/MiuraNSN17",
        "ACL": "W17-4709",
        "DOI": "10.18653/v1/W17-4709",
        "CorpusId": 2467720
      },
      "url": "https://www.semanticscholar.org/paper/b131cf78363993e4126b2562a156bd9d046c8bc4",
      "title": "Tree as a Pivot: Syntactic Matching Methods in Pivot Translation",
      "abstract": "Pivot translation is a useful method for translating between languages with little or no parallel data by utilizing parallel data in an intermediate language such as English. A popular approach for pivot translation used in phrase-based or tree-based translation models combines source-pivot and pivot-target translation models into a source-target model, as known as triangulation. However, this combination is based on the constituent words\u2019 surface forms and often produces incorrect source-target phrase pairs due to semantic ambiguity in the pivot language, and interlingual differences. This degrades translation accuracy. In this paper, we propose a approach for the triangulation using syntactic subtrees in the pivot language to distinguish pivot language words by their syntactic roles to avoid incorrect phrase combinations. Experimental results on the United Nations Parallel Corpus show the proposed method gains in all tested combinations of language, up to 2.3 BLEU points.1",
      "year": 2017,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W17-4709.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "90-98"
      },
      "authors": [
        {
          "authorId": "31504732",
          "name": "Akiva Miura"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1790811",
          "name": "Katsuhito Sudoh"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "b8b5b95a0471e0553a0e6cd5086f384cf0f4d4d8",
      "externalIds": {
        "DBLP": "conf/interspeech/DoTSNTN15",
        "MAG": "2242221029",
        "DOI": "10.1109/TASLP.2016.2643280",
        "CorpusId": 15868658
      },
      "url": "https://www.semanticscholar.org/paper/b8b5b95a0471e0553a0e6cd5086f384cf0f4d4d8",
      "title": "Preserving Word-Level Emphasis in Speech-to-Speech Translation",
      "abstract": "Speech-to-speech translation (S2ST) is a technology that translates speech across languages, which can remove barriers in cross-lingual communication. In the conventional S2ST systems, the linguistic meaning of speech was translated, but paralinguistic information conveying other features of the speech such as emotion or emphasis were ignored. In this paper, we propose a method to translate paralinguistic information, specifically focusing on emphasis. The method consists of a series of components that can accurately translate emphasis using all acoustic features of speech. First, linear-regression hidden semi-Markov models (LR-HSMMs) are used to estimate a real-numbered emphasis value for every word in an utterance, resulting in a sequence of values for the utterance. After that the emphasis translation module translates the estimated emphasis sequence into a target language emphasis sequence using a conditional random field model considering the features of emphasis levels, words, and part-of-speech tags. Finally, the speech synthesis module synthesizes emphasized speech with LR-HSMMs, taking into account the translated emphasis sequence and transcription. The results indicate that our translation model can translate emphasis information, correctly emphasizing words in the target language with 91.6% $F$ -measure by objective evaluation. A listening test with human subjects further showed that they could identify the emphasized words with 87.8% $F$ -measure, and that the naturalness of the audio was preserved.",
      "year": 2017,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://naist.repo.nii.ac.jp/record/4025/files/TASLP2643280.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "25",
        "pages": "544-556",
        "name": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
      },
      "authors": [
        {
          "authorId": "2527751",
          "name": "Quoc Truong Do"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "c3490ec9b8f695bed2187fb4a4164b1509389ca8",
      "externalIds": {
        "MAG": "2801021082",
        "DOI": "10.18653/v1/w17-32",
        "CorpusId": 66979511
      },
      "url": "https://www.semanticscholar.org/paper/c3490ec9b8f695bed2187fb4a4164b1509389ca8",
      "title": "Proceedings of the First Workshop on Neural Machine Translation",
      "abstract": null,
      "year": 2017,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "1821711",
          "name": "Thang Luong"
        },
        {
          "authorId": "2539211",
          "name": "Alexandra Birch"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2987548",
          "name": "A. Finch"
        }
      ]
    },
    {
      "paperId": "c382c8392d7692a70fd67dc5693dc2cf8888ac9d",
      "externalIds": {
        "DBLP": "conf/sigdial/RavichanderMGNF17",
        "MAG": "2785952547",
        "ACL": "W17-5545",
        "DOI": "10.18653/v1/W17-5545",
        "CorpusId": 6177202
      },
      "url": "https://www.semanticscholar.org/paper/c382c8392d7692a70fd67dc5693dc2cf8888ac9d",
      "title": "How Would You Say It? Eliciting Lexically Diverse Dialogue for Supervised Semantic Parsing",
      "abstract": "Building dialogue interfaces for real-world scenarios often entails training semantic parsers starting from zero examples. How can we build datasets that better capture the variety of ways users might phrase their queries, and what queries are actually realistic? Wang et al. (2015) proposed a method to build semantic parsing datasets by generating canonical utterances using a grammar and having crowdworkers paraphrase them into natural wording. A limitation of this approach is that it induces bias towards using similar language as the canonical utterances. In this work, we present a methodology that elicits meaningful and lexically diverse queries from users for semantic parsing tasks. Starting from a seed lexicon and a generative grammar, we pair logical forms with mixed text-image representations and ask crowdworkers to paraphrase and confirm the plausibility of the queries that they generated. We use this method to build a semantic parsing dataset from scratch for a dialog agent in a smart-home simulation. We find evidence that this dataset, which we have named SmartHome, is demonstrably more lexically diverse and difficult to parse than existing domain-specific semantic parsing datasets.",
      "year": 2017,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "374-383"
      },
      "authors": [
        {
          "authorId": "3023068",
          "name": "Abhilasha Ravichander"
        },
        {
          "authorId": "2632776",
          "name": "Thomas Manzini"
        },
        {
          "authorId": "2869551",
          "name": "Matthias Grabmair"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "26253744",
          "name": "Jonathan M Francis"
        },
        {
          "authorId": "144287919",
          "name": "Eric Nyberg"
        }
      ]
    },
    {
      "paperId": "d14afc470cd90521147130e153c0d3e1324cd104",
      "externalIds": {
        "DBLP": "journals/corr/MalaviyaNL17",
        "MAG": "2952225984",
        "ArXiv": "1707.09569",
        "ACL": "D17-1268",
        "DOI": "10.18653/v1/D17-1268",
        "CorpusId": 23943173
      },
      "url": "https://www.semanticscholar.org/paper/d14afc470cd90521147130e153c0d3e1324cd104",
      "title": "Learning Language Representations for Typology Prediction",
      "abstract": "One central mystery of neural NLP is what neural models \u201cknow\u201d about their subject matter. When a neural machine translation system learns to translate from one language to another, does it learn the syntax or semantics of the languages? Can this knowledge be extracted from the system to fill holes in human scientific knowledge? Existing typological databases contain relatively full feature specifications for only a few hundred languages. Exploiting the existence of parallel texts in more than a thousand languages, we build a massive many-to-one NMT system from 1017 languages into English, and use this to predict information missing from typological databases. Experiments show that the proposed method is able to infer not only syntactic, but also phonological and phonetic inventory features, and improves over a baseline that has access to information about the languages geographic and phylogenetic neighbors.",
      "year": 2017,
      "influentialCitationCount": 16,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D17-1268.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1707.09569",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "8805254",
          "name": "Chaitanya Malaviya"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3070462",
          "name": "Patrick Littell"
        }
      ]
    },
    {
      "paperId": "d18d8d364bb18d66924919feebb2e892ebe6761c",
      "externalIds": {
        "ArXiv": "1711.00309",
        "MAG": "2963259630",
        "DBLP": "conf/ijcnlp/ZhangUSNN17",
        "ACL": "I17-1016",
        "CorpusId": 23339307
      },
      "url": "https://www.semanticscholar.org/paper/d18d8d364bb18d66924919feebb2e892ebe6761c",
      "title": "Improving Neural Machine Translation through Phrase-based Forced Decoding",
      "abstract": "Compared to traditional statistical machine translation (SMT), neural machine translation (NMT) often sacrifices adequacy for the sake of fluency. We propose a method to combine the advantages of traditional SMT and NMT by exploiting an existing phrase-based SMT model to compute the phrase-based decoding cost for an NMT output and then using the phrase-based decoding cost to rerank the n-best NMT outputs. The main challenge in implementing this approach is that NMT outputs may not be in the search space of the standard phrase-based decoding algorithm, because the search space of phrase-based SMT is limited by the phrase-based translation rule table. We propose a soft forced decoding algorithm, which can always successfully find a decoding path for any NMT output. We show that using the forced decoding cost to rerank the NMT outputs can successfully improve translation quality on four different language pairs.",
      "year": 2017,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "152-162"
      },
      "authors": [
        {
          "authorId": "4279134",
          "name": "Jingyi Zhang"
        },
        {
          "authorId": "1802277",
          "name": "M. Utiyama"
        },
        {
          "authorId": "1698363",
          "name": "E. Sumita"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "db253b17043b6a86e02173b6aa597664b0c7f256",
      "externalIds": {
        "ArXiv": "1704.04859",
        "ACL": "P17-1188",
        "MAG": "2964325863",
        "DBLP": "conf/acl/LiuLLN17",
        "DOI": "10.18653/v1/P17-1188",
        "CorpusId": 18263067
      },
      "url": "https://www.semanticscholar.org/paper/db253b17043b6a86e02173b6aa597664b0c7f256",
      "title": "Learning Character-level Compositionality with Visual Features",
      "abstract": "Previous work has modeled the compositionality of words by creating character-level models of meaning, reducing problems of sparsity for rare words. However, in many writing systems compositionality has an effect even on the character-level: the meaning of a character is derived by the sum of its parts. In this paper, we model this effect by creating embeddings for characters based on their visual characteristics, creating an image for the character and running it through a convolutional neural network to produce a visual character embedding. Experiments on a text classification task demonstrate that such model allows for better processing of instances with rare characters in languages such as Chinese, Japanese, and Korean. Additionally, qualitative analyses demonstrate that our proposed model learns to focus on the parts of characters that carry topical content which resulting in embeddings that are coherent in visual space.",
      "year": 2017,
      "influentialCitationCount": 9,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P17-1188.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2059-2068"
      },
      "authors": [
        {
          "authorId": "2155134",
          "name": "Frederick Liu"
        },
        {
          "authorId": "2115605894",
          "name": "Han Lu"
        },
        {
          "authorId": "144645869",
          "name": "Chieh Lo"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "de3c3eb590065a6d78ec8566161f8236ab2a7435",
      "externalIds": {
        "MAG": "2977587102",
        "DBLP": "conf/aclwat/NakazawaHDMGKON17",
        "ACL": "W17-5701",
        "CorpusId": 219301866
      },
      "url": "https://www.semanticscholar.org/paper/de3c3eb590065a6d78ec8566161f8236ab2a7435",
      "title": "Overview of the 4th Workshop on Asian Translation",
      "abstract": "This paper presents the results of the shared tasks from the 4th workshop on Asian translation (WAT2017) including J\u2194E, J\u2194C scientific paper translation subtasks, C\u2194J, K\u2194J, E\u2194J patent translation subtasks, H\u2194E mixed domain subtasks, J\u2194E newswire subtasks and J\u2194E recipe subtasks. For the WAT2017, 12 institutions participated in the shared tasks. About 300 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated.",
      "year": 2017,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-54"
      },
      "authors": [
        {
          "authorId": "40948140",
          "name": "Toshiaki Nakazawa"
        },
        {
          "authorId": "2011045",
          "name": "S. Higashiyama"
        },
        {
          "authorId": "2126835",
          "name": "Chenchen Ding"
        },
        {
          "authorId": "40315879",
          "name": "Hideya Mino"
        },
        {
          "authorId": "2205832",
          "name": "Isao Goto"
        },
        {
          "authorId": "1754386",
          "name": "H. Kazawa"
        },
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1795664",
          "name": "S. Kurohashi"
        }
      ]
    },
    {
      "paperId": "f0a498014c4ef67c0b72ceb18d95e0d25087fd57",
      "externalIds": {
        "ArXiv": "1704.06918",
        "MAG": "2609102890",
        "ACL": "P17-1079",
        "DBLP": "conf/acl/OdaANYN17",
        "DOI": "10.18653/v1/P17-1079",
        "CorpusId": 13266502
      },
      "url": "https://www.semanticscholar.org/paper/f0a498014c4ef67c0b72ceb18d95e0d25087fd57",
      "title": "Neural Machine Translation via Binary Code Prediction",
      "abstract": "In this paper, we propose a new method for calculating the output layer in neural machine translation systems. The method is based on predicting a binary code for each word and can reduce computation time/memory requirements of the output layer to be logarithmic in vocabulary size in the best case. In addition, we also introduce two advanced approaches to improve the robustness of the proposed model: using error-correcting codes and combining softmax and binary codes. Experiments on two English-Japanese bidirectional translation tasks show proposed models achieve BLEU scores that approach the softmax, while reducing memory usage to the order of less than 1/10 and improving decoding speed on CPUs by x5 to x10.",
      "year": 2017,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P17-1079.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1704.06918",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "144332819",
          "name": "Philip Arthur"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "f4c8539bed600c9c652aba76a996b8188761d3fe",
      "externalIds": {
        "MAG": "2952009362",
        "DBLP": "conf/aclnmt/DenkowskiN17",
        "ArXiv": "1706.09733",
        "ACL": "W17-3203",
        "DOI": "10.18653/v1/W17-3203",
        "CorpusId": 8140780
      },
      "url": "https://www.semanticscholar.org/paper/f4c8539bed600c9c652aba76a996b8188761d3fe",
      "title": "Stronger Baselines for Trustable Results in Neural Machine Translation",
      "abstract": "Interest in neural machine translation has grown rapidly as its effectiveness has been demonstrated across language and data scenarios. New research regularly introduces architectural and algorithmic improvements that lead to significant gains over \u201cvanilla\u201d NMT implementations. However, these new techniques are rarely evaluated in the context of previously published techniques, specifically those that are widely used in state-of-the-art production and shared-task systems. As a result, it is often difficult to determine whether improvements from research will carry over to systems deployed for real-world use. In this work, we recommend three specific methods that are relatively easy to implement and result in much stronger experimental systems. Beyond reporting significantly higher BLEU scores, we conduct an in-depth analysis of where improvements originate and what inherent weaknesses of basic NMT models are being addressed. We then compare the relative gains afforded by several other techniques proposed in the literature when starting with vanilla systems versus our stronger baselines, showing that experimental conclusions may change depending on the baseline chosen. This indicates that choosing a strong baseline is crucial for reporting reliable experimental results.",
      "year": 2017,
      "influentialCitationCount": 12,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W17-3203.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "18-27"
      },
      "authors": [
        {
          "authorId": "2157958",
          "name": "Michael J. Denkowski"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "fee62123e1d2ac56065675983475b079e1e9106f",
      "externalIds": {
        "MAG": "2963899977",
        "DBLP": "journals/corr/abs-1708-00111",
        "ArXiv": "1708.00111",
        "DOI": "10.1609/aaai.v32i1.11806",
        "CorpusId": 19200845
      },
      "url": "https://www.semanticscholar.org/paper/fee62123e1d2ac56065675983475b079e1e9106f",
      "title": "A Continuous Relaxation of Beam Search for End-to-end Training of Neural Sequence Models",
      "abstract": "\n \n Beam search is a desirable choice of test-time decoding algorithm for neural sequence models because it potentially avoids search errors made by simpler greedy methods. However, typical cross entropy training procedures for these models do not directly consider the behaviour of the final decoding method. As a result, for cross-entropy trained models, beam decoding can sometimes yield reduced test performance when compared with greedy decoding. In order to train models that can more effectively make use of beam search, we propose a new training procedure that focuses on the final loss metric (e.g. Hamming loss) evaluated on the output of beam search. While well-defined, this \"direct loss\" objective is itself discontinuous and thus difficult to optimize. Hence, in our approach, we form a sub-differentiable surrogate objective by introducing a novel continuous approximation of the beam search decoding procedure.In experiments, we show that optimizing this new training objective yields substantially better results on two sequence tasks (Named Entity Recognition and CCG Supertagging) when compared with both cross entropy trained greedy decoding and cross entropy trained beam decoding baselines.\n \n",
      "year": 2017,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://ojs.aaai.org/index.php/AAAI/article/download/11806/11665",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "volume": "abs/1708.00111",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "37195917",
          "name": "Kartik Goyal"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1745899",
          "name": "Chris Dyer"
        },
        {
          "authorId": "1400419309",
          "name": "Taylor Berg-Kirkpatrick"
        }
      ]
    },
    {
      "paperId": "ff7e60b8d336aef5ed974609a63610641085177e",
      "externalIds": {
        "ArXiv": "1705.07136",
        "DBLP": "journals/corr/MaYLNH17",
        "MAG": "2618606525",
        "CorpusId": 28470390
      },
      "url": "https://www.semanticscholar.org/paper/ff7e60b8d336aef5ed974609a63610641085177e",
      "title": "Softmax Q-Distribution Estimation for Structured Prediction: A Theoretical Interpretation for RAML",
      "abstract": "Reward augmented maximum likelihood (RAML), a simple and effective learning framework to directly optimize towards the reward function in structured prediction tasks, has led to a number of impressive empirical successes. RAML incorporates task-specific reward by performing maximum-likelihood updates on candidate outputs sampled according to an exponentiated payoff distribution, which gives higher probabilities to candidates that are close to the reference output. While RAML is notable for its simplicity, efficiency, and its impressive empirical successes, the theoretical properties of RAML, especially the behavior of the exponentiated payoff distribution, has not been examined thoroughly. In this work, we introduce softmax Q-distribution estimation, a novel theoretical interpretation of RAML, which reveals the relation between RAML and Bayesian decision theory. The softmax Q-distribution can be regarded as a smooth approximation of the Bayes decision boundary, and the Bayes decision rule is achieved by decoding with this Q-distribution. We further show that RAML is equivalent to approximately estimating the softmax Q-distribution, with the temperature $\\tau$ controlling approximation error. We perform two experiments, one on synthetic data of multi-class classification and one on real data of image captioning, to demonstrate the relationship between RAML and the proposed softmax Q-distribution estimation method, verifying our theoretical analysis. Additional experiments on three structured prediction tasks with rewards defined on sequential (named entity recognition), tree-based (dependency parsing) and irregular (machine translation) structures show notable improvements over maximum likelihood baselines.",
      "year": 2017,
      "influentialCitationCount": 3,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Mathematics"
      ],
      "journal": {
        "volume": "abs/1705.07136",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "2378954",
          "name": "Xuezhe Ma"
        },
        {
          "authorId": "38253388",
          "name": "Pengcheng Yin"
        },
        {
          "authorId": "48211835",
          "name": "J. Liu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144547315",
          "name": "E. Hovy"
        }
      ]
    },
    {
      "paperId": "04b876e95ac3e4754c8f0c8a9355e7acc3dc70b9",
      "externalIds": {
        "DBLP": "journals/lre/MoriN16",
        "MAG": "2398458169",
        "DOI": "10.1007/s10579-016-9354-7",
        "CorpusId": 31534902
      },
      "url": "https://www.semanticscholar.org/paper/04b876e95ac3e4754c8f0c8a9355e7acc3dc70b9",
      "title": "A comparative study of dictionaries and corpora as methods for language resource addition",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "50",
        "pages": "245-261",
        "name": "Language Resources and Evaluation"
      },
      "authors": [
        {
          "authorId": "144873535",
          "name": "Shinsuke Mori"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "1678eccf0f3895dbea6dfac44fc9d4f86de15ff6",
      "externalIds": {
        "CorpusId": 201863768
      },
      "url": "https://www.semanticscholar.org/paper/1678eccf0f3895dbea6dfac44fc9d4f86de15ff6",
      "title": "Predicting Emotional Responses From Spontaneous Social-Affective Interaction Data \u2217 \u2606",
      "abstract": "The study of social-affective communication is concerned with the role of emotion in human interaction. Emotion plays a two way role in human conversation; we express our emotion and are affected by our conversational partner. This is yet to be completely replicated in human-computer interaction (HCI). In addition to the more traditional works on recognition and simulation, there has recently been an increasing interest in emotional triggers, studying what causes emotion in the first place. A recent study by Hasegawa et al. [1] addresses this issue by predicting and eliciting emotion in online conversation. We have followed up this study by performing a similar task on natural spoken conversation. We recognized emotion based on a speaker\u2019s utterances and analyzed the conversational partner\u2019s as the emotional triggers [2, 3]. In this paper, we extend upon the previous works by trying to predict a person\u2019s emotional reaction in a social-affective conversation. We examine not only the person having the reaction, but also their conversational partner. The prediction is aimed to accommodate the ability to predict the response to an emotion trigger. Two languages are examined: English and Indonesian.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "143604111",
          "name": "Nurul Lubis"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "18289b2b04fc8a7a86f474236e55a3b1070a98ad",
      "externalIds": {
        "MAG": "2507168796",
        "CorpusId": 184921999
      },
      "url": "https://www.semanticscholar.org/paper/18289b2b04fc8a7a86f474236e55a3b1070a98ad",
      "title": "\u6587\u732e\u6284\u9332 \u6d44\u6c34\u51e6\u7406\u306b\u304a\u3051\u308b\u819c\u30d5\u30a1\u30a6\u30ea\u30f3\u30b0\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u306e\u305f\u3081\u306e\u51dd\u96c6\u53ca\u3073\u9178\u5316\u51e6\u7406 : \u6d78\u6f2c\u819c\u69fd\u3078\u306e\u4f4e\u6fc3\u5ea6\u30aa\u30be\u30f3\u6ce8\u5165\u306e\u9069\u7528",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "85",
        "pages": "13-15",
        "name": ""
      },
      "authors": [
        {
          "authorId": "122310299",
          "name": "Wen-Jung Yu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2069316420",
          "name": "G. D. Fowler"
        }
      ]
    },
    {
      "paperId": "1ddf9d306ae27113f55ea3d4eee12c8441235656",
      "externalIds": {
        "ACL": "W16-4601",
        "DBLP": "conf/aclwat/NakazawaDMGNK16",
        "CorpusId": 34267140
      },
      "url": "https://www.semanticscholar.org/paper/1ddf9d306ae27113f55ea3d4eee12c8441235656",
      "title": "Overview of the 3rd Workshop on Asian Translation",
      "abstract": "This paper presents the results of the shared tasks from the 3rd workshop on Asian translation (WAT2016) including J \u2194 E, J \u2194 C scientific paper translation subtasks, C \u2194 J, K \u2194 J, E \u2194 J patent translation subtasks, I \u2194 E newswire subtasks and H \u2194 E, H \u2194 J mixed domain subtasks. For the WAT2016, 15 institutions participated in the shared tasks. About 500 translation results have been submitted to the automatic evaluation server, and selected submissions were manually evaluated.",
      "year": 2016,
      "influentialCitationCount": 5,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-46"
      },
      "authors": [
        {
          "authorId": "40948140",
          "name": "Toshiaki Nakazawa"
        },
        {
          "authorId": "2126835",
          "name": "Chenchen Ding"
        },
        {
          "authorId": "40315879",
          "name": "Hideya Mino"
        },
        {
          "authorId": "2205832",
          "name": "Isao Goto"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1795664",
          "name": "S. Kurohashi"
        }
      ]
    },
    {
      "paperId": "26c2aad87810418b09e0f5b80352dd4d2536afe3",
      "externalIds": {
        "DBLP": "journals/tiis/TanakaSNTNIN16",
        "MAG": "2500714156",
        "DOI": "10.1145/2937757",
        "CorpusId": 14174976
      },
      "url": "https://www.semanticscholar.org/paper/26c2aad87810418b09e0f5b80352dd4d2536afe3",
      "title": "Teaching Social Communication Skills Through Human-Agent Interaction",
      "abstract": "There are a large number of computer-based systems that aim to train and improve social skills. However, most of these do not resemble the training regimens used by human instructors. In this article, we propose a computer-based training system that follows the procedure of social skills training (SST), a well-established method to decrease human anxiety and discomfort in social interaction, and acquire social skills. We attempt to automate the process of SST by developing a dialogue system named the automated social skills trainer, which teaches social communication skills through human-agent interaction. The system includes a virtual avatar that recognizes user speech and language information and gives feedback to users. Its design is based on conventional SST performed by human participants, including defining target skills, modeling, role-play, feedback, reinforcement, and homework. We performed a series of three experiments investigating (1) the advantages of using computer-based training systems compared to human-human interaction (HHI) by subjectively evaluating nervousness, ease of talking, and ability to talk well; (2) the relationship between speech language features and human social skills; and (3) the effect of computer-based training using our proposed system. Results of our first experiment show that interaction with an avatar decreases nervousness and increases the user's subjective impression of his or her ability to talk well compared to interaction with an unfamiliar person. The experimental evaluation measuring the relationship between social skill and speech and language features shows that these features have a relationship with social skills. Finally, experiments measuring the effect of performing SST with the proposed application show that participants significantly improve their skill, as assessed by separate evaluators, by using the system for 50 minutes. A user survey also shows that the users thought our system is useful and easy to use, and that interaction with the avatar felt similar to HHI.",
      "year": 2016,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://naist.repo.nii.ac.jp/record/4026/files/Tanaka_tiis2016.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "6",
        "pages": "18:1-18:26",
        "name": "ACM Trans. Interact. Intell. Syst."
      },
      "authors": [
        {
          "authorId": "50426585",
          "name": "Hiroki Tanaka"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1867578",
          "name": "Hideki Negoro"
        },
        {
          "authorId": "49429065",
          "name": "H. Iwasaka"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "298ddceada580c46e40e2a0323c0e3b16ed5f3c9",
      "externalIds": {
        "MAG": "2747491239",
        "CorpusId": 117214321
      },
      "url": "https://www.semanticscholar.org/paper/298ddceada580c46e40e2a0323c0e3b16ed5f3c9",
      "title": "Incongruity Detection on ASR Outputs based on EEG Signals",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Engineering"
      ],
      "journal": {
        "volume": "2016",
        "pages": "6",
        "name": ""
      },
      "authors": [
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "103273215",
          "name": "Odagaki Yu"
        },
        {
          "authorId": "102574149",
          "name": "Sasakura Takafumi"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "2a0cb1a1e78b77fe9981e4935410cf3ea900e370",
      "externalIds": {
        "CorpusId": 231778345
      },
      "url": "https://www.semanticscholar.org/paper/2a0cb1a1e78b77fe9981e4935410cf3ea900e370",
      "title": "Building Speech Recognition System from Untranscribed Data Report from JHU workshop 2016",
      "abstract": "Luk\u00e1\u0161 Burget , Sanjeev Khudanpur , Najim Dehak , Jan Trmal , Reinhold Haeb-Umbach , Graham Neubig , Shinji Watanabe , Daichi Mochihashi , Takahiro Shinozaki , Ming Sun , Chunxi Liu , Matthew Wiesner , Raghavendra Pappagari , Lucas Ondel , Mirko Hannemann , Santosh Kesiraju , Thomas Glarner , Leda Sari , Jinyi Yang , Ond\u0159ej \u0106\u0131fka , Yibo Yang , Alena Rott , and Jan \u201cHonza\u201d \u010cernock\u00fd (editor of the report) 1",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "1816892",
          "name": "L. Burget"
        },
        {
          "authorId": "2803071",
          "name": "S. Khudanpur"
        },
        {
          "authorId": "3135554",
          "name": "N. Dehak"
        },
        {
          "authorId": "2587544",
          "name": "J. Trmal"
        },
        {
          "authorId": "1405500402",
          "name": "R. Haeb-Umbach"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1746678",
          "name": "Shinji Watanabe"
        },
        {
          "authorId": "2619938",
          "name": "D. Mochihashi"
        },
        {
          "authorId": "1732454",
          "name": "T. Shinozaki"
        },
        {
          "authorId": "2118254816",
          "name": "Ming Sun"
        },
        {
          "authorId": "2145159028",
          "name": "Chunxi Liu"
        },
        {
          "authorId": "1500652334",
          "name": "Matthew Wiesner"
        },
        {
          "authorId": "18081502",
          "name": "R. Pappagari"
        },
        {
          "authorId": "2167829",
          "name": "Lucas Ondel"
        },
        {
          "authorId": "2592983",
          "name": "M. Hannemann"
        },
        {
          "authorId": "2481684",
          "name": "Santosh Kesiraju"
        },
        {
          "authorId": "10769826",
          "name": "Thomas Glarner"
        },
        {
          "authorId": "2769735",
          "name": "Leda Sari"
        },
        {
          "authorId": "47987949",
          "name": "Jinyi Yang"
        },
        {
          "authorId": "41022618",
          "name": "Ond\u0159ej C\u00edfka"
        },
        {
          "authorId": "2108619355",
          "name": "Yibo Yang"
        },
        {
          "authorId": "11016433",
          "name": "Alena Rott"
        }
      ]
    },
    {
      "paperId": "2a64da1ed300e49f2d665312146c8bb2f66920b7",
      "externalIds": {
        "ACL": "J16-1001",
        "DBLP": "journals/coling/NeubigW16",
        "MAG": "2300537905",
        "DOI": "10.1162/COLI_a_00241",
        "CorpusId": 2578447
      },
      "url": "https://www.semanticscholar.org/paper/2a64da1ed300e49f2d665312146c8bb2f66920b7",
      "title": "Optimization for Statistical Machine Translation: A Survey",
      "abstract": "In statistical machine translation (SMT), the optimization of the system parameters to maximize translation accuracy is now a fundamental part of virtually all modern systems. In this article, we survey 12 years of research on optimization for SMT, from the seminal work on discriminative models (Och and Ney 2002) and minimum error rate training (Och 2003), to the most recent advances. Starting with a brief introduction to the fundamentals of SMT systems, we follow by covering a wide variety of optimization algorithms for use in both batch and online optimization. Specifically, we discuss losses based on direct error minimization, maximum likelihood, maximum margin, risk minimization, ranking, and more, along with the appropriate methods for minimizing these losses. We also cover recent topics, including large-scale optimization, nonlinear models, domain-dependent optimization, and the effect of MT evaluation measures or search on optimization. Finally, we discuss the current state of affairs in MT optimization, and point out some unresolved problems that will likely be the target of further research in optimization for MT.",
      "year": 2016,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.mitpressjournals.org/doi/pdf/10.1162/COLI_a_00241",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "42",
        "pages": "1-54",
        "name": "Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2110694221",
          "name": "Taro Watanabe"
        }
      ]
    },
    {
      "paperId": "31884a623af77136413d997049b5787b394db461",
      "externalIds": {
        "CorpusId": 201825083
      },
      "url": "https://www.semanticscholar.org/paper/31884a623af77136413d997049b5787b394db461",
      "title": "Encoder-Decoder \u30e2\u30c7\u30eb\u306b\u304a\u3051\u308b\u51fa\u529b\u9577\u5236\u5fa1 \u83ca\u6c60\u60a0\u592a 1 , a )",
      "abstract": "\u6a5f\u68b0\u7ffb\u8a33\u306b\u9069\u7528\u3055\u308c\u3066\u4ee5\u964d [4], [18], [35],\u753b\u50cf\u30ad\u30e3\u30d7 \u30b7\u30e7\u30f3\u751f\u6210 [38], [42],\u69cb\u6587\u89e3\u6790 [37],\u5bfe\u8a71\u5fdc\u7b54\u751f\u6210 [21], [34] \u3084\u6587\u8981\u7d04 [5], [32]\u306a\u3069\u3092\u542b\u3080\u591a\u304f\u306e\u7cfb\u5217\u751f\u6210\u30bf\u30b9\u30af\u306b\u304a\u3044 \u3066 encoder-decoder\u30e2\u30c7\u30eb\u3092\u9069\u7528\u3057\u305f\u7814\u7a76\u304c\u5831\u544a\u3055\u308c\u3066\u3044 \u308b.\u672c\u7a3f\u306b\u304a\u3044\u3066\u306f\u3053\u306e\u3046\u3061\u6587\u8981\u7d04\u306b\u7126\u70b9\u3092\u5f53\u3066\u308b.\u6587\u8981 \u7d04\u306f,\u6587\u66f8\u8981\u7d04 [29]\u3084\u30d8\u30c3\u30c9\u30e9\u30a4\u30f3\u751f\u6210 [8]\u306e\u3088\u3046\u306b,\u4e0e \u3048\u3089\u308c\u305f\u539f\u6587 (\u66f8)\u96c6\u5408\u306e\u5185\u5bb9\u3092\u7c21\u6f54\u306b\u307e\u3068\u3081\u305f\u77ed\u3044\u6587 (\u66f8) \u3092\u751f\u6210\u3059\u308b,\u30c6\u30ad\u30b9\u30c8\u81ea\u52d5\u8981\u7d04\u306e\u4e00\u7a2e\u3067\u3042\u308b.Rush\u3089 [32] \u304c\u30cb\u30e5\u30fc\u30b9\u8a18\u4e8b\u3068\u30bf\u30a4\u30c8\u30eb\u304b\u3089\u5927\u898f\u6a21\u306a\u8a13\u7df4\u30c7\u30fc\u30bf\u3092\u6574 \u5099\u3057\u3066\u4ee5\u964d,\u6587\u8981\u7d04\u306f\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u57fa\u3065\u304f encoder-decoder\u30e2\u30c7\u30eb\u306e\u65b0\u305f\u306a\u9069\u7528\u30bf\u30b9\u30af\u3068\u3057\u3066\u76db\u3093\u306b\u7814 \u7a76\u3055\u308c\u59cb\u3081\u3066\u3044\u308b [1], [5], [14], [15], [24], [26], [30]. \u8981\u7d04\u5668\u304c\u6301\u3064\u3079\u304d\u91cd\u8981\u306a\u80fd\u529b\u306e\u4e00\u3064\u3068\u3057\u3066,\u51fa\u529b\u3059\u308b\u8981 \u7d04\u306e\u9577\u3055\u3092\u5236\u5fa1\u3067\u304d\u308b\u3053\u3068\u304c\u6319\u3052\u3089\u308c\u308b.\u30e6\u30fc\u30b6\u304c\u6240\u671b\u3059 \u308b\u8981\u7d04\u306e\u9577\u3055\u306f,\u539f\u6587\u66f8\u304b\u3089\u628a\u63e1\u3057\u305f\u3044\u60c5\u5831\u306e\u7c92\u5ea6\u3084\u8981\u7d04 \u3092\u8aad\u3080\u30c7\u30d0\u30a4\u30b9\u306e\u5927\u304d\u3055\u306a\u3069,\u5229\u7528\u3059\u308b\u6761\u4ef6\u306b\u5f37\u304f\u4f9d\u5b58\u3059 \u308b.\u305d\u306e\u305f\u3081,\u30e6\u30fc\u30b6\u306b\u3088\u308a\u5165\u529b\u3055\u308c\u305f\u6240\u671b\u306e\u9577\u3055\u306b\u5fdc\u3058 \u3066\u67d4\u8edf\u306b\u51fa\u529b\u3059\u308b\u8981\u7d04\u9577\u3092\u5236\u5fa1\u3067\u304d\u308b\u3053\u3068\u306f\u975e\u5e38\u306b\u91cd\u8981 \u3067\u3042\u308a,\u5fc5\u9808\u306e\u80fd\u529b\u3067\u3042\u308b.\u5b9f\u969b,\u6587\u66f8\u8981\u7d04\u5206\u91ce\u306b\u304a\u3051\u308b \u5b9f\u9a13\u8a2d\u5b9a\u3067\u306f\u539f\u6587\u66f8\u3068\u540c\u6642\u306b\u8981\u7d04\u306e\u9577\u3055\u5236\u9650\u3092\u5165\u529b\u3059\u308b \u306e\u304c\u6a19\u6e96\u3068\u306a\u3063\u3066\u3044\u308b.\u3057\u304b\u3057,\u305d\u306e\u91cd\u8981\u3055\u306b\u95a2\u308f\u3089\u305a, encoder-decoder\u30e2\u30c7\u30eb\u306b\u3088\u308b\u6587\u8981\u7d04\u306b\u53d6\u308a\u7d44\u3093\u3060\u5f93\u6765\u306e\u7814 \u7a76\u3067\u306f,\u3053\u306e\u51fa\u529b\u9577\u306e\u5236\u5fa1\u3068\u3044\u3046\u70b9\u306b\u3064\u3044\u3066\u660e\u78ba\u306b\u53d6\u308a\u7d44 1 \u6771\u4eac\u5de5\u696d\u5927\u5b66 Tokyo Institute of Technology 2 \u5948\u826f\u5148\u7aef\u79d1\u5b66\u6280\u8853\u5927\u5b66\u9662\u5927\u5b66 Nara Institute of Science and Technology a) kikuchi@lr.pi.titech.ac.jp b) neubig@is.naist.jp c) sasano@pi.titech.ac.jp d) takamura@pi.titech.ac.jp e) oku@pi.titech.ac.jp \u307e\u308c\u3066\u3053\u306a\u304b\u3063\u305f. \u672c\u7814\u7a76\u3067\u306f,encoder-decoder\u30e2\u30c7\u30eb\u306b\u304a\u3044\u3066\u51fa\u529b\u9577\u3092\u5236 \u5fa1\u3092\u3059\u308b\u305f\u3081\u306e 4\u3064\u306e\u624b\u6cd5\u3092\u63d0\u6848\u3059\u308b.\u305d\u306e\u3046\u3061 2\u624b\u6cd5\u306f \u30d3\u30fc\u30e0\u63a2\u7d22\u306b\u5909\u66f4\u3092\u52a0\u3048\u308b\u624b\u6cd5\u3067\u3042\u308a,\u6a19\u6e96\u7684\u306a encoderdecoder\u30e2\u30c7\u30eb\u3092\u8a13\u7df4\u3057\u305f\u306e\u3061,\u63a2\u7d22\u6642\u306b\u9577\u3055\u306e\u5165\u529b\u3092\u53d7 \u3051\u53d6\u308b.\u6b8b\u308a\u306e\u4e8c\u624b\u6cd5\u306f\u5b66\u7fd2\u306b\u57fa\u3065\u304f\u3082\u306e\u3067\u3042\u308a,\u51fa\u529b\u7cfb \u5217\u306e\u9577\u3055\u306e\u6307\u5b9a\u3092\u5165\u529b\u3068\u3057\u3066\u53d7\u3051\u53d6\u308b\u3088\u3046\u30e2\u30c7\u30eb\u306e\u62e1\u5f35\u3092 \u884c\u3044,\u5b66\u7fd2\u3092\u901a\u3057\u3066\u51fa\u529b\u9577\u306e\u5236\u5fa1\u80fd\u529b\u3092\u7372\u5f97\u3059\u308b. \u63d0\u6848\u624b\u6cd5\u306e\u6709\u52b9\u6027\u3092\u78ba\u304b\u3081\u308b\u305f\u3081\u306b,\u6587\u8981\u7d04\u306b\u304a\u3051\u308b\u6a19 \u6e96\u7684\u306a\u8a55\u4fa1\u30bb\u30c3\u30c8\u306b\u304a\u3044\u3066\u8981\u7d04\u306e\u9577\u3055\u5236\u9650\u3092\u5909\u5316\u3055\u305b\u305f\u6642 \u306e\u8a55\u4fa1\u5024\u306e\u5909\u5316\u3092\u78ba\u8a8d\u3057\u305f.\u5b9f\u9a13\u306b\u3088\u308a,\u5b66\u7fd2\u30d9\u30fc\u30b9\u306e\u624b \u6cd5\u306f\u9577\u3044\u5236\u9650\u9577 (50 \u30d0\u30a4\u30c8\u304a\u3088\u3073 75 \u30d0\u30a4\u30c8) \u306e\u3068\u304d\u306b\u63a2 \u7d22\u30d9\u30fc\u30b9\u306e\u624b\u6cd5\u3092\u4e0a\u56de\u308b\u50be\u5411\u306b\u3042\u308b\u3053\u3068\u304c\u78ba\u8a8d\u3067\u304d\u305f.\u307e \u305f,\u5f93\u6765\u624b\u6cd5\u3068\u306e\u6bd4\u8f03\u306b\u3088\u308a,\u6211\u3005\u306e\u5b66\u7fd2\u30d9\u30fc\u30b9\u306e\u624b\u6cd5\u306f \u51fa\u529b\u9577\u306e\u5236\u5fa1\u80fd\u529b\u3092\u7372\u5f97\u3057\u3064\u3064\u3082\u305d\u306e\u8981\u7d04\u7cbe\u5ea6\u306e\u52a3\u5316\u3092\u907f \u3051\u3066\u3044\u308b\u3053\u3068\u3092\u78ba\u8a8d\u3057\u305f.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "320278b24a3c53a44f95e8ef5465bebe56f24225",
      "externalIds": {
        "CorpusId": 201866023
      },
      "url": "https://www.semanticscholar.org/paper/320278b24a3c53a44f95e8ef5465bebe56f24225",
      "title": "A Joint Model for Pause Prediction and Dependency Parsing using Latent Variables The",
      "abstract": "Prediction of prosodic information from text is a basic technology used in a number of speech-related applications. In particular, pauses prediction is used in speech synthesis to allow for more natural prosodic boundaries [1]. While early studies in pause prediction only relied on lexical information such as POS tags or punctuation [1, 2], recent works have shown that using syntactic structure helps achieve better accuracy [3]. This work focuses on the use of dependency structure for pause prediction. Our method is inspired by recent work by Honnibal and Johnson [4] on the related, but quite different task, of disfluency detection. In this work, they propose a model that jointly performs dependency parsing and disfluency detection, and demonstrate that a joint model that considers these two tasks improves over solving these two tasks individually. Thus, in this paper, we propose a method for joint dependency parsing and pause prediction. One of the most widely used methods for dependency parsing is the transition-based method based on the shift-reduce algorithm [5]. As shown in the black text of Figure 1, and explained in detail in Section 2, the shift-reduce method builds a dependency tree expressing the syntactic structure of the sentence by performing a series of \u201cshift\u201d and \u201creduce\u201d actions, and if the correct action sequence is chosen, the correct dependency tree will be created. A classifier to choose the correct answer is trained from syntactically annotated data. In our proposed model, we further expand the action set of the shift-reduce algorithm by adding actions that predict pauses, as shown in the bold, red text in Figure 1, and is described in detail in Section 3. This presents more difficulties in training, however, as it is necessary to have data that is annotated with both pauses and syntactic information, which cannot be obtained in large quantities. To solve this problem, we treat the pauses as latent variables, allowing our parsing model to be trained on data fully annotated with syntax and pauses as well as data only annotated with syntactic trees. In the experiments described in Section 4, we find that the proposed model exceeds all baselines, and that the proposed latent variable allows for effective use of data not explicitly annotated with pauses, resulting in an 11.6% absolute improvement in pause F -measure.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "40538640",
          "name": "T. Nguyen"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "73467110",
          "name": "Hiroyuki Shindo"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "3426f000673aae995a55ade9273c842bb484ad18",
      "externalIds": {
        "MAG": "2514012605",
        "DBLP": "conf/interspeech/VetterMHNNSW16",
        "DOI": "10.21437/Interspeech.2016-1440",
        "CorpusId": 21670731
      },
      "url": "https://www.semanticscholar.org/paper/3426f000673aae995a55ade9273c842bb484ad18",
      "title": "Unsupervised Phoneme Segmentation of Previously Unseen Languages",
      "abstract": "In this paper we investigate the automatic detection of phoneme boundaries in audio recordings of an unknown language. This work is motivated by the needs of the project BULB which aims to support linguists in documenting unwritten languages. The automatic phonemic transcription of recordings of the unwritten language is part of this. We cannot use multilingual phoneme recognizers as their phoneme inventory might not completely cover that of the new language. Thus we opted for pursuing a two step approach which is inspired by work from speech synthesis for previously unknown languages. First, we detect boundaries for phonemes, and then we classify the detected segments into phoneme units. In this paper we address the \ufb01rst step, i.e. the detection of the phoneme boundaries. For this we again used multilingual and crosslingual phoneme recognizers but were only interested in the phoneme boundaries detected by them and not the phoneme identities. We measured the quality of the segmentations obtained this way using precision, re-call and F-measure. We compared the performance of different con\ufb01gurations of mono-and multilingual phoneme recognizers among each other and against a monolingual gold standard. Finally we applied the technique to Basaa, a Bantu language.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3544-3548"
      },
      "authors": [
        {
          "authorId": "32892953",
          "name": "Marco Vetter"
        },
        {
          "authorId": "48588187",
          "name": "Markus M\u00fcller"
        },
        {
          "authorId": "3400252",
          "name": "F. Hamlaoui"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        },
        {
          "authorId": "11126660",
          "name": "Sebastian St\u00fcker"
        },
        {
          "authorId": "1724972",
          "name": "A. Waibel"
        }
      ]
    },
    {
      "paperId": "34fb3e21a63fb2987f7a87f88ecf49aea53cff36",
      "externalIds": {
        "MAG": "2519892770",
        "DBLP": "journals/speech/HiraokaNSTN16",
        "DOI": "10.1016/j.specom.2016.09.002",
        "CorpusId": 38007369
      },
      "url": "https://www.semanticscholar.org/paper/34fb3e21a63fb2987f7a87f88ecf49aea53cff36",
      "title": "Learning cooperative persuasive dialogue policies using framing",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "84",
        "pages": "83-96",
        "name": "Speech Commun."
      },
      "authors": [
        {
          "authorId": "3027595",
          "name": "Takuya Hiraoka"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "35c710f5fdacc71a675832f6beaa2dbfe301d0ce",
      "externalIds": {
        "MAG": "2563741217",
        "DBLP": "conf/iwsds/HiraokaNYT016",
        "DOI": "10.1007/978-981-10-2585-3_5",
        "CorpusId": 7387579
      },
      "url": "https://www.semanticscholar.org/paper/35c710f5fdacc71a675832f6beaa2dbfe301d0ce",
      "title": "Active Learning for Example-Based Dialog Systems",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "67-78"
      },
      "authors": [
        {
          "authorId": "3027595",
          "name": "Takuya Hiraoka"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "3a6334953cd2775fab7a8e7b72ed63468c71dee7",
      "externalIds": {
        "DBLP": "conf/embc/TanakaSNNIN16",
        "MAG": "2538192626",
        "DOI": "10.1109/EMBC.2016.7591180",
        "CorpusId": 367538,
        "PubMed": "28226970"
      },
      "url": "https://www.semanticscholar.org/paper/3a6334953cd2775fab7a8e7b72ed63468c71dee7",
      "title": "Automated social skills training with audiovisual information",
      "abstract": "People with social communication difficulties tend to have superior skills using computers, and as a result computer-based social skills training systems are flourishing. Social skills training, performed by human trainers, is a well-established method to obtain appropriate skills in social interaction. Previous works have attempted to automate one or several parts of social skills training through human-computer interaction. However, while previous work on simulating social skills training considered only acoustic and linguistic features, human social skills trainers take into account visual features (e.g. facial expression, posture). In this paper, we create and evaluate a social skills training system that closes this gap by considering audiovisual features regarding ratio of smiling, yaw, and pitch. An experimental evaluation measures the difference in effectiveness of social skill training when using audio features and audiovisual features. Results showed that the visual features were effective to improve users' social skills.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Psychology",
        "Medicine",
        "Computer Science"
      ],
      "journal": {
        "pages": "2262-2265",
        "name": "2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
      },
      "authors": [
        {
          "authorId": "50426585",
          "name": "Hiroki Tanaka"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1867578",
          "name": "Hideki Negoro"
        },
        {
          "authorId": "49429065",
          "name": "H. Iwasaka"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "3cfdec4f1664fcdc20fd5a6d3f86e7b40cf19f70",
      "externalIds": {
        "DBLP": "journals/corr/KikuchiNSTO16",
        "MAG": "2528130257",
        "ACL": "D16-1140",
        "ArXiv": "1609.09552",
        "DOI": "10.18653/v1/D16-1140",
        "CorpusId": 11157751
      },
      "url": "https://www.semanticscholar.org/paper/3cfdec4f1664fcdc20fd5a6d3f86e7b40cf19f70",
      "title": "Controlling Output Length in Neural Encoder-Decoders",
      "abstract": "Neural encoder-decoder models have shown great success in many sequence generation tasks. However, previous work has not investigated situations in which we would like to control the length of encoder-decoder outputs. This capability is crucial for applications such as text summarization, in which we have to generate concise summaries with a desired length. In this paper, we propose methods for controlling the output sequence length for neural encoder-decoder models: two decoding-based methods and two learning-based methods. Results show that our learning-based methods have the capability to control length without degrading summary quality in a summarization task.",
      "year": 2016,
      "influentialCitationCount": 25,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D16-1140.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1328-1338"
      },
      "authors": [
        {
          "authorId": "2056484958",
          "name": "Yuta Kikuchi"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2293543",
          "name": "Ryohei Sasano"
        },
        {
          "authorId": "36514372",
          "name": "Hiroya Takamura"
        },
        {
          "authorId": "144859189",
          "name": "M. Okumura"
        }
      ]
    },
    {
      "paperId": "4533fd4cf13d2f4dd105edaf612934a1bd85ad5a",
      "externalIds": {
        "MAG": "2409994170",
        "DBLP": "journals/ieicet/MakiTSNN16",
        "DOI": "10.1587/TRANSINF.2015CBP0008",
        "CorpusId": 10838472
      },
      "url": "https://www.semanticscholar.org/paper/4533fd4cf13d2f4dd105edaf612934a1bd85ad5a",
      "title": "Enhancing Event-Related Potentials Based on Maximum a Posteriori Estimation with a Spatial Correlation Prior",
      "abstract": "In this paper a new method for noise removal from singletrial event-related potentials recorded with a multi-channel electroencephalogram is addressed. An observed signal is separated into multiple signals with a multi-channel Wiener filter whose coefficients are estimated based on parameter estimation of a probabilistic generative model that locally models the amplitude of each separated signal in the time-frequency domain. Effectiveness of using prior information about covariance matrices to estimate model parameters and frequency dependent covariance matrices were shown through an experiment with a simulated event-related potential data set. key words: electroencephalogram (EEG), event-related potential (ERP), generative model, independent component analysis (ICA), Wiener filter, noise removal, Wishart distribution, spatial correlation prior",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.jstage.jst.go.jp/article/transinf/E99.D/6/E99.D_2015CBP0008/_pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "99-D",
        "pages": "1437-1446",
        "name": "IEICE Trans. Inf. Syst."
      },
      "authors": [
        {
          "authorId": "3030934",
          "name": "Hayato Maki"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "45eea76ac46b402f3a209de57e469275419fdc9e",
      "externalIds": {
        "MAG": "2741908889",
        "CorpusId": 67350343
      },
      "url": "https://www.semanticscholar.org/paper/45eea76ac46b402f3a209de57e469275419fdc9e",
      "title": "Eye-gaze based Unknown Word Detection in Non-native Language Reading considering Part-of-speech Tag and Personalization",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "116",
        "pages": "29-34",
        "name": ""
      },
      "authors": [
        {
          "authorId": "72274088",
          "name": "H. Rui"
        },
        {
          "authorId": "65817231",
          "name": "Tanaka Hiroki"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "4b890b6ded71f005414e55adb87c23efd437ef95",
      "externalIds": {
        "MAG": "2293049663",
        "DBLP": "journals/taslp/TakamichiTBNSN16",
        "DOI": "10.1109/TASLP.2016.2522655",
        "CorpusId": 12597237
      },
      "url": "https://www.semanticscholar.org/paper/4b890b6ded71f005414e55adb87c23efd437ef95",
      "title": "Postfilters to Modify the Modulation Spectrum for Statistical Parametric Speech Synthesis",
      "abstract": "This paper presents novel approaches based on modulation spectrum (MS) for high-quality statistical parametric speech synthesis, including text-to-speech (TTS) and voice conversion (VC). Although statistical parametric speech synthesis offers various advantages over concatenative speech synthesis, the synthetic speech quality is still not as good as that of concatenative speech synthesis or the quality of natural speech. One of the biggest issues causing the quality degradation is the over-smoothing effect often observed in the generated speech parameter trajectories. Global variance (GV) is known as a feature well correlated with the over-smoothing effect, and the effectiveness of keeping the GV of the generated speech parameter trajectories similar to those of natural speech has been confirmed. However, the quality gap between natural speech and synthetic speech is still large. In this paper, we propose using the MS of the generated speech parameter trajectories as a new feature to effectively quantify the over-smoothing effect. Moreover, we propose postfilters to modify the MS utterance by utterance or segment by segment to make the MS of synthetic speech close to that of natural speech. The proposed postfilters are applicable to various synthesizers based on statistical parametric speech synthesis. We first perform an evaluation of the proposed method in the framework of hidden Markov model (HMM)-based TTS, examining its properties from different perspectives. Furthermore, effectiveness of the proposed postfilters are also evaluated in Gaussian mixture model (GMM)-based VC and classification and regression trees (CART)-based TTS (a.k.a., CLUSTERGEN). The experimental results demonstrate that 1) the proposed utterance-level postfilter achieves quality comparable to the conventional generation algorithm considering the GV, and yields significant improvements by applying to the GV-based generation algorithm in HMM-based TTS, 2) the proposed segment-level postfilter capable of achieving low-delay synthesis also yields significant improvements in synthetic speech quality, and 3) the proposed postfilters are also effective in not only HMM-based TTS but also GMM-based VC and CLUSTERGEN.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "24",
        "pages": "755-767",
        "name": "IEEE/ACM Transactions on Audio, Speech, and Language Processing"
      },
      "authors": [
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1690706",
          "name": "A. Black"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "4cfc7d3c6a61f6db48b1f3c75235592c1609a54f",
      "externalIds": {
        "ACL": "L16-1314",
        "MAG": "2574465720",
        "DBLP": "conf/lrec/SperberNNW16",
        "CorpusId": 21666821
      },
      "url": "https://www.semanticscholar.org/paper/4cfc7d3c6a61f6db48b1f3c75235592c1609a54f",
      "title": "Optimizing Computer-Assisted Transcription Quality with Iterative User Interfaces",
      "abstract": "Computer-assisted transcription promises high-quality speech transcription at reduced costs. This is achieved by limiting human effort to transcribing parts for which automatic transcription quality is insufficient. Our goal is to improve the human transcription quality via appropriate user interface design. We focus on iterative interfaces that allow humans to solve tasks based on an initially given suggestion, in this case an automatic transcription. We conduct a user study that reveals considerable quality gains for three variations of iterative interfaces over a non-iterative from-scratch transcription interface. Our iterative interfaces included post-editing, confidence-enhanced post-editing, and a novel retyping interface. All three yielded similar quality on average, but we found that the proposed retyping interface was less sensitive to the difficulty of the segment, and superior when the automatic transcription of the segment contained relatively many errors. An analysis using mixed-effects models allows us to quantify these and other factors and draw conclusions over which interface design should be chosen in which circumstance.",
      "year": 2016,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "pages": "1986-1992",
        "name": ""
      },
      "authors": [
        {
          "authorId": "3011998",
          "name": "Matthias Sperber"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        },
        {
          "authorId": "1724972",
          "name": "A. Waibel"
        }
      ]
    },
    {
      "paperId": "64280761641d8f1eb285165160bd96efac0bb5f5",
      "externalIds": {
        "MAG": "2586665961",
        "DBLP": "conf/slt/SaktiKNYN16",
        "DOI": "10.1109/SLT.2016.7846242",
        "CorpusId": 9404883
      },
      "url": "https://www.semanticscholar.org/paper/64280761641d8f1eb285165160bd96efac0bb5f5",
      "title": "Deep bottleneck features and sound-dependent i-vectors for simultaneous recognition of speech and environmental sounds",
      "abstract": "In speech interfaces, it is often necessary to understand the overall auditory environment, not only recognizing what is being said, but also being aware of the location or actions surrounding the utterance. However, automatic speech recognition (ASR) becomes difficult when recognizing speech with environmental sounds. Standard solutions treat environmental sounds as noise, and remove them to improve ASR performance. On the other hand, most studies on environmental sounds construct classifiers for environmental sounds only, without interference of spoken utterances. But, in reality, such separate situations almost never exist. This study attempts to address the problem of simultaneous recognition of speech and environmental sounds. Particularly, we examine the possibility of using deep neural network (DNN) techniques to recognize speech and environmental sounds simultaneously, and improve the accuracy of both tasks under respective noisy conditions. First, we investigate DNN architectures including two parallel single-task DNNs, and a single multi-task DNN. However, we found direct multi-task learning of simultaneous speech and environmental recognition to be difficult. Therefore, we further propose a method that combines bottleneck features and sound-dependent i-vectors within this framework. Experimental evaluation results reveal that the utilizing bottleneck features and i-vectors as the input of DNNs can help to improve accuracy of each recognition task.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "35-42",
        "name": "2016 IEEE Spoken Language Technology Workshop (SLT)"
      },
      "authors": [
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "153528451",
          "name": "S. Kawanishi"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "68aa7c7b65365c3303d5024b1273408fb435d178",
      "externalIds": {
        "ACL": "W16-3640",
        "DBLP": "conf/sigdial/MizukamiYNTN16",
        "MAG": "2524481654",
        "DOI": "10.18653/v1/W16-3640",
        "CorpusId": 8324365
      },
      "url": "https://www.semanticscholar.org/paper/68aa7c7b65365c3303d5024b1273408fb435d178",
      "title": "Analyzing the Effect of Entrainment on Dialogue Acts",
      "abstract": "Entrainment is a factor in dialogue that affects not only human-human but also human-machine interaction. While entrainment on the lexical level is well documented, less is known about how entrainment affects dialogue on a more abstract, structural level. In this paper, we investigate the effect of entrainment on dialogue acts and on lexical choice given dialogue acts, as well as how entrainment changes during a dialogue. We also define a novel measure of entrainment to measure these various types of entrainment. These results may serve as guidelines for dialogue systems that would like to entrain with users in a similar manner.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "310-318"
      },
      "authors": [
        {
          "authorId": "2387338",
          "name": "M. Mizukami"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144518646",
          "name": "D. Traum"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "6c170fe3fec5a477c938d07fa00935bb6f7b87cc",
      "externalIds": {
        "DBLP": "journals/ieicet/TakamichiTNSN16",
        "MAG": "2524591323",
        "DOI": "10.1587/TRANSINF.2016SLP0020",
        "CorpusId": 19616213
      },
      "url": "https://www.semanticscholar.org/paper/6c170fe3fec5a477c938d07fa00935bb6f7b87cc",
      "title": "A Statistical Sample-Based Approach to GMM-Based Voice Conversion Using Tied-Covariance Acoustic Models",
      "abstract": "This paper presents a novel statistical sample-based approach for Gaussian Mixture Model (GMM)-based Voice Conversion (VC). Although GMM-based VC has the promising flexibility of model adaptation, quality in converted speech is significantly worse than that of natural speech. This paper addresses the problem of inaccurate modeling, which is one of the main reasons causing the quality degradation. Recently, we have proposed statistical sample-based speech synthesis using rich context models for high-quality and flexible Hidden Markov Model (HMM)-based Text-To-Speech (TTS) synthesis. This method makes it possible not only to produce high-quality speech by introducing ideas from unit selection synthesis, but also to preserve flexibility of the original HMM-based TTS. In this paper, we apply this idea to GMM-based VC. The rich context models are first trained for individual joint speech feature vectors, and then we gather them mixture by mixture to form a Rich context-GMM (R-GMM). In conversion, an iterative generation algorithm using R-GMMs is used to convert speech parameters, after initialization using over-trained probability distributions. Because the proposed method utilizes individual speech features, and its formulation is the same as that of conventional GMMbased VC, it makes it possible to produce high-quality speech while keeping flexibility of the original GMM-based VC. The experimental results demonstrate that the proposed method yields significant improvements in term of speech quality and speaker individuality in converted speech. key words: GMM-based voice conversion, sample-based speech synthesis, speech parameter conversion, rich context model",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.jstage.jst.go.jp/article/transinf/E99.D/10/E99.D_2016SLP0020/_pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "99-D",
        "pages": "2490-2498",
        "name": "IEICE Trans. Inf. Syst."
      },
      "authors": [
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "6c520d983923dbe1e437c01086424fdcdd8f430a",
      "externalIds": {
        "CorpusId": 203627126
      },
      "url": "https://www.semanticscholar.org/paper/6c520d983923dbe1e437c01086424fdcdd8f430a",
      "title": "Post-Filters to Modify the Modulation Spectrum for Statistical Parametric Speech Synthesis",
      "abstract": "This paper presents novel approaches based on Modulation Spectrum (MS) for high-quality statistical parametric speech synthesis, including Text-To-Speech (TTS) and Voice Conversion (VC). Although statistical parametric speech synthesis offers various advantages over concatenative speech synthesis, the synthetic speech quality is still not as good as that of concatenative speech synthesis or the quality of natural speech. One of the biggest issues causing the quality degradation is the over-smoothing effect often observed in the generated speech parameter trajectories. Global Variance (GV) is known as a feature well correlated with the over-smoothing effect, and the effectiveness of keeping the GV of the generated speech parameter trajectories similar to those of natural speech has been confirmed. However, the quality gap between natural speech and synthetic speech is still large. In this paper, we propose using the MS of the generated speech parameter trajectories as a new feature to effectively quantify the over-smoothing effect. Moreover, we propose post-filters to modify the MS utterance by utterance or segment by segment to make the MS of synthetic speech close to that of natural speech. The proposed postfilters are applicable to various synthesizers based on statistical parametric speech synthesis. We first perform an evaluation of the proposed method in the framework of Hidden Markov Model (HMM)-based TTS, examining its properties from different perspectives. Furthermore, effectiveness of the proposed post-filters are also evaluated in Gaussian Mixture Model (GMM)-based VC and Classification And Regression Trees (CART)-based TTS (a.k.a., CLUSTERGEN). The experimental results demonstrate that (1) the proposed utterance-level post-filter achieves quality comparable to the conventional generation algorithm considering the GV, and yields significant improvements by applying to the GV-based generation algorithm in HMM-based TTS. (2) the proposed segment-level post-filter capable of achieving lowdelay synthesis also yields significant improvements in synthetic speech quality, and (3) the proposed post-filters are also effective in not only HMM-based TTS but also GMM-based VC and CLUSTERGEN.",
      "year": 2016,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1690706",
          "name": "A. Black"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "730950df0dbbb4c56212a67287097dc2799f8802",
      "externalIds": {
        "DOI": "10.1007/s10579-016-9354-7",
        "CorpusId": 254380435
      },
      "url": "https://www.semanticscholar.org/paper/730950df0dbbb4c56212a67287097dc2799f8802",
      "title": "A comparative study of dictionaries and corpora as methods for language resource addition",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "50",
        "pages": "245 - 261",
        "name": "Language Resources and Evaluation"
      },
      "authors": [
        {
          "authorId": "144873535",
          "name": "Shinsuke Mori"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "811531c959b0543a8e7abe1e827770e36b96f817",
      "externalIds": {
        "MAG": "3009392219",
        "CorpusId": 166226177
      },
      "url": "https://www.semanticscholar.org/paper/811531c959b0543a8e7abe1e827770e36b96f817",
      "title": "Word-level Emphasis Transfer in Speech-to-speech Translation | Article Information | J-GLOBAL",
      "abstract": "Speech-to-speech (S2S) translation systems [1] combine various technologies to help to translate speech across languages. However, most S2S systems ignore paralinguistic information such as emphasis. This paper attempts to solve the problem by proposing two new components: word-level emphasis estimation [2] using linear regression hidden semi-Markov models (LR-HSMM) [3], and emphasis translation that translates the word-level emphasis to a target language with conditional random fields (CRFs) [4]. The result shows that our system can accurately translate emphasis with 91.6% F -measure according to objective evaluation. A listening test with human subjects further showed that they could identify emphasized words with 87.8% F measure. This paper is a shortened version of our work presented in [5].",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "2016",
        "name": ""
      },
      "authors": [
        {
          "authorId": "2527751",
          "name": "Quoc Truong Do"
        },
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "91e605a125f64207a242693d0dc1c862080f6c27",
      "externalIds": {
        "ACL": "D16-1263",
        "DBLP": "conf/emnlp/AdamsNCBDN16",
        "MAG": "2563850823",
        "DOI": "10.18653/v1/D16-1263",
        "CorpusId": 6225155
      },
      "url": "https://www.semanticscholar.org/paper/91e605a125f64207a242693d0dc1c862080f6c27",
      "title": "Learning a Lexicon and Translation Model from Phoneme Lattices",
      "abstract": "Language documentation begins by gathering speech. Manual or automatic transcription at the word level is typically not possible because of the absence of an orthography or prior lexicon, and though manual phone-mic transcription is possible, it is prohibitively slow. On the other hand, translations of the minority language into a major language are more easily acquired. We propose a method to harness such translations to improve automatic phoneme recognition. The method assumes no prior lexicon or translation model, instead learning them from phoneme lattices and translations of the speech being transcribed. Experiments demonstrate phoneme error rate improvements against two baselines and the model\u2019s ability to learn useful bilingual lexical entries.",
      "year": 2016,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D16-1263.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2377-2382"
      },
      "authors": [
        {
          "authorId": "38535429",
          "name": "Oliver Adams"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143620680",
          "name": "Trevor Cohn"
        },
        {
          "authorId": "21308992",
          "name": "Steven Bird"
        },
        {
          "authorId": "2527751",
          "name": "Quoc Truong Do"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "923ddc71f8a453c7995e97b0681a674224a5fc09",
      "externalIds": {
        "MAG": "2343298328",
        "DOI": "10.5715/JNLP.23.87",
        "CorpusId": 63473450
      },
      "url": "https://www.semanticscholar.org/paper/923ddc71f8a453c7995e97b0681a674224a5fc09",
      "title": "Error Selection Methods for Machine Translation Error Analysis",
      "abstract": "Error analysis is used to improve accuracy of machine translation (MT) systems. Various methods of analyzing MT errors have been proposed; however, most of these methods are based on differences between translations and references that are translated independently by human translators, and few methods have been proposed for manual error analysis. This work proposes a method that uses a machine learning framework to identify errors in MT output, and improves efficiency of manual error analysis. Our method builds models that classify low and high quality translations, then identifies features of low quality translations to improve efficiency of the manual analysis. Experiments showed that by using our methods, we could improve the efficiency of MT error analysis.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.jstage.jst.go.jp/article/jnlp/23/1/23_87/_pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "23",
        "pages": "87-117",
        "name": ""
      },
      "authors": [
        {
          "authorId": "1783871",
          "name": "Koichi Akabe"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "93a55f3341aa70bb42c0f76b112e2e8da27b3df2",
      "externalIds": {
        "CorpusId": 201860514
      },
      "url": "https://www.semanticscholar.org/paper/93a55f3341aa70bb42c0f76b112e2e8da27b3df2",
      "title": "Response Selection on EBDM-Dialogue System based on Entrainment Analysis",
      "abstract": "Entrainment is a factor in dialogue that affects not only human-human dialogue but also human-machine interaction. While the entrainment on the lexical level is well documented, less is known about how entrainment affects dialogue on a more abstract, and structural level. In previous works, we investigated the effect of entrainment on lexical choice given dialogue acts. From these results, we build the example selection on the EBDM dialogue system that would like to entrain with users in a similar manner.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2108944785",
          "name": "Masahiro Nakamura"
        }
      ]
    },
    {
      "paperId": "95cedaeb3178a4671703a05171a144e6b964a819",
      "externalIds": {
        "DBLP": "conf/emnlp/NeubigD16",
        "ArXiv": "1606.00499",
        "MAG": "2951913798",
        "ACL": "D16-1124",
        "DOI": "10.18653/v1/D16-1124",
        "CorpusId": 11054466
      },
      "url": "https://www.semanticscholar.org/paper/95cedaeb3178a4671703a05171a144e6b964a819",
      "title": "Generalizing and Hybridizing Count-based and Neural Language Models",
      "abstract": "Language models (LMs) are statistical models that calculate probabilities over sequences of words or other discrete symbols. Currently two major paradigms for language modeling exist: count-based n-gram models, which have advantages of scalability and test-time speed, and neural LMs, which often achieve superior modeling performance. We demonstrate how both varieties of models can be unified in a single modeling framework that defines a set of probability distributions over the vocabulary of words, and then dynamically calculates mixture weights over these distributions. This formulation allows us to create novel hybrid models that combine the desirable features of count-based and neural LMs, and experiments demonstrate the advantages of these approaches.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D16-1124.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1163-1172"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1745899",
          "name": "Chris Dyer"
        }
      ]
    },
    {
      "paperId": "a182a8a0678857df5c513d52469fa707c32e69ec",
      "externalIds": {
        "DBLP": "conf/acl/ZhangUSNN16",
        "ACL": "P16-1130",
        "MAG": "2515116787",
        "DOI": "10.18653/v1/P16-1130",
        "CorpusId": 10140027
      },
      "url": "https://www.semanticscholar.org/paper/a182a8a0678857df5c513d52469fa707c32e69ec",
      "title": "A Continuous Space Rule Selection Model for Syntax-based Statistical Machine Translation",
      "abstract": "One of the major challenges for statistical machine translation (SMT) is to choose the appropriate translation rules based on the sentence context. This paper proposes a continuous space rule selection (CSRS) model for syntax-based SMT to perform this context-dependent rule selection. In contrast to existing maximum entropy based rule selection (MERS) models, which use discrete representations of words as features, the CSRS model is learned by a feed-forward neural network and uses real-valued vector representations of words, allowing for better generalization. In addition, we propose a method to train the rule selection models only on minimal rules, which are more frequent and have richer training data compared to non-minimal rules. We tested our model on different translation tasks and the CSRS model outperformed a base-line without rule selection and the previous MERS model by up to 2 . 2 and 1 . 1 points of BLEU score respectively.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P16-1130.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "1",
        "pages": "1372-1381",
        "name": ""
      },
      "authors": [
        {
          "authorId": "4279134",
          "name": "Jingyi Zhang"
        },
        {
          "authorId": "1802277",
          "name": "M. Utiyama"
        },
        {
          "authorId": "1698363",
          "name": "E. Sumita"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "a4577911d247e472772e2101d21aeaf8f46053cc",
      "externalIds": {
        "CorpusId": 201866207
      },
      "url": "https://www.semanticscholar.org/paper/a4577911d247e472772e2101d21aeaf8f46053cc",
      "title": "Semantic Parsing of Ambiguous Input using Multi Synchronous Grammars",
      "abstract": "Semantic parsing (SP) is the problem of parsing a given natural language (NL) sentence into a meaning representation (MR) conducive to further processing by applications. One of the major challenges in SP stems from the fact that NL is rife with ambiguities. Previous works using statistical models along with formalisms such as combinatorial categorial grammars, synchronous context free grammars, and dependency-based compositional semantics have shown notable success in resolving these ambiguities [7, 9, 12, 11, 14]. However, in many cases, the input for NL applications is underspecified and ungrammatical. We illustrate the example of search queries in Table 1. From these queries (Column 1) and their MRs (Column 2), we can see that there are several kinds of ambiguity. For example the parser must make the distinction between Kobe as a city or a basketball player. This problem of word sense ambiguity occurs in standard semantic parsing tasks, but there are also more pernicious problems unique to the more ambiguous input posed by search queries. Focusing on the queries \u201cKobe hotels\u201d and \u201cKobe flight\u201d we can see that it is also necessary to estimate the latent relationship between words, such as \u201clocation\u201d or \u201cdestination.\u201d However it should be noted that if we take the keyword query and re-express it as a more explicit paraphrase, we can reduce this ambiguity to the point where there is only one reasonable interpretation. For example, in the second line, if we add the preposition \u201cto\u201d the user is likely asking for flights that arriving in Kobe, and if we add \u201cfrom\u201d the user is asking for departures. In this paper, we focus on SP of ambiguous input and propose a new method for dealing with the problem of ambiguity. Specifically we describe a framework where an ambiguous input (Column 1) is simultaneously transformed into both its MR (Column 2) and a more explicit, less ambiguous paraphrase (Column 3). The advantage of this method is that it is then possible to verify that the paraphrase indeed expresses the intended meaning of the underspecified input. Specifically, this verification can be done either manually by the system user or automatically using a probabilistic model trained to judge the naturalness of the paraphrases. As a concrete approach, we build upon the formalism of synchronous context free grammars (SCFG). Unlike traditional SCFGs, which usually only generate one target string (in semantic parsing, an MR), we introduce a new variety of SCFGs called \u201ctri-synchronous\u201d grammars, which generate multiple strings on the target side. This allows us to not only generate the MR, but also jointly generate the more explicit paraphrase. We then use a language model (LM) over the paraphrases generated by each derivation to help determine which derivations, and consequently which MRs, are more likely. Evaluation was performed using the Geoquery benchmark of 880 query-logic pairs representing questions about US geography [13]. The baseline SCFG parser achieves reasonable accuracy on regular questions but when the same method is used with underspecified input, the system accuracy decreases significantly. On the other hand, when incorporating the proposed tri-synchronous grammar to generate paraphrases and verify them with an LM, we find that it is possible to recover the loss of accuracy, resulting in a model that is able to parse the ambiguous input with significantly better accuracy. 1",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "144332819",
          "name": "Philip Arthur"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "a73d83e50b5687455336a2adce32a069c77ba163",
      "externalIds": {
        "MAG": "2747561225",
        "CorpusId": 187488350
      },
      "url": "https://www.semanticscholar.org/paper/a73d83e50b5687455336a2adce32a069c77ba163",
      "title": "\u7d71\u8a08\u7684F_0\u8f2a\u90ed\u4e88\u6e2c\u306b\u57fa\u3065\u304f\u4eba\u5de5\u5589\u982d\u306e\u5b9f\u6642\u9593\u632f\u52d5\u5236\u5fa1\u3010Powered by NICT\u3011",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "2016",
        "pages": "1337",
        "name": ""
      },
      "authors": [
        {
          "authorId": "2073016145",
          "name": "Tanaka Kou"
        },
        {
          "authorId": "2054735420",
          "name": "Toda Tomoki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2075534130",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "b13e9d23983273c0c67b91ae70c55d4c3f745b8b",
      "externalIds": {
        "DBLP": "conf/eacl/NeubigCGL17",
        "MAG": "2950853958",
        "ACL": "E17-1099",
        "ArXiv": "1610.00388",
        "DOI": "10.18653/V1/E17-1099",
        "CorpusId": 2782776
      },
      "url": "https://www.semanticscholar.org/paper/b13e9d23983273c0c67b91ae70c55d4c3f745b8b",
      "title": "Learning to Translate in Real-time with Neural Machine Translation",
      "abstract": "Translating in real-time, a.k.a.simultaneous translation, outputs translation words before the input sentence ends, which is a challenging problem for conventional machine translation methods. We propose a neural machine translation (NMT) framework for simultaneous translation in which an agent learns to make decisions on when to translate from the interaction with a pre-trained NMT environment. To trade off quality and delay, we extensively explore various targets for delay and design a method for beam-search applicable in the simultaneous MT setting. Experiments against state-of-the-art baselines on two language pairs demonstrate the efficacy of the proposed framework both quantitatively and qualitatively.",
      "year": 2016,
      "influentialCitationCount": 37,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/E17-1099.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1610.00388",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1979489",
          "name": "Kyunghyun Cho"
        },
        {
          "authorId": "3016273",
          "name": "Jiatao Gu"
        },
        {
          "authorId": "2052674293",
          "name": "V. Li"
        }
      ]
    },
    {
      "paperId": "b2baf9e053c32abfb3c8658b9bc6d6790ae671cb",
      "externalIds": {
        "CorpusId": 51800940
      },
      "url": "https://www.semanticscholar.org/paper/b2baf9e053c32abfb3c8658b9bc6d6790ae671cb",
      "title": "Eye Gaze-based Unknown Word Detection in Non-native Language Reading using SVMs and Random Forests",
      "abstract": "This paper proposes machine-learning based method to detect unknown words during natural reading by using eye-tracking features. A previous approach [1] utilizes gaze duration and word rarity features for detection. However, the performance of the previous approach is not sufficient during natural reading. To improve detection performance, we try to 1) apply support vector machines (SVM) [2] and Random Forests (RF) [3], and 2) use novel eye movement features that were not considered in the previous work. The experimental results demonstrate that SVM and proposed eye movement features are capable of improving detection performance as measured by F-measure.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "50426585",
          "name": "Hiroki Tanaka"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "b5991b1018bb89b053a2c8229248f97956391bb5",
      "externalIds": {
        "DBLP": "conf/interspeech/TsujiokaSYNN16",
        "MAG": "2510428086",
        "DOI": "10.21437/Interspeech.2016-919",
        "CorpusId": 21566225
      },
      "url": "https://www.semanticscholar.org/paper/b5991b1018bb89b053a2c8229248f97956391bb5",
      "title": "Unsupervised Joint Estimation of Grapheme-to-Phoneme Conversion Systems and Acoustic Model Adaptation for Non-Native Speech Recognition",
      "abstract": "Non-native speech differs significantly from native speech, often resulting in a degradation of the performance of automatic speech recognition (ASR). Hand-crafted pronunciation lexicons used in standard ASR systems generally fail to cover non-native pronunciations, and design of new ones by linguistic experts is time consuming and costly. In this work, we propose acoustic data-driven iterative pronunciation learning for non-native speech recognition, which automatically learns non-native pronunciations directly from speech using an iterative estimation procedure. Grapheme-to-Phoneme (G2P) conversion is used to predict multiple candidate pronunciations for each word, occurrence frequency of pronunciation variations is estimated from the acoustic data of non-native speakers, and these automatically estimated pronunciation variations are used to perform acoustic model adaptation. We investigate various cases such as learning (1) without knowledge of non-native pronunciation, and (2) when we adapt to the speaker\u2019s proficiency level. In experiments on speech from non-native speakers of various levels, the proposed method was able to achieve an 8.9% average improvement in accuracy.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3091-3095"
      },
      "authors": [
        {
          "authorId": "49703062",
          "name": "Satoshi Tsujioka"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "ba56bb1eb67b188a89060058ef8ad02ce3c660ac",
      "externalIds": {
        "MAG": "2805853209",
        "CorpusId": 187419108
      },
      "url": "https://www.semanticscholar.org/paper/ba56bb1eb67b188a89060058ef8ad02ce3c660ac",
      "title": "Proceedings of the 3rd Workshop on Asian Translation (WAT2016)",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Political Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "40948140",
          "name": "Toshiaki Nakazawa"
        },
        {
          "authorId": "40315879",
          "name": "Hideya Mino"
        },
        {
          "authorId": "2126835",
          "name": "Chenchen Ding"
        },
        {
          "authorId": "2205832",
          "name": "Isao Goto"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1795664",
          "name": "S. Kurohashi"
        },
        {
          "authorId": "146997818",
          "name": "Ir. Hammam Riza"
        },
        {
          "authorId": "145532184",
          "name": "P. Bhattacharyya"
        }
      ]
    },
    {
      "paperId": "bc1bf0a21d7838ec167e77c76163afc1f5f76c3d",
      "externalIds": {
        "DBLP": "conf/embc/MakiTSNN16",
        "MAG": "2537033055",
        "DOI": "10.1109/EMBC.2016.7591538",
        "CorpusId": 18759325,
        "PubMed": "28227325"
      },
      "url": "https://www.semanticscholar.org/paper/bc1bf0a21d7838ec167e77c76163afc1f5f76c3d",
      "title": "Removing noise from event-related potentials using a probabilistic generative model with grouped covariance matrices",
      "abstract": "Analysis of electroencephalograms (EEG) usually suffers from a variety of noises. In this paper, we propose a new method for background noise removal from single-trial event-related potentials (ERPs) recorded with a multi-channel EEG. An observed signal is separated into multiple signals with a multi-channel Wiener filter, whose coefficients are estimated based on a probabilistic generative model in the time-frequency domain. The main contribution is a method to estimate covariance matrices for each frequency bins of short-time Fourier transform (STFT) representing different spatial spread of a multi-channel EEG signal according to frequencies. An experiment using a pseudo-ERP data set demonstrates the effectiveness of our proposed method.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Medicine",
        "Mathematics"
      ],
      "journal": {
        "pages": "3728-3731",
        "name": "2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
      },
      "authors": [
        {
          "authorId": "3030934",
          "name": "Hayato Maki"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "c2b22a18ea2ed444c6c1f5bb27ab55bda2b44567",
      "externalIds": {
        "MAG": "2741682708",
        "CorpusId": 195986481
      },
      "url": "https://www.semanticscholar.org/paper/c2b22a18ea2ed444c6c1f5bb27ab55bda2b44567",
      "title": "Hard-Attentional Neural Network Models for Emphasis Speech Translation",
      "abstract": "Traditional speech translation systems are oblivious to paralinguistic information. A recent work has tried to tackle this task by utilizing conditional random fields (CRFs). Although CRFs allow for consideration of rich features and local context, they have difficulty in handling continuous variables, and cannot capture long-distance dependencies easily. In this paper, we propose a new model for emphasis transfer in speech translation using an approach based on neural networks. Our experiments showed a significant improvement of the proposed model over the previous model by 4% target-language emphasis prediction F-measure according to objective evaluation.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "116",
        "pages": "7-8",
        "name": ""
      },
      "authors": [
        {
          "authorId": "2097158185",
          "name": "TruongDo Quoc"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "c3d2c60e70cad17ea37cb116ab30e1239405dbdd",
      "externalIds": {
        "MAG": "3008140499",
        "CorpusId": 217655712
      },
      "url": "https://www.semanticscholar.org/paper/c3d2c60e70cad17ea37cb116ab30e1239405dbdd",
      "title": "EEG\u3092\u7528\u3044\u305fASR\u51fa\u529b\u306b\u9055\u548c\u611f\u306e\u691c\u51fa | \u6587\u732e\u60c5\u5831 | J-GLOBAL \u79d1\u5b66\u6280\u8853\u7dcf\u5408\u30ea\u30f3\u30af\u30bb\u30f3\u30bf\u30fc",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "2016",
        "name": ""
      },
      "authors": [
        {
          "authorId": "71957193",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "2073045276",
          "name": "Odagaki Yu"
        },
        {
          "authorId": "2093681254",
          "name": "Sasakura Takafumi"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2054735420",
          "name": "Toda Tomoki"
        },
        {
          "authorId": "2075534130",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "c3f9c1f702d0c3b35b99502674757b3d8e7dd352",
      "externalIds": {
        "DBLP": "journals/ieicet/OshimaTTNSN16",
        "MAG": "2559039559",
        "DOI": "10.1587/TRANSINF.2016EDP7231",
        "CorpusId": 7573692
      },
      "url": "https://www.semanticscholar.org/paper/c3f9c1f702d0c3b35b99502674757b3d8e7dd352",
      "title": "Non-Native Text-to-Speech Preserving Speaker Individuality Based on Partial Correction of Prosodic and Phonetic Characteristics",
      "abstract": "This paper presents a novel non-native speech synthesis technique that preserves the individuality of a non-native speaker. Crosslingual speech synthesis based on voice conversion or Hidden Markov Model (HMM)-based speech synthesis is a technique to synthesize foreign language speech using a target speaker\u2019s natural speech uttered in his/her mother tongue. Although the technique holds promise to improve a wide variety of applications, it tends to cause degradation of target speaker\u2019s individuality in synthetic speech compared to intra-lingual speech synthesis. This paper proposes a new approach to speech synthesis that preserves speaker individuality by using non-native speech spoken by the target speaker. Although the use of non-native speech makes it possible to preserve the speaker individuality in the synthesized target speech, naturalness is significantly degraded as the synthesized speech waveform is directly affected by unnatural prosody and pronunciation often caused by differences in the linguistic systems of the source and target languages. To improve naturalness while preserving speaker individuality, we propose (1) a prosody correction method based on model adaptation, and (2) a phonetic correction method based on spectrum replacement for unvoiced consonants. The experimental results using English speech uttered by native Japanese speakers demonstrate that (1) the proposed methods are capable of significantly improving naturalness while preserving the speaker individuality in synthetic speech, and (2) the proposed methods also improve intelligibility as confirmed by a dictation test. key words: cross-lingual speech synthesis, English-Read-by-Japanese, speaker individuality, HMM-based speech synthesis, prosody correction, phonetic correction",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.jstage.jst.go.jp/article/transinf/E99.D/12/E99.D_2016EDP7231/_pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "99-D",
        "pages": "3132-3139",
        "name": "IEICE Trans. Inf. Syst."
      },
      "authors": [
        {
          "authorId": "145006640",
          "name": "Yuji Oshima"
        },
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "cc549a11d277d86f6228443cb16c231c9bda6c96",
      "externalIds": {
        "DBLP": "conf/interspeech/DoTNSN16",
        "MAG": "2510638983",
        "DOI": "10.21437/Interspeech.2016-930",
        "CorpusId": 35742194
      },
      "url": "https://www.semanticscholar.org/paper/cc549a11d277d86f6228443cb16c231c9bda6c96",
      "title": "A Hybrid System for Continuous Word-Level Emphasis Modeling Based on HMM State Clustering and Adaptive Training",
      "abstract": "Emphasis is an important aspect of speech that conveys the focus of utterances, and modeling of this emphasis has been an active research field. Previous work has modeled emphasis using state clustering with an emphasis contextual factor indicating whether or not a word is emphasized. In addition, cluster adaptive training (CAT) makes it possible to directly optimize model parameters for clusters with different characteristics. In this paper, we first make a straightforward extension of CAT to emphasis adaptive training using continuous emphasis representations. We then compare it to state clustering, and propose a hybrid approach that combines both the emphasis contextual factor and adaptive training. Experiments demonstrated the effectiveness of adaptive training both stand-alone or combined with the state clustering approach (hybrid system) with it improving emphasis estimation by 2-5% F -measure and producing more natural audio.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3196-3200"
      },
      "authors": [
        {
          "authorId": "2527751",
          "name": "Quoc Truong Do"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "cfb1b39d1a6733f42cc5e8cfd60dc68cafa01d28",
      "externalIds": {
        "CorpusId": 52402161
      },
      "url": "https://www.semanticscholar.org/paper/cfb1b39d1a6733f42cc5e8cfd60dc68cafa01d28",
      "title": "Cross Modal Content-Based Objective for Learning Adequate Multimodal Representations",
      "abstract": "EDUCATION Carnegie Mellon University Pittsburgh, PA, USA Master of Science in Language and Information Technologies Aug 2015-Aug 2017 (Expected) \u2022 Research Master\u2019s degree, with emphasis on the intersection between natural language processing and machine learning. \u2022 Advisor: Professor Chris Dyer (also collaborate closely with Professors Noah Smith and Graham Neubig). \u2022 Research area: Natural Language Processing for low-resource languages, including machine learning methods (especially neural networks) in structured prediction tasks, multi-lingual learning, and multi-modal machine learning, \u2022 Coursework: Algorithms for NLP (A), Introduction to Machine Learning PhD Level (A), Advanced Multi-Modal Machine Learning (A+), Machine Translation (A), Deep Learning (A+), Language and Statistics (A), Directed Research (A+) \u2022 CGPA: 4.07 out of 4.33",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "3376845",
          "name": "A. Kuncoro"
        },
        {
          "authorId": "1745899",
          "name": "Chris Dyer"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144365875",
          "name": "Noah A. Smith"
        },
        {
          "authorId": "2528900",
          "name": "Akash Bharadwaj"
        },
        {
          "authorId": "29072828",
          "name": "Seungwhan Moon"
        },
        {
          "authorId": "3149518",
          "name": "Volkan Cirik"
        },
        {
          "authorId": "49933077",
          "name": "Louis-Philippe Morency"
        }
      ]
    },
    {
      "paperId": "d21a0e01514732f241b9c138eceb76ecaef17a27",
      "externalIds": {
        "MAG": "2754890530",
        "DBLP": "conf/naacl/MiuraNPN16",
        "ACL": "N16-1003",
        "DOI": "10.18653/v1/N16-1003",
        "CorpusId": 12183141
      },
      "url": "https://www.semanticscholar.org/paper/d21a0e01514732f241b9c138eceb76ecaef17a27",
      "title": "Selecting Syntactic, Non-redundant Segments in Active Learning for Machine Translation",
      "abstract": "Active learning is a framework that makes it possible to efficiently train statistical models by selecting informative examples from a pool of unlabeled data. Previous work has found this framework effective for machine translation (MT), making it possible to train better translation models with less effort, particularly when annotators translate short phrases instead of full sentences. However, previous methods for phrase-based active learning in MT fail to consider whether the selected units are coherent and easy for human translators to translate, and also have problems with selecting redundant phrases with similar content. In this paper, we tackle these problems by proposing two new methods for selecting more syntactically coherent and less redundant segments in active learning for MT. Experiments using both simulation and extensive manual translation by professional translators find the proposed method effective, achieving both greater gain of BLEU score for the same number of translated words, and allowing translators to be more confident in their translations1.",
      "year": 2016,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/N16-1003.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "20-29"
      },
      "authors": [
        {
          "authorId": "31504732",
          "name": "Akiva Miura"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144734729",
          "name": "Michael Paul"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "d26254cf3ec537f37708afaaf7f5a76a7922d4a2",
      "externalIds": {
        "MAG": "2294311690",
        "DBLP": "journals/mt/ZhangUSZNN16",
        "DOI": "10.1007/s10590-016-9178-7",
        "CorpusId": 16887612
      },
      "url": "https://www.semanticscholar.org/paper/d26254cf3ec537f37708afaaf7f5a76a7922d4a2",
      "title": "Learning local word reorderings for hierarchical phrase-based statistical machine translation",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "30",
        "pages": "1-18",
        "name": "Machine Translation"
      },
      "authors": [
        {
          "authorId": "4279134",
          "name": "Jingyi Zhang"
        },
        {
          "authorId": "1802277",
          "name": "M. Utiyama"
        },
        {
          "authorId": "1698363",
          "name": "E. Sumita"
        },
        {
          "authorId": "36225434",
          "name": "Zhao Hai"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "d9b89de5c2a39479768c6e32f13ac3e816635cc1",
      "externalIds": {
        "MAG": "2513832136",
        "DBLP": "conf/interspeech/AdamsNCB16",
        "DOI": "10.21437/Interspeech.2016-862",
        "CorpusId": 10018535
      },
      "url": "https://www.semanticscholar.org/paper/d9b89de5c2a39479768c6e32f13ac3e816635cc1",
      "title": "Learning a Translation Model from Word Lattices",
      "abstract": "Translation models have been used to improve automatic speech recognition when speech input is paired with a written translation, primarily for the task of computer-aided translation. Existing approaches require large amounts of parallel text for training the translation models, but for many language pairs this data is not available. We propose a model for learning lexical translation parameters directly from the word lattices for which a transcription is sought. The model is expressed through composition of each lattice with a weighted \ufb01nite-state transducer representing the translation model, where inference is performed by sampling paths through the composed \ufb01nite-state transducer. We show consistent word error rate reductions in two datasets, using between just 20 minutes and 4 hours of speech input, additionally outperforming a translation model trained on the 1-best path.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2518-2522"
      },
      "authors": [
        {
          "authorId": "38535429",
          "name": "Oliver Adams"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143620680",
          "name": "Trevor Cohn"
        },
        {
          "authorId": "21308992",
          "name": "Steven Bird"
        }
      ]
    },
    {
      "paperId": "dc984ea8be018a0244b40468d13f7b734ab55bac",
      "externalIds": {
        "DBLP": "journals/corr/ArthurNN16",
        "MAG": "2949978460",
        "ArXiv": "1606.02006",
        "ACL": "D16-1162",
        "DOI": "10.18653/v1/D16-1162",
        "CorpusId": 10086161
      },
      "url": "https://www.semanticscholar.org/paper/dc984ea8be018a0244b40468d13f7b734ab55bac",
      "title": "Incorporating Discrete Translation Lexicons into Neural Machine Translation",
      "abstract": "Neural machine translation (NMT) often makes mistakes in translating low-frequency content words that are essential to understanding the meaning of the sentence. We propose a method to alleviate this problem by augmenting NMT systems with discrete translation lexicons that efficiently encode translations of these low-frequency words. We describe a method to calculate the lexicon probability of the next word in the translation candidate by using the attention vector of the NMT model to select which source word lexical probabilities the model should focus on. We test two methods to combine this probability with the standard NMT probability: (1) using it as a bias, and (2) linear interpolation. Experiments on two corpora show an improvement of 2.0-2.3 BLEU and 0.13-0.44 NIST score, and faster convergence time.",
      "year": 2016,
      "influentialCitationCount": 12,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://doi.org/10.18653/v1/d16-1162",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1606.02006",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "144332819",
          "name": "Philip Arthur"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "ddfd297531f56121b8383bd1eb2bb09189ab2e2b",
      "externalIds": {
        "MAG": "2507561499",
        "DBLP": "conf/interspeech/DoSNN16",
        "DOI": "10.21437/Interspeech.2016-898",
        "CorpusId": 8158818
      },
      "url": "https://www.semanticscholar.org/paper/ddfd297531f56121b8383bd1eb2bb09189ab2e2b",
      "title": "Transferring Emphasis in Speech Translation Using Hard-Attentional Neural Network Models",
      "abstract": "While traditional speech translation systems are oblivious to paralinguistic information, there has been a recent focus on speech translation systems that transfer not only the linguistic content but also emphasis information across languages. A recent work has tried to tackle this task by developing a method for mapping emphasis between languages utilizing conditional random fields (CRFs). Although CRFs allow for consideration of rich features and local context, they have difficulty in handling continuous variables, and cannot capture long-distance dependencies easily. In this paper, we propose a new model for emphasis transfer in speech translation using an approach based on neural networks. The proposed model can handle long-distance dependencies by using long short-term memory (LSTM) neural networks, and is able to handle continuous emphasis values through a novel hard-attention mechanism, which uses word alignments to decide which emphasis values to map from the source to the target sentence. Our experiments on the emphasis translation task showed a significant improvement of the proposed model over the previous state-of-the-art model by 4% target-language emphasis prediction F -measure according to objective evaluation and 2% F -measure according to subjective evaluation.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2533-2537"
      },
      "authors": [
        {
          "authorId": "2527751",
          "name": "Quoc Truong Do"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "dfa7bdea128b899d348ed32a84a7ccb1da4340e4",
      "externalIds": {
        "DBLP": "journals/ieicet/NioSNYN16",
        "MAG": "2525257151",
        "DOI": "10.1587/TRANSINF.2016SLP0018",
        "CorpusId": 51998252
      },
      "url": "https://www.semanticscholar.org/paper/dfa7bdea128b899d348ed32a84a7ccb1da4340e4",
      "title": "Neural Network Approaches to Dialog Response Retrieval and Generation",
      "abstract": "In this work, we propose a new statistical model for building robust dialog systems using neural networks to either retrieve or generate dialog response based on an existing data sources. In the retrieval task, we propose an approach that uses paraphrase identification during the retrieval process. This is done by employing recursive autoencoders and dynamic pooling to determine whether two sentences with arbitrary length have the same meaning. For both the generation and retrieval tasks, we propose a model using long short term memory (LSTM) neural networks that works by first using an LSTM encoder to read in the user\u2019s utterance into a continuous vector-space representation, then using an LSTM decoder to generate the most probable word sequence. An evaluation based on objective and subjective metrics shows that the new proposed approaches have the ability to deal with user inputs that are not well covered in the database compared to standard example-based dialog baselines. key words: example-based dialog system, dialog system, response retrieval, response generation, long short term memory neural network",
      "year": 2016,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.jstage.jst.go.jp/article/transinf/E99.D/10/E99.D_2016SLP0018/_pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "99-D",
        "pages": "2508-2517",
        "name": "IEICE Trans. Inf. Syst."
      },
      "authors": [
        {
          "authorId": "2115734",
          "name": "Lasguido Nio"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "e2198b039ee5bfa233cf06e65f26a9f3233ada9f",
      "externalIds": {
        "MAG": "2603902376",
        "CorpusId": 64680577
      },
      "url": "https://www.semanticscholar.org/paper/e2198b039ee5bfa233cf06e65f26a9f3233ada9f",
      "title": "Word and Dialogue Act Entrainment Analysis based on User Profile",
      "abstract": "Patterns of dialogue act and word selection are observable in dialogue. Entrainment is the factor that might account for these patterns. We test the entrainment hypotheses using the switchboard corpus, comparing speech of different speakers from different parts of the dialogue, but also speech of the same speaker at different points. Our findings replicate previous studies that dialogue participants converge toward each other in word choice, but we also investigate novel measures of entrainment of dialogue act selection, and word choice for specific dialogue acts. These studies inform a design for dialogue systems that would show human-like degrees of entrainment.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "2387338",
          "name": "M. Mizukami"
        },
        {
          "authorId": "144518646",
          "name": "D. Traum"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "e2ebf18e0b88752bd3ff905d2fba74213dcd2c51",
      "externalIds": {
        "MAG": "2563698958",
        "DBLP": "conf/eusipco/TanakaTNN16",
        "DOI": "10.1109/EUSIPCO.2016.7760465",
        "CorpusId": 18880839
      },
      "url": "https://www.semanticscholar.org/paper/e2ebf18e0b88752bd3ff905d2fba74213dcd2c51",
      "title": "Real-time vibration control of an electrolarynx based on statistical F0 contour prediction",
      "abstract": "An electrolarynx is a speaking aid device to artificially generate excitation sounds to help laryngectomees produce electrolaryngeal (EL) speech. Although EL speech is quite intelligible, its naturalness significantly suffers from the unnatural fundamental frequency (F0) patterns of the mechanical excitation sounds. To make it possible to produce more naturally sounding EL speech, we have proposed a method to automatically control F0 patterns of the excitation sounds generated from the electrolarynx based on the statistical F0 prediction, which predicts F0 patterns from the produced EL speech in real-time. In our previous work, we have developed a prototype system by implementing the proposed real-time prediction method in an actual, physical electrolarynx, and through the use of the prototype system, we have found that improvements of the naturalness of EL speech yielded by the prototype system tend to be lower than that yielded by the batch-type prediction. In this paper, we examine negative impacts caused by latency of the real-time prediction on the F0 prediction accuracy, and to alleviate them, we also propose two methods, 1) modeling of segmented continuous F0 (CF0) patterns and 2) prediction of forthcoming F0 values. The experimental results demonstrate that 1) the conventional real-time prediction method needs a large delay to predict CF0 patterns and 2) the proposed methods have positive impacts on the real-time prediction.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1333-1337",
        "name": "2016 24th European Signal Processing Conference (EUSIPCO)"
      },
      "authors": [
        {
          "authorId": "2109245369",
          "name": "Kou Tanaka"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "e8135016ff3bd33ace936e50247fd650fcc58a7a",
      "externalIds": {
        "DBLP": "journals/corr/Neubig16",
        "MAG": "2533220144",
        "ArXiv": "1610.06542",
        "ACL": "W16-4610",
        "CorpusId": 10128510
      },
      "url": "https://www.semanticscholar.org/paper/e8135016ff3bd33ace936e50247fd650fcc58a7a",
      "title": "Lexicons and Minimum Risk Training for Neural Machine Translation: NAIST-CMU at WAT2016",
      "abstract": "This year, the Nara Institute of Science and Technology (NAIST)/Carnegie Mellon University (CMU) submission to the Japanese-English translation track of the 2016 Workshop on Asian Translation was based on attentional neural machine translation (NMT) models. In addition to the standard NMT model, we make a number of improvements, most notably the use of discrete translation lexicons to improve probability estimates, and the use of minimum risk training to optimize the MT system for BLEU score. As a result, our system achieved the highest translation evaluation scores for the task.",
      "year": 2016,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1610.06542",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "f2818da69bb72526fff9d601677db38f24a62ecc",
      "externalIds": {
        "MAG": "2571306822",
        "DOI": "10.1527/TJSAI.DSF-517",
        "CorpusId": 64215699
      },
      "url": "https://www.semanticscholar.org/paper/f2818da69bb72526fff9d601677db38f24a62ecc",
      "title": "Example Based Dialogue System Based on Satisfaction Prediction",
      "abstract": "In dialogue systems, dialogue modeling is one of the most important factors contributing to user satisfaction. Especially in example-based dialogue modeling (EBDM), effective methods for dialog example databases and selecting response utterances from examples improve dialogue quality. Conventional EBDM-based systems use example database consisting of pair of user query and system response. However, the best responses for the same user query are different depending on the user\u2019s preference. We propose an EBDM framework that predicts user satisfaction to select the best system response for the user from multiple response candidates. We define two methods for user satisfaction prediction; prediction using user query and system response pairs, and prediction using user feedback for the system response. Prediction using query/response pairs allows for evaluation of examples themselves, while prediction using user feedback can be used to adapt the system responses to user feedback. We also propose two response selection methods for example-based dialog, one static and one user adaptive, based on these satisfaction prediction methods. Experimental results showed that the proposed methods can estimate user satisfaction and adapt to user preference, improving user satisfaction score.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.jstage.jst.go.jp/article/tjsai/31/1/31_DSF-517/_pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "31",
        "name": "Transactions of The Japanese Society for Artificial Intelligence"
      },
      "authors": [
        {
          "authorId": "2387338",
          "name": "M. Mizukami"
        },
        {
          "authorId": "2115734",
          "name": "Lasguido Nio"
        },
        {
          "authorId": "49558617",
          "name": "Hideaki Kizuki"
        },
        {
          "authorId": "2053134639",
          "name": "Toshio Nomura"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "f3271e61dc0507183ee399393129d7888c2f82b9",
      "externalIds": {
        "ACL": "C16-1292",
        "DBLP": "conf/coling/SperberNNSW16",
        "MAG": "2573658159",
        "CorpusId": 12665845
      },
      "url": "https://www.semanticscholar.org/paper/f3271e61dc0507183ee399393129d7888c2f82b9",
      "title": "Lightly Supervised Quality Estimation",
      "abstract": "Evaluating the quality of output from language processing systems such as machine translation or speech recognition is an essential step in ensuring that they are sufficient for practical use. However, depending on the practical requirements, evaluation approaches can differ strongly. Often, reference-based evaluation measures (such as BLEU or WER) are appealing because they are cheap and allow rapid quantitative comparison. On the other hand, practitioners often focus on manual evaluation because they must deal with frequently changing domains and quality standards requested by customers, for which reference-based evaluation is insufficient or not possible due to missing in-domain reference data (Harris et al., 2016). In this paper, we attempt to bridge this gap by proposing a framework for lightly supervised quality estimation. We collect manually annotated scores for a small number of segments in a test corpus or document, and combine them with automatically predicted quality scores for the remaining segments to predict an overall quality estimate. An evaluation shows that our framework estimates quality more reliably than using fully automatic quality estimation approaches, while keeping annotation effort low by not requiring full references to be available for the particular domain.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3103-3113"
      },
      "authors": [
        {
          "authorId": "3011998",
          "name": "Matthias Sperber"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2920247",
          "name": "J. Niehues"
        },
        {
          "authorId": "11126660",
          "name": "Sebastian St\u00fcker"
        },
        {
          "authorId": "1724972",
          "name": "A. Waibel"
        }
      ]
    },
    {
      "paperId": "f430c43018f17cabccd3a2e9258aff3da508afe1",
      "externalIds": {
        "MAG": "2582842948",
        "CorpusId": 63464999
      },
      "url": "https://www.semanticscholar.org/paper/f430c43018f17cabccd3a2e9258aff3da508afe1",
      "title": "Unknown Word Detection in Foreign Language based on Eye Gaze Features by SVM",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "116",
        "pages": "213-218",
        "name": ""
      },
      "authors": [
        {
          "authorId": "72274088",
          "name": "H. Rui"
        },
        {
          "authorId": "65817231",
          "name": "Tanaka Hiroki"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "70636081",
          "name": "Yoshino Koichiro"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "f765b23f0b0d2a196bc0fe562ad24278d0c9cee4",
      "externalIds": {
        "MAG": "2806852391",
        "CorpusId": 125470180
      },
      "url": "https://www.semanticscholar.org/paper/f765b23f0b0d2a196bc0fe562ad24278d0c9cee4",
      "title": "Eve: A Gradient Based Optimization Method with Locally and Globally Adaptive Learning Rates",
      "abstract": "Adaptive gradient methods for stochastic optimization adjust the learning rate for each parameter locally. However, there is also a global learning rate which must be tuned in order to get the best performance. In this paper, we present a new algorithm that adapts the learning rate locally for each parameter separately, and also globally for all parameters together. Specifically, we modify Adam, a popular method for training deep learning models, with a coefficient that captures properties of the objective function. Empirically, we show that our method, which we call Eve, outperforms Adam and other popular methods in training deep neural networks, like convolutional neural networks for image classification, and recurrent neural networks for language tasks.",
      "year": 2016,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Mathematics"
      ],
      "journal": {
        "volume": "",
        "name": "arXiv: Learning"
      },
      "authors": [
        {
          "authorId": "50376014",
          "name": "Hiroaki Hayashi"
        },
        {
          "authorId": "3407381",
          "name": "Jayanth Koushik"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "f82ae0a87cae2f3a43d4c0289d0cdf7ca57461d0",
      "externalIds": {
        "MAG": "3002965978",
        "CorpusId": 217887153
      },
      "url": "https://www.semanticscholar.org/paper/f82ae0a87cae2f3a43d4c0289d0cdf7ca57461d0",
      "title": "\u30cf\u30fc\u30c9\u30a2\u30c6\u30f3\u30b7\u30e7\u30f3\u7528\u3044\u305f\u6ce8\u610f\u578b\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u3088\u308b\u97f3\u58f0\u7ffb\u8a33 | \u6587\u732e\u60c5\u5831 | J-GLOBAL \u79d1\u5b66\u6280\u8853\u7dcf\u5408\u30ea\u30f3\u30af\u30bb\u30f3\u30bf\u30fc",
      "abstract": null,
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "116",
        "pages": "7-8",
        "name": ""
      },
      "authors": [
        {
          "authorId": "1665451048",
          "name": "Truong Do Quoc"
        },
        {
          "authorId": "71957193",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2075534130",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "f8b32c2edcd7ef098ce40b7fd2e68448ac818191",
      "externalIds": {
        "MAG": "2548595117",
        "DBLP": "conf/icmi/HiraokaTSNN16",
        "DOI": "10.1145/2993148.2993167",
        "CorpusId": 14598787
      },
      "url": "https://www.semanticscholar.org/paper/f8b32c2edcd7ef098ce40b7fd2e68448ac818191",
      "title": "Personalized unknown word detection in non-native language reading using eye gaze",
      "abstract": "This paper proposes a method to detect unknown words during natural reading of non-native language text by using eye-tracking features. A previous approach utilizes gaze duration and word rarity features to perform this detection. However, while this system can be used by trained users, its performance is not sufficient during natural reading by untrained users. In this paper, we 1) apply support vector machines (SVM) with novel eye movement features that were not considered in the previous work and 2) examine the effect of personalization. The experimental results demonstrate that learning using SVMs and proposed eye movement features improves detection performance as measured by F-measure and that personalization further improves results.",
      "year": 2016,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "name": "Proceedings of the 18th ACM International Conference on Multimodal Interaction"
      },
      "authors": [
        {
          "authorId": "7529805",
          "name": "Rui Hiraoka"
        },
        {
          "authorId": "50426585",
          "name": "Hiroki Tanaka"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "fb38451ff87254ac1ff15e79154ef958b4efb6a6",
      "externalIds": {
        "MAG": "3088771895",
        "CorpusId": 226944649
      },
      "url": "https://www.semanticscholar.org/paper/fb38451ff87254ac1ff15e79154ef958b4efb6a6",
      "title": "Practical Neural Networks for NLP: From Theory to Code",
      "abstract": "This tutorial aims to bring NLP researchers up to speed with the current techniques in deep learning and neural networks, and show them how they can turn their ideas into practical implementations. We will start with simple classification models (logistic regression and multilayer perceptrons) and cover more advanced patterns that come up in NLP such as recurrent networks for sequence tagging and prediction problems, structured networks (e.g., compositional architectures based on syntax trees), structured output spaces (sequences and trees), attention for sequence-to-sequence transduction, and feature induction for complex algorithm states. A particular emphasis will be on learning to represent complex objects as recursive compositions of simpler objects. This representation will reflect characterize standard objects in NLP, such as the composition of characters and morphemes into words, and words into sentences and documents. In addition, new opportunities such as learning to embed \"algorithm states\" such as those used in transition-based parsing and other sequential structured prediction models (for which effective features may be difficult to engineer by hand) will be covered.Everything in the tutorial will be grounded in code \u2014 we will show how to program seemingly complex neural-net models using toolkits based on the computation-graph formalism. Computation graphs decompose complex computations into a DAG, with nodes representing inputs, target outputs, parameters, or (sub)differentiable functions (e.g., \"tanh\", \"matrix multiply\", and \"softmax\"), and edges represent data dependencies. These graphs can be run \"forward\" to make predictions and compute errors (e.g., log loss, squared error) and then \"backward\" to compute derivatives with respect to model parameters. In particular we'll cover the Python bindings of the CNN library. CNN has been designed from the ground up for NLP applications, dynamically structured NNs, rapid prototyping, and a transparent data and execution model.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "1745899",
          "name": "Chris Dyer"
        },
        {
          "authorId": "79775260",
          "name": "Yoav Goldberg"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "fce10a1a9727cbda33d44b62409e303f1009417a",
      "externalIds": {
        "MAG": "2950191064",
        "ACL": "E17-1117",
        "DBLP": "journals/corr/KuncoroBKDNS16",
        "ArXiv": "1611.05774",
        "DOI": "10.18653/V1/E17-1117",
        "CorpusId": 5545615
      },
      "url": "https://www.semanticscholar.org/paper/fce10a1a9727cbda33d44b62409e303f1009417a",
      "title": "What Do Recurrent Neural Network Grammars Learn About Syntax?",
      "abstract": "Recurrent neural network grammars (RNNG) are a recently proposed probablistic generative modeling family for natural language. They show state-of-the-art language modeling and parsing performance. We investigate what information they learn, from a linguistic perspective, through various ablations to the model and the data, and by augmenting the model with an attention mechanism (GA-RNNG) to enable closer inspection. We find that explicit modeling of composition is crucial for achieving the best performance. Through the attention mechanism, we find that headedness plays a central role in phrasal representation (with the model\u2019s latent attention largely agreeing with predictions made by hand-crafted head rules, albeit with some important differences). By training grammars without nonterminal labels, we find that phrasal representations depend minimally on nonterminals, providing support for the endocentricity hypothesis.",
      "year": 2016,
      "influentialCitationCount": 16,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/E17-1117.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1611.05774",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "144365875",
          "name": "Noah A. Smith"
        },
        {
          "authorId": "1745899",
          "name": "Chris Dyer"
        },
        {
          "authorId": "143668305",
          "name": "Miguel Ballesteros"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "47648549",
          "name": "Lingpeng Kong"
        },
        {
          "authorId": "3376845",
          "name": "A. Kuncoro"
        }
      ]
    },
    {
      "paperId": "fe54832083f65eade8e2847627d330a24df22488",
      "externalIds": {
        "DOI": "10.1109/EMBC.2016.7591538",
        "CorpusId": 54549060,
        "PubMed": "28269100"
      },
      "url": "https://www.semanticscholar.org/paper/fe54832083f65eade8e2847627d330a24df22488",
      "title": "Removing noise from event-related potentials using a probabilistic generative model with grouped covariance matrices.",
      "abstract": "Analysis of electroencephalograms (EEG) usually suffers from a variety of noises. In this paper, we propose a new method for background noise removal from single-trial event-related potentials (ERPs) recorded with a multi-channel EEG. An observed signal is separated into multiple signals with a multi-channel Wiener filter, whose coefficients are estimated based on a probabilistic generative model in the time-frequency domain. The main contribution is a method to estimate covariance matrices for each frequency bins of short-time Fourier transform (STFT) representing different spatial spread of a multi-channel EEG signal according to frequencies. An experiment using a pseudo-ERP data set demonstrates the effectiveness of our proposed method.",
      "year": 2016,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Medicine"
      ],
      "journal": {
        "volume": "2016",
        "pages": "\n          3728-3731\n        ",
        "name": "Conference proceedings : ... Annual International Conference of the IEEE Engineering in Medicine and Biology Society. IEEE Engineering in Medicine and Biology Society. Annual Conference"
      },
      "authors": [
        {
          "authorId": "3030934",
          "name": "Hayato Maki"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "00717c695e4a33318fe5655e2b69e1ba8b61f981",
      "externalIds": {
        "DBLP": "conf/asru/DoHSNTN15",
        "MAG": "2289963167",
        "DOI": "10.1109/ASRU.2015.7404858",
        "CorpusId": 2187076
      },
      "url": "https://www.semanticscholar.org/paper/00717c695e4a33318fe5655e2b69e1ba8b61f981",
      "title": "The NAIST ASR system for the 2015 Multi-Genre Broadcast challenge: On combination of deep learning systems using a rank-score function",
      "abstract": "The Multi-Genre Broadcast challenge is an official challenge of the IEEE Automatic Speech Recognition and Understanding Workshop. This paper presents NAISTs contribution to the premiere of this challenge. The presented speech-to-text system for English makes use of various front-ends (e.g., MFCC, i-vector and FBANK), DNN acoustic models and several language models for decoding and rescoring (N-gram, RNNLM). Subsets of the training data with varying sizes were evaluated with respect to the overall training quality. Two speech segmentation systems were developed for the challenge, based on DNNs and GMM-HMMs. Recognition was performed in three stages: Decoding, lattice rescoring and system combination. This paper focuses on the system combination experiments and presents a rank-score based system weighting approach, which gave better performance compared to a normal system combination strategy. The DNN based ASR system trained on MFCC + i-vector features with the sMBR training criterion gives the best performance of 27.8% WER, and thus significantly outperforms the baseline DNN-HMM sMBR yielding 33.7% WER.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "654-659",
        "name": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)"
      },
      "authors": [
        {
          "authorId": "2527751",
          "name": "Quoc Truong Do"
        },
        {
          "authorId": "113469882",
          "name": "Michael Heck"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "00c8d88abef116d8d3d673a28ff4098115cf8da3",
      "externalIds": {
        "DBLP": "books/sp/15/HiraokaNSTN15",
        "MAG": "2340414918",
        "DOI": "10.1007/978-3-319-19291-8_15",
        "CorpusId": 692214
      },
      "url": "https://www.semanticscholar.org/paper/00c8d88abef116d8d3d673a28ff4098115cf8da3",
      "title": "Evaluation of a Fully Automatic Cooperative Persuasive Dialogue System",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "153-167"
      },
      "authors": [
        {
          "authorId": "3027595",
          "name": "Takuya Hiraoka"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "024091a3c0223f27d6456b1a27db18fb08d41e5a",
      "externalIds": {
        "DBLP": "conf/emnlp/ZhangUSNN15",
        "MAG": "2251269336",
        "ACL": "D15-1250",
        "DOI": "10.18653/v1/D15-1250",
        "CorpusId": 16588388
      },
      "url": "https://www.semanticscholar.org/paper/024091a3c0223f27d6456b1a27db18fb08d41e5a",
      "title": "A Binarized Neural Network Joint Model for Machine Translation",
      "abstract": "The neural network joint model (NNJM), which augments the neural network language model (NNLM) with an m-word source context window, has achieved large gains in machine translation accuracy, but also has problems with high normalization cost when using large vocabularies. Training the NNJM with noise-contrastive estimation (NCE), instead of standard maximum likelihood estimation (MLE), can reduce computation cost. In this paper, we propose an alternative to NCE, the binarized NNJM (BNNJM), which learns a binary classifier that takes both the context and target words as input, and can be efficiently trained using MLE. We compare the BNNJM and NNJM trained by NCE on various translation tasks.",
      "year": 2015,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/D15-1250.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2094-2099"
      },
      "authors": [
        {
          "authorId": "4279134",
          "name": "Jingyi Zhang"
        },
        {
          "authorId": "1802277",
          "name": "M. Utiyama"
        },
        {
          "authorId": "1698363",
          "name": "E. Sumita"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "05fb5a180214bf092eeda30baf9f16fb6bd15727",
      "externalIds": {
        "MAG": "2582220667",
        "CorpusId": 63422528
      },
      "url": "https://www.semanticscholar.org/paper/05fb5a180214bf092eeda30baf9f16fb6bd15727",
      "title": "Evaluation and Analysis of Duration Correction for Non-Native Speech Based on Waveform Modification",
      "abstract": "There are several attempts at correcting durational patterns of non-native speech towards language learning. One of the typical approaches modifies a speech parameter sequence with Dynamic Time Warping (DTW) using native speech as the reference, generating corrected speech from the modified speech parameter sequence. Although this approach makes it possible to flexibly modify durational patterns of non-native speech, quality of the corrected speech significantly degrades due to the use of analysis-synthesis process to generate the corrected speech. In this report, we propose a method for correcting durational patterns using direct waveform modification for performing DTW. In calculating a temporal warping function, statistical voice conversion is effectively used to reduce an adverse effect caused by speaker differences. Moreover, phoneme insertion often observed in non-native speech is also handled. We conducted an experimental evaluation using English speech read by Japanese, demonstrating that the proposed method was capable of flexibly modifying durational patterns while avoiding quality degradation caused by the analysis-synthesis process. Furthermore, waveform segments suffering from quality degradation caused by temporal warping was analyzed using the modulation spectrum of spectral parameters.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "115",
        "pages": "19-24",
        "name": ""
      },
      "authors": [
        {
          "authorId": "71779039",
          "name": "Kura Shinya"
        },
        {
          "authorId": "73282804",
          "name": "Takamichi Shinnosuke"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "092687dc06b0b264a524c6d4ea151780ba85a02a",
      "externalIds": {
        "MAG": "1170807523",
        "DBLP": "journals/ieicet/TanakaSNTN15",
        "DOI": "10.1587/TRANSINF.2014EDP7400",
        "CorpusId": 9898281
      },
      "url": "https://www.semanticscholar.org/paper/092687dc06b0b264a524c6d4ea151780ba85a02a",
      "title": "NOCOA+: Multimodal Computer-Based Training for Social and Communication Skills",
      "abstract": "SUMMARY Non-verbal communication incorporating visual, audio, and contextual information is important to make sense of and navigate the social world. Individuals who have trouble with social situations often have difficulty recognizing these sorts of non-verbal social signals. In this article, we propose a training tool NOCOA+ (Non-verbal COmmuniation for Autism plus) that uses utterances in visual and audio modalities in nonverbal communication training. We describe the design of NOCOA+ ,a nd further perform an experimental evaluation in which we examine its potential as a tool for computer-based training of non-verbal communication skills for people with social and communication difficulties. In a series of four experiments, we investigated 1) the effect of temporal context on the ability to recognize social signals in testing context, 2) the effect of modality of presentation of social stimulus on ability to recognize non-verbal information, 3) the correlation between autistic traits as measured by the autism spectrum quotient (AQ) and non-verbal behavior recognition skills measured by NOCOA+ ,4 ) the effectiveness of computer-based training in improving social skills. We found that context information was helpful for recognizing non-verbal behaviors, and the effect of modality was different. The results also showed a significant relationship between the AQ communication and socialization scores and non-verbal communication skills, and that social skills were significantly improved through computer-based training.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "98-D",
        "pages": "1536-1544",
        "name": "IEICE Trans. Inf. Syst."
      },
      "authors": [
        {
          "authorId": "50426585",
          "name": "Hiroki Tanaka"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "0f61621206e363367db85b39e8e4325e425afcb4",
      "externalIds": {
        "MAG": "2401296648",
        "DBLP": "conf/interspeech/KobayashiTNSN15",
        "DOI": "10.21437/Interspeech.2015-580",
        "CorpusId": 761995
      },
      "url": "https://www.semanticscholar.org/paper/0f61621206e363367db85b39e8e4325e425afcb4",
      "title": "Statistical singing voice conversion based on direct waveform modification with global variance",
      "abstract": "This paper presents techniques to improve the quality of voices generated through statistical singing voice conversion with direct waveform modification based on spectrum differential (DIFFSVC). The DIFFSVC method makes it possible to convert singing voice characteristics of a source singer into those of a target singer without using vocoder-based waveform generation. However, quality of the converted singing voice still degrades compared to that of a natural singing voice due to various factors, such as the over-smoothing of the converted spectral parameter trajectory. To alleviate this over-smoothing, we propose a technique to restore the global variance of the converted spectral parameter trajectory within the framework of the DIFFSVC method. We also propose another technique to specifically avoid over-smoothing at unvoiced frames. Results of subjective and objective evaluations demonstrate that the proposed techniques significantly improve speech quality of the converted singing voice while preserving the conversion accuracy of singer identity compared to the conventional DIFFSVC.",
      "year": 2015,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2754-2758"
      },
      "authors": [
        {
          "authorId": "65851830",
          "name": "Kazuhiro Kobayashi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "16326359081a42c0b254ee6be39824fd2db07e48",
      "externalIds": {
        "MAG": "2251349730",
        "ACL": "P15-2094",
        "DBLP": "conf/acl/MiuraNSTN15",
        "DOI": "10.3115/v1/P15-2094",
        "CorpusId": 14599263
      },
      "url": "https://www.semanticscholar.org/paper/16326359081a42c0b254ee6be39824fd2db07e48",
      "title": "Improving Pivot Translation by Remembering the Pivot",
      "abstract": "Pivot translation allows for translation of language pairs with little or no parallel data by introducing a third language for which data exists. In particular, the triangulation method, which translates by combining source-pivot and pivot-target translation models into a source-target model, is known for its high translation accuracy. However, in the conventional triangulation method, information of pivot phrases is forgotten and not used in the translation process. In this paper, we propose a novel approach torememberthe pivot phrases in the triangulation stage, and use a pivot language model as an additional information source at translation time. Experimental results on the Europarl corpus showed gains of 0.4-1.2 BLEU points in all tested combinations of languages 1 .",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P15-2094.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "573-577"
      },
      "authors": [
        {
          "authorId": "31504732",
          "name": "Akiva Miura"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "205d67dfe0112df846bc4b221fa2665b0434d441",
      "externalIds": {
        "MAG": "2800320125",
        "CorpusId": 188396536
      },
      "url": "https://www.semanticscholar.org/paper/205d67dfe0112df846bc4b221fa2665b0434d441",
      "title": "Proceedings of the 2nd Workshop on Asian Translation (WAT2015)",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Political Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "40948140",
          "name": "Toshiaki Nakazawa"
        },
        {
          "authorId": "40315879",
          "name": "Hideya Mino"
        },
        {
          "authorId": "2205832",
          "name": "Isao Goto"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1795664",
          "name": "S. Kurohashi"
        },
        {
          "authorId": "1698363",
          "name": "E. Sumita"
        }
      ]
    },
    {
      "paperId": "22b6e88a2f234fc5646f6239f9040a776e841a97",
      "externalIds": {
        "DBLP": "conf/iwslt/AdamsNCB15",
        "ACL": "2015.iwslt-papers.18",
        "CorpusId": 201855895
      },
      "url": "https://www.semanticscholar.org/paper/22b6e88a2f234fc5646f6239f9040a776e841a97",
      "title": "Inducing bilingual lexicons from small quantities of sentence-aligned phonemic transcriptions",
      "abstract": "We investigate induction of a bilingual lexicon from a corpus of phonemic transcriptions that have been sentence-aligned with English translations. We evaluate existing models that have been used for this purpose and report on two additional models, which demonstrate performance improvements. The first performs monolingual segmentation followed by alignment, while the second performs both tasks jointly. We show that monolingual and bilingual lexical entries can be learnt with high precision from corpora having just 1k\u201310k sentences. We explain how our results support the application of alignment algorithms to the task of documenting endangered languages.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": null,
      "authors": [
        {
          "authorId": "38535429",
          "name": "Oliver Adams"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "143620680",
          "name": "Trevor Cohn"
        },
        {
          "authorId": "21308992",
          "name": "Steven Bird"
        }
      ]
    },
    {
      "paperId": "27e1dbe9f7c71cd6cc1b0357f49aef497e572d09",
      "externalIds": {
        "DBLP": "conf/kbse/OdaFNHSTN15",
        "MAG": "2242083635",
        "DOI": "10.1109/ASE.2015.36",
        "CorpusId": 15979705
      },
      "url": "https://www.semanticscholar.org/paper/27e1dbe9f7c71cd6cc1b0357f49aef497e572d09",
      "title": "Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)",
      "abstract": "Pseudo-code written in natural language can aid the comprehension of source code in unfamiliar programming languages. However, the great majority of source code has no corresponding pseudo-code, because pseudo-code is redundant and laborious to create. If pseudo-code could be generated automatically and instantly from given source code, we could allow for on-demand production of pseudo-code without human effort. In this paper, we propose a method to automatically generate pseudo-code from source code, specifically adopting the statistical machine translation (SMT) framework. SMT, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort. In experiments, we generated English or Japanese pseudo-code from Python statements using SMT, and find that the generated pseudo-code is largely accurate, and aids code understanding.",
      "year": 2015,
      "influentialCitationCount": 37,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://naist.repo.nii.ac.jp/record/5127/files/10061_12734.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "574-584",
        "name": "2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)"
      },
      "authors": [
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "2281695",
          "name": "Hiroyuki Fudaba"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145050815",
          "name": "Hideaki Hata"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "3429a6b440fb6f71990bbeda9d097d709634a913",
      "externalIds": {
        "DBLP": "conf/iwslt/MorishitaAHNYN15",
        "ACL": "2015.iwslt-papers.16",
        "MAG": "2564674211",
        "DOI": "10.5715/JNLP.23.353",
        "CorpusId": 34147115
      },
      "url": "https://www.semanticscholar.org/paper/3429a6b440fb6f71990bbeda9d097d709634a913",
      "title": "Parser self-training for syntax-based machine translation",
      "abstract": "In syntax-based machine translation, it is known that the accuracy of parsing greatly affects the translation accuracy. Self-training, which uses parser output as training data, is one method to improve the parser accuracy. However, because parsing errors cause noisy data to be mixed with the training data, automatically generated parse trees do not always contribute to improving accuracy. In this paper, we propose a method for selecting self-training data by performing syntaxbased machine translation using a variety of parse trees, using automatic evaluation metrics to select which translation is better, and using that translation\u2019s parse tree for parser selftraining. This method allows us to automatically choose the trees that contribute to improving translation accuracy, improving the effectiveness of self-training. In experiments, we found that our self-trained parsers significantly improve a state-of-the-art syntax-based machine translation system in two language pairs.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "23",
        "pages": "353-376",
        "name": ""
      },
      "authors": [
        {
          "authorId": "2731589",
          "name": "Makoto Morishita"
        },
        {
          "authorId": "1783871",
          "name": "Koichi Akabe"
        },
        {
          "authorId": "2061275",
          "name": "Yuto Hatakoshi"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "34fc6da7a88433478fd976fd0b9de3cf7134e652",
      "externalIds": {
        "MAG": "2563875945",
        "CorpusId": 63297341
      },
      "url": "https://www.semanticscholar.org/paper/34fc6da7a88433478fd976fd0b9de3cf7134e652",
      "title": "Automated Commucation Training System with Multimodal information",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "115",
        "pages": "45-50",
        "name": ""
      },
      "authors": [
        {
          "authorId": "65817231",
          "name": "Tanaka Hiroki"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "72028057",
          "name": "Negoro Hideki"
        },
        {
          "authorId": "71553524",
          "name": "Iwasaka Hidemi"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "3d1318bc66d534eefac7c665fd7cc891fba27b87",
      "externalIds": {
        "MAG": "1241982707",
        "CorpusId": 195612996
      },
      "url": "https://www.semanticscholar.org/paper/3d1318bc66d534eefac7c665fd7cc891fba27b87",
      "title": "\u6a5f\u68b0\u7ffb\u8a33\u6280\u8853 (\u5c0f\u7279\u96c6 2020\u5e74,\u8a00\u8449\u306e\u58c1\u3092\u8d85\u3048\u308b\u97f3\u58f0\u7ffb\u8a33 : \u65b0\u3057\u3044\u6280\u8853\u3068\u7814\u7a76\u306e\u53ef\u80fd\u6027)",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "98",
        "pages": "718-725",
        "name": "The Journal of Institute of Electronics, Information and Communication Engineers"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "467b14cc8337dd7efe1d374f9a7feb90ae9d2c12",
      "externalIds": {
        "CorpusId": 167214311
      },
      "url": "https://www.semanticscholar.org/paper/467b14cc8337dd7efe1d374f9a7feb90ae9d2c12",
      "title": "An Evaluation of Articulatory Controllable Speech Modification based on Gaussian Mixture Models with Direct Waveform Modification",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "3269936",
          "name": "Patrick Lumban Tobing"
        },
        {
          "authorId": "65851830",
          "name": "Kazuhiro Kobayashi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "49f9afa4d0405019d01b55529ce4167380acc103",
      "externalIds": {
        "DBLP": "conf/interspeech/TobingKTNSN15",
        "MAG": "2406009389",
        "DOI": "10.21437/Interspeech.2015-138",
        "CorpusId": 15904341
      },
      "url": "https://www.semanticscholar.org/paper/49f9afa4d0405019d01b55529ce4167380acc103",
      "title": "Articulatory controllable speech modification based on Gaussian mixture models with direct waveform modification using spectrum differential",
      "abstract": "In our previous work, we have developed a speech modification system capable of manipulating unobserved articulatory movements by sequentially performing speech-to-articulatory inversion mapping and articulatory-to-speech production mapping based on a Gaussian mixture model (GMM)-based statistical feature mapping technique. One of the biggest issues to be addressed in this system is quality degradation of the synthetic speech caused by modeling and conversion errors in a vocoderbased waveform generation framework. To address this issue, we propose several implementation methods of direct waveform modification. The proposed methods directly filter an input speech waveform with a time sequence of spectral differential parameters calculated between unmodified and modified spectral envelop parameters in order to avoid using vocoderbased excitation signal generation. The experimental results show that the proposed direct waveform modification methods yield significantly larger quality improvements in the synthetic speech while also keeping a capability of intuitively modifying phoneme sounds by manipulating the unobserved articulatory movements.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3350-3354"
      },
      "authors": [
        {
          "authorId": "3269936",
          "name": "Patrick Lumban Tobing"
        },
        {
          "authorId": "65851830",
          "name": "Kazuhiro Kobayashi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "4cb3275ec95f4ad407f153aa9dc2d527bc2744e5",
      "externalIds": {
        "CorpusId": 208509021
      },
      "url": "https://www.semanticscholar.org/paper/4cb3275ec95f4ad407f153aa9dc2d527bc2744e5",
      "title": "PROSODY-CONTROLLABLE HMM-BASED SPEECH SYNTHESIS USING SPEECH INPUT",
      "abstract": "This paper proposes an HMM-based speech synthesis system that makes it possible to control the prosody of the synthesized speech through speech input. As creative activities using speech synthesis technologies have been rapidly growing in popularity, there is great demand for interfaces to synthesize speech of a specific target speaker as the users want. The proposed system allows the users to guide prosody of synthetic speech of the target speaker by using their own speech while preserving the original functionality of the HMM-based speech synthesis as a text-to-speech synthesis system. The proposed system consists of 3 main modules: a duration determination module, a F0 modification module, and a speech parameter generation module. The first 2 modules ensure that the duration and F0 of the input speech are reflected in the synthetic speech, and the last module generates synthetic speech parameters according to the determined duration. We examine properties of each module on speech quality and prosodic mimicking ability of synthetic speech, with experimental resulting demonstrate the effectiveness of the proposed system.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "2314594",
          "name": "Y. Nishigaki"
        },
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "55b61befce42280c3d57331121c7d349dd8be4cf",
      "externalIds": {
        "DBLP": "conf/interspeech/MienoNSTN15",
        "MAG": "2395821692",
        "DOI": "10.21437/Interspeech.2015-498",
        "CorpusId": 7925206
      },
      "url": "https://www.semanticscholar.org/paper/55b61befce42280c3d57331121c7d349dd8be4cf",
      "title": "Speed or accuracy? a study in evaluation of simultaneous speech translation",
      "abstract": "Simultaneous speech translation is a technology that attempts to reduce the delay inherent in speech translation by beginning translation before the end of explicit sentence boundaries. Despite best efforts, there is still often a trade-off between speed and accuracy in these systems, with systems with less delay also achieving lower accuracy. However, somewhat surprisingly, there is no previous work examining the relative importance of speed and accuracy, and thus given two systems with various speeds and accuracies, it is difficult to say with certainty which is better. In this paper, we make the first steps towards evaluation of simultaneous speech translation systems in consideration of both speed and accuracy. We collect user evaluations of speech translation results with different levels of accuracy and delay, and using this data to learn the parameters of an evaluation measure that can judge the trade-off between these two factors. Based on these results, we find that considering both accuracy and delay in the evaluation of speech translation results helps improve correlations with human judgements, and that users placed higher relative importance on reducing delay when results were presented through text, rather than speech.",
      "year": 2015,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2267-2271"
      },
      "authors": [
        {
          "authorId": "47066672",
          "name": "Takashi Mieno"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "55faed1fbb1575ffa2609bdc4490586e30df441a",
      "externalIds": {
        "MAG": "2597273215",
        "ACL": "W15-3057",
        "DBLP": "conf/wmt/SugiyamaMNYSTN15",
        "DOI": "10.18653/v1/W15-3057",
        "CorpusId": 16795042
      },
      "url": "https://www.semanticscholar.org/paper/55faed1fbb1575ffa2609bdc4490586e30df441a",
      "title": "An Investigation of Machine Translation Evaluation Metrics in Cross-lingual Question Answering",
      "abstract": "Through using knowledge bases, question answering (QA) systems have come to be able to answer questions accurately over a variety of topics. However, knowledge bases are limited to only a few major languages, and thus it is often necessary to build QA systems that answer questions in one language based on an information source in another (cross-lingual QA: CLQA). Machine translation (MT) is one tool to achieve CLQA, and it is intuitively clear that a better MT system improves QA accuracy. However, it is not clear whether an MT system that is better for human consumption is also better for CLQA. In this paper, we investigate the relationship between manual and automatic translation evaluation metrics and CLQA accuracy by creating a data set using both manual and machine translations and perform CLQA using this created data set. 1 As a result, we find that QA accuracy is closely related with a metric that considers frequency of words, and as a result of manual analysis, we identify 3 factors of translation results that affect CLQA accuracy.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/W15-3057.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "442-449"
      },
      "authors": [
        {
          "authorId": "3387334",
          "name": "Kyoshiro Sugiyama"
        },
        {
          "authorId": "2387338",
          "name": "M. Mizukami"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "5695847f8ffbb3da078842c3683ef74175eb59e5",
      "externalIds": {
        "CorpusId": 201867156
      },
      "url": "https://www.semanticscholar.org/paper/5695847f8ffbb3da078842c3683ef74175eb59e5",
      "title": "English-Read-By-Japanese Speech Synthesis Preserving Speaker Individuality Based on Partial Correction of Prosody and Phonetic Sounds and Effects of English Proficiency Level on Its Performance",
      "abstract": "Cross-lingual speech synthesis for generating naturally sounding English speech uttered by Japanese speakers based on voice conversion and HMM-based speech synthesis tends to cause the degradation of speaker individuality in synthetic speech compared to intra-lingual speech synthesis. To address this issue, we have proposed an ERJ(English Read by Japanese) speech synthesis method to preserve speaker individuality in synthetic speech and a prosody correction method to improve its naturalness. However, their effectiveness has never been evaluated by native listeners: the effects of each speaker\u2019s English proficiency level on their performance have never been evaluated; and incorrect phonetic sounds of ERJ have never been addressed. In this paper, we evaluate these points by applying the proposed method to multiple speakers with various English proficiency levels and also propose a correction method of some incorrect phonetic sounds based on spectrum swapping for unvoiced consonants. The experimental results demonstrate that (1) the effectiveness of power correction is well confirmed by native listeners; (2) the naturalness of ERJ synthetic speech is successfully improved over various English prociency levels by the prosody correction method; and (3) the proposed phonetic sound correction method is also effective for further improving its naturalness.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "145006640",
          "name": "Yuji Oshima"
        },
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "579e01c3c864cc98e57c728f84fcf553c5b1bcba",
      "externalIds": {
        "MAG": "2597475098",
        "CorpusId": 64173140
      },
      "url": "https://www.semanticscholar.org/paper/579e01c3c864cc98e57c728f84fcf553c5b1bcba",
      "title": "Non-Audible Murmur Enhancement Method using Air- and Body-Conductive Microphones in Noisy Environments and its Evaluation",
      "abstract": "As one of the silent speech interfaces, Non-Audible Murmur (NAM) microphone which can detect an extremely soft whispered voice has been developed. Although NAM is a promising medium for silent speech communication, its intelligibility and naturalness are significantly degraded by acoustic changes caused by body-conductive recording. To address this issue, several enhancement methods based on statistical voice conversion techniques have been proposed, and their effectiveness has been confirmed in quiet environments. However, it can be expected that NAM will be used not only in quiet, but also in noisy environments, and it is thus necessary to develop enhancement methods that will also work in these cases. In this report, we propose a framework for NAM enhancement using the NAM microphone and an air-conductive microphone. Experimental results demonstrate that the proposed framework is capable of significantly improving enhancement performance in noisy environments by considering not only the effect of noise contamination but also speaking style changes caused by the noise.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "115",
        "pages": "59-64",
        "name": ""
      },
      "authors": [
        {
          "authorId": "71178078",
          "name": "Tajiri Yusuke"
        },
        {
          "authorId": "66310397",
          "name": "Tanaka Kou"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "5801974fcebc11b4a8085fb02e77f792454caf7c",
      "externalIds": {
        "DBLP": "conf/iui/TanakaSNTNIN15",
        "MAG": "2122293883",
        "DOI": "10.1145/2678025.2701368",
        "CorpusId": 16695661
      },
      "url": "https://www.semanticscholar.org/paper/5801974fcebc11b4a8085fb02e77f792454caf7c",
      "title": "Automated Social Skills Trainer",
      "abstract": "Social skills training is a well-established method to decrease human anxiety and discomfort in social interaction, and acquire social skills. In this paper, we attempt to automate the process of social skills training by developing a dialogue system named \"automated social skills trainer,\" which provides social skills training through human-computer interaction. The system includes a virtual avatar that recognizes user speech and language information and gives feedback to users to improve their social skills. Its design is based on conventional social skills training performed by human participants, including defining target skills, modeling, role-play, feedback, reinforcement, and homework. An experimental evaluation measuring the relationship between social skill and speech and language features shows that these features have a relationship with autistic traits. Additional experiments measuring the effect of performing social skills training with the proposed application show that most participants improve their skill by using the system for 50 minutes.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "name": "Proceedings of the 20th International Conference on Intelligent User Interfaces"
      },
      "authors": [
        {
          "authorId": "50426585",
          "name": "Hiroki Tanaka"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1867578",
          "name": "Hideki Negoro"
        },
        {
          "authorId": "49429065",
          "name": "H. Iwasaka"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "62a5b47def8d21825d06f7407a505ff0b64ecb1a",
      "externalIds": {
        "MAG": "2295682796",
        "ACL": "Q15-1041",
        "DBLP": "journals/tacl/ArthurNSTN15",
        "DOI": "10.1162/tacl_a_00159",
        "CorpusId": 16170214
      },
      "url": "https://www.semanticscholar.org/paper/62a5b47def8d21825d06f7407a505ff0b64ecb1a",
      "title": "Semantic Parsing of Ambiguous Input through Paraphrasing and Verification",
      "abstract": "We propose a new method for semantic parsing of ambiguous and ungrammatical input, such as search queries. We do so by building on an existing semantic parsing framework that uses synchronous context free grammars (SCFG) to jointly model the input sentence and output meaning representation. We generalize this SCFG framework to allow not one, but multiple outputs. Using this formalism, we construct a grammar that takes an ambiguous input string and jointly maps it into both a meaning representation and a natural language paraphrase that is less ambiguous than the original input. This paraphrase can be used to disambiguate the meaning representation via verification using a language model that calculates the probability of each paraphrase.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "3",
        "pages": "571-584",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "144332819",
          "name": "Philip Arthur"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "634bbe75c34b82e664f1e9f083314b5bdb6ba187",
      "externalIds": {
        "MAG": "2295480719",
        "DBLP": "conf/embc/MakiTSNN15",
        "DOI": "10.1109/EMBC.2015.7318967",
        "CorpusId": 4491204,
        "PubMed": "26736867"
      },
      "url": "https://www.semanticscholar.org/paper/634bbe75c34b82e664f1e9f083314b5bdb6ba187",
      "title": "An evaluation of EEG ocular artifact removal with a multi-channel wiener filter based on probabilistic generative model",
      "abstract": "Data contamination by ocular artifacts such as eye blinks and eye movements is a major barrier that must be overcome when attempting to analyze electroencephalogram (EEG) and event-related potential (ERP) data. To handle this problem, a number of artifact removal methods has been proposed. Specifically, we focus on a method using a multi-channel Wiener filters based on a probabilistic generative model. This method assumes that the observed signal is the sum of multiple signals elicited by psychological or physical events, and separates the observed signal into each event signal using estimated model parameters. Based on this scheme, we have proposed a model parameter estimation method using prior information of each event signal. In this paper, we examine the potential of this model to deal with highly contaminated signals by collecting EEG data intentionally contaminated by eye blinks and relatively clean ERP data, and using them as prior information of each event signal. We conducted an experimental evaluation using a classical attention task. The results showed the proposed method effectively enhances the target ERP component while reducing the contamination caused by eye blinks.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Medicine"
      ],
      "journal": {
        "pages": "2775-2778",
        "name": "2015 37th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC)"
      },
      "authors": [
        {
          "authorId": "3030934",
          "name": "Hayato Maki"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "6413e6a4f68be0ea6aed0082b205147d9f893699",
      "externalIds": {
        "MAG": "2963965928",
        "DBLP": "journals/corr/FaruquiTND15",
        "ArXiv": "1512.06110",
        "ACL": "N16-1077",
        "DOI": "10.18653/v1/N16-1077",
        "CorpusId": 3089175
      },
      "url": "https://www.semanticscholar.org/paper/6413e6a4f68be0ea6aed0082b205147d9f893699",
      "title": "Morphological Inflection Generation Using Character Sequence to Sequence Learning",
      "abstract": "Morphological inflection generation is the task of generating the inflected form of a given lemma corresponding to a particular linguistic transformation. We model the problem of inflection generation as a character sequence to sequence learning problem and present a variant of the neural encoder-decoder model for solving it. Our model is language independent and can be trained in both supervised and semi-supervised settings. We evaluate our system on seven datasets of morphologically rich languages and achieve either better or comparable results to existing state-of-the-art models of inflection generation.",
      "year": 2015,
      "influentialCitationCount": 17,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://doi.org/10.18653/v1/n16-1077",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "634-643"
      },
      "authors": [
        {
          "authorId": "1779225",
          "name": "Manaal Faruqui"
        },
        {
          "authorId": "145317727",
          "name": "Yulia Tsvetkov"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1745899",
          "name": "Chris Dyer"
        }
      ]
    },
    {
      "paperId": "64c575bb8b3e11097605028de5c289b0b2d839a4",
      "externalIds": {
        "MAG": "2395237272",
        "DBLP": "conf/interspeech/OshimaTTNSN15",
        "DOI": "10.21437/Interspeech.2015-121",
        "CorpusId": 16548296
      },
      "url": "https://www.semanticscholar.org/paper/64c575bb8b3e11097605028de5c289b0b2d839a4",
      "title": "Non-native speech synthesis preserving speaker individuality based on partial correction of prosodic and phonetic characteristics",
      "abstract": "This paper presents a novel non-native speech synthesis technique that preserves the individuality of a non-native speaker. Cross-lingual speech synthesis based on voice conversion or HMM-based speech synthesis, which synthesizes foreign language speech of a specific non-native speaker reflecting the speaker-dependent acoustic characteristics extracted from the speaker\u2019s natural speech in his/her mother tongue, tends to cause a degradation of speaker individuality in synthetic speech compared to intra-lingual speech synthesis. This paper proposes a new approach to cross-lingual speech synthesis that preserves speaker individuality by explicitly using non-native speech spoken by the target speaker. Although the use of nonnative speech makes it possible to preserve the speaker individuality in the synthesized target speech, naturalness is significantly degraded as the speech is directly affected by unnatural prosody and pronunciation often caused by differences in the linguistic systems of the source and target languages. To improve naturalness while preserving speaker individuality, we propose (1) a prosodic correction method based on model adaptation, and (2) a phonetic correction method based on spectrum replacement for unvoiced consonants. The experimental results demonstrate that these proposed methods are capable of significantly improving naturalness while preserving the speaker individuality in synthetic speech.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "299-303"
      },
      "authors": [
        {
          "authorId": "145006640",
          "name": "Yuji Oshima"
        },
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "6aac35ec3bfaf7e835ac633414419c9623838007",
      "externalIds": {
        "MAG": "1550985492",
        "DBLP": "conf/icassp/TjandraSNTAN15",
        "DOI": "10.1109/ICASSP.2015.7178827",
        "CorpusId": 15262312
      },
      "url": "https://www.semanticscholar.org/paper/6aac35ec3bfaf7e835ac633414419c9623838007",
      "title": "Combination of two-dimensional cochleogram and spectrogram features for deep learning-based ASR",
      "abstract": "This paper explores the use of auditory features based on cochleograms; two dimensional speech features derived from gammatone filters within the convolutional neural network (CNN) framework. Furthermore, we also propose various possibilities to combine cochleogram features with log-mel filter banks or spectrogram features. In particular, we combine within low and high levels of CNN framework which we refer to as low-level and high-level feature combination. As comparison, we also construct the similar configuration with deep neural network (DNN). Performance was evaluated in the framework of hybrid neural network - hidden Markov model (NN-HMM) system on TIMIT phoneme sequence recognition task. The results reveal that cochleogram-spectrogram feature combination provides significant advantages. The best accuracy was obtained by high-level combination of two dimensional cochleogram-spectrogram features using CNN, achieved up to 8.2% relative phoneme error rate (PER) reduction from CNN single features or 19.7% relative PER reduction from DNN single features.",
      "year": 2015,
      "influentialCitationCount": 4,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "4525-4529",
        "name": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
      },
      "authors": [
        {
          "authorId": "2894428",
          "name": "Andros Tjandra"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1719005",
          "name": "M. Adriani"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "6fae71765a5e86dfef2f93bbe03c4a2e20f827b5",
      "externalIds": {
        "DBLP": "conf/iwslt/HeckDSNN15",
        "ACL": "2015.iwslt-evaluation.17",
        "MAG": "2188409090",
        "CorpusId": 17137326
      },
      "url": "https://www.semanticscholar.org/paper/6fae71765a5e86dfef2f93bbe03c4a2e20f827b5",
      "title": "The NAIST English speech recognition system for IWSLT 2015",
      "abstract": "The International Workshop for Spoken Language Translation (IWSLT) is an annual evaluation campaign for core speech processing technologies. This paper presents Nara Institute of Science and Technology\u2019s (NAIST\u2019s) contribution to the English automatic speech recognition (ASR) track for the 2015 evaluation campaign. The ASR systems presented in this paper make use of various frontends, varying deep neural net (DNN) acoustic models and separate language models for decoding and rescoring. Recognition is performed in three stages: Decoding, lattice rescoring and system combination via recognizer output voting error reduction (ROVER). We discuss the application of a rank-score based weighting approach for the system combination. Also, a Gaussian mixture model hidden Markov model (GMM-HMM) based speech/non-speech segmenter makes use of said combination scheme. The primary submission achieves a word error rate (WER) of 9.5% and 10.1% on the official development set, given manual and automatic segmentation respectively.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "113469882",
          "name": "Michael Heck"
        },
        {
          "authorId": "2527751",
          "name": "Quoc Truong Do"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "814421bb20ba1fba88928fc168db1b7175cca6ac",
      "externalIds": {
        "MAG": "2912493453",
        "CorpusId": 86664799
      },
      "url": "https://www.semanticscholar.org/paper/814421bb20ba1fba88928fc168db1b7175cca6ac",
      "title": "Implementation of Direct F0 Control of an Electrolarynx based on Real-time Excitation Prediction",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "115",
        "pages": "47-52",
        "name": ""
      },
      "authors": [
        {
          "authorId": "66310397",
          "name": "Tanaka Kou"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "8163c4010fc103343518d49db5974577593972f6",
      "externalIds": {
        "DBLP": "books/sp/15/TsunomoriNSTN15",
        "MAG": "2338045124",
        "DOI": "10.1007/978-3-319-19291-8_17",
        "CorpusId": 15542674
      },
      "url": "https://www.semanticscholar.org/paper/8163c4010fc103343518d49db5974577593972f6",
      "title": "An Analysis Towards Dialogue-Based Deception Detection",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Psychology"
      ],
      "journal": {
        "pages": "177-187"
      },
      "authors": [
        {
          "authorId": "9530075",
          "name": "Yuiko Tsunomori"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "89ba434b30a3f1b61bcbcf917842899fe3d2eea4",
      "externalIds": {
        "MAG": "2341005300",
        "DBLP": "books/sp/15/MizukamiNSTN15",
        "DOI": "10.1007/978-3-319-19291-8_13",
        "CorpusId": 14272905
      },
      "url": "https://www.semanticscholar.org/paper/89ba434b30a3f1b61bcbcf917842899fe3d2eea4",
      "title": "Linguistic Individuality Transformation for Spoken Language",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "129-143"
      },
      "authors": [
        {
          "authorId": "2387338",
          "name": "M. Mizukami"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "8d64be0d3bb2650ff99a4c1ae8049eb5fece27a1",
      "externalIds": {
        "CorpusId": 201824161
      },
      "url": "https://www.semanticscholar.org/paper/8d64be0d3bb2650ff99a4c1ae8049eb5fece27a1",
      "title": "Bottleneck Features for Emotional Speech Recognition",
      "abstract": "Automatic speech recognition (ASR) system is used for emotional speech. However emotion infulence speech signal. Therefore emotional speech degrades ASR quality. In this study, we focus on bottleneck features for emotional speech recognition. The bottleneck features are made by deep neural network hidden layer which has small number nodes than other layer. We think bottoleneck sturucture can extract features and bottleneck features represent phoneme essential features. By using bottleneck features for emotional speech recognition, we confirm improvement results compared with emotion adaptation model and normal model.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "2235899",
          "name": "Kohei Mukaihara"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "8ef0ca924ae88ac2bb2803c49589722b52efc5b4",
      "externalIds": {
        "MAG": "2207442312",
        "DBLP": "conf/ococosda/LubisSNTN15",
        "DOI": "10.1109/ICSDA.2015.7357892",
        "CorpusId": 23188412
      },
      "url": "https://www.semanticscholar.org/paper/8ef0ca924ae88ac2bb2803c49589722b52efc5b4",
      "title": "Construction and analysis of social-affective interaction corpus in English and Indonesian",
      "abstract": "Social-affective aspects of interaction play a vital role in making human communication a rich and dynamic experience. Observation of complex emotional phenomena requires rich sets of labeled data of natural interaction. Although there has been an increase of interest in constructing corpora containing social interactions, there is still a lack of spontaneous and emotionally rich corpora. This paper presents a corpus of social-affective interactions in English and Indonesian, constructed from various television talk shows, containing natural conversations and real emotion occurrences. We carefully annotate the corpus in terms of emotion and discourse structure to allow for the aforementioned observation. The corpus is still in its early stage of development, yielding wide-ranging possibilities for future work.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Psychology"
      ],
      "journal": {
        "pages": "202-206",
        "name": "2015 International Conference Oriental COCOSDA held jointly with 2015 Conference on Asian Spoken Language Research and Evaluation (O-COCOSDA/CASLRE)"
      },
      "authors": [
        {
          "authorId": "143604111",
          "name": "Nurul Lubis"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "9b1057e1f6eb17abf3962d6cd2f49468d27b94c6",
      "externalIds": {
        "DBLP": "conf/iwslt/DoSNTN15",
        "ACL": "2015.iwslt-papers.12",
        "CorpusId": 198949765
      },
      "url": "https://www.semanticscholar.org/paper/9b1057e1f6eb17abf3962d6cd2f49468d27b94c6",
      "title": "Improving translation of emphasis with pause prediction in speech-to-speech translation systems",
      "abstract": "Prosodic emphasis is a vital element of speech-based communicating, and machine translation of emphasis has been an active research target. For example, there is some previous work on translation of word-level emphasis through the cross-lingual transfer of F0, power, or duration. However, no previous work has covered a type of information that might have a large potential benefit in emphasizing speech, pauses between words. In this paper, we first investigate the importance of pauses in emphasizing speech by analyzing the number of pauses inserted surrounding emphasized words. Then, we develop a pause prediction model that can be integrated into an existing emphasis translation system. Experiments showed that the proposed emphasis translation system integrating the pause prediction model made it easier for human listeners to identify emphasis in the target language, with an overall gain of 2% in human subjects\u2019 emphasis prediction F -measure.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": null,
      "authors": [
        {
          "authorId": "2527751",
          "name": "Quoc Truong Do"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "9bd6cdae71506eb307507e44df7abe0c285b3ca7",
      "externalIds": {
        "MAG": "2953080212",
        "DBLP": "journals/corr/NeubigMN15",
        "ArXiv": "1510.05203",
        "ACL": "W15-5003",
        "CorpusId": 110317
      },
      "url": "https://www.semanticscholar.org/paper/9bd6cdae71506eb307507e44df7abe0c285b3ca7",
      "title": "Neural Reranking Improves Subjective Quality of Machine Translation: NAIST at WAT2015",
      "abstract": "This year, the Nara Institute of Science and Technology (NAIST)'s submission to the 2015 Workshop on Asian Translation was based on syntax-based statistical machine translation, with the addition of a reranking component using neural attentional machine translation models. Experiments re-confirmed results from previous work stating that neural MT reranking provides a large gain in objective evaluation measures such as BLEU, and also confirmed for the first time that these results also carry over to manual evaluation. We further perform a detailed analysis of reasons for this increase, finding that the main contributions of the neural models lie in improvement of the grammatical correctness of the output, as opposed to improvements in lexical choice of content words.",
      "year": 2015,
      "influentialCitationCount": 9,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "abs/1510.05203",
        "name": "ArXiv"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2731589",
          "name": "Makoto Morishita"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "9f832bdcbc9d9566f7ab07b7455364bee62086fb",
      "externalIds": {
        "MAG": "2240848044",
        "DBLP": "conf/kbse/FudabaOANHSTN15",
        "DOI": "10.1109/ASE.2015.107",
        "CorpusId": 252711
      },
      "url": "https://www.semanticscholar.org/paper/9f832bdcbc9d9566f7ab07b7455364bee62086fb",
      "title": "Pseudogen: A Tool to Automatically Generate Pseudo-Code from Source Code",
      "abstract": "Understanding the behavior of source code written in an unfamiliar programming language is difficult. One way to aid understanding of difficult code is to add corresponding pseudo-code, which describes in detail the workings of the code in a natural language such as English. In spite of its usefulness, most source code does not have corresponding pseudo-code because it is tedious to create. This paper demonstrates a tool Pseudogen that makes it possible to automatically generate pseudo-code from source code using statistical machine translation (SMT). Pseudogen currently supports generation of English or Japanese pseudo-code from Python source code, and the SMT framework makes it easy for users to create new generators for their preferred source code/pseudo-code pairs.",
      "year": 2015,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://naist.repo.nii.ac.jp/record/5128/files/10061_12735.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "824-829",
        "name": "2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)"
      },
      "authors": [
        {
          "authorId": "2281695",
          "name": "Hiroyuki Fudaba"
        },
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "1783871",
          "name": "Koichi Akabe"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145050815",
          "name": "Hideaki Hata"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "a0f00d5ea3727151b1c2fc8c407404f0c6641051",
      "externalIds": {
        "DBLP": "conf/naacl/OdaNSTN15",
        "ACL": "N15-3009",
        "MAG": "2250841445",
        "DOI": "10.3115/v1/N15-3009",
        "CorpusId": 9001123
      },
      "url": "https://www.semanticscholar.org/paper/a0f00d5ea3727151b1c2fc8c407404f0c6641051",
      "title": "Ckylark: A More Robust PCFG-LA Parser",
      "abstract": "This paper describes Ckylark, a PCFG-LA style phrase structure parser that is more robust than other parsers in the genre. PCFG-LA parsers are known to achieve highly competitive performance, but sometimes the parsing process fails completely, and no parses can be generated. Ckylark introduces three new techniques that prevent possible causes for parsing failure: outputting intermediate results when coarse-to-fine analysis fails, smoothing lexicon probabilities, and scaling probabilities to avoid underflow. An experiment shows that this allows millions of sentences can be parsed without any failures, in contrast to other publicly available PCFG-LA parsers. Ckylark is implemented in C++, and is available opensource under the LGPL license.1",
      "year": 2015,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://doi.org/10.3115/v1/n15-3009",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "41-45"
      },
      "authors": [
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "a4ce6cd06bc73d81651f7888efa4337fd82a60f0",
      "externalIds": {
        "DBLP": "books/sp/15/SasakuraSNTN15",
        "MAG": "2337125036",
        "DOI": "10.1007/978-3-319-19291-8_16",
        "CorpusId": 10182392
      },
      "url": "https://www.semanticscholar.org/paper/a4ce6cd06bc73d81651f7888efa4337fd82a60f0",
      "title": "Unknown Word Detection Based on Event-Related Brain Desynchronization Responses",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "169-175"
      },
      "authors": [
        {
          "authorId": "2073379",
          "name": "Takafumi Sasakura"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "a56dba9cabfc110df231051d7c9d6e439f6757dd",
      "externalIds": {
        "DBLP": "conf/pacling/MoriNNS15",
        "MAG": "2293644338",
        "DOI": "10.1007/978-981-10-0515-2_1",
        "CorpusId": 17024666
      },
      "url": "https://www.semanticscholar.org/paper/a56dba9cabfc110df231051d7c9d6e439f6757dd",
      "title": "Pointwise Prediction and Sequence-Based Reranking for Adaptable Part-of-Speech Tagging",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "3-17"
      },
      "authors": [
        {
          "authorId": "144873535",
          "name": "Shinsuke Mori"
        },
        {
          "authorId": "2056151867",
          "name": "Yosuke Nakata"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "46510146",
          "name": "Tetsuro Sasada"
        }
      ]
    },
    {
      "paperId": "abcaec70b463ed925c29180437ed581c971952cf",
      "externalIds": {
        "DBLP": "conf/asru/MizukamiKNNYSTN15",
        "MAG": "2291522004",
        "DOI": "10.1109/ASRU.2015.7404868",
        "CorpusId": 8998508
      },
      "url": "https://www.semanticscholar.org/paper/abcaec70b463ed925c29180437ed581c971952cf",
      "title": "Adaptive selection from multiple response candidates in example-based dialogue",
      "abstract": "In spoken dialogue systems, dialogue modeling is one of the most important factors for contributing to user satisfaction improvement. Especially in Example-Based Dialogue Modeling (EBDM), effective methods to build dialogue example databases and to select response utterances from examples are the keys for improving dialogue quality. In dialogue corpora, it often have plural appropriate responses for one utterance. However, the system merges these plural appropriate responses into the one system response, thus, it does not try to use plural responses properly by user preference. In fact, responses that each user thinks to be preferable are different. In this paper, we propose a framework that select an appropriate response from plural appropriate response candidates satisfies users. It has a multi-response example database, and selects an appropriate response based on collaborative filtering. Experimental results showed that the proposed framework were successfully choosing appropriate responses, considering multi-response candidates improves user satisfaction to 4.1 from 3.7 of single response, and the adaptive response selection method increased user satisfaction from 3.7 to 4.3.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "784-790",
        "name": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)"
      },
      "authors": [
        {
          "authorId": "2387338",
          "name": "M. Mizukami"
        },
        {
          "authorId": "49558617",
          "name": "Hideaki Kizuki"
        },
        {
          "authorId": "2053134639",
          "name": "Toshio Nomura"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "b2b0fbf9033f1c36bea8bb11c173f14378c60db9",
      "externalIds": {
        "CorpusId": 201857608
      },
      "url": "https://www.semanticscholar.org/paper/b2b0fbf9033f1c36bea8bb11c173f14378c60db9",
      "title": "Study on Word-Level Emphasis Across English and Japanese \u2217 \u2606",
      "abstract": "Speech-to-speech (S2S) translation systems [1] allow us to communicate with other people in different languages. However, conventional S2S systems ignore emphasis, which is an important factor of paralinguistic information. In order to construct a S2S system considering emphasis, it is important to study on how emphasis is expressed in each language and also across languages. In this paper, we analyzed emphasis is word-level. We first estimate a real-numbered value of word-level emphasis using many speech features such as F0, duration and power. Based on this estimation we perform an analysis of how emphasis is expressed in individual languages and also across languages.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "70459053",
          "name": "D. Q. Truong"
        },
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "b4bc1a98eb79545f8da4385a6dfb643b0c62a07e",
      "externalIds": {
        "MAG": "2251766657",
        "ACL": "P15-1020",
        "DBLP": "conf/acl/OdaNSTN15",
        "DOI": "10.3115/v1/P15-1020",
        "CorpusId": 17272416
      },
      "url": "https://www.semanticscholar.org/paper/b4bc1a98eb79545f8da4385a6dfb643b0c62a07e",
      "title": "Syntax-based Simultaneous Translation through Prediction of Unseen Syntactic Constituents",
      "abstract": "Simultaneous translation is a method to reduce the latency of communication through machine translation (MT) by dividing the input into short segments before performing translation. However, short segments pose problems for syntaxbased translation methods, as it is difficult to generate accurate parse trees for sub-sentential segments. In this paper, we perform the first experiments applying syntax-based SMT to simultaneous translation, and propose two methods to prevent degradations in accuracy: a method to predict unseen syntactic constituents that help generate complete parse trees, and a method that waits for more input when the current utterance is not enough to generate a fluent translation. Experiments on English-Japanese translation show that the proposed methods allow for improvements in accuracy, particularly with regards to word order of the target sentences.",
      "year": 2015,
      "influentialCitationCount": 4,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.aclweb.org/anthology/P15-1020.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "198-207"
      },
      "authors": [
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "be0ad0710bfb09f6c875dd6cd834ac643713c93d",
      "externalIds": {
        "MAG": "2595549297",
        "CorpusId": 185438339
      },
      "url": "https://www.semanticscholar.org/paper/be0ad0710bfb09f6c875dd6cd834ac643713c93d",
      "title": "\u81ea\u7136\u8a00\u8a9e\u51e6\u7406\u6280\u8853\u306e\u73fe\u72b6\u3068\u5c55\u671b -\u30a8\u30e9\u30fc\u5206\u6790\u30d7\u30ed\u30b8\u30a7\u30af\u30c8\u3092\u901a\u3057\u3066-\uff1a\uff3b\u7ffb\u8a33\uff0c\u6587\u4f5c\u6210\u652f\u63f4\uff0c\u5bfe\u8a71\uff3d3.14 \u7ffb\u8a33",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "57",
        "pages": "36-37",
        "name": ""
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "69522967",
          "name": "\u62d3 \u5de5\u85e4"
        },
        {
          "authorId": "149130255",
          "name": "\u6643\u4e00 \u8d64\u90e8"
        }
      ]
    },
    {
      "paperId": "c02da00857c33fa39b115c0eb6c655ff6cf96878",
      "externalIds": {
        "DBLP": "conf/aclwat/NakazawaMGNKS15",
        "ACL": "W15-5001",
        "CorpusId": 219304806
      },
      "url": "https://www.semanticscholar.org/paper/c02da00857c33fa39b115c0eb6c655ff6cf96878",
      "title": "Overview of the 2nd Workshop on Asian Translation",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 10,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-28"
      },
      "authors": [
        {
          "authorId": "40948140",
          "name": "Toshiaki Nakazawa"
        },
        {
          "authorId": "40315879",
          "name": "Hideya Mino"
        },
        {
          "authorId": "2205832",
          "name": "Isao Goto"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1795664",
          "name": "S. Kurohashi"
        },
        {
          "authorId": "1698363",
          "name": "E. Sumita"
        }
      ]
    },
    {
      "paperId": "c0cce8955bf10b21753161ffaa1978a7c8b78a16",
      "externalIds": {
        "DBLP": "conf/interspeech/TajiriTTNSN15",
        "MAG": "2395552930",
        "DOI": "10.21437/Interspeech.2015-583",
        "CorpusId": 15018401
      },
      "url": "https://www.semanticscholar.org/paper/c0cce8955bf10b21753161ffaa1978a7c8b78a16",
      "title": "Non-audible murmur enhancement based on statistical conversion using air- and body-conductive microphones in noisy environments",
      "abstract": "Non-Audible Murmur (NAM) is an extremely soft whispered voice detected by a special body-conductive microphone called a NAM microphone. Although NAM is a promising medium for silent speech communication, its quality is significantly degraded by its faint volume and spectral changes caused by body-conductive recording. To improve the quality of NAM, several enhancement methods based on statistical voice conversion (VC) techniques have been proposed, and their effectiveness has been confirmed in quiet environments. However, it can be expected that NAM will be used not only in quiet, but also in noisy environments, and it is thus necessary to develop enhancement methods that will also work in these cases. In this paper, we propose a framework for NAM enhancement using not only the NAM microphone but also an air-conductive microphone. Airand body-conducted NAM signals are used as the input of VC to estimate a more naturally sounding speech signal. To clarify adverse effects of external noises on the performance of the proposed framework and investigate a possibility to alleviate them by revising VC models, we also implement noise-dependent VC models within the proposed framework. Experimental results demonstrate that the proposed framework yields significant improvements in the spectral conversion accuracy and listenability of enhanced speech under both quiet and noisy environments.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2769-2773"
      },
      "authors": [
        {
          "authorId": "40410236",
          "name": "Y. Tajiri"
        },
        {
          "authorId": "2109245369",
          "name": "Kou Tanaka"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "c37ecbccecab1774b545a5a5804b575718218f7d",
      "externalIds": {
        "CorpusId": 201825000
      },
      "url": "https://www.semanticscholar.org/paper/c37ecbccecab1774b545a5a5804b575718218f7d",
      "title": "Exploring CNN and DNN Bottleneck Features for Emotional Speech Recognition",
      "abstract": "Emotion influences the speech and degrades. Therefore emotional speech degrades ASR quality due to the mismatch between input speech and the acoustic model. In this study, we focus on feature transformation methods to solve this mismatch. We propose a tandem approach using DNN bottleneck features and CNN bottleneck features for emotional speech recognition. The bottleneck features are made by a deep neural network hidden layer that has a smaller number of nodes than other layers. We hypothesize that bottleneck sturucture can extract features and bottleneck features represent essential features of phonemes. By using bottleneck features for emotional speech recognition, we confirm that results improve results compared with other feature transformation methods. In addition, we combine the proposed methods and other feature transformation methods to improve emotional speech recognition.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "2235899",
          "name": "Kohei Mukaihara"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "c4536a5c7f47bfc48df202ba882002531248f955",
      "externalIds": {
        "MAG": "2041370616",
        "DBLP": "conf/assets/TanakaTNSN15",
        "DOI": "10.1145/2700648.2811340",
        "CorpusId": 16932612
      },
      "url": "https://www.semanticscholar.org/paper/c4536a5c7f47bfc48df202ba882002531248f955",
      "title": "An Enhanced Electrolarynx with Automatic Fundamental Frequency Control based on Statistical Prediction",
      "abstract": "An electrolarynx is a type of speaking aid device which is able to mechanically generate excitation sounds to help laryngectomees produce electrolaryngeal (EL) speech. Although EL speech is quite intelligible, its naturalness suffers from monotonous fundamental frequency patterns of the mechanical excitation sounds. To make it possible to generate more natural excitation sounds, we have proposed a method to automatically control the fundamental frequency of the sounds generated by the electrolarynx based on a statistical prediction model, which predicts the fundamental frequency patterns from the produced EL speech in real-time. In this paper, we develop a prototype system by implementing the proposed control method in an actual, physical electrolarynx and evaluate its performance.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "name": "Proceedings of the 17th International ACM SIGACCESS Conference on Computers & Accessibility"
      },
      "authors": [
        {
          "authorId": "2109245369",
          "name": "Kou Tanaka"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "c5a323f8744838093ee36bee3739dea599ce62f0",
      "externalIds": {
        "MAG": "1840346973",
        "CorpusId": 178696077
      },
      "url": "https://www.semanticscholar.org/paper/c5a323f8744838093ee36bee3739dea599ce62f0",
      "title": "D-15-29 NIRS\u3092\u7528\u3044\u305f\u8133\u6d3b\u52d5\u8a08\u6e2c\u306b\u3088\u308b\u8a8d\u77e5\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\u306e\u5ba2\u89b3\u7684\u8a55\u4fa1(D-15.\u6559\u80b2\u5de5\u5b66,\u4e00\u822c\u30bb\u30c3\u30b7\u30e7\u30f3)",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "2015",
        "pages": "208",
        "name": ""
      },
      "authors": [
        {
          "authorId": "136473979",
          "name": "\u84d1\u7530 \u7531\u82b1\u91cc"
        },
        {
          "authorId": "51961171",
          "name": "\u7530\u4e2d \u5b8f"
        },
        {
          "authorId": "71957193",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "52004333",
          "name": "\u6238\u7530 \u667a\u57fa"
        },
        {
          "authorId": "52456834",
          "name": "\u4e2d\u6751 \u54f2\u90ce"
        }
      ]
    },
    {
      "paperId": "c7af06170f3d81ab761873a4c1fe0af2736eb0a2",
      "externalIds": {
        "DBLP": "conf/asru/LubisSNYTN15",
        "MAG": "2290103709",
        "DOI": "10.1109/ASRU.2015.7404867",
        "CorpusId": 1633243
      },
      "url": "https://www.semanticscholar.org/paper/c7af06170f3d81ab761873a4c1fe0af2736eb0a2",
      "title": "A study of social-affective communication: Automatic prediction of emotion triggers and responses in television talk shows",
      "abstract": "Advancements in spoken language technologies have allowed users to interact with computers in an increasingly natural manner. However, most conversational agents or dialogue systems are yet to consider emotional awareness in interaction. To consider emotion in these situations, social-affective knowledge in conversational agents is essential. In this paper, we present a study of the social-affective process in natural conversation from television talk shows. We analyze occurrences of emotion (emotional responses), and the events that elicit them (emotional triggers). We then utilize our analysis for prediction to model the ability of a dialogue system to decide an action and response in an affective interaction. This knowledge has great potential to incorporate emotion into human-computer interaction. Experiments in two languages, English and Indonesian, show that automatic prediction performance surpasses random guessing accuracy.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "777-783",
        "name": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)"
      },
      "authors": [
        {
          "authorId": "143604111",
          "name": "Nurul Lubis"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "ce268e0942ce0d1f6942e4d7e7e2aa6464f1b577",
      "externalIds": {
        "CorpusId": 201863612
      },
      "url": "https://www.semanticscholar.org/paper/ce268e0942ce0d1f6942e4d7e7e2aa6464f1b577",
      "title": "Non-native ASR Utilizing Acoustic Data-driven Pronunciation Learning with Zero Knowledge of Non-native Pronunciation",
      "abstract": "Non-native speech differs significantly from native speech, often resulting in a degradation of the performance of automatic speech recognition (ASR). Handcrafted pronunciation lexicons used in standard ASR systems generally fail to cover non-native pronunciations, and design of new ones by linguistic experts is time consuming and costly. A previous study proposed a method to automatically learn a pronunciation lexicon in an iterative fashion using knowledge of non-native pronunciation. However, this previous method needs a handcrafted non-native pronunciation lexicon to train a grapheme-to-phoneme (G2P) converter used to generate non-native pronunciation variations, including pronunciations of new words. This non-native pronunciation lexicon is difficult to obtain, and lacks versatility to be applied to other non-native speakers. This study proposes a method for non-native ASR using acoustic evidence for pronunciation learning without knowledge of non-native pronunciation. In experiments, we evaluate our ASR systems for speakers with three degrees of English proficiency level. The results reveal that the proposed method can achieve almost same recognition accuracy with a system using knowledge of a non-native pronunciation, and is able to achieve an improvement of about 2.4% in recognition accuracy, particularly for high-proficiency speakers.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "49703062",
          "name": "Satoshi Tsujioka"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "ce5a57c0ccc8993f4a8e3a07101140a757024d9f",
      "externalIds": {
        "DBLP": "books/sp/15/KotoSNTAN15",
        "MAG": "2337050382",
        "DOI": "10.1007/978-3-319-19291-8_14",
        "CorpusId": 12229952
      },
      "url": "https://www.semanticscholar.org/paper/ce5a57c0ccc8993f4a8e3a07101140a757024d9f",
      "title": "A Study on Natural Expressive Speech: Automatic Memorable Spoken Quote Detection",
      "abstract": null,
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "145-152"
      },
      "authors": [
        {
          "authorId": "2789148",
          "name": "Fajri Koto"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1719005",
          "name": "M. Adriani"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "cece2d2f7cc38a512325122401f8aa658121b80e",
      "externalIds": {
        "CorpusId": 201822849
      },
      "url": "https://www.semanticscholar.org/paper/cece2d2f7cc38a512325122401f8aa658121b80e",
      "title": "A Dialog System to Detect Deception",
      "abstract": "When humans attempt to detect deception, they perform two actions: looking for telltale signs of deception, and asking questions to attempt to unveil a deceptive conversational partner. There is a significant amount of prior work on automatic deception detection, which focuses on the former. On the other hand, we focus on the latter, constructing a dialog system that asks questions to attempt to catch a potentially deceptive conversation partner. We propose several dialog strategies for this task, and measure the deception detection accuracy of each, finding that a more intelligent dialog strategy is slightly more effective at detecting deception.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "9530075",
          "name": "Yuiko Tsunomori"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "3027595",
          "name": "Takuya Hiraoka"
        },
        {
          "authorId": "2387338",
          "name": "M. Mizukami"
        },
        {
          "authorId": "2105082252",
          "name": "Sakriani"
        },
        {
          "authorId": "2229357938",
          "name": "Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "d723630c585aa0e4084fdd6e71bc6586cfa30e9d",
      "externalIds": {
        "MAG": "2398584126",
        "DBLP": "conf/interspeech/NguyenNSSTN15",
        "DOI": "10.21437/Interspeech.2015-573",
        "CorpusId": 17944619
      },
      "url": "https://www.semanticscholar.org/paper/d723630c585aa0e4084fdd6e71bc6586cfa30e9d",
      "title": "A latent variable model for joint pause prediction and dependency parsing",
      "abstract": "The prosody of speech is closely related to syntactic structure of the spoken sentence, and thus analysis models that jointly consider these two types of information are promising. However, manual annotation of syntactic information and prosodic information such as pauses is laborious, and thus it can be difficult to obtain sufficient data to train such joint models. In this paper, we tackle this problem by introducing a joint pause prediction and dependency parsing model that treats pauses between consecutive words as latent variables. Using this model, it is possible to learn from not only data labeled with both syntax and pause information, but also data labeled with only syntactic information, which can be obtained in larger quantities. Experiments find that a joint pause prediction and dependency parsing model obtains better pause prediction F-measure than a decision-tree-based baseline trained on the same data, and that the addition of more data using the proposed latent variable model leads for further gains of up to 11.6 points in F-measure.",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2719-2723"
      },
      "authors": [
        {
          "authorId": "37920773",
          "name": "Nguyen The Tung"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "73467110",
          "name": "Hiroyuki Shindo"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "e050cd9cec5eed73bd56cb2c9726ea85e985384b",
      "externalIds": {
        "MAG": "2289918019",
        "DBLP": "conf/asru/SaktiINTPN15",
        "DOI": "10.1109/ASRU.2015.7404802",
        "CorpusId": 11753467
      },
      "url": "https://www.semanticscholar.org/paper/e050cd9cec5eed73bd56cb2c9726ea85e985384b",
      "title": "Incremental sentence compression using LSTM recurrent networks",
      "abstract": "Many of the current sentence compression techniques attempt to produce a shortened form of a sentence by relying on syntactic structure such as dependency tree representations. While the performance of sentence compression has been improving, these approaches require a full parse of the sentence before performing sentence compression, making it difficult to perform compression in real time. In this paper, we examine the possibilities of performing incremental sentence compression using long short-term memory (LSTM) recurrent neural networks (RNN). The decision of whether to remove a word is done at each time step, without waiting for the end of the sentence. Various RNN parameters are investigated, including the number of layers and network connections. Furthermore, we also propose using a pretraining method in which the network is pretrained as an autoencoder. Experimental results reveal that our method obtains compression rates similar to human references and a better accuracy than the state-of-the-art tree transduction models.",
      "year": 2015,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "252-258",
        "name": "2015 IEEE Workshop on Automatic Speech Recognition and Understanding (ASRU)"
      },
      "authors": [
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "2672446",
          "name": "Faiz Ilham"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1962263",
          "name": "A. Purwarianti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "e212f788c701370af02b138d2a61e180cddfb138",
      "externalIds": {
        "ACL": "N15-1033",
        "DBLP": "conf/naacl/NeubigAD15",
        "MAG": "2181989146",
        "DOI": "10.3115/v1/N15-1033",
        "CorpusId": 3106450
      },
      "url": "https://www.semanticscholar.org/paper/e212f788c701370af02b138d2a61e180cddfb138",
      "title": "Multi-Target Machine Translation with Multi-Synchronous Context-free Grammars",
      "abstract": "We propose a method for simultaneously translating from a single source language to multiple target languages T1, T2, etc. The motivation behind this method is that if we only have a weak language model for T1 and translations in T1 and T2 are associated, we can use the information from a strong language model over T2 to disambiguate the translations in T1, providing better translation results. As a specific framework to realize multi-target translation, we expand the formalism of synchronous context-free grammars to handle multiple targets, and describe methods for rule extraction, scoring, pruning, and search with these models. Experiments find that multi-target translation with a strong language model in a similar second target language can provide gains of up to 0.8-1.5 BLEU points. 1",
      "year": 2015,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "293-302"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "144332819",
          "name": "Philip Arthur"
        },
        {
          "authorId": "1800354",
          "name": "Kevin Duh"
        }
      ]
    },
    {
      "paperId": "ee24fb876e6f1b345d492101c499bc5dd6b8196b",
      "externalIds": {
        "MAG": "1795524366",
        "DBLP": "conf/icassp/MakiTSNN15",
        "DOI": "10.1109/ICASSP.2015.7178449",
        "CorpusId": 6268863
      },
      "url": "https://www.semanticscholar.org/paper/ee24fb876e6f1b345d492101c499bc5dd6b8196b",
      "title": "EEG signal enhancement using multi-channel wiener filter with a spatial correlation prior",
      "abstract": "Event-related potentials (ERPs) of electroencephalogram (EEG) are often used as features for brain machine interfaces or for analysis of brain activities. However, as EEG signals easily suffer from various artifacts, ERPs are often collapsed and hard to observe. There are several attempts at using multi-channel EEG signals to enhance EEG signals of interest and make ERPs more clearly observed. For example, a previous work has proposed a blind EEG signal separation method using a multi-channel Wiener filter designed with a probabilistic generative model of observed EEG signals. This method copes with the under-determination of EEG signal separation by assuming sparseness of each EEG component in the time-frequency domain. Although this method blindly separates EEG signals into individual EEG components using time-varying scaled spatial correlation matrices, target EEG components, such as P300 of ERP, are often known in advance in some applications. In this paper, inspired by this previous work, we propose a probabilistic EEG signal enhancement method using a multi-channel Wiener filter, newly incorporating prior information of the spatial correlation matrices related to the target EEG component in the probabilistic generative model to improve performance of EEG signal enhancement. An experimental evaluation for P300 enhancement shows that the proposed method significantly reduces artifacts.",
      "year": 2015,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2639-2643",
        "name": "2015 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
      },
      "authors": [
        {
          "authorId": "3030934",
          "name": "Hayato Maki"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "f7ce4c7ec30c846cc122393deee98f1eacd24049",
      "externalIds": {
        "CorpusId": 12502987
      },
      "url": "https://www.semanticscholar.org/paper/f7ce4c7ec30c846cc122393deee98f1eacd24049",
      "title": "Dialogue State Tracking using Long Short Term Memory Neural Networks",
      "abstract": "We propose a dialogue state tracker based on long short term memory (LSTM) neural networks. LSTM is an extension of a recurrent neural network (RNN), which can better consider distant dependencies in sequential input. We construct a LSTM network that receives utterances of dialogue participants as input, and outputs the dialogue state of the current utterance. The input utterances are separated into vectors of words with their orders, which are further converted to word embeddings to avoid sparsity problems. In experiments, we combined this system with the baseline system of the dialogue state tracking challenge (DSTC), and achieved improved dialogue state tracking accuracy.",
      "year": 2015,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "2237192",
          "name": "Koichiro Yoshino"
        },
        {
          "authorId": "3027595",
          "name": "Takuya Hiraoka"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "07a5536c0570804f816fdb5a0a5ae890630e61bd",
      "externalIds": {
        "DBLP": "journals/ieicet/TanakaTNSN14",
        "MAG": "2023694213",
        "DOI": "10.1587/TRANSINF.E97.D.1429",
        "CorpusId": 3571827
      },
      "url": "https://www.semanticscholar.org/paper/07a5536c0570804f816fdb5a0a5ae890630e61bd",
      "title": "A Hybrid Approach to Electrolaryngeal Speech Enhancement Based on Noise Reduction and Statistical Excitation Generation",
      "abstract": "SUMMARY This paper presents an electrolaryngeal (EL) speech enhancement method capable of significantly improving naturalness of EL speech while causing no degradation in its intelligibility. An electrolarynx is an external device that artificially generates excitation sounds to enable laryngectomees to produce EL speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. Moreover, the excitation sounds produced by the device often leak outside, adding to EL speech as noise. To address these issues, there are mainly two conventional approached to EL speech enhancement through either noise reduction or statistical voice conversion (VC). The former approach usually causes no degradation in intelligibility but yields only small improvements in naturalness as the mechanical excitation sounds remain essentially unchanged. On the other hand, the latter approach significantly improves naturalness of EL speech using spectral and excitation parameters of natural voices converted from acoustic parameters of EL speech, but it usually causes degradation in intelligibility owing to errors in conversion. We propose a hybrid approach using a noise reduction method for enhancing spectral parameters and statistical voice conversion method for predicting excitation parameters. Moreover, we further modify the prediction process of the excitation parameters to improve its prediction accuracy and reduce adverse effects caused by unvoiced/voiced prediction errors. The experimental results demonstrate the proposed method yields significant improvements in naturalness compared with EL speech while keeping intelligibility high enough.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.jstage.jst.go.jp/article/transinf/E97.D/6/E97.D_1429/_pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science",
        "Engineering"
      ],
      "journal": {
        "volume": "97-D",
        "pages": "1429-1437",
        "name": "IEICE Trans. Inf. Syst."
      },
      "authors": [
        {
          "authorId": "2109245369",
          "name": "Kou Tanaka"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "084ddb77fce5a7f0b6418ef4e38dbb1bedf4ae78",
      "externalIds": {
        "DBLP": "conf/apsipa/TanakaTNSN14",
        "MAG": "2011331090",
        "DOI": "10.1109/APSIPA.2014.7041593",
        "CorpusId": 182251
      },
      "url": "https://www.semanticscholar.org/paper/084ddb77fce5a7f0b6418ef4e38dbb1bedf4ae78",
      "title": "An inter-speaker evaluation through simulation of electrolarynx control based on statistical F0 prediction",
      "abstract": "An electrolarynx is a device that artificially generates excitation sounds to produce electrolaryngeal (EL) speech. Although proficient laryngectomees can produce intelligible EL speech by using this device, it sounds quite unnatural due to the mechanical excitation. To address this issue, we have proposed several EL speech enhancement methods using statistical voice conversion and showed that statistical prediction of excitation parameters, such as F0 patterns, was essential to significantly improve naturalness of EL speech. Based on this result, we have also proposed a direct control method of F0 patterns of excitation sounds generated from the electrolarynx based on the statistical excitation prediction, which may allow EL speech enhancement to be applied to face-to-face conversation. In our previous work, this direct control method was evaluated through simulation using only a single laryngectomee's EL speech and it was demonstrated that this method allows for improved naturalness of EL speech while preserving listenability. However, because quality of EL speech highly depends on the proficiency of each laryngectomee, it is still not clear whether these methods will generalize to other speakers. In addition, while previous work only evaluated the naturalness and listenability, intelligibility is also an important factor that has not been evaluated. In this paper, we apply the direct control method to multiple speakers consisting of two real laryngectomees and one non-laryngectomee and evaluate its performance through simulations in terms of naturalness, listenability, and intelligibility. The experimental results demonstrate that the proposed method yields significant improvements in naturalness of EL speech for multiple laryngectomees while maintaining listenability and intelligibility.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-4",
        "name": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific"
      },
      "authors": [
        {
          "authorId": "2109245369",
          "name": "Kou Tanaka"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "0a4dd1e51616b422aa2d437610dbfbdd3733a114",
      "externalIds": {
        "CorpusId": 17483117
      },
      "url": "https://www.semanticscholar.org/paper/0a4dd1e51616b422aa2d437610dbfbdd3733a114",
      "title": "A Dialog System with Human-to-Human Conversation Example",
      "abstract": "Example-based dialog modeling (EBDM) is datadriven approach for deploying dialog systems [1, 2]. Some studies propose constructing dialog examples from available human-to human conversation log databases [3, 4] or movie scripts [5]. However, these works did not filter any uncorrelated consecutive lines in the movie data. As the authors state, this causes failures and diminishes the ability to maintain a consistent conversation. In this paper, we summarize our work on a dialog agent that utilizes human-to-human conversation examples from movies and Twitter to reduce the time requirement for database design and collection, and allow the agent to interact with the user in as natural as fashion as possible. Next, we investigate various data-driven approaches to dialog management, including two EBDM techniques (syntactic-semantic similarity retrieval and TF-IDF based cosine similarity retrieval) and using statistical machine translation (SMT) to learn a conversational mapping between user-input and systemoutput dialog pairs.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "2115734",
          "name": "Lasguido Nio"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "0db557c4315b1e08ef65ff15b96eb7630014bf72",
      "externalIds": {
        "DBLP": "conf/apsipa/YoshidaHNSTN14",
        "MAG": "2001957674",
        "DOI": "10.1109/APSIPA.2014.7041572",
        "CorpusId": 16179519
      },
      "url": "https://www.semanticscholar.org/paper/0db557c4315b1e08ef65ff15b96eb7630014bf72",
      "title": "Unnecessary utterance detection for avoiding digressions in discussion",
      "abstract": "In this paper, we propose a method for avoiding digressions in discussion by detecting unnecessary utterances and having a dialogue system intervene. The detector is based on the features using word frequency and topic shifts. The performance (i.e. accuracy, recall, precision, and F-measure) of the unnecessary utterance detector is evaluated through leave-one-dialogue-out cross-validation. In the evaluation, we find that the performance of the proposed detector is higher than that of a typical automatic summarization method.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-4",
        "name": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific"
      },
      "authors": [
        {
          "authorId": "28695904",
          "name": "Riki Yoshida"
        },
        {
          "authorId": "3027595",
          "name": "Takuya Hiraoka"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "101d619b5911e9c2fda6f02365c593ae61617cb6",
      "externalIds": {
        "MAG": "2136152952",
        "DBLP": "conf/coling/HiraokaNSTN14",
        "ACL": "C14-1161",
        "CorpusId": 13214001
      },
      "url": "https://www.semanticscholar.org/paper/101d619b5911e9c2fda6f02365c593ae61617cb6",
      "title": "Reinforcement Learning of Cooperative Persuasive Dialogue Policies using Framing",
      "abstract": "In this paper, we apply reinforcement learning for automatically learning cooperative persuasive dialogue system policies using framing, the use of emotionally charged statements common in persuasive dialogue between humans. In order to apply reinforcement learning, we describe a method to construct user simulators and reward functions specifically tailored to persuasive dialogue based on a corpus of persuasive dialogues between human interlocutors. Then, we evaluate the learned policy and the effect of framing through experiments both with a user simulator and with real users. The experimental evaluation indicates that applying reinforcement learning is effective for construction of cooperative persuasive dialogue systems which use framing.",
      "year": 2014,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1706-1717"
      },
      "authors": [
        {
          "authorId": "3027595",
          "name": "Takuya Hiraoka"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "1392df13a80a962057e979a294a850a50b7deb7e",
      "externalIds": {
        "ACL": "Q14-1014",
        "MAG": "2184055828",
        "DBLP": "journals/tacl/SperberSNNW14",
        "DOI": "10.1162/tacl_a_00174",
        "CorpusId": 11390483
      },
      "url": "https://www.semanticscholar.org/paper/1392df13a80a962057e979a294a850a50b7deb7e",
      "title": "Segmentation for Efficient Supervised Language Annotation with an Explicit Cost-Utility Tradeoff",
      "abstract": "In this paper, we study the problem of manually correcting automatic annotations of natural language in as efficient a manner as possible. We introduce a method for automatically segmenting a corpus into chunks such that many uncertain labels are grouped into the same chunk, while human supervision can be omitted altogether for other segments. A tradeoff must be found for segment sizes. Choosing short segments allows us to reduce the number of highly confident labels that are supervised by the annotator, which is useful because these labels are often already correct and supervising correct labels is a waste of effort. In contrast, long segments reduce the cognitive effort due to context switches. Our method helps find the segmentation that optimizes supervision efficiency by defining user models to predict the cost and utility of supervising each segment and solving a constrained optimization problem balancing these contradictory objectives. A user study demonstrates noticeable gains over pre-segmented, confidence-ordered baselines on two natural language processing tasks: speech transcription and word segmentation.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://www.mitpressjournals.org/doi/pdf/10.1162/tacl_a_00174",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "2",
        "pages": "169-180",
        "name": "Transactions of the Association for Computational Linguistics"
      },
      "authors": [
        {
          "authorId": "3011998",
          "name": "Matthias Sperber"
        },
        {
          "authorId": "1897178",
          "name": "Mirjam Simantzik"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        },
        {
          "authorId": "1724972",
          "name": "A. Waibel"
        }
      ]
    },
    {
      "paperId": "15513c732d6af975f312307be3b5e2bd674ac0ef",
      "externalIds": {
        "DBLP": "conf/apsipa/SaktiOSNTN14",
        "MAG": "2067208615",
        "DOI": "10.1109/APSIPA.2014.7041620",
        "CorpusId": 1110988
      },
      "url": "https://www.semanticscholar.org/paper/15513c732d6af975f312307be3b5e2bd674ac0ef",
      "title": "An event-related brain potential study on the impact of speech recognition errors",
      "abstract": "Most automatic speech recognition (ASR) systems, which aim for perfect transcription of utterances, are trained and tuned by minimizing the word error rate (WER). In this framework, even though the impact of all errors is not the same, all errors (substitutions, deletions, insertions) from any words are treated in a uniform manner. The size of the impact and exactly what the differences are remain unknown. Several studies have proposed possible alternatives to the WER metric. But no analysis has investigated how the human brain processes language and perceives the effect of mistaken output by ASR systems. In this research we utilize event-related brain potential (ERP) studies and directly analyze the brain activities on the impact of ASR errors. Our results reveal that the peak amplitudes of the positive shift after the substitution and deletion violations are much bigger than the insertion violations. This finding indicates that humans perceived each error differently based on its impact of the whole sentence. To investigate the effect of this study, we formulated a new weighted word error rate metric based on the ERP results: ERP-WWER. We re-evaluated the ASR performance using the new ERP-WWER metric and compared and discussed the results with the standard WER.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-4",
        "name": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific"
      },
      "authors": [
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "2122293",
          "name": "Y. Odagaki"
        },
        {
          "authorId": "2073379",
          "name": "Takafumi Sasakura"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "162c3cf78af48ddf826ec76a1a3767a88a730170",
      "externalIds": {
        "MAG": "1601191451",
        "CorpusId": 208856028
      },
      "url": "https://www.semanticscholar.org/paper/162c3cf78af48ddf826ec76a1a3767a88a730170",
      "title": "\u8133\u6ce2\u306e\u30c1\u30e3\u30cd\u30eb\u9593\u76f8\u95a2\u306e\u4e8b\u524d\u5206\u5e03\u3092\u5229\u7528\u3057\u305f\u78ba\u7387\u7684\u76ee\u7684\u6210\u5206\u5f37\u8abf(\u6a5f\u68b0\u5b66\u7fd2\u306b\u3088\u308b\u30d0\u30a4\u30aa\u30c7\u30fc\u30bf\u30de\u30a4\u30f3\u30cb\u30f3\u30b0,\u4e00\u822c)",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Mathematics"
      ],
      "journal": null,
      "authors": [
        {
          "authorId": "1451617729",
          "name": "\u771f\u6728 \u52c7\u4eba"
        },
        {
          "authorId": "52004333",
          "name": "\u6238\u7530 \u667a\u57fa"
        },
        {
          "authorId": "47828326",
          "name": "Sakriani Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "52456834",
          "name": "\u4e2d\u6751 \u54f2\u90ce"
        }
      ]
    },
    {
      "paperId": "1f76ee8472ec3a41511540f62e4676317df14ea5",
      "externalIds": {
        "MAG": "2031321541",
        "DBLP": "conf/icassp/KobayashiTNGNSN14",
        "DOI": "10.1109/ICASSP.2014.6855139",
        "CorpusId": 2291901
      },
      "url": "https://www.semanticscholar.org/paper/1f76ee8472ec3a41511540f62e4676317df14ea5",
      "title": "Regression approaches to perceptual age control in singing voice conversion",
      "abstract": "The perceptual age of a singing voice is the age of the singer as perceived by the listener, and is one of the notable characteristics that determines perceptions of a song. In this paper, we describe a novel voice timbre control technique based on the perceptual age for singing voice conversion (SVC). Singers can sing expressively by controlling prosody and voice timbre, but the varieties of voices that singers can produce are limited by physical constraints. Previous work has attempted to overcome the limitation through the use of statistical voice conversion. This technique makes it possible to convert singing voice timbre of an arbitrary source singer into that of an arbitrary target singer. However, it is still difficult to intuitively control singing voice characteristics by manipulating parameters corresponding to specific physical traits, such as gender and age. In this paper, we develop a technique for controlling the voice timbre based on perceptual age that maintains the singer's individuality. The experimental results show that the proposed voice timbre control method makes it possible to change the singer's perceptual age while not having an adverse effect on the perceived individuality.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Psychology"
      ],
      "journal": {
        "pages": "7904-7908",
        "name": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
      },
      "authors": [
        {
          "authorId": "65851830",
          "name": "Kazuhiro Kobayashi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1712260",
          "name": "Tomoyasu Nakano"
        },
        {
          "authorId": "144968710",
          "name": "Masataka Goto"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "2526c510610c7220ecc56e6b08d09c4cbaf58c3c",
      "externalIds": {
        "MAG": "2184331203",
        "CorpusId": 55915097
      },
      "url": "https://www.semanticscholar.org/paper/2526c510610c7220ecc56e6b08d09c4cbaf58c3c",
      "title": "Anaphora Resolution for Transforming Regular Expressions into Honorifics in Japanese",
      "abstract": "The Japanese language places a heavy emphasis in establishing hierarchical relations among people, or paying respects to elders or those of higher social rank. The Japanese language includes the normal speech (\u5e38\u8a9e) and honorific speech (\u656c \u8a9e), with honorific speech playing an important role in establishing these relationships. The honorifics in Japanese include different levels of respectful (\u5c0a\u656c\u8a9e), humble (\u8b19\u8b72 \u8a9e), and polite (\u4e01\u5be7\u8a9e) speech, which are frequently used in various social or business situations. Because of this, one verb in Japanese will have at least four ways to express the same meaning in different situations. For example, word \u884c \u304f (go), it will have the four expressions as in Table 1. The mechanism of honorifics in Japanese is complicated, and many non-native Japanese speakers, as well as members of the young generations in Japan, have trouble mastering it. This situation has encouraged the study of automatic systems that identify the proper form of honorifics in Japanese including automatic translating of learners text into correct honorifics [1, 2]. However these previous works do not have a mechanism to determine which kind of honorific form should be applied in sentences without subjects. As subject omission is a common phenomenon in Japanese, this limits the practical utility of these methods. In this paper, we propose the use of anaphora resolution for this task. We incorporate anaphora resolution into a rulebased machine translation system to translate regular expressions into appropriate honorific of Japanese, and examine the effectiveness of using correct subjects annotated by a human, and those automatically predicted by anaphora resolution.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "2152598150",
          "name": "Miao Yu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "2899eb53cddf050e3a34f07bbc0bc0ee7907d5d0",
      "externalIds": {
        "ACL": "L14-1515",
        "DBLP": "conf/lrec/MoriN14",
        "MAG": "2251455968",
        "CorpusId": 17242981
      },
      "url": "https://www.semanticscholar.org/paper/2899eb53cddf050e3a34f07bbc0bc0ee7907d5d0",
      "title": "Language Resource Addition: Dictionary or Corpus?",
      "abstract": "In this paper, we investigate the relative effect of two strategies of language resource additions to the word segmentation problem and part-of-speech tagging problem in Japanese. The first strategy is adding entries to the dictionary and the second is adding annotated sentences to the training corpus. The experimental results showed that the annotated sentence addition to the training corpus is better than the entries addition to the dictionary. And the annotated sentence addition is efficient especially when we add new words with contexts of three real occurrences as partially annotated sentences. According to this knowledge, we executed annotation on the invention disclosure texts and observed word segmentation accuracy.",
      "year": 2014,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1631-1636"
      },
      "authors": [
        {
          "authorId": "144873535",
          "name": "Shinsuke Mori"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "2a81081c987da2bb8184b8e9a884cf6a73712ee8",
      "externalIds": {
        "MAG": "2292201470",
        "CorpusId": 218697291
      },
      "url": "https://www.semanticscholar.org/paper/2a81081c987da2bb8184b8e9a884cf6a73712ee8",
      "title": "\u968e\u5c64\u7684\u30d5\u30ec\u30fc\u30ba\u30d9\u30fc\u30b9\u7ffb\u8a33\u306b\u304a\u3051\u308b\u30d4\u30dc\u30c3\u30c8\u7ffb\u8a33\u624b\u6cd5\u306e\u5fdc\u7528(\u5fdc\u7528\u51e6\u7406,\u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0)",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "119-125",
        "name": "IEICE technical report. Natural language understanding and models of communication"
      },
      "authors": [
        {
          "authorId": "135231196",
          "name": "\u4e09\u6d66 \u660e\u6ce2"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "47828326",
          "name": "Sakriani Sakti"
        },
        {
          "authorId": "52004333",
          "name": "\u6238\u7530 \u667a\u57fa"
        },
        {
          "authorId": "52456834",
          "name": "\u4e2d\u6751 \u54f2\u90ce"
        }
      ]
    },
    {
      "paperId": "2f1743d1a1be46452ab90691ead8bf916ffd912b",
      "externalIds": {
        "CorpusId": 201855398
      },
      "url": "https://www.semanticscholar.org/paper/2f1743d1a1be46452ab90691ead8bf916ffd912b",
      "title": "Probabilistic Enhancement of EEG Component Using Prior Distribution of Correlations Between Channels",
      "abstract": "\u3042\u3089\u307e\u3057 \u8133\u6ce2\u306e\u632f\u5e45\u306f\u975e\u5e38\u306b\u5c0f\u3055\u304f,\u5916\u90e8\u304b\u3089\u306e\u30a2\u30fc\u30c1\u30d5\u30a1\u30af\u30c8\u6df7\u5165\u306b\u5bfe\u3057\u3066\u3082\u6975\u3081\u3066\u8106\u5f31\u3067\u3042\u308b\u305f\u3081,\u305d\u306e\u307e \u307e\u3067\u306f\u7279\u5fb4\u3092\u5206\u6790\u3059\u308b\u306e\u304c\u96e3\u3057\u3044.\u8133\u6ce2\u306e S/N \u6bd4\u3092\u5411\u4e0a\u3055\u305b\u308b\u305f\u3081\u306b,\u52a0\u7b97\u5e73\u5747\u6cd5\u3084\u72ec\u7acb\u6210\u5206\u5206\u6790\u3092\u7528\u3044\u308b\u306e\u304c \u4e00\u822c\u7684\u3067\u3042\u308b.\u52a0\u7b97\u5e73\u5747\u6cd5\u306f\u30ea\u30a2\u30eb\u30bf\u30a4\u30e0\u51e6\u7406\u304c\u884c\u3048\u306a\u304f\u306a\u308b\u7b49\u306e\u554f\u984c\u304c\u751f\u3058\u308b\u4e00\u65b9,\u72ec\u7acb\u6210\u5206\u5206\u6790\u306b\u306f\u5206\u96e2\u53ef\u80fd \u306a\u4fe1\u53f7\u6e90\u306e\u6570\u306b\u554f\u984c\u304c\u3042\u308b.\u97f3\u6e90\u5206\u96e2\u306e\u5206\u91ce\u3067\u306f,\u8133\u6ce2\u306e\u89b3\u6e2c\u3092\u6642\u9593\u5468\u6ce2\u6570\u9818\u57df\u4e0a\u306e\u78ba\u7387\u30e2\u30c7\u30eb\u3067\u4eee\u5b9a\u3057,\u6700\u5c24\u63a8 \u5b9a\u3067\u5f97\u3089\u308c\u305f\u30d1\u30e9\u30e1\u30bf\u3092\u7528\u3044\u3066\u8a2d\u8a08\u3055\u308c\u305f\u30de\u30eb\u30c1\u30c1\u30e3\u30cd\u30eb\u30a6\u30a3\u30fc\u30ca\u30fc\u30d5\u30a3\u30eb\u30bf\u306b\u3088\u308a\u4fe1\u53f7\u5206\u96e2\u3092\u884c\u3046\u624b\u6cd5\u304c\u63d0\u5531\u3055 \u308c\u305f.\u3053\u306e\u624b\u6cd5\u306b\u5bfe\u3057, \u672c\u7814\u7a76\u3067\u306f\u4e8b\u524d\u306e\u8133\u6ce2\u8a08\u6e2c\u306b\u3088\u308a,\u30d1\u30e9\u30e1\u30bf\u306e 1 \u3064\u3067\u3042\u308b\u7a7a\u9593\u76f8\u95a2\u884c\u5217\u306b\u95a2\u3059\u308b\u4e8b\u524d\u77e5 \u8b58\u3092\u7372\u5f97\u3057,\u4e8b\u524d\u5206\u5e03\u3068\u3057\u3066\u5b9a\u5f0f\u5316\u3059\u308b\u3053\u3068\u306b\u3088\u308a\u30d1\u30e9\u30e1\u30bf\u63a8\u5b9a\u306e\u7cbe\u5ea6\u5411\u4e0a\u3092\u56f3\u3063\u305f.\u5f37\u8abf\u7cbe\u5ea6\u306e\u8a55\u4fa1\u306e\u305f\u3081\u6ce2\u5f62 \u306e\u30d1\u30bf\u30fc\u30f3\u8a8d\u8b58\u3092\u884c\u3044,\u4e8b\u524d\u77e5\u8b58\u306e\u5c0e\u5165\u306b\u3088\u308a\u5f37\u8abf\u7cbe\u5ea6\u304c\u5411\u4e0a\u3059\u308b\u3053\u3068\u3092\u793a\u3057\u305f.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "3030934",
          "name": "Hayato Maki"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "2f369845ae7191196d65310210db2485feb3aa86",
      "externalIds": {
        "MAG": "2088260968",
        "DBLP": "conf/icassp/KuboSNTN14",
        "DOI": "10.1109/ICASSP.2014.6854068",
        "CorpusId": 17713103
      },
      "url": "https://www.semanticscholar.org/paper/2f369845ae7191196d65310210db2485feb3aa86",
      "title": "Narrow Adaptive Regularization of weights for grapheme-to-phoneme conversion",
      "abstract": "As the speech recognition field proceeds to open domain and multilingual tasks, the need for robust g2p conversion has been increasing. Towards this objective, we propose a new g2p conversion training method based on the Narrow Adaptive Regularization of Weights (NAROW) online learning algorithm. NAROW improves over its predecessor AROW by automatically adjusting hyperparameters to reduce mistake bounds, and ensuring that the learning rate is not updated when features for the input data have already been updated enough. The contribution of this paper is first to extend NAROW to structured learning, and show the inequality to bound the maximum number of errors in structured NAROW. In experiments, our proposed approach significantly improved over MIRA with consistent phoneme error rate reductions of 1.3-3.8% on a variety of dictionaries.",
      "year": 2014,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2589-2593",
        "name": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
      },
      "authors": [
        {
          "authorId": "143745924",
          "name": "Keigo Kubo"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "3dc20be709818630e2249ab28b35b0666b4b544d",
      "externalIds": {
        "CorpusId": 166227113
      },
      "url": "https://www.semanticscholar.org/paper/3dc20be709818630e2249ab28b35b0666b4b544d",
      "title": "Analysis of Emphasis on Japanese-English Bilingual Corpora",
      "abstract": "Conventional speech-to-speech (S2S) translation [1] systems only translate the content of the utterance and ignore paralinguistic information included in the input speech. As a step towards addressing this limitation, in this paper we analyze paralinguistic information across languages. Among the various types of paralinguistic information, we focus on emphasis, a type of information that is used to convey the focus of the sentence. Emphasis is an important factor especially when repeating an initially misheard sentence (a situation that occurs often when using less-than perfect speech translation systems). This paper describes an analysis regarding how emphasis is expressed across two languages: Japanese and English.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "3ed07f6643856b9ac4687b3bc667767f3ab4b563",
      "externalIds": {
        "MAG": "2585427789",
        "CorpusId": 63121693
      },
      "url": "https://www.semanticscholar.org/paper/3ed07f6643856b9ac4687b3bc667767f3ab4b563",
      "title": "Design of control parameters for voice quality control based on multiple-regression Gaussian mixture model",
      "abstract": "This report presents a method for designing control parameters in statistical voice quality control. As a method for intuitively controlling converted voice quality, a voice quality control method based on multiple-regression Gaussian mixture model (MR-GMM) was proposed. In this method, perceptual scores corresponding to voice quality expression words are manually assigned to individual pre-stored target speakers for statistically modeling a correspondence between acoustic features and the perceptual scores. Therefore, performance of voice quality control highly depends on a selection of voice quality expression words and accuracy of the perceptual score assignment. In this report, to achieve better voice quality control using multiple voice quality expression words, we propose a method for selecting voice quality expression words and assigning corresponding perceptual scores considering both independency of the perceptual scores and independency of acoustic features corresponding to them in the MR-GMM. Experimental results show that the performance of voice quality control is significantly improved by the",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "65-70",
        "name": ""
      },
      "authors": [
        {
          "authorId": "71072955",
          "name": "Kubo Kazutaka"
        },
        {
          "authorId": "52552669",
          "name": "Kobayashi Kazuhiro"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "42be8ed9973b3326a6e3d838c4742bc1d7704704",
      "externalIds": {
        "MAG": "2293872531",
        "DBLP": "conf/interspeech/TobingTNSNP14",
        "DOI": "10.21437/Interspeech.2014-185",
        "CorpusId": 18641034
      },
      "url": "https://www.semanticscholar.org/paper/42be8ed9973b3326a6e3d838c4742bc1d7704704",
      "title": "Articulatory controllable speech modification based on statistical feature mapping with Gaussian mixture models",
      "abstract": "This paper presents a novel speech modification method capable of controlling unobservable articulatory parameters based on a statistical feature mapping technique with Gaussian Mixture Models (GMMs). In previous work [1], the GMM-based statistical feature mapping was successfully applied to acousticto-articulatory inversion mapping and articulatory-to-acoustic production mapping separately. In this paper, these two mapping frameworks are integrated to a unified framework to develop a novel speech modification system. The proposed system sequentially performs the inversion and the production mapping, making it possible to modify phonemic sounds of an input speech signal by intuitively manipulating articulatory parameters estimated from the input speech signal. We also propose a manipulation method to automatically compensate for unmodified articulatory movements considering inter-dimensional correlation of the articulatory parameters. The proposed system is implemented for a single English speaker and its effectiveness is evaluated experimentally. The experimental results demonstrate that the proposed system is capable of modifying phonemic sounds by manipulating the estimated articulatory movements and higher speech quality is achieved by considering the inter-dimensional correlation in the manipulation. Index Terms: speech modification, acoustic-to-articulatory inversion mapping, articulatory-to-acoustic production mapping, Gaussian mixture model, inter-dimensional correlation",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "2298-2302"
      },
      "authors": [
        {
          "authorId": "3269936",
          "name": "Patrick Lumban Tobing"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        },
        {
          "authorId": "1962263",
          "name": "A. Purwarianti"
        }
      ]
    },
    {
      "paperId": "4358335263622fe189cf95c613f4d6fdcb67fbea",
      "externalIds": {
        "MAG": "2244685650",
        "CorpusId": 208929218
      },
      "url": "https://www.semanticscholar.org/paper/4358335263622fe189cf95c613f4d6fdcb67fbea",
      "title": "\u97f3\u58f0\u5165\u529b\u306b\u57fa\u3065\u304f\u97fb\u5f8b\u5236\u5fa1\u6a5f\u80fd\u3092\u6709\u3059\u308bHMM\u97f3\u58f0\u5408\u6210\u30b7\u30b9\u30c6\u30e0(\u30dd\u30b9\u30bf\u30fc\u30fb\u30c7\u30e2\u30bb\u30c3\u30b7\u30e7\u30f3,\u7b2c16\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0)",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "81-86",
        "name": "Scientific Programming"
      },
      "authors": [
        {
          "authorId": "149322620",
          "name": "\u897f\u57a3 \u53cb\u7406"
        },
        {
          "authorId": "69327003",
          "name": "\u9ad8\u9053 \u614e\u4e4b\u4ecb"
        },
        {
          "authorId": "52004333",
          "name": "\u6238\u7530 \u667a\u57fa"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "47828326",
          "name": "Sakriani Sakti"
        },
        {
          "authorId": "52456834",
          "name": "\u4e2d\u6751 \u54f2\u90ce"
        }
      ]
    },
    {
      "paperId": "4572ded23106285cbd8ebbe6c3b354973ac06ff7",
      "externalIds": {
        "MAG": "2341116233",
        "CorpusId": 63865171
      },
      "url": "https://www.semanticscholar.org/paper/4572ded23106285cbd8ebbe6c3b354973ac06ff7",
      "title": "Recognition and Analysis of Emotion in Indonesian Conversational Speech (\u97f3\u58f0) -- (\u7b2c16\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0)",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Psychology",
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "1-6",
        "name": "Scientific Programming"
      },
      "authors": [
        {
          "authorId": "143604111",
          "name": "Nurul Lubis"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "3161999",
          "name": "D. Lestari"
        },
        {
          "authorId": "1962263",
          "name": "A. Purwarianti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "4731f89169604cd0d8b5352380baa1b4728bca0b",
      "externalIds": {
        "ACL": "C14-1106",
        "MAG": "2251796512",
        "DBLP": "conf/coling/AkabeNSTN14",
        "CorpusId": 17541809
      },
      "url": "https://www.semanticscholar.org/paper/4731f89169604cd0d8b5352380baa1b4728bca0b",
      "title": "Discriminative Language Models as a Tool for Machine Translation Error Analysis",
      "abstract": "In this paper, we propose a new method for effective error analysis of machine translation (MT) systems. In previous work on error analysis of MT, error trends are often shown by frequency. However, if we attempt to perform a more detailed analysis based on frequently erroneous word strings, the word strings also often occur in correct translations, and analyzing these correct sentences decreases the overall efficiency of error analysis. In this paper, we propose the use of regularized discriminative language models (LMs) to allow for more focused MT error analysis. In experiments, we demonstrate that our method is more efficient than frequency-based analysis, and examine differences across systems, language pairs, and evaluation measures. 1",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1124-1132"
      },
      "authors": [
        {
          "authorId": "1783871",
          "name": "Koichi Akabe"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "4f4da6fdb9496b0295764b2db11381dd390de02d",
      "externalIds": {
        "MAG": "1992371874",
        "DBLP": "conf/slt/SperberNNW14",
        "DOI": "10.1109/SLT.2014.7078618",
        "CorpusId": 12434311
      },
      "url": "https://www.semanticscholar.org/paper/4f4da6fdb9496b0295764b2db11381dd390de02d",
      "title": "On-the-fly user modeling for cost-sensitive correction of speech transcripts",
      "abstract": "We propose an on-the-fly updating framework for cost-sensitive manual correction of automatically recognized speech transcripts. This framework trains cost-models during the transcription process, and does not require the transcriber enrollment necessary in previous work. We use a baseline method that optimizes a segmentation into segments to supervise or not to supervise in a cost-sensitive fashion that minimizes human effort, and introduce a much faster algorithm for computing such a segmentation that can be used for on-the-fly updates. Besides removing the need to carry out enrollments, experiments show that our updating framework results in 28% higher human supervision efficiency than previous cost-sensitive approaches.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "460-465",
        "name": "2014 IEEE Spoken Language Technology Workshop (SLT)"
      },
      "authors": [
        {
          "authorId": "3011998",
          "name": "Matthias Sperber"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        },
        {
          "authorId": "1724972",
          "name": "A. Waibel"
        }
      ]
    },
    {
      "paperId": "4f74be7e5dd4b8e9113e86132cf792da2c32ca3d",
      "externalIds": {
        "MAG": "598222209",
        "CorpusId": 181366964
      },
      "url": "https://www.semanticscholar.org/paper/4f74be7e5dd4b8e9113e86132cf792da2c32ca3d",
      "title": "\u30d1\u30e9\u30d5\u30ec\u30fc\u30ba\u3092\u8003\u616e\u3057\u305f\u6a5f\u68b0\u7ffb\u8a33\u306e\u8aa4\u308a\u7b87\u6240\u9078\u629e (\u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3) -- (\u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0)",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "114",
        "pages": "111-118",
        "name": "IEICE technical report. Speech"
      },
      "authors": [
        {
          "authorId": "136550032",
          "name": "\u8d64\u90e8 \u6643\u4e00"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "47828326",
          "name": "Sakriani Sakti"
        }
      ]
    },
    {
      "paperId": "54e7de06a97b4b6c41e185c0bee60c838a15265a",
      "externalIds": {
        "MAG": "577088675",
        "CorpusId": 57087151
      },
      "url": "https://www.semanticscholar.org/paper/54e7de06a97b4b6c41e185c0bee60c838a15265a",
      "title": "Articulatory Controllable Speech Modification using Sequential Inversion and Production Mapping with Gaussian Mixture Models (\u97f3\u58f0) -- (\u7b2c16\u56de\u97f3\u58f0\u8a00\u8a9e\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0)",
      "abstract": "In this report, we propose an articulatory controllable speech modification framework using statistical inversion and production mapping with Gaussian Mixture Models. The proposed framework enables us to modify speech waveforms by manipulating unobserved articulatory parameters estimated in the inversion mapping and generating the modified speech waveforms from the manipulated articulatory parameters in the production mapping. We also propose an articulatory manipulation method that considers inter-dimensional correlation between articulators. The experimental results show that the proposed framework is capable of successfully modifying phoneme sounds by manually controlling related articulators.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "57-62",
        "name": "Scientific Programming"
      },
      "authors": [
        {
          "authorId": "3269936",
          "name": "Patrick Lumban Tobing"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        },
        {
          "authorId": "1962263",
          "name": "A. Purwarianti"
        }
      ]
    },
    {
      "paperId": "5c159745fce2b87e8b00307b76f0948b9fa8b1d7",
      "externalIds": {
        "ACL": "W14-7002",
        "DBLP": "conf/aclwat/Neubig14",
        "MAG": "2760372894",
        "CorpusId": 16171971
      },
      "url": "https://www.semanticscholar.org/paper/5c159745fce2b87e8b00307b76f0948b9fa8b1d7",
      "title": "Forest-to-String SMT for Asian Language Translation: NAIST at WAT 2014",
      "abstract": "This paper describes the Nara Institute of Science and Technology\u2019s (NAIST) submission to the 2014 Workshop on Asian Translation\u2019s four translation tasks. All systems are based on forest-to-string (F2S) translation, in which the input sentence is first parsed using a syntactic parser, then a forest of possible syntactic analyses is translated into the target language. In addition to the baseline F2S system, we add rescoring using a recurrent neural network language model (RNNLM), which allows for more fluent output. The resulting system achieved the highest results in both automatic and manual evaluation for all four of the language pairs targeted by the workshop.",
      "year": 2014,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "20-25"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "5dcbdb9bf80575953b5d21f378d8139f0a44168b",
      "externalIds": {
        "DBLP": "conf/iwsds/LubisSNTPN14",
        "MAG": "2497386712",
        "DOI": "10.1007/978-3-319-21834-2_10",
        "CorpusId": 16892307
      },
      "url": "https://www.semanticscholar.org/paper/5dcbdb9bf80575953b5d21f378d8139f0a44168b",
      "title": "Emotion and Its Triggers in Human Spoken Dialogue: Recognition and Analysis",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "103-110"
      },
      "authors": [
        {
          "authorId": "143604111",
          "name": "Nurul Lubis"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1962263",
          "name": "A. Purwarianti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "5e51edfcef2b28594c63cce97c08752dfd438af0",
      "externalIds": {
        "DBLP": "conf/interspeech/KuboSNTN14",
        "MAG": "2295592722",
        "DOI": "10.21437/Interspeech.2014-316",
        "CorpusId": 18860074
      },
      "url": "https://www.semanticscholar.org/paper/5e51edfcef2b28594c63cce97c08752dfd438af0",
      "title": "Structured soft margin confidence weighted learning for grapheme-to-phoneme conversion",
      "abstract": "In recent years, structured online discriminative learning methods using second order statistics have been shown to outperform conventional generative and discriminative models in the grapheme-to-phoneme (g2p) conversion task. However, these methods update the parameters by sequentially using N-best hypotheses predicted with the current parameters. Thus, the parameters appearing in early hypotheses are overfitted compared with those in later hypotheses. In this paper, we propose a novel method called structured soft margin confidence weighted learning, which extends multi-class confidence weighted learning to structured learning. The proposed method extends multiclass CW in two ways, allowing for improved robustness to overfitting: (1) regularization inspired by soft margin support vector machines, allowing for margin error, and (2) update using N-best hypotheses simultaneously and interdependently. In an evaluation experiment on the g2p conversion task, the proposed method improved over all other approaches in terms of phoneme error rate with a significant difference. Index Terms: g2p conversion, out-of-vocabulary word, online discriminative training, structured learning, confidence weighted algorithm",
      "year": 2014,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1263-1267"
      },
      "authors": [
        {
          "authorId": "143745924",
          "name": "Keigo Kubo"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "601408d6617bf72894c9f41ae54cf9c17905903a",
      "externalIds": {
        "MAG": "2251678492",
        "ACL": "P14-2024",
        "DBLP": "conf/acl/NeubigD14",
        "DOI": "10.3115/v1/P14-2024",
        "CorpusId": 8004243
      },
      "url": "https://www.semanticscholar.org/paper/601408d6617bf72894c9f41ae54cf9c17905903a",
      "title": "On the Elements of an Accurate Tree-to-String Machine Translation System",
      "abstract": "While tree-to-string (T2S) translation theoretically holds promise for efficient, accurate translation, in previous reports T2S systems have often proven inferior to other machine translation (MT) methods such as phrase-based or hierarchical phrase-based MT. In this paper, we attempt to clarify the reason for this performance gap by investigating a number of peripheral elements that affect the accuracy of T2S systems, including parsing, alignment, and search. Based on detailed experiments on the English-Japanese and JapaneseEnglish pairs, we show how a basic T2S system that performs on par with phrasebased systems can be improved by 2.6-4.6 BLEU, greatly exceeding existing stateof-the-art methods. These results indicate that T2S systems indeed hold much promise, but the above-mentioned elements must be taken seriously in construction of these systems.",
      "year": 2014,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "143-149"
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1800354",
          "name": "Kevin Duh"
        }
      ]
    },
    {
      "paperId": "64bc7fe1c46c4d4106afba4621ff1bd4376c077a",
      "externalIds": {
        "MAG": "2562843482",
        "CorpusId": 15825788
      },
      "url": "https://www.semanticscholar.org/paper/64bc7fe1c46c4d4106afba4621ff1bd4376c077a",
      "title": "An Evaluation of a Hybrid Approach to Electrolaryngeal Speech Enhancement Based on Noise Reduction and Statistical Excitation Prediction",
      "abstract": "An Evaluation of a Hybrid Approach to Electrolaryngeal Speech Enhancement Based on Noise Reduction and Statistical Excitation Prediction Kou TANAKA\u2020, Tomoki TODA\u2020, Graham NEUBIG\u2020, Sakriani SAKTI\u2020, and Satoshi NAKAMURA\u2020 \u2020 Graduate School of Information Science, Nara Institute of Science and Technology, 8916-5 Takayama-cho, Ikoma-shi, 630-0101, Japan E-mail: \u2020{ko-t,tomoki,neubig,ssakti,s-nakamura}@is.naist.jp",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "331-336",
        "name": "IEICE technical report. Speech"
      },
      "authors": [
        {
          "authorId": "66310397",
          "name": "Tanaka Kou"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "6992f54509c139455c3cffa9b0e4ae5c19ebff82",
      "externalIds": {
        "CorpusId": 201866748
      },
      "url": "https://www.semanticscholar.org/paper/6992f54509c139455c3cffa9b0e4ae5c19ebff82",
      "title": "The Network-based Multilingual ASR System Towards Multilingual Conversations in Medical Domain",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "143745924",
          "name": "Keigo Kubo"
        },
        {
          "authorId": "2864759",
          "name": "Sho Matsumiya"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        },
        {
          "authorId": "152712879",
          "name": "Fumihiro Adachi"
        },
        {
          "authorId": "2216243",
          "name": "R. Isotani"
        }
      ]
    },
    {
      "paperId": "6d19d73909ffaa6c94cae6a2535ed52d138cb63b",
      "externalIds": {
        "MAG": "2107667213",
        "DBLP": "journals/ieicet/NioSNTN14",
        "DOI": "10.1587/TRANSINF.E97.D.1497",
        "CorpusId": 11031204
      },
      "url": "https://www.semanticscholar.org/paper/6d19d73909ffaa6c94cae6a2535ed52d138cb63b",
      "title": "Utilizing Human-to-Human Conversation Examples for a Multi Domain Chat-Oriented Dialog System",
      "abstract": "SUMMARY This paper describes the design and evaluation of a method for developing a chat-oriented dialog system by utilizing real human-to-human conversation examples from movie scripts and Twitter conversations. The aim of the proposed method is to build a conversational agent that can interact with users in as natural a fashion as possible, while reducing the time requirement for database design and collection. A number of the challenging design issues we faced are described, including (1) constructing an appropriate dialog corpora from raw movie scripts and Twitter data, and (2) developing an multi domain chat-oriented dialog management system which can retrieve a proper system response based on the current user query. To build a dialog corpus, we propose a unit of conversation called a tri-turn (a trigram conversation turn), as well as extraction and semantic similarity analysis techniques to help ensure that the content extracted from raw movie/drama script files forms appropriate dialog-pair (query-response) examples. The constructed dialog corpora are then utilized in a data-driven dialog management system. Here, various approaches are investigated including example-based (EBDM) and response generation using phrase-based statistical machine translation (SMT). In particular, we use two EBDM: syntactic-semantic similarity retrieval and TF-IDF based cosine similarity retrieval. Experiments are conducted to compare and contrast EBDM and SMT approaches in building a chat-oriented dialog system, and we investigate a combined method that addresses the advantages and disadvantages of both approaches. System performance was evaluated based on objective metrics (semantic similarity and cosine similarity) and human subjective evaluation from a small user study. Experimental results show that the proposed filtering approach effectively improve the performance. Furthermore, the results also show that by combing both EBDM",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.jstage.jst.go.jp/article/transinf/E97.D/6/E97.D_1497/_pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "97-D",
        "pages": "1497-1505",
        "name": "IEICE Trans. Inf. Syst."
      },
      "authors": [
        {
          "authorId": "2115734",
          "name": "Lasguido Nio"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "6d9603be7e79ff33677327a0edd5bd3f7da6347b",
      "externalIds": {
        "MAG": "603879209",
        "CorpusId": 181725826
      },
      "url": "https://www.semanticscholar.org/paper/6d9603be7e79ff33677327a0edd5bd3f7da6347b",
      "title": "\u968e\u5c64\u7684\u30d5\u30ec\u30fc\u30ba\u30d9\u30fc\u30b9\u7ffb\u8a33\u306b\u304a\u3051\u308b\u30d4\u30dc\u30c3\u30c8\u7ffb\u8a33\u624b\u6cd5\u306e\u5fdc\u7528 (\u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3) -- (\u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0)",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "114",
        "pages": "119-125",
        "name": "IEICE technical report. Speech"
      },
      "authors": [
        {
          "authorId": "135231196",
          "name": "\u4e09\u6d66 \u660e\u6ce2"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "47828326",
          "name": "Sakriani Sakti"
        }
      ]
    },
    {
      "paperId": "713844009469478141671c53a3b73cd12caf9df0",
      "externalIds": {
        "MAG": "2316018042",
        "CorpusId": 218698916
      },
      "url": "https://www.semanticscholar.org/paper/713844009469478141671c53a3b73cd12caf9df0",
      "title": "\u540c\u6642\u97f3\u58f0\u7ffb\u8a33\u306b\u304a\u3051\u308b\u7ffb\u8a33\u7cbe\u5ea6\u3068\u9045\u5ef6\u6642\u9593\u3092\u540c\u6642\u306b\u8003\u616e\u3057\u305f\u8a55\u4fa1\u5c3a\u5ea6(\u7ffb\u8a33\u8a55\u4fa1,\u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0)",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "114",
        "pages": "11-15",
        "name": ""
      },
      "authors": [
        {
          "authorId": "136676805",
          "name": "\u9686\u53f2 \u4e09\u91cd\u91ce"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "47828326",
          "name": "Sakriani Sakti"
        },
        {
          "authorId": "52004333",
          "name": "\u6238\u7530 \u667a\u57fa"
        },
        {
          "authorId": "52456834",
          "name": "\u4e2d\u6751 \u54f2\u90ce"
        }
      ]
    },
    {
      "paperId": "75983c55a489d526427fe399ce2670376168a2f0",
      "externalIds": {
        "MAG": "2538545555",
        "DBLP": "conf/ococosda/MizukamiNSTN14",
        "DOI": "10.1109/ICSDA.2014.7051433",
        "CorpusId": 15884891
      },
      "url": "https://www.semanticscholar.org/paper/75983c55a489d526427fe399ce2670376168a2f0",
      "title": "Building a free, general-domain paraphrase database for Japanese",
      "abstract": "Previous works have used parallel corpora and alignment techniques from phrase-based statistical machine translation to extract and generate paraphrases. In Japanese, paraphrases for a number of paraphrase categories or domains have been extracted by this method. However, most of these resources focus on a particular phenomenon in Japanese, and there are still no Japanese paraphrase resources that cover all varieties of phrases from several domains, and are freely available. In addition, because Japanese and English vary in grammar and word ordering, we perform syntax-based preprocessing to reduce this mismatch and extract paraphrases similar in quality to those extracted using more similar language pairs. The data used in creating the Japanese paraphrases is either in the public domain, or available under the Creative Commons license, and spans a variety of genres for wide coverage.",
      "year": 2014,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-4",
        "name": "2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)"
      },
      "authors": [
        {
          "authorId": "2387338",
          "name": "M. Mizukami"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "7657b56d2ac9269b32e8bcbe2a20f99ea17afe09",
      "externalIds": {
        "MAG": "2009926692",
        "DBLP": "conf/apsipa/KobayashiTNGNSN14",
        "DOI": "10.1109/APSIPA.2014.7041590",
        "CorpusId": 17712401
      },
      "url": "https://www.semanticscholar.org/paper/7657b56d2ac9269b32e8bcbe2a20f99ea17afe09",
      "title": "Gender-dependent spectrum differential models for perceived age control based on direct waveform modification in singing voice conversion",
      "abstract": "The perceived age of a singing voice, which is the age of the singer as perceived by the listener, is one of the intuitively understandable measures to describe voice characteristics of the singing voice. Singers can sing expressively by controlling voice timbre to some extent but the varieties of voice timbre that singers can produce are limited by physical constraints. To overcome this limitation, previous work has proposed statistical voice timbre control technique based on the perceived age. This technique makes it possible to control the perceived age of singing voice while retaining singer individuality by the use of statistical voice conversion (SVC) with a multiple-regression Gaussian mixture model (MR-GMM). However, the range of controllable perceived age is limited and speech quality of the converted singing voice is significantly degraded compared to that of a natural singing voice. In this paper, we propose a method for perceived age control using direct waveform modification based on spectrum differential and gender-dependent modeling. The experimental results show that the proposed method makes the range of controllable perceived age wider and quality of converted singing voice higher compared to the conventional method.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://staff.aist.go.jp/m.goto/PAPER/APSIPA2014kobayashi.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-4",
        "name": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific"
      },
      "authors": [
        {
          "authorId": "65851830",
          "name": "Kazuhiro Kobayashi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1712260",
          "name": "Tomoyasu Nakano"
        },
        {
          "authorId": "144968710",
          "name": "Masataka Goto"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "78e838bcd2268260ddce6be6db4907df6f29f04f",
      "externalIds": {
        "MAG": "2184040158",
        "DBLP": "conf/interspeech/MatsumiyaSNTN14",
        "DOI": "10.21437/Interspeech.2014-410",
        "CorpusId": 18103577
      },
      "url": "https://www.semanticscholar.org/paper/78e838bcd2268260ddce6be6db4907df6f29f04f",
      "title": "Data-driven generation of text balloons based on linguistic and acoustic features of a comics-anime corpus",
      "abstract": "Most automatic speech recognition systems existing today are still limited to recognizing what is being said, without being concerned with how it is being said. On the other hand, research on emotion recognition from speech has recently gained considerable interest, but how those emotions could be expressed in text-based communication has not been widely investigated. Our long-term goal is to construct expressive speech-to-text systems that conveys all information from acoustic speech, including verbal message, emotional state, speaker condition, and background noise, into unified text-based communication. In this preliminary study, we start with developing a system that can convey emotional speech into text-based communication by way of text balloons. As there exist many possible ways to generate the text balloons, we propose to utilize linguistic and acoustic features based on comic books and anime films. Experimental results reveal that expressive text is more preferable than static text, and the system is able to estimate the shape of text balloons with 87.01% accuracy. Index Terms: data-driven approaches, expressive text generation, linguistic and acoustic features",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1801-1805"
      },
      "authors": [
        {
          "authorId": "2864759",
          "name": "Sho Matsumiya"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "7c3a2e953d2c07ff4f150865112e4ceec14090ea",
      "externalIds": {
        "DBLP": "conf/icassp/TanakaTNSN14",
        "MAG": "1990967464",
        "DOI": "10.1109/ICASSP.2014.6854451",
        "CorpusId": 17870028
      },
      "url": "https://www.semanticscholar.org/paper/7c3a2e953d2c07ff4f150865112e4ceec14090ea",
      "title": "An evaluation of excitation feature prediction in a hybrid approach to electrolaryngeal speech enhancement",
      "abstract": "We implement removing micro-prosody with low-pass filtering and avoiding Unvoiced/Voiced (U/V) prediction as part of a hybrid approach to improve statistical excitation prediction in electrolaryngeal (EL) speech enhancement. An electrolarynx is a device that artificially generates excitation sounds to enable laryngectomees to produce EL speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. Moreover, the excitation sounds produced by the device often leak outside, adding noise to EL speech. To address these issues, in our previous work, we proposed a hybrid method using a noise reduction method for enhancing spectral parameters and voice conversion method for predicting excitation parameters. In this paper, we evaluate the effect of removing micro-prosody with low-pass filtering and avoiding U/V prediction in the hybrid enhancement process.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "4488-4492",
        "name": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
      },
      "authors": [
        {
          "authorId": "2109245369",
          "name": "Kou Tanaka"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "81ea04f822a1d5317e5846783900ac424a8f7528",
      "externalIds": {
        "ACL": "W14-3211",
        "DBLP": "conf/acl-clpsych/TanakaSNTN14",
        "MAG": "2125302588",
        "DOI": "10.3115/v1/W14-3211",
        "CorpusId": 9824522
      },
      "url": "https://www.semanticscholar.org/paper/81ea04f822a1d5317e5846783900ac424a8f7528",
      "title": "Linguistic and Acoustic Features for Automatic Identification of Autism Spectrum Disorders in Children\u2019s Narrative",
      "abstract": "Autism spectrum disorders are developmental disorders characterised as deficits in social and communication skills, and they affect both verbal and non-verbal communication. Previous works measured differences in children with and without autism spectrum disorders in terms of linguistic and acoustic features, although they do not mention automatic identification using integration of these features. In this paper, we perform an exploratory study of several language and speech features of both single utterances and full narratives. We find that there are characteristic differences between children with autism spectrum disorders and typical development with respect to word categories, prosody, and voice quality, and that these differences can be used in automatic classifiers. We also examine the differences between American and Japanese children and find significant differences with regards to pauses before new turns and linguistic cues.",
      "year": 2014,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://aclanthology.org/W14-3211.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Psychology",
        "Computer Science"
      ],
      "journal": {
        "pages": "88-96"
      },
      "authors": [
        {
          "authorId": "50426585",
          "name": "Hiroki Tanaka"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "834fb0d09e764b88ef76ee77e0befb8faeaad7fe",
      "externalIds": {
        "MAG": "2109444541",
        "DBLP": "journals/jstsp/TakamichiTSSNN14",
        "DOI": "10.1109/JSTSP.2013.2288599",
        "CorpusId": 13042356
      },
      "url": "https://www.semanticscholar.org/paper/834fb0d09e764b88ef76ee77e0befb8faeaad7fe",
      "title": "Parameter Generation Methods With Rich Context Models for High-Quality and Flexible Text-To-Speech Synthesis",
      "abstract": "In this paper, we propose parameter generation methods using rich context models as yet another hybrid method combining Hidden Markov Model (HMM)-based speech synthesis and unit selection synthesis. Traditional HMM-based speech synthesis enables flexible modeling of acoustic features based on a statistical approach. However, the speech parameters tend to be excessively smoothed. To address this problem, several hybrid methods combining HMM-based speech synthesis and unit selection synthesis have been proposed. Although they significantly improve quality of synthetic speech, they usually lose flexibility of the original HMM-based speech synthesis. In the proposed methods, we use rich context models, which are statistical models that represent individual acoustic parameter segments. In training, the rich context models are reformulated as Gaussian Mixture Models (GMMs). In synthesis, initial speech parameters are generated from probability distributions over-fitted to individual segments, and the speech parameter sequence is iteratively generated from GMMs using a parameter generation method based on the maximum likelihood criterion. Since the basic framework of the proposed methods is still the same as the traditional framework, the capability of flexibly modeling acoustic features remains. The experimental results demonstrate: (1) the use of approximation with a single Gaussian component sequence yields better synthetic speech quality than the use of EM algorithm in the proposed parameter generation method, (2) the state-based model selection yields quality improvements at the same level as the frame-based model selection, (3) the use of the initial parameters generated from the over-fitted speech probability distributions is very effective to further improve speech quality, and (4) the proposed methods for spectral and F0 components yields significant improvements in synthetic speech quality compared with the traditional HMM-based speech synthesis.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "8",
        "pages": "239-250",
        "name": "IEEE Journal of Selected Topics in Signal Processing"
      },
      "authors": [
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "2068230",
          "name": "Y. Shiga"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "83c7335904002d2b7c7cb403f3538703c9a69025",
      "externalIds": {
        "MAG": "2076689693",
        "DBLP": "conf/apsipa/TsurutaTTNSN14",
        "DOI": "10.1109/APSIPA.2014.7041618",
        "CorpusId": 14048926
      },
      "url": "https://www.semanticscholar.org/paper/83c7335904002d2b7c7cb403f3538703c9a69025",
      "title": "An evaluation of target speech for a nonaudible murmur enhancement system in noisy environments",
      "abstract": "Nonaudible murmur (NAM) is a soft whispered voice recorded with NAM microphone through body conduction. NAM allows for silent speech communication as it makes it possible for the speaker to convey their message in a nonaudible voice. However, its intelligibility and naturalness are significantly degraded compared to those of natural speech owing to acoustic changes caused by body conduction. To address this issue, statistical voice conversion (VC) methods from NAM to normal speech (NAM-to-Speech) and to a whispered voice (NAM-to-Whisper) have been proposed. It has been reported that these NAM enhancement methods significantly improve speech quality and intelligibility of NAM, and NAM-to-Whisper is more effective than NAM-to-Speech. However, it is still not obvious which method is more effective if a listener listens to the enhanced speech in noisy environments, a situation that often happens in silent speech communication. In this paper, assuming a typical situation in which NAM is uttered by a speaker in a quiet environment and conveyed to a listener in noisy environments, we investigate what kinds of target speech are more effective for NAM enhancement. We also propose NAM enhancement methods for converting NAM to other types of target voiced speech. Experiments show that the conversion process into voiced speech is more effective than that into unvoiced speech for generating more intelligible speech in noisy environments.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-4",
        "name": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific"
      },
      "authors": [
        {
          "authorId": "2272018",
          "name": "Sakura Tsuruta"
        },
        {
          "authorId": "2109245369",
          "name": "Kou Tanaka"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "83cf7b9611fabe9da2d08722445039023f1b19e9",
      "externalIds": {
        "MAG": "777757745",
        "CorpusId": 208900355
      },
      "url": "https://www.semanticscholar.org/paper/83cf7b9611fabe9da2d08722445039023f1b19e9",
      "title": "\u7d71\u8a08\u7684\u624b\u6cd5\u306b\u57fa\u3065\u304f\u6b4c\u58f0\u306e\u77e5\u899a\u5e74\u9f62\u5236\u5fa1\u6cd5(\u30dd\u30b9\u30bf\u30fc\u30bb\u30c3\u30b7\u30e7\u30f3,\u97f3\u5b66\u30b7\u30f3\u30dd\u30b8\u30a6\u30e02014)",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "114",
        "pages": "321-326",
        "name": "Scientific Programming"
      },
      "authors": [
        {
          "authorId": "2098878189",
          "name": "\u5c0f\u6797 \u548c\u5f18"
        },
        {
          "authorId": "52004333",
          "name": "\u6238\u7530 \u667a\u57fa"
        },
        {
          "authorId": "69450714",
          "name": "\u4e2d\u91ce \u502b\u9756"
        },
        {
          "authorId": "52464345",
          "name": "\u5f8c\u85e4 \u771f\u5b5d"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "47828326",
          "name": "Sakriani Sakti"
        },
        {
          "authorId": "52456834",
          "name": "\u4e2d\u6751 \u54f2\u90ce"
        }
      ]
    },
    {
      "paperId": "89b8153a86708b411bd21357c5b6006142104fc9",
      "externalIds": {
        "DBLP": "conf/ococosda/KotoSNTAN14",
        "MAG": "2545729695",
        "DOI": "10.1109/ICSDA.2014.7051435",
        "CorpusId": 11266639
      },
      "url": "https://www.semanticscholar.org/paper/89b8153a86708b411bd21357c5b6006142104fc9",
      "title": "Memorable spoken quote corpora of TED public speaking",
      "abstract": "In this paper we present the construction and analysis of memorable spoken quote corpora from TED public speaking. Memorable quotes are interesting and useful words which usually contain generic pearls of wisdom that could achieve public awareness and retained in people consciousness. Our study aims to reveal why can some public speeches can be retained in people mind and make their consciousness to like and share it, while some others can not. To achieve this purpose, the relevance corpora is required to perform system quantitative evaluation. In this study, we start with the collection of the corpus from TED public speaking. Specifically, we utilize 899 video files of TED Talks and more than 2000 speech quotes annotated by TED team. We then complement the data with non-memorable quotes. According to shares number of quotes which are provided by TED, we also annotate memorable quotes with popularity factor. Analysis of memorable spoken quotes is done based on speech duration, F0, and popularity.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Psychology"
      ],
      "journal": {
        "pages": "1-4",
        "name": "2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)"
      },
      "authors": [
        {
          "authorId": "2789148",
          "name": "Fajri Koto"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1719005",
          "name": "M. Adriani"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "89c148d3d4edcb7b13c35da36b97ffb881c38058",
      "externalIds": {
        "MAG": "2572309529",
        "CorpusId": 63225875
      },
      "url": "https://www.semanticscholar.org/paper/89c148d3d4edcb7b13c35da36b97ffb881c38058",
      "title": "An Evaluation through Simulation for Direct F0 Control of an Electrolarynx based on Statistical Excitation Feature Prediction",
      "abstract": "An electrolarynx is a device that artificially generates excitation sounds to enable laryngectomees to produce electrolaryngeal (EL) speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. To address this issue, we have proposed several EL speech enhancement methods using statistical excitation prediction, which was essential to significantly improve naturalness by predicting excitation parameters of normal speech. In these methods, the original EL speech is recorded with a microphone and the enhanced EL speech is presented from a loudspeaker in real time. This framework is effective for telecommunication but it is not suitable to face-to-face conversation because both the original EL speech and the enhanced EL speech are presented to listeners. In this paper, we propose direct F0 control of the electrolarynx based on the statistical excitation prediction also effective for face-to-face conversation. A simulation experiment is conducted to evaluate the effectiveness of the proposed method. The experimental result shows that our proposed system enables laryngectomees to produce more natural EL speech.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "33-38",
        "name": "IEICE technical report. Speech"
      },
      "authors": [
        {
          "authorId": "66310397",
          "name": "Tanaka Kou"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "8e56db786a685b4b9c7f1b750f60a81baebff0b5",
      "externalIds": {
        "MAG": "2732934789",
        "CorpusId": 57779410
      },
      "url": "https://www.semanticscholar.org/paper/8e56db786a685b4b9c7f1b750f60a81baebff0b5",
      "title": "An evaluation of target speech for nonaudible murmur enhancement focusing on its intelligibility under noisy environments",
      "abstract": "Nonaudible murmur (NAM) is a soft whispered voice recorded with a NAM microphone through body conduction. NAM is effective for silent speech communication as it allows a speaker to speak without making an audible sound. However, its intelligibility and naturalness are significantly degraded compared to natural speech owing to acoustic changes caused by body conduction. To address this issue, statistical voice conversion methods from NAM to normal speech (NAM-to-Speech) and to a whispered voice (NAM-to-Whisper) have been proposed. It has been reported that these NAM enhancement methods significantly improve speech quality and intelligibility of NAM, and that NAM-to-Whisper is more effective than NAM-to-Speech. However, it is still not obvious which method is more effective if a listener listens to the enhanced speech in noisy environments, a situation that often happens in silent speech communication. In this report, we assume a situation where NAM is uttered by a speaker in a quiet environment and conveyed to a listener in a noisy environment, and investigate what kinds of target speech is more effective for NAM enhancement. We also propose NAM enhancement methods for converting NAM to other types of target voiced speech. Experimental results show that the conversion process into voiced speech is more effective than that into unvoiced speech for generating more intelligible speech in noisy environments.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "71-76",
        "name": ""
      },
      "authors": [
        {
          "authorId": "66871291",
          "name": "Tsuruta Sakura"
        },
        {
          "authorId": "66310397",
          "name": "Tanaka Kou"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "9165d5e99b2106825dd00b9f5daf60e454434399",
      "externalIds": {
        "MAG": "2251542837",
        "ACL": "L14-1178",
        "DBLP": "conf/lrec/ShimizuNSTN14",
        "CorpusId": 9574685
      },
      "url": "https://www.semanticscholar.org/paper/9165d5e99b2106825dd00b9f5daf60e454434399",
      "title": "Collection of a Simultaneous Translation Corpus for Comparative Analysis",
      "abstract": "This paper describes the collection of an English-Japanese/Japanese-English simultaneous interpretation corpus. There are two main features of the corpus. The first is that professional simultaneous interpreters with different amounts of experience cooperated with the collection. By comparing data from simultaneous interpretation of each interpreter, it is possible to compare better interpretations to those that are not as good. The second is that for part of our corpus there are already translation data available. This makes it possible to compare translation data with simultaneous interpretation data. We recorded the interpretations of lectures and news, and created time-aligned transcriptions. A total of 387k words of transcribed data were collected. The corpus will be helpful to analyze differences in interpretations styles and to construct simultaneous interpretation systems.",
      "year": 2014,
      "influentialCitationCount": 8,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "670-673"
      },
      "authors": [
        {
          "authorId": "2114102929",
          "name": "Hiroaki Shimizu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "91ef95907dc637ad3c29ac3cc0e682b9c1985a37",
      "externalIds": {
        "ACL": "P14-2090",
        "MAG": "2121457870",
        "DBLP": "conf/acl/OdaNSTN14",
        "DOI": "10.3115/v1/P14-2090",
        "CorpusId": 724894
      },
      "url": "https://www.semanticscholar.org/paper/91ef95907dc637ad3c29ac3cc0e682b9c1985a37",
      "title": "Optimizing Segmentation Strategies for Simultaneous Speech Translation",
      "abstract": "In this paper, we propose new algorithms for learning segmentation strategies for simultaneous speech translation. In contrast to previously proposed heuristic methods, our method finds a segmentation that directly maximizes the performance of the machine translation system. We describe two methods based on greedy search and dynamic programming that search for the optimal segmentation strategy. An experimental evaluation finds that our algorithm is able to segment the input two to three times more frequently than conventional methods in terms of number of words, while maintaining the same score of automatic evaluation. 1",
      "year": 2014,
      "influentialCitationCount": 10,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "551-556"
      },
      "authors": [
        {
          "authorId": "143800179",
          "name": "Yusuke Oda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "924e43c4de98743d2e7c14c241b03b2109325b90",
      "externalIds": {
        "MAG": "2897033592",
        "CorpusId": 8610728
      },
      "url": "https://www.semanticscholar.org/paper/924e43c4de98743d2e7c14c241b03b2109325b90",
      "title": "Simple , Correct Parallelization for Blocked Gibbs Sampling Graham Neubig November",
      "abstract": "We present a method for distributing collapsed Gibbs sampling over multiple processors that is simple, statistically correct, and memory efficient. The method uses blocked sampling, dividing the training data into relatively large sized blocks, and distributing the sampling of each block over multiple processors. At the end of each parallel run, MetropolisHastings rejection sampling is performed to ensure that samples are being drawn from the correct distribution. Empirical results on part-of-speech tagging and word segmentation tasks show that the proposed blocked sampling method can sample from the true distribution while achieving convergence speed comparable with previous parallel sampling methods.",
      "year": 2014,
      "influentialCitationCount": 1,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "name": ""
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "92bd9e8a83e82dbbcafd8cde4f5a42d7bb4a5859",
      "externalIds": {
        "CorpusId": 166227020
      },
      "url": "https://www.semanticscholar.org/paper/92bd9e8a83e82dbbcafd8cde4f5a42d7bb4a5859",
      "title": "Language Model Adaptation and Analysis for Individuality Transforming \u6c34\u4e0a \u96c5\u535a",
      "abstract": "In text and speech, there are various features that express the individuality of the writer or speaker. We proposed a method for transforming individuality using a technique inspired by statistical machine translation (SMT), and showed the effectiveness. In previous work, we proposed a method for paraphrasing for characteristic words using n-gram clustering. However, the method can be improved, because it considers only short context. In this paper, we propose a model of transforming individuality that considers longer contexts. To achieve this, we suggest adaptation of the language models and expansion of paraphrasing for characteristic words.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": null,
      "authors": [
        {
          "authorId": "2387338",
          "name": "M. Mizukami"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "9b3fd2525a2d1abc44145308e013f117d3d7bdee",
      "externalIds": {
        "DBLP": "conf/interspeech/TanakaTNSN14",
        "MAG": "2185076889",
        "DOI": "10.21437/Interspeech.2014-7",
        "CorpusId": 14237872
      },
      "url": "https://www.semanticscholar.org/paper/9b3fd2525a2d1abc44145308e013f117d3d7bdee",
      "title": "Direct F0 control of an electrolarynx based on statistical excitation feature prediction and its evaluation through simulation",
      "abstract": "An electrolarynx is a device that artificially generates excitation sounds to enable laryngectomees to produce electrolaryngeal (EL) speech. Although proficient laryngectomees can produce quite intelligible EL speech, it sounds very unnatural due to the mechanical excitation produced by the device. To address this issue, we have proposed several EL speech enhancement methods using statistical voice conversion and showed that statistical prediction of excitation parameters, such as F0 patterns, was essential to significantly improve naturalness of EL speech. In these methods, the original EL speech is recorded with a microphone and the enhanced EL speech is presented from a loudspeaker in real time. This framework is effective for telecommunication but it is not suitable to face-to-face conversation because both the original EL speech and the enhanced EL speech are presented to listeners. In this paper, we propose direct F0 control of the electrolarynx based on statistical excitation prediction to develop an EL speech enhancement technique also effective for face-to-face conversation. F0 patterns of excitation signals produced by the electrolarynx are predicted in real time from the EL speech produced by the laryngectomee\u2019s articulation of the excitation signals with previously predicted F0 values. A simulation experiment is conducted to evaluate the effectiveness of the proposed method. The experimental results demonstrate that the proposed method yields significant improvements in naturalness of EL speech while keeping its intelligibility high enough. Index Terms: laryngectomee, electrolarynx, electrolaryngeal speech, statistical excitation prediction, simulation evaluation",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "31-35"
      },
      "authors": [
        {
          "authorId": "2109245369",
          "name": "Kou Tanaka"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "a13d9c8e5a2fc028ad597e2bd46a9c60aca0ede4",
      "externalIds": {
        "MAG": "2586474482",
        "CorpusId": 55669765
      },
      "url": "https://www.semanticscholar.org/paper/a13d9c8e5a2fc028ad597e2bd46a9c60aca0ede4",
      "title": "HMM-Based Speech Synthesis System with Prosody Modification Based on Speech Input",
      "abstract": "As a creative activity using speech synthesis technologies has been grown rapidly, it is desired to develop an interface to synthesize speech of a specific target speaker as users want. In this report, we propose a prosody modification method using user\u2019s speech inputs in HMM-based speech synthesis system in order to achieve high-quality and expressive speech synthesis. The propose method allows users to guide prosody of synthetic speech of the target speaker by using their own voices while preserving original functions of the HMM-based speech synthesis system as a text-to-speech synthesis system. Both duration information of the input speech extracted by performing HMM state alignment and F0 patterns of the input speech are effectively used to control the duration and F0 patterns of synthetic speech of the target speaker. To alleviate the degradation of naturalness caused by prosodic mismatches between the input speech and the synthetic speech, we investigate an appropriate unit for the HMM state alignment, model adaptation for building an HMM used for the alignment, and correction of unvoiced/voiced information. Experimental evaluations are conducted for multiple input speakers, which demonstrates the effectiveness of the proposed method.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "81-86",
        "name": "IEICE technical report. Speech"
      },
      "authors": [
        {
          "authorId": "115481467",
          "name": "Nishigaki Yuri"
        },
        {
          "authorId": "73282804",
          "name": "Takamichi Shinnosuke"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "a16cecbaf87d965e396e610f251f710a807b70ad",
      "externalIds": {
        "MAG": "2294395636",
        "DBLP": "conf/interspeech/JinboTTNSN14",
        "DOI": "10.21437/Interspeech.2014-122",
        "CorpusId": 6991506
      },
      "url": "https://www.semanticscholar.org/paper/a16cecbaf87d965e396e610f251f710a807b70ad",
      "title": "A hearing impairment simulation method using audiogram-based approximation of auditory charatecteristics",
      "abstract": "Hearing impairment simulation is an effective technique to educate normal-hearing people about auditory perception of the hearing-impaired. Because auditory characteristics of the hearing impaired vary greatly from person-to-person, personalization of the hearing impairment simulation systems is essential to accurately simulate these individual differences. However, measurement of auditory characteristics of individuals is time-consuming work. In this paper, we propose a hearing impairment simulation method that is easily applied to individual hearing-impaired persons. Auditory filter characteristics and gain characteristics are estimated from easily measurable audiograms of each individual. We also implement a method for manually adjusting the hearing impairment level to improve accuracy of the proposed hearing impairment simulation. An experimental evaluation is conducted to compare intelligibility between hearing-impaired and normal-hearing persons with the proposed hearing impairment simulation. The experimental results show that the proposed method effectively makes the word correct rate and phoneme confusion tendency of the normal hearing persons similar to those of the hearing impaired persons. Index Terms: hearing-impairment simulation, personalization, auditory filter characteristics, gain characteristics, audiogram,",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "490-494"
      },
      "authors": [
        {
          "authorId": "2645099",
          "name": "N. Jinbo"
        },
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "a4dd375c18709b1554249cc5cb88d8ba6acfea10",
      "externalIds": {
        "MAG": "2729669909",
        "CorpusId": 67686607
      },
      "url": "https://www.semanticscholar.org/paper/a4dd375c18709b1554249cc5cb88d8ba6acfea10",
      "title": "Machine Translation -- Why couldn't we do it? Why are we starting to be able to now?",
      "abstract": "Abstract While machine translation has been a long-held dream dating back to the appearance of the first computers, for many years it had not reached a level that was able to stand up to real use. However, over the past ten or so years, there have been huge advances in the technology, and machine translation is finally seeing large-scale use in real situations. This paper describes 5 major elements that have contributed to these enormous steps forward in machine translation systems.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "35-38",
        "name": ""
      },
      "authors": [
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        }
      ]
    },
    {
      "paperId": "a636768c2fc6cadccd8bb4d704f651dd54dad395",
      "externalIds": {
        "MAG": "1510971382",
        "CorpusId": 30359273
      },
      "url": "https://www.semanticscholar.org/paper/a636768c2fc6cadccd8bb4d704f651dd54dad395",
      "title": "Recognition and Analysis of Emotion in Indonesian Conversational Speech",
      "abstract": "The importance of incorporating emotional aspect in human computer interaction continues to arise. Unfortunately, exploration of the subject in Indonesian is still very lacking. This paper presents the first study of emotion recognition in Indonesian on conversational speech. We construct our corpus, IDESC, by making use of television talk show recordings in various topics of discussion, yielding colorful emotional utterances. Using the corpus, we then build a support vector machine (SVM) that classifies Indonesian speech in terms of emotion based on its acoustic features. We perform feature selection and parameter optimization while building the classifier to optimize the recognition performance, resulting in absolute 11.9% increase of accuracy. Lastly, we perform analyses on our corpus and evaluation result to gain better insight of emotion occurence in Indonesian speech.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Psychology"
      ],
      "journal": {
        "volume": "2014",
        "pages": "1-6",
        "name": ""
      },
      "authors": [
        {
          "authorId": "117341185",
          "name": "Lubis Nurul"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "114509818",
          "name": "L. Dessi"
        },
        {
          "authorId": "79600263",
          "name": "P. Ayu"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "a6a7374c5ddac1446ceab9d7cbe5a3305238d0ee",
      "externalIds": {
        "DBLP": "conf/ococosda/NioSNTN14",
        "MAG": "2542999985",
        "DOI": "10.1109/ICSDA.2014.7051436",
        "CorpusId": 10906260
      },
      "url": "https://www.semanticscholar.org/paper/a6a7374c5ddac1446ceab9d7cbe5a3305238d0ee",
      "title": "Conversation dialog corpora from television and movie scripts",
      "abstract": "Example-based dialogue systems often require natural conversation templates as examples for response generation. However, in previous work most conversation corpora have been created by hand and do not well portray actual conversations between two people. One way to overcome this problem is to record and transcribe real human-to-human conversation. However, this work is tedious and time consuming. In this work, we utilize conversation scripts from television and movies. We extract conversations from television and movie scripts from the web and perform various types of filtering. In order to ensure that the conversation is performed by two speakers, we introduce a unit of conversation called a tri-turn (a trigram conversation turn) which allow us to filter conversations with more than two speakers. In the end, our conversation corpora contains 86,719 query-response pairs that represent conversation turns performed by two speakers talking to each other.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-4",
        "name": "2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)"
      },
      "authors": [
        {
          "authorId": "2115734",
          "name": "Lasguido Nio"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "a711e02f85fa52c15df0a830a8ba88df2c3928ec",
      "externalIds": {
        "MAG": "2765999834",
        "CorpusId": 208026955
      },
      "url": "https://www.semanticscholar.org/paper/a711e02f85fa52c15df0a830a8ba88df2c3928ec",
      "title": "Robust Example-based Dialog Retrieval using Distributed Word Representations and Recursive Autoencoders",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "",
        "pages": "01-06",
        "name": ""
      },
      "authors": [
        {
          "authorId": "71893651",
          "name": "Nio Lasguido"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "af85c67a1f30f8359be1091234118492b511a088",
      "externalIds": {
        "MAG": "643377137",
        "CorpusId": 176462869
      },
      "url": "https://www.semanticscholar.org/paper/af85c67a1f30f8359be1091234118492b511a088",
      "title": "\u540c\u6642\u97f3\u58f0\u7ffb\u8a33\u306b\u304a\u3051\u308b\u7ffb\u8a33\u7cbe\u5ea6\u3068\u9045\u5ef6\u6642\u9593\u3092\u540c\u6642\u306b\u8003\u616e\u3057\u305f\u8a55\u4fa1\u5c3a\u5ea6 (\u8a00\u8a9e\u7406\u89e3\u3068\u30b3\u30df\u30e5\u30cb\u30b1\u30fc\u30b7\u30e7\u30f3) -- (\u7b2c6\u56de\u96c6\u5408\u77e5\u30b7\u30f3\u30dd\u30b8\u30a6\u30e0)",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": null,
      "journal": {
        "volume": "114",
        "pages": "11-15",
        "name": "IEICE technical report. Speech"
      },
      "authors": [
        {
          "authorId": "136676805",
          "name": "\u9686\u53f2 \u4e09\u91cd\u91ce"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "47828326",
          "name": "Sakriani Sakti"
        }
      ]
    },
    {
      "paperId": "b0b1112b06898733faefc32f54940aa4e84bc383",
      "externalIds": {
        "MAG": "2547277727",
        "DBLP": "conf/ococosda/DoNSTN14",
        "DOI": "10.1109/ICSDA.2014.7051424",
        "CorpusId": 15914283
      },
      "url": "https://www.semanticscholar.org/paper/b0b1112b06898733faefc32f54940aa4e84bc383",
      "title": "Collection and analysis of a Japanese-English emphasized speech corpora",
      "abstract": "Speech-to-speech (S2S) translation [10] is gradually starting to break down the language barrier, bringing opportunities for people to understand each other using different languages. However, one of the limitations of current S2S systems that they usually do not translate the paralinguistic information included in the input speech. Among the various types of paralinguistic information, we focus on emphasis, a type of information that is used to convey the focus of the sentence, emotion of the speaker, or other high level information useful for communication. This paper describes the collection of an Japanese-English emphasized speech corpora that can be used in the study of how emphasis is expressed across languages. We constructed 2 corpora, one containing digit strings and one with a utterances from a conversational setting. The speakers who can speak both Japanese and English were selected for the recording. 500 parallel digit strings for the digit corpus and 2030 parallel sentences for the conversation corpus were collected. The corpora may be used to analyze emphasis of one language or between languages, or develop emphasized speech translation systems.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-5",
        "name": "2014 17th Oriental Chapter of the International Committee for the Co-ordination and Standardization of Speech Databases and Assessment Techniques (COCOSDA)"
      },
      "authors": [
        {
          "authorId": "2527751",
          "name": "Quoc Truong Do"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "b169c4b6c23efe8cbd4dc29eb97939cbcfba0f28",
      "externalIds": {
        "MAG": "2491310222",
        "DBLP": "conf/iwsds/HiraokaNSTN14",
        "DOI": "10.1007/978-3-319-21834-2_12",
        "CorpusId": 16551313
      },
      "url": "https://www.semanticscholar.org/paper/b169c4b6c23efe8cbd4dc29eb97939cbcfba0f28",
      "title": "Construction and Analysis of a Persuasive Dialogue Corpus",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Psychology"
      ],
      "journal": {
        "pages": "125-138"
      },
      "authors": [
        {
          "authorId": "3027595",
          "name": "Takuya Hiraoka"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "b5241fcbfbf30f6fd8ff1ae19d947dd2ca23244f",
      "externalIds": {
        "ACL": "E14-4025",
        "DBLP": "conf/eacl/VuNSTN14",
        "MAG": "2161431802",
        "DOI": "10.3115/v1/E14-4025",
        "CorpusId": 15373046
      },
      "url": "https://www.semanticscholar.org/paper/b5241fcbfbf30f6fd8ff1ae19d947dd2ca23244f",
      "title": "Acquiring a Dictionary of Emotion-Provoking Events",
      "abstract": "This paper is concerned with the discovery and aggregation of events that provoke a particular emotion in the person who experiences them, or emotion-provoking events. We first describe the creation of a small manually-constructed dictionary of events through a survey of 30 subjects. Next, we describe first attempts at automatically acquiring and aggregating these events from web data, with a baseline from previous work and some simple extensions using seed expansion and clustering. Finally, we propose several evaluation measures for evaluating the automatically acquired events, and perform an evaluation of the effectiveness of automatic event extraction.",
      "year": 2014,
      "influentialCitationCount": 2,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "128-132"
      },
      "authors": [
        {
          "authorId": "1934086",
          "name": "H. Vu"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "b53689b8c28353106f327f0981b108eb67816053",
      "externalIds": {
        "DBLP": "conf/ssst/HatakoshiNSTN14",
        "ACL": "W14-4004",
        "MAG": "2139693518",
        "DOI": "10.3115/v1/W14-4004",
        "CorpusId": 1305434
      },
      "url": "https://www.semanticscholar.org/paper/b53689b8c28353106f327f0981b108eb67816053",
      "title": "Rule-based Syntactic Preprocessing for Syntax-based Machine Translation",
      "abstract": "Several preprocessing techniques using syntactic information and linguistically motivated rules have been proposed to improve the quality of phrase-based machine translation (PBMT) output. On the other hand, there has been little work on similar techniques in the context of other translation formalisms such as syntax-based SMT. In this paper, we examine whether the sort of rule-based syntactic preprocessing approaches that have proved beneficial for PBMT can contribute to syntax-based SMT. Specifically, we tailor a highly successful preprocessing method for EnglishJapanese PBMT to syntax-based SMT, andfind that while the gains achievable are smaller than those for PBMT, significant improvements in accuracy can be realized.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "http://www.aclweb.org/anthology/W/W14/W14-4004.pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "34-42"
      },
      "authors": [
        {
          "authorId": "2061275",
          "name": "Yuto Hatakoshi"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "bdfb9f1c79ad726049a3563c741311391e18532a",
      "externalIds": {
        "MAG": "2969312850",
        "CorpusId": 201881091
      },
      "url": "https://www.semanticscholar.org/paper/bdfb9f1c79ad726049a3563c741311391e18532a",
      "title": "Speech style manipulation using entrainment for speech recognition system",
      "abstract": null,
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "113",
        "pages": "27-28",
        "name": ""
      },
      "authors": [
        {
          "authorId": "1396296990",
          "name": "Sugiyama Koutaro"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "c3177616ad35ef7850ea1e62da1fa3be36943e8b",
      "externalIds": {
        "MAG": "2078494675",
        "DBLP": "conf/apsipa/NioSNTN14",
        "DOI": "10.1109/APSIPA.2014.7041777",
        "CorpusId": 7285408
      },
      "url": "https://www.semanticscholar.org/paper/c3177616ad35ef7850ea1e62da1fa3be36943e8b",
      "title": "Recursive neural network paraphrase identification for example-based dialog retrieval",
      "abstract": "An example-based dialog model often require a lot of data collections to achieve a good performance. However, when it comes on handling an out of vocabulary (OOV) database queries, this approach resulting in weakness and inadequate handling of interactions between words in the sentence. In this work, we try to overcome this problem by utilizing recursive neural network paraphrase identification to improve the robustness of example-based dialog response retrieval. We model our dialog-pair database and user input query with distributed word representations, and employ recursive autoencoders and dynamic pooling to determine whether two sentences with arbitrary length have the same meaning. The distributed representations have the potential to improve handling of OOV cases, and the recursive structure can reduce confusion in example matching.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-4",
        "name": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific"
      },
      "authors": [
        {
          "authorId": "2115734",
          "name": "Lasguido Nio"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "c55d5805a6eb8b1482f21581fe893484eaf9ffb5",
      "externalIds": {
        "MAG": "2288228893",
        "DBLP": "journals/ieicet/KobayashiTDNGNSN14",
        "DOI": "10.1587/TRANSINF.E97.D.1419",
        "CorpusId": 3570101
      },
      "url": "https://www.semanticscholar.org/paper/c55d5805a6eb8b1482f21581fe893484eaf9ffb5",
      "title": "Voice Timbre Control Based on Perceived Age in Singing Voice Conversion",
      "abstract": "The perceived age of a singing voice is the age of the singer as perceived by the listener, and is one of the notable characteristics that determines perceptions of a song. In this paper, we describe an investigation of acoustic features that have an effect on the perceived age, and a novel voice timbre control technique based on the perceived age for singing voice conversion (SVC). Singers can sing expressively by controlling prosody and voice timbre, but the varieties of voices that singers can produce are limited by physical constraints. Previous work has attempted to overcome this limitation through the use of statistical voice conversion. This technique makes it possible to convert singing voice timbre of an arbitrary source singer into those of an arbitrary target singer. However, it is still difficult to intuitively control singing voice characteristics by manipulating parameters corresponding to specific physical traits, such as gender and age. In this paper, we first perform an investigation of the factors that play a part in the listener\u2019s perception of the singer\u2019s age at first. Then, we applied a multiple-regression Gaussian mixture models (MR-GMM) to SVC for the purpose of controlling voice timbre based on the perceived age and we propose SVC based on the modified MR-GMM for manipulating the perceived age while maintaining singer\u2019s individuality. The experimental results show that 1) the perceived age of singing voices corresponds relatively well to the actual age of the singer, 2) prosodic features have a larger effect on the perceived age than spectral features, 3) the individuality of a singer is influenced more heavily by segmental features than prosodic features 4) the proposed voice timbre control method makes it possible to change the singer\u2019s perceived age while not having an adverse effect on the perceived individuality. key words: singing voice, voice conversion, perceived age, spectral and prosodic features, subjective evaluations",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": true,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science",
        "Psychology"
      ],
      "journal": {
        "volume": "97-D",
        "pages": "1419-1428",
        "name": "IEICE Trans. Inf. Syst."
      },
      "authors": [
        {
          "authorId": "65851830",
          "name": "Kazuhiro Kobayashi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1701762",
          "name": "Hironori Doi"
        },
        {
          "authorId": "1712260",
          "name": "Tomoyasu Nakano"
        },
        {
          "authorId": "144968710",
          "name": "Masataka Goto"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "ceefd51b4b391668e313afe8edb3588197002e37",
      "externalIds": {
        "MAG": "1987992317",
        "DBLP": "conf/icassp/TakamichiTNSN14",
        "DOI": "10.1109/ICASSP.2014.6853604",
        "CorpusId": 14170126
      },
      "url": "https://www.semanticscholar.org/paper/ceefd51b4b391668e313afe8edb3588197002e37",
      "title": "A postfilter to modify the modulation spectrum in HMM-based speech synthesis",
      "abstract": "In this paper, we propose a postfilter to compensate modulation spectrum in HMM-based speech synthesis. In order to alleviate over-smoothing effects which is a main cause of quality degradation in HMM-based speech synthesis, it is necessary to consider features that can capture over-smoothing. Global Variance (GV) is one well-known example of such a feature, and the effectiveness of parameter generation algorithm considering GV have been confirmed. However, the quality gap between natural speech and synthetic speech is still large. In this paper, we introduce the Modulation Spectrum (MS) of speech parameter trajectory as a new feature to effectively capture the over-smoothing effect, and we propose a postfilter based on the MS. The MS is represented as a power spectrum of the parameter trajectory. The generated speech parameter sequence is filtered to ensure that its MS has a pattern similar to natural speech. Experimental results show quality improvements when the proposed methods are applied to spectral and F0 components, compared with conventional methods considering GV.",
      "year": 2014,
      "influentialCitationCount": 2,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "290-294",
        "name": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
      },
      "authors": [
        {
          "authorId": "2424104",
          "name": "Shinnosuke Takamichi"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "cf08bef866885edb8b001deb18e582eec94c51de",
      "externalIds": {
        "MAG": "1963679279",
        "DBLP": "conf/apsipa/KotoSNTAN14",
        "DOI": "10.1109/APSIPA.2014.7041625",
        "CorpusId": 2177041
      },
      "url": "https://www.semanticscholar.org/paper/cf08bef866885edb8b001deb18e582eec94c51de",
      "title": "The use of semantic and acoustic features for open-domain TED talk summarization",
      "abstract": "In this paper, we address the problem of automatic speech summarization on open-domain TED talks. The large vocabulary and diversity of topics from speaker-to-speaker presents significant difficulties. The challenges increase not only how to handle disfluencies and fillers, but also how to extract topic-related meaningful messages within the free talks. Here, we propose to incorporate semantic and acoustic features within the speech summarization technique. In addition, we also propose a new evaluation method for speech summarization by checking semantic similarity between system and human summarization. Experiments results reveal that the proposed methods are effective in spontaneous speech summarization.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "pages": "1-4",
        "name": "Signal and Information Processing Association Annual Summit and Conference (APSIPA), 2014 Asia-Pacific"
      },
      "authors": [
        {
          "authorId": "2789148",
          "name": "Fajri Koto"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "1719005",
          "name": "M. Adriani"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "d338bcd1e34a8259e123465203b05c5bf21aa12a",
      "externalIds": {
        "MAG": "2568742097",
        "CorpusId": 54987972
      },
      "url": "https://www.semanticscholar.org/paper/d338bcd1e34a8259e123465203b05c5bf21aa12a",
      "title": "Prosody Correction Preserving Speaker Individuality in English-Read-By-Japanese Speech Synthesis Based on HMM",
      "abstract": "To build an English acoustic model that well captures speaker individuality of each Japanese speaker, a framework using English-Read-by-Japanese (ERJ) voices is effective as it enables to directly model speaker-dependent acoustic characteristics. However, naturalness of English speech synthesized by such an ERJ acoustic model is significantly degraded as it is directly affected by prosodic differences and pronunciation errors often caused by differences of a language system between Japanese and English. To synthesize more natural English speech while preserving speaker individuality of individual Japanese speakers, we propose a technique to correct prosody of ERJ voices based on that of a native English speaker. The duration and power of the native English speaker are effectively used to develop the ERJ acoustic model for each Japanese speaker by using model adaptation techniques in HMM-based speech synthesis. The experimental results show that our proposed method is capable of significantly improving naturalness of ERJ synthetic speech while preserving its speaker individuality.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "63-68",
        "name": "IEICE technical report. Speech"
      },
      "authors": [
        {
          "authorId": "2097235515",
          "name": "Oshima Yuji"
        },
        {
          "authorId": "73282804",
          "name": "Takamichi Shinnosuke"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    },
    {
      "paperId": "d7851e80f6072991bc99e2157f05515564f894f4",
      "externalIds": {
        "MAG": "2026609073",
        "DBLP": "journals/ieicet/KuboSNTN14",
        "DOI": "10.1587/TRANSINF.E97.D.1468",
        "CorpusId": 7284176
      },
      "url": "https://www.semanticscholar.org/paper/d7851e80f6072991bc99e2157f05515564f894f4",
      "title": "Structured Adaptive Regularization of Weight Vectors for a Robust Grapheme-to-Phoneme Conversion Model",
      "abstract": "Grapheme-to-phoneme (g2p) conversion, used to estimate the pronunciations of out-of-vocabulary (OOV) words, is a highly important part of recognition systems, as well as text-to-speech systems. The current state-of-the-art approach in g2p conversion is structured learning based on the Margin Infused Relaxed Algorithm (MIRA), which is an online discriminative training method for multiclass classification. However, it is known that the aggressive weight update method of MIRA is prone to overfitting, even if the current example is an outlier or noisy. Adaptive Regularization of Weight Vectors (AROW) has been proposed to resolve this problem for binary classification. In addition, AROW\u2019s update rule is simpler and more efficient than that of MIRA, allowing for more efficient training. Although AROW has these advantages, it has not been applied to g2p conversion yet. In this paper, we first apply AROW on g2p conversion task which is structured learning problem. In an evaluation that employed a dataset generated from the collective knowledge on the Web, our proposed approach achieves a 6.8% error reduction rate compared to MIRA in terms of phoneme error rate. Also the learning time of our proposed approach was shorter than that of MIRA in almost datasets. key words: g2p conversion, out-of-vocabulary word, online discriminative training, structured learning, AROW",
      "year": 2014,
      "influentialCitationCount": 1,
      "isOpenAccess": true,
      "openAccessPdf": {
        "url": "https://www.jstage.jst.go.jp/article/transinf/E97.D/6/E97.D_1468/_pdf",
        "status": null
      },
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "97-D",
        "pages": "1468-1476",
        "name": "IEICE Trans. Inf. Syst."
      },
      "authors": [
        {
          "authorId": "143745924",
          "name": "Keigo Kubo"
        },
        {
          "authorId": "1783949",
          "name": "S. Sakti"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "1726559",
          "name": "T. Toda"
        },
        {
          "authorId": "145223960",
          "name": "Satoshi Nakamura"
        }
      ]
    },
    {
      "paperId": "d82592f3a110308366dfc7c42565d437b5bf59af",
      "externalIds": {
        "MAG": "2733801877",
        "CorpusId": 65080677
      },
      "url": "https://www.semanticscholar.org/paper/d82592f3a110308366dfc7c42565d437b5bf59af",
      "title": "Automating Social Skills Training",
      "abstract": "Social skills training is a well-established method to decrease human anxiety and discomfort in social interaction, and get appropriate skills. In this paper, we attempt to automate the process of social skills training by developing a dialogue system named \u201dautomated social skills trainer,\u201d which provides the social skills training through human-computer interaction. The system includes a virtual avatar that recognizes user speech and language information and gives feedback to users to improve their social skills. Its design is based on conventional individual social skills training performed by human participants including defining target skills, modeling, role-play, feedback, reinforcement, and homework. Experiments measuring the relationship between social skill and speech and language features shows that these features have a relationship with autistic traits. Additional experiments measuring the effect of performing social skills training with the proposed application show participants significantly improve their skill by using the system for 50 minutes.",
      "year": 2014,
      "influentialCitationCount": 0,
      "isOpenAccess": false,
      "openAccessPdf": null,
      "fieldsOfStudy": [
        "Computer Science"
      ],
      "journal": {
        "volume": "114",
        "pages": "1-6",
        "name": ""
      },
      "authors": [
        {
          "authorId": "65817231",
          "name": "Tanaka Hiroki"
        },
        {
          "authorId": "66143114",
          "name": "Sakti Sakriani"
        },
        {
          "authorId": "1700325",
          "name": "Graham Neubig"
        },
        {
          "authorId": "65812230",
          "name": "T. Tomoki"
        },
        {
          "authorId": "72028057",
          "name": "Negoro Hideki"
        },
        {
          "authorId": "71553524",
          "name": "Iwasaka Hidemi"
        },
        {
          "authorId": "52343068",
          "name": "Nakamura Satoshi"
        }
      ]
    }
  ]
}